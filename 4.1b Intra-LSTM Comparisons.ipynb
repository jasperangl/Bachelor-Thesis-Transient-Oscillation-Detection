{"cells":[{"cell_type":"markdown","metadata":{"id":"NZa9qbao_gID"},"source":["# Model Comparisons\n","\n","Take the learnings and use 500 seq_len and 16 batch_size\n","\n","1. LSTM\n","1. BiLSTM\n","3. LSTM + Dense\n","3. BiLSTM + Dense\n","2. 2 Hidden LSTM\n","2. 2 Hidden BiLSTM\n","2. 2 Hidden LSTM + Dense\n","2. 2 Hidden BiLSTM + Dense\n","5. Threshold"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":14894,"status":"ok","timestamp":1744732768909,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"6fbxE6nIELL-"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, mean_absolute_error, confusion_matrix, f1_score, classification_report, matthews_corrcoef, recall_score\n","from sklearn.model_selection import train_test_split, KFold\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import sys\n","\n","import json\n","from pprint import pprint\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import matthews_corrcoef, accuracy_score\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1744732768910,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"e-6oNkoR8HkB","outputId":"f8905232-b170-405c-8d76-8a71706678cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.18.0\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23679,"status":"ok","timestamp":1744732792586,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"8egAzyeLvEBl","outputId":"404c6014-4c67-4cc9-f3b1-dda1659f199a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Get the current working directory (where your notebook is)\n","current_directory = os.getcwd()\n","\n","\n","# Construct the full path to your data_utils.py file\n","data_utils_path = '/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Thesis Files/data_utils.py'  # Replace with the actual path\n","\n","# Add the directory containing data_utils.py to the Python path\n","sys.path.append(os.path.dirname(data_utils_path))  # Add parent directory of data_utils.py\n","# Now you can import the custom module\n","import data_utils as du\n","import analysis_utils as au\n","\n","sys.path = current_directory\n","print(sys.path)"]},{"cell_type":"markdown","metadata":{"id":"MdevXsq95Ji5"},"source":["## Model Set-Up"]},{"cell_type":"markdown","metadata":{"id":"9al3TxnZMugD"},"source":["### Simple Models for comparison"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1744732792638,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"G9hU8UrN5LQU"},"outputs":[],"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Attention, Dropout\n","\n","# 1a. LSTM\n","def create_lstm(input_shape, binary):\n","    model = Sequential([\n","        Input(shape=input_shape),\n","        LSTM(32, return_sequences=True),\n","        Dense(1 if binary else 5, activation=\"sigmoid\" if binary else \"softmax\")\n","    ], name = \"LSTM\")\n","    loss = \"binary_crossentropy\" if binary else \"sparse_categorical_crossentropy\"\n","    model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n","    return model\n","\n","# 1b. BiLSTM\n","def create_bidirectional_lstm(input_shape, binary):\n","    model = Sequential([\n","        Input(shape=input_shape),\n","        Bidirectional(LSTM(32, return_sequences=True)),\n","        Dense(1 if binary else 5, activation=\"sigmoid\" if binary else \"softmax\")\n","    ], name = \"BiLSTM\")\n","    loss = \"binary_crossentropy\" if binary else \"sparse_categorical_crossentropy\"\n","    model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n","    return model\n","\n","# 2a. LSTM + Dense\n","def create_lstm_with_dense(input_shape, binary):\n","    model = Sequential([\n","        Input(shape=input_shape),\n","        LSTM(32, return_sequences=True),\n","        Dense(16, activation=\"relu\"),  # Dense layer added\n","        Dense(1 if binary else 5, activation=\"sigmoid\" if binary else \"softmax\")\n","    ], name = \"LSTM_Dense\")\n","    loss = \"binary_crossentropy\" if binary else \"sparse_categorical_crossentropy\"\n","    model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n","    return model\n","\n","# 2b. BiLSTM + Dense\n","def create_bidirectional_lstm_with_dense(input_shape, binary):\n","    model = Sequential([\n","        Input(shape=input_shape),\n","        Bidirectional(LSTM(32, return_sequences=True)),\n","        Dense(16, activation=\"relu\"),  # Dense layer added\n","        Dense(1 if binary else 5, activation=\"sigmoid\" if binary else \"softmax\")\n","    ], name = \"BiLSTM_Dense\")\n","    loss = \"binary_crossentropy\" if binary else \"sparse_categorical_crossentropy\"\n","    model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n","    return model\n","\n","# 3a. 2 Hidden LSTM + Dense\n","def create_2_hidden_lstm_with_dense(input_shape, binary):\n","    model = Sequential([\n","        Input(shape=input_shape),\n","        LSTM(64, return_sequences=True),  # First LSTM layer\n","        LSTM(32, return_sequences=True),  # Second LSTM layer\n","        Dense(16, activation=\"relu\"),  # Dense layer added\n","        Dense(1 if binary else 5, activation=\"sigmoid\" if binary else \"softmax\")\n","    ], name = \"LSTM_Deep\")\n","    loss = \"binary_crossentropy\" if binary else \"sparse_categorical_crossentropy\"\n","    model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n","    return model\n","\n","# 3b. 2 Hidden BiLSTM + Dense\n","def create_2_hidden_bidirectional_lstm_with_dense(input_shape, binary):\n","    model = Sequential([\n","        Input(shape=input_shape),\n","        Bidirectional(LSTM(64, return_sequences=True)),  # First BiLSTM layer\n","        Bidirectional(LSTM(32, return_sequences=True)),  # Second BiLSTM layer\n","        Dense(16, activation=\"relu\"),  # Dense layer added\n","        Dense(1 if binary else 5, activation=\"sigmoid\" if binary else \"softmax\")\n","    ], name = \"BiLSTM_Deep\")\n","    loss = \"binary_crossentropy\" if binary else \"sparse_categorical_crossentropy\"\n","    model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"rCua7L8cT8fH"},"source":["### Create Model Dicts"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1744732792643,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"-prXETVUrl_u"},"outputs":[],"source":["def create_models_simple(input_shape, binary):\n","    \"\"\"\n","    Creates and compiles all the models, returning them in a dictionary.\n","\n","    Args:\n","        input_shape: Tuple specifying the input shape for the models.\n","        binary: Boolean indicating if the problem is binary classification.\n","\n","    Returns:\n","        A dictionary where keys are model names and values are dictionaries\n","        containing the model and its parameter count.\n","    \"\"\"\n","\n","    # 1. LSTM Standard\n","    lstm_model = create_lstm(input_shape, binary)\n","\n","    # 2. BiLSTM\n","    bilstm_model = create_bidirectional_lstm(input_shape, binary)\n","\n","    # 3. LSTM + Dense\n","    lstm_dense_model = create_lstm_with_dense(input_shape, binary)\n","\n","    # 4. BiLSTM + Dense\n","    bilstm_dense_model = create_bidirectional_lstm_with_dense(input_shape, binary)\n","\n","    # 5. 2 Hidden LSTM + Dense\n","    lstm_2_hidden_dense_model = create_2_hidden_lstm_with_dense(input_shape, binary)\n","\n","    # 6. 2 Hidden BiLSTM + Dense\n","    bilstm_2_hidden_dense_model = create_2_hidden_bidirectional_lstm_with_dense(input_shape, binary)\n","\n","    models = {\n","        \"LSTM\": {\"model\": lstm_model, \"parameters\": lstm_model.count_params()},\n","        \"LSTM_Dense\": {\"model\": lstm_dense_model, \"parameters\": lstm_dense_model.count_params()},\n","        \"LSTM_Deep\": {\"model\": lstm_2_hidden_dense_model, \"parameters\": lstm_2_hidden_dense_model.count_params()},\n","        \"BiLSTM\": {\"model\": bilstm_model, \"parameters\": bilstm_model.count_params()},\n","        \"BiLSTM_Dense\": {\"model\": bilstm_dense_model, \"parameters\": bilstm_dense_model.count_params()},\n","        \"BiLSTM_Deep\": {\"model\": bilstm_2_hidden_dense_model, \"parameters\": bilstm_2_hidden_dense_model.count_params()},\n","    }\n","\n","    return models"]},{"cell_type":"code","source":["models_dict = create_models_simple((500, 1), True)\n","print(models_dict)\n","models_dict = create_models_simple((500, 5), True)\n","print(models_dict)\n","models_dict = create_models_simple((500, 25), True)\n","print(models_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3PubGQJCXF_X","executionInfo":{"status":"ok","timestamp":1744732799559,"user_tz":-120,"elapsed":6915,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"37b9408e-43ee-4157-84bc-0f32a07011ec"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'LSTM': {'model': <Sequential name=LSTM, built=True>, 'parameters': 4385}, 'LSTM_Dense': {'model': <Sequential name=LSTM_Dense, built=True>, 'parameters': 4897}, 'LSTM_Deep': {'model': <Sequential name=LSTM_Deep, built=True>, 'parameters': 29857}, 'BiLSTM': {'model': <Sequential name=BiLSTM, built=True>, 'parameters': 8769}, 'BiLSTM_Dense': {'model': <Sequential name=BiLSTM_Dense, built=True>, 'parameters': 9761}, 'BiLSTM_Deep': {'model': <Sequential name=BiLSTM_Deep, built=True>, 'parameters': 76065}}\n","{'LSTM': {'model': <Sequential name=LSTM, built=True>, 'parameters': 4897}, 'LSTM_Dense': {'model': <Sequential name=LSTM_Dense, built=True>, 'parameters': 5409}, 'LSTM_Deep': {'model': <Sequential name=LSTM_Deep, built=True>, 'parameters': 30881}, 'BiLSTM': {'model': <Sequential name=BiLSTM, built=True>, 'parameters': 9793}, 'BiLSTM_Dense': {'model': <Sequential name=BiLSTM_Dense, built=True>, 'parameters': 10785}, 'BiLSTM_Deep': {'model': <Sequential name=BiLSTM_Deep, built=True>, 'parameters': 78113}}\n","{'LSTM': {'model': <Sequential name=LSTM, built=True>, 'parameters': 7457}, 'LSTM_Dense': {'model': <Sequential name=LSTM_Dense, built=True>, 'parameters': 7969}, 'LSTM_Deep': {'model': <Sequential name=LSTM_Deep, built=True>, 'parameters': 36001}, 'BiLSTM': {'model': <Sequential name=BiLSTM, built=True>, 'parameters': 14913}, 'BiLSTM_Dense': {'model': <Sequential name=BiLSTM_Dense, built=True>, 'parameters': 15905}, 'BiLSTM_Deep': {'model': <Sequential name=BiLSTM_Deep, built=True>, 'parameters': 88353}}\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"id":"LrjefpHIl6H1","executionInfo":{"status":"ok","timestamp":1744732799581,"user_tz":-120,"elapsed":2,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["def model_funcs_simple():\n","    \"\"\"\n","    Creates a dictionary of model creation functions.\n","\n","    Args:\n","        input_shape: Tuple specifying the input shape for the models.\n","        binary: Boolean indicating if the problem is binary classification.\n","\n","    Returns:\n","        A dictionary where keys are model names and values are the\n","        corresponding model creation functions.\n","    \"\"\"\n","    models = {\n","        \"LSTM\": create_lstm,\n","        \"LSTM_Dense\": create_lstm_with_dense,\n","        \"LSTM_Deep\": create_2_hidden_lstm_with_dense,\n","        \"BiLSTM\": create_bidirectional_lstm,\n","        \"BiLSTM_Dense\": create_bidirectional_lstm_with_dense,\n","        \"BiLSTM_Deep\": create_2_hidden_bidirectional_lstm_with_dense,\n","    }\n","\n","    return models"]},{"cell_type":"markdown","source":["Threshold Model"],"metadata":{"id":"OC6Hp1C2YC5u"}},{"cell_type":"code","source":["import numpy as np\n","\n","def compute_thresholds(X, std_factor=1.0):\n","    \"\"\"\n","    Computes dynamic thresholds using mean + std dev, ensuring no negative thresholds.\n","\n","    :param X: Training data of shape (n_samples, seq_len, num_features)\n","    :param std_factor: Multiplier for standard deviation (e.g., 1.0 means mean + 1 std dev)\n","    :return: Computed thresholds of shape (1, 1, num_features)\n","    \"\"\"\n","    feature_data = X.reshape(-1, X.shape[-1])  # Flatten across samples & time: (n_samples * seq_len, num_features)\n","    mean_values = np.mean(feature_data, axis=0)\n","    std_values = np.std(feature_data, axis=0)\n","\n","    # Compute thresholds (only allow positive values)\n","    thresholds = np.maximum(mean_values + std_factor * std_values, 0)\n","\n","    # Reshape to broadcast properly: (1, 1, num_features)\n","    return thresholds.reshape(1, 1, -1)\n","\n","def predict_binary(X, thresholds):\n","    \"\"\"\n","    Binary classification: Predict whether each timestep contains a burst (1) or not (0).\n","\n","    :param X: Input data of shape (n_samples, seq_len, num_features)\n","    :param thresholds: Thresholds of shape (1, 1, num_features)\n","    :return: Binary predictions of shape (n_samples, seq_len)\n","    \"\"\"\n","    # Compare with thresholds for each feature, then check if any feature exceeds its threshold\n","    return (X > thresholds).any(axis=-1).astype(int)\n","\n","def predict_multiclass(X, thresholds):\n","    \"\"\"\n","    Multiclass classification: Count how many features exceed their threshold per timestep.\n","\n","    :param X: Input data of shape (n_samples, seq_len, num_features)\n","    :param thresholds: Thresholds of shape (1, 1, num_features)\n","    :return: Multiclass predictions of shape (n_samples, seq_len), values 0-4\n","    \"\"\"\n","    return np.sum(X > thresholds, axis=-1)\n","\n","# Example Usage\n","np.random.seed(42)\n","X_train = np.random.rand(100, 500, 4)  # Fake training data (100 samples, 500 timesteps, 4 features)\n","X_test = np.random.rand(20, 500, 4)    # Fake test data (20 samples, 500 timesteps, 4 features)\n","\n","# Compute dynamic thresholds from training data\n","thresholds = compute_thresholds(X_train, std_factor=1.5)\n","\n","# Predict bursts using binary and multiclass models\n","binary_preds = predict_binary(X_test, thresholds)\n","multiclass_preds = predict_multiclass(X_test, thresholds)\n","\n","print(f\"Binary Predictions Shape: {binary_preds.shape} (Should be (20, 500))\")\n","print(f\"Multiclass Predictions Shape: {multiclass_preds.shape} (Should be (20, 500))\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSMt-HL4YE70","executionInfo":{"status":"ok","timestamp":1744732799631,"user_tz":-120,"elapsed":8,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"838461dc-8a6a-47d3-dc55-686c67b88910"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Binary Predictions Shape: (20, 500) (Should be (20, 500))\n","Multiclass Predictions Shape: (20, 500) (Should be (20, 500))\n"]}]},{"cell_type":"markdown","metadata":{"id":"kBNCJqQLuSth"},"source":["## Data Import"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11476,"status":"ok","timestamp":1744732811107,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"bulzztRauP-J","outputId":"5a53d0a7-af9e-4e2b-83f5-72cf18196db6"},"outputs":[{"output_type":"stream","name":"stdout","text":["DATASET INFO:\n","Shape: (10, 20, 5, 7500, 25)\n","\n","No of Samples: 10\n","\n","No of Frequencies: 20\n","Freqency values: [np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(11), np.int64(13), np.int64(16), np.int64(18), np.int64(22), np.int64(26), np.int64(31), np.int64(36), np.int64(43), np.int64(51), np.int64(60), np.int64(71), np.int64(84), np.int64(100)]\n","\n","No of noise ratios: 5\n","Signal to Noise ratios (in db) [np.int64(-10), np.int64(-8), np.int64(-6), np.int64(-4), np.int64(-2), np.int64(0), np.int64(2), np.int64(4), np.int64(6), np.int64(8)]\n","\n","No of Datapoints: 7500\n","\n","No of Features per Datapoint: 25 (signal, hilbert amp, 20 wavelets for each freq)\n"]}],"source":["data_path = \"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Including Features/5_class_mid_noise_30s_features_vec.npy\"\n","label_path = \"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Including Features/5_class_mid_noise_30s_numeric_label_vec.npy\"\n","\n","data_vec, label_vec = du.load_data(data_path, label_path)\n","\n","du.data_info(data_vec)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1744732811161,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"yi75Jy_NvlCV","outputId":"c6f255cc-6659-4537-c4af-3a6162b67ead"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 20, 5, 7500, 25)\n","(10, 20, 5, 7500)\n"]}],"source":["data_vec_trim = data_vec[:, :, :]\n","label_vec_trim = label_vec[:, :, :]\n","\n","print(data_vec_trim.shape)\n","print(label_vec_trim.shape)"]},{"cell_type":"code","source":["## Class distribution\n","unique, counts = np.unique(label_vec_trim.flatten(), return_counts=True)\n","print(f\"Class distribution: {dict(zip(unique, counts))}\")\n","print(f\"Noise  Percentage: {(counts[0]/(sum(counts))):.4f}\")\n","print(f\"Theta Percentage: {(counts[1]/(sum(counts))):.4f}\")\n","print(f\"Alpha Percentage: {(counts[2]/(sum(counts))):.4f}\")\n","print(f\"Beta Percentage: {(counts[3]/(sum(counts))):.4f}\")\n","print(f\"Gamma Percentage: {(counts[4]/(sum(counts))):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRpA41twg0ln","executionInfo":{"status":"ok","timestamp":1744732811218,"user_tz":-120,"elapsed":93,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"d8ba965a-06b7-4291-d73d-97d76228c59d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution: {np.int64(0): np.int64(4008043), np.int64(1): np.int64(882021), np.int64(2): np.int64(882177), np.int64(3): np.int64(886981), np.int64(4): np.int64(840778)}\n","Noise  Percentage: 0.5344\n","Theta Percentage: 0.1176\n","Alpha Percentage: 0.1176\n","Beta Percentage: 0.1183\n","Gamma Percentage: 0.1121\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"KjL-9suE23DC","executionInfo":{"status":"ok","timestamp":1744732811278,"user_tz":-120,"elapsed":53,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["def preprocess_data(data_vector, label_vector):\n","    \"\"\"\n","    Preprocesses data and label vectors to the desired shape (S, timesteps, I).\n","    Handles various input shapes, including timesteps from 1000 and up:\n","    - S: N_of_samples\n","    - F: N_of_frequencies\n","    - N: N_of_noise_burst_ratios\n","    - I: N_of_Input_Features\n","    - 2D: (S, timesteps)\n","    - 3D: (S, F, timesteps) or (S, timesteps, I)\n","    - 4D: (S, F, N, timesteps) or (S, F, timesteps, I)\n","    - 5D: (S, F, N, timesteps, I)\n","\n","    Args:\n","        data_vector (np.ndarray): The input data vector.\n","        label_vector (np.ndarray): The input label vector.\n","\n","    Returns:\n","        tuple: A tuple containing the preprocessed data and label vectors.\n","    \"\"\"\n","\n","    def reshape_vector(vector):\n","        \"\"\"Helper function to reshape a single vector.\"\"\"\n","        if vector.ndim == 2:  # (S, timesteps)\n","            # Reshape 2D to 3D by adding a feature dimension with I = 1\n","            vector = vector.reshape(vector.shape[0], vector.shape[1], 1)\n","        elif vector.ndim == 3:\n","            if vector.shape[1] < 1000 and vector.shape[2] >= 1000:  # (S, F, timesteps)\n","                # Assume timesteps is the last dimension, move it to the second position\n","                vector = vector.transpose(0, 2, 1)\n","            # else: (S, timesteps, I) - already in desired format\n","        elif vector.ndim == 4:\n","            if vector.shape[2] < 1000 and vector.shape[3] >= 1000:  # (S, F, N, timesteps)\n","                # Reshape by flattening the first three dimensions\n","                N = vector.shape[0] * vector.shape[1] * vector.shape[2]\n","                vector = vector.reshape(N, vector.shape[3], 1)\n","            # Assume its (S, F, timesteps, I), flatten first 2 to get (N, timesteps, I)\n","            elif vector.shape[2] >= 1000 and vector.shape[3] < 1000:\n","                N = vector.shape[0] * vector.shape[1]\n","                vector = vector.reshape(N, vector.shape[2], vector.shape[3])\n","            else:\n","                raise ValueError(\"4D input shape not recognized. Please check the dimensions.\")\n","        elif vector.ndim == 5:  # (S, F, N, timesteps, I)\n","            # Reshape by flattening the first three dimensions\n","            N = vector.shape[0] * vector.shape[1] * vector.shape[2]\n","            vector = vector.reshape(N, vector.shape[3], vector.shape[4])\n","        else:\n","            raise ValueError(\"Input vector must be 2D, 3D, 4D, or 5D.\")\n","        return vector\n","\n","\n","    # Reshape data and label vectors independently\n","    data_vector = reshape_vector(data_vector)\n","    label_vector = reshape_vector(label_vector)\n","\n","    return data_vector, label_vector"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"kg4x5Pbl23A8","executionInfo":{"status":"ok","timestamp":1744732811294,"user_tz":-120,"elapsed":57,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["import numpy as np\n","\n","def split_sequences_keep_data(X, y, new_seq_len):\n","    \"\"\"\n","    Splits input sequences and their per-timestep labels into smaller sequences of length `new_seq_len`.\n","\n","    Args:\n","        X (numpy array): Input data of shape (samples, seq_len, features).\n","        y (numpy array): Labels of shape (samples, seq_len) or (samples, seq_len, ...).\n","        new_seq_len (int): Desired sequence length.\n","\n","    Returns:\n","        X_new: New input data with shape (new_samples, new_seq_len, features).\n","        y_new: New labels with shape (new_samples, new_seq_len, ...).\n","    \"\"\"\n","    num_samples, original_seq_len, num_features = X.shape\n","    if original_seq_len % new_seq_len != 0:\n","        print(f\"Warning: Original seq_len ({original_seq_len}) is not a multiple of new_seq_len ({new_seq_len}).\")\n","\n","    # Number of new samples per original sequence\n","    num_segments = original_seq_len // new_seq_len\n","\n","    # Create new datasets by reshaping\n","    X_new = np.reshape(X[:, :num_segments * new_seq_len, :],\n","                       (-1, new_seq_len, num_features))  # Shape: (samples * num_segments, new_seq_len, features)\n","    y_new = np.reshape(y[:, :num_segments * new_seq_len, ...],\n","                       (-1, new_seq_len, *y.shape[2:]))  # Adjust labels to match input shape\n","\n","    return X_new, y_new\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1744732811299,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"wc43pWiwvpsH","outputId":"95f344da-2d53-4aa8-9dff-619b3ce197b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1000, 7500, 25)\n","(1000, 7500, 1)\n"]}],"source":["data_vec_shaped, label_vec_shaped = preprocess_data(data_vector=data_vec_trim, label_vector=label_vec_trim)\n","\n","print(data_vec_shaped.shape)\n","print(label_vec_shaped.shape)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"PYtFDIjr5t_6","executionInfo":{"status":"ok","timestamp":1744732811315,"user_tz":-120,"elapsed":3,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["new_seq_len = 500\n","\n","sample_increase_factor = int(data_vec_shaped.shape[1] / new_seq_len)\n","\n","data_vec_reshaped, label_vec_reshaped = split_sequences_keep_data(data_vec_shaped, label_vec_shaped, new_seq_len=new_seq_len)\n","\n","# Extract noise levels\n","noise_levels = np.arange(data_vec_trim.shape[2])  # Assuming noise levels are 0, 1, 2, 3, 4\n","noise_levels = np.repeat(noise_levels, data_vec_trim.shape[0] * data_vec_trim.shape[1] * sample_increase_factor)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1744732811344,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"RYWhCZB8298N","outputId":"c1d43d09-ebc6-43a9-b7fb-854a40149eb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["(15000, 500, 25)\n","(15000, 500, 1)\n","(15000,)\n"]}],"source":["\n","print(data_vec_reshaped.shape)\n","print(label_vec_reshaped.shape)\n","print(noise_levels.shape)"]},{"cell_type":"code","source":["# For testing make training data very small\n","# data_vec_reshaped = data_vec_reshaped[:50]\n","# label_vec_reshaped = label_vec_reshaped[:50]"],"metadata":{"id":"m9R_uCqZpsR5","executionInfo":{"status":"ok","timestamp":1744732811348,"user_tz":-120,"elapsed":3,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"id":"MdIkhIxMWpVu","executionInfo":{"status":"ok","timestamp":1744732811381,"user_tz":-120,"elapsed":18,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["# label_one_hot = []\n","# for label in label_vec_reshaped:\n","#     label_one_hot.append(du.one_hot_encode_single_sample(label, [0,1,2,3,4]))\n","\n","# label_one_hot = np.array(label_one_hot)\n","# label_one_hot.shape"]},{"cell_type":"markdown","metadata":{"id":"aJw1XMp2uUVw"},"source":["## Data Splits\n","\n","Be careful about choosing binary or not in the TT Splits"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153,"status":"ok","timestamp":1744732811518,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"8ueXnyIwuZIW","outputId":"b8c1e719-8d60-4dda-b55d-90c237309740"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training set shape: (12000, 500, 5) (12000, 500, 1)\n","Test set shape: (3000, 500, 5) (3000, 500, 1)\n"]}],"source":["# Assuming `X` is your input data and `y` is your labels\n","test_size = 0.2  # 20% test set\n","\n","hilbert_data_vec = data_vec_reshaped[:,:,:5]\n","signal_data_vec = data_vec_reshaped[:,:, :1]\n","all_data_vec = data_vec_reshaped\n","\n","label_multi = label_vec_reshaped\n","label_binary = label_vec_reshaped.copy()\n","label_binary[label_binary >= 1] = 1  # Convert to binary labels\n","\n","# First, split into training+validation and test sets\n","X_train, X_test, y_train, y_test = train_test_split(hilbert_data_vec, label_multi, test_size=test_size, random_state=42)\n","\n","\n","# Check the shapes\n","print(\"Training set shape:\", X_train.shape, y_train.shape)\n","print(\"Test set shape:\", X_test.shape, y_test.shape)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1744732811927,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"8aXIx5z63k5R","outputId":"d6ae3002-b40e-45f8-f755-01818b98b15e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'LSTM': {'model': <Sequential name=LSTM, built=True>, 'parameters': 5029},\n"," 'LSTM_Dense': {'model': <Sequential name=LSTM_Dense, built=True>,\n","  'parameters': 5477},\n"," 'LSTM_Deep': {'model': <Sequential name=LSTM_Deep, built=True>,\n","  'parameters': 30949},\n"," 'BiLSTM': {'model': <Sequential name=BiLSTM, built=True>,\n","  'parameters': 10053},\n"," 'BiLSTM_Dense': {'model': <Sequential name=BiLSTM_Dense, built=True>,\n","  'parameters': 10853},\n"," 'BiLSTM_Deep': {'model': <Sequential name=BiLSTM_Deep, built=True>,\n","  'parameters': 78181}}"]},"metadata":{},"execution_count":20}],"source":["models_dict = create_models_simple((1500, 5), binary=False)\n","models_dict"]},{"cell_type":"markdown","metadata":{"id":"zs3IQP22uoB4"},"source":["## Train Models (deprecated)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"yXWqvPEzupkc","executionInfo":{"status":"ok","timestamp":1744703961856,"user_tz":-120,"elapsed":2,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["def evaluate_model(model, X_train, y_train, X_val, y_val, binary):\n","    \"\"\"\n","    Trains and evaluates the model, returning the metrics and evaluation data.\n","    \"\"\"\n","\n","\n","    model.summary()\n","\n","    start_time = time.time()\n","\n","    # Train the model\n","    history = model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1, validation_data=(X_val, y_val))\n","\n","    # Measure training time\n","    training_time = time.time() - start_time\n","\n","    start_time = time.time()\n","\n","    # Evaluate\n","    y_pred = model.predict(X_val).argmax(axis=-1) if not binary else (model.predict(X_val) > 0.5).astype(int)\n","    inference_time = time.time() - start_time\n","\n","    y_pred = y_pred.flatten()\n","    y_val = y_val.flatten()\n","\n","\n","    accuracy = accuracy_score(y_val, y_pred)\n","    mae = mean_absolute_error(y_val, y_pred)\n","    recall = recall_score(y_val, y_pred, average=\"binary\" if binary else \"macro\")\n","    f1 = f1_score(y_val, y_pred, average=\"binary\" if binary else \"macro\")\n","    mcc = matthews_corrcoef(y_val, y_pred)\n","\n","    print(f\"Accuracy: {accuracy}\")\n","    print(f\"MAE: {mae}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"F1-Score: {f1}\")\n","    print(f\"Confusion Matrix:\\n{cm}\")\n","    print(f\"MCC: {mcc}\")\n","\n","    return model , {\n","        \"accuracy\": accuracy,\n","        \"mcc\": mcc,\n","        \"mae\": mae,\n","        \"recall\": recall,\n","        \"f1\": f1,\n","        \"training_time\": training_time,\n","        \"inference_time\": inference_time\n","\n","    }\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"xuvtDw_v3PN6","executionInfo":{"status":"ok","timestamp":1744703961861,"user_tz":-120,"elapsed":2,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["def train_and_evaluate_models(models, X_train, y_train, X_val, y_val, binary):\n","    \"\"\"\n","    Trains and evaluates all models in the dictionary, storing performance metrics.\n","\n","    Args:\n","        models: Dictionary containing models with subkeys \"model\" and \"parameters\".\n","        X_train: Training data.\n","        y_train: Training labels.\n","        X_val: Validation data.\n","        y_val: Validation labels.\n","\n","    Returns:\n","        The updated models dictionary with a new \"performances\" subkey for each model.\n","    \"\"\"\n","\n","    for model_name, model_data in models.items():#\n","        print(f\"Training and evaluating {model_name}...\")  # Optional: Print progress\n","        model = model_data[\"model\"]  # Get the model object\n","\n","        trained_model, performance_metrics = evaluate_model(model, X_train, y_train, X_val, y_val, binary)\n","\n","        # Save the trained model in Dict\n","        model_data[\"model\"] = trained_model\n","\n","        # Store the performance metrics in the \"performances\" subkey\n","        model_data[\"performances\"] = performance_metrics\n","\n","    return models  # Return the updated models dictionary"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1744703961870,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"GHqO2yXZhCuV","outputId":"8d58eed4-da34-4fdf-bee4-7fe7adfdceaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["(12000, 500, 5)\n","(12000, 500, 1)\n","(3000, 500, 5)\n","(3000, 500, 1)\n"]}],"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"3VeSFTOS4Ruk","colab":{"base_uri":"https://localhost:8080/","height":581},"collapsed":true,"executionInfo":{"status":"error","timestamp":1744663247358,"user_tz":-120,"elapsed":6870,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"50f491f4-f4a5-4c5c-c615-34197953033f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training and evaluating LSTM...\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"LSTM\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ lstm_24 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,864\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m5\u001b[0m)        │           \u001b[38;5;34m165\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ lstm_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,029\u001b[0m (19.64 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,029</span> (19.64 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,029\u001b[0m (19.64 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,029</span> (19.64 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.4683 - loss: 1.2962 - val_accuracy: 0.7341 - val_loss: 0.7439\n","Epoch 2/10\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-37ba5b3a22ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using Only the signal as an input feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msignal_only_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-84f7c99a17c4>\u001b[0m in \u001b[0;36mtrain_and_evaluate_models\u001b[0;34m(models, X_train, y_train, X_val, y_val, binary)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Get the model object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Save the trained model in Dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-24f382c8bcfa>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_train, y_train, X_val, y_val, binary)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Measure training time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Using Only the signal as an input feature\n","\n","signal_only_models = train_and_evaluate_models(models_dict, X_train, y_train, X_test, y_test, binary=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1738447926852,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-60},"id":"6gR_O7s5PRCl","outputId":"3d34201e-f39c-4487-f858-c63521073865"},"outputs":[{"data":{"text/plain":["{'LSTM': {'model': <keras.engine.sequential.Sequential at 0x78eb28a7b850>,\n","  'parameters': 4385,\n","  'performances': {'accuracy': 0.8542,\n","   'mcc': 0.70743184074071,\n","   'mae': 0.1458,\n","   'recall': 0.8183711048158641,\n","   'f1': 0.8408574921411107,\n","   'confusion_matrix': array([[70353,  9047],\n","          [12823, 57777]]),\n","   'training_time': 144.01327061653137,\n","   'inference_time': 0.8876395225524902}},\n"," 'BiLSTM': {'model': <keras.engine.sequential.Sequential at 0x78eb2929d150>,\n","  'parameters': 5041,\n","  'performances': {'accuracy': 0.8995066666666667,\n","   'mcc': 0.7982790522855711,\n","   'mae': 0.10049333333333334,\n","   'recall': 0.8911756373937677,\n","   'f1': 0.8930223975927556,\n","   'confusion_matrix': array([[72009,  7391],\n","          [ 7683, 62917]]),\n","   'training_time': 153.8624861240387,\n","   'inference_time': 1.3268213272094727}},\n"," 'RNN': {'model': <keras.engine.sequential.Sequential at 0x78eb2b4e1090>,\n","  'parameters': 4289,\n","  'performances': {'accuracy': 0.83588,\n","   'mcc': 0.6710292799915758,\n","   'mae': 0.16412,\n","   'recall': 0.7867705382436261,\n","   'f1': 0.8185984820573281,\n","   'confusion_matrix': array([[69836,  9564],\n","          [15054, 55546]]),\n","   'training_time': 61.187647342681885,\n","   'inference_time': 0.49288153648376465}},\n"," 'GRU': {'model': <keras.engine.sequential.Sequential at 0x78eb2b308190>,\n","  'parameters': 3393,\n","  'performances': {'accuracy': 0.83762,\n","   'mcc': 0.6738597065538349,\n","   'mae': 0.16238,\n","   'recall': 0.8157223796033994,\n","   'f1': 0.8254441474304306,\n","   'confusion_matrix': array([[68053, 11347],\n","          [13010, 57590]]),\n","   'training_time': 143.50585746765137,\n","   'inference_time': 1.0248956680297852}},\n"," 'TCN': {'model': <keras.engine.sequential.Sequential at 0x78eb29250e90>,\n","  'parameters': 5601,\n","  'performances': {'accuracy': 0.8420866666666667,\n","   'mcc': 0.6829179889773844,\n","   'mae': 0.15791333333333332,\n","   'recall': 0.8263597733711048,\n","   'f1': 0.8312519145965277,\n","   'confusion_matrix': array([[67972, 11428],\n","          [12259, 58341]]),\n","   'training_time': 43.51044297218323,\n","   'inference_time': 0.5817756652832031}},\n"," 'FNN': {'model': <keras.engine.sequential.Sequential at 0x78eb2b3b0890>,\n","  'parameters': 2241,\n","  'performances': {'accuracy': 0.6877533333333333,\n","   'mcc': 0.374887498919398,\n","   'mae': 0.3122466666666667,\n","   'recall': 0.5487393767705382,\n","   'f1': 0.6232514740305183,\n","   'confusion_matrix': array([[64422, 14978],\n","          [31859, 38741]]),\n","   'training_time': 11.091627836227417,\n","   'inference_time': 0.1857895851135254}}}"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["signal_only_models\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1738448092058,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-60},"id":"zkLoQWoxNSg7","outputId":"9c77bece-bace-448a-9315-8a40539587fa"},"outputs":[{"data":{"text/html":["\n","    <style>\n","    table {\n","        border-collapse: collapse;\n","        width: 100%;\n","    }\n","    th, td {\n","        text-align: center;\n","        padding: 8px;\n","        border: 1px solid #ddd;\n","    }\n","    th {\n","        background-color: #4f50b3;\n","    }\n","    </style>\n","    <table><thead><tr><th>Model</th><th>Parameters</th><th>Accuracy</th><th>MCC</th><th>Recall</th><th>F1</th><th>Training Time</th><th>Inference Time</th></tr></thead><tbody><tr><td>LSTM</td><td>4385</td><td>0.854</td><td>0.707</td><td>0.818</td><td>0.841</td><td>144.0</td><td>0.888</td></tr><tr><td>BiLSTM</td><td>5041</td><td>0.900</td><td>0.798</td><td>0.891</td><td>0.893</td><td>153.9</td><td>1.327</td></tr><tr><td>RNN</td><td>4289</td><td>0.836</td><td>0.671</td><td>0.787</td><td>0.819</td><td>61.2</td><td>0.493</td></tr><tr><td>GRU</td><td>3393</td><td>0.838</td><td>0.674</td><td>0.816</td><td>0.825</td><td>143.5</td><td>1.025</td></tr><tr><td>TCN</td><td>5601</td><td>0.842</td><td>0.683</td><td>0.826</td><td>0.831</td><td>43.5</td><td>0.582</td></tr><tr><td>FNN</td><td>2241</td><td>0.688</td><td>0.375</td><td>0.549</td><td>0.623</td><td>11.1</td><td>0.186</td></tr></tbody></table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import HTML\n","\n","def display_models_with_performances(models):\n","    \"\"\"\n","    Displays models with existing 'performances' in an HTML table.\n","\n","    Args:\n","        models: A dictionary where keys are model names and values are dictionaries\n","               containing model information, including 'performances' if available.\n","    \"\"\"\n","\n","    html = \"\"\"\n","    <style>\n","    table {\n","        border-collapse: collapse;\n","        width: 100%;\n","    }\n","    th, td {\n","        text-align: center;\n","        padding: 8px;\n","        border: 1px solid #ddd;\n","    }\n","    th {\n","        background-color: #4f50b3;\n","    }\n","    </style>\n","    <table><thead><tr><th>Model</th><th>Parameters</th><th>Accuracy</th><th>MCC</th><th>Recall</th><th>F1</th><th>Training Time</th><th>Inference Time</th></tr></thead><tbody>\"\"\"\n","\n","    for model_name, model_data in models.items():\n","        if \"performances\" in model_data:\n","            performances = model_data[\"performances\"]\n","            html += f\"<tr><td>{model_name}</td>\"\n","            html += f\"<td>{model_data['parameters']}</td>\"\n","            html += f\"<td>{performances['accuracy']:.3f}</td>\"\n","            html += f\"<td>{performances['mcc']:.3f}</td>\"\n","            html += f\"<td>{performances['recall']:.3f}</td>\"\n","            html += f\"<td>{performances['f1']:.3f}</td>\"\n","            html += f\"<td>{performances['training_time']:.1f}</td>\"\n","            html += f\"<td>{performances['inference_time']:.3f}</td></tr>\"\n","\n","    html += \"</tbody></table>\"\n","    display(HTML(html))\n","\n","display_models_with_performances(signal_only_models)"]},{"cell_type":"markdown","metadata":{"id":"lwhQ88ipzjmj"},"source":["## K-Fold Analysis\n","\n","- Stratified k-fold\n","- Make function that takes in only the model_dict and returns result dict\n","- Create Heatlike result visualizations"]},{"cell_type":"markdown","source":["### Set-Up"],"metadata":{"id":"TPlVW-vplMFH"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"pro7Jn4SKK72","executionInfo":{"status":"ok","timestamp":1744732818518,"user_tz":-120,"elapsed":6565,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["# Setting Up the different model dicts for different configs\n","\n","models_siganl_beta_binary = create_models_simple((data_vec_reshaped.shape[1], 1), True)\n","models_singal_multi =       create_models_simple((data_vec_reshaped.shape[1], 1), False)\n","\n","models_hilbert_beta_binary = create_models_simple((data_vec_reshaped.shape[1], 2), True)\n","models_hilbert_multi =       create_models_simple((data_vec_reshaped.shape[1], 5), False)\n","\n","models_wavelet_beta_binary = create_models_simple((data_vec_reshaped.shape[1], 6), True)\n","models_wavelet_multi =       create_models_simple((data_vec_reshaped.shape[1], 21), False)\n","\n","models_all_features_beta_binary = create_models_simple((data_vec_reshaped.shape[1], 7), True)\n","models_all_features_multi =       create_models_simple((data_vec_reshaped.shape[1], 25), False)\n"]},{"cell_type":"code","source":["models_wavelet_multi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwXXLS3QBDv9","executionInfo":{"status":"ok","timestamp":1744732818544,"user_tz":-120,"elapsed":9,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"ad08cd77-1781-45f7-d03d-7acb57926aa2"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'LSTM': {'model': <Sequential name=LSTM, built=True>, 'parameters': 7077},\n"," 'LSTM_Dense': {'model': <Sequential name=LSTM_Dense, built=True>,\n","  'parameters': 7525},\n"," 'LSTM_Deep': {'model': <Sequential name=LSTM_Deep, built=True>,\n","  'parameters': 35045},\n"," 'BiLSTM': {'model': <Sequential name=BiLSTM, built=True>,\n","  'parameters': 14149},\n"," 'BiLSTM_Dense': {'model': <Sequential name=BiLSTM_Dense, built=True>,\n","  'parameters': 14949},\n"," 'BiLSTM_Deep': {'model': <Sequential name=BiLSTM_Deep, built=True>,\n","  'parameters': 86373}}"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":23,"metadata":{"id":"Wmd2kVu4k15R","executionInfo":{"status":"ok","timestamp":1744732818618,"user_tz":-120,"elapsed":27,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["\n","def stratified_kfold_split(X, y, num_folds=5):\n","    \"\"\"\n","    Performs Stratified K-Fold cross-validation on sequence data.\n","\n","    Args:\n","        X: ndarray of shape (samples, timesteps, features) -> (7500, 500, ?)\n","        y: ndarray of shape (samples, timesteps, 1) -> (7500, 500, 1)\n","        num_folds: Number of folds for cross-validation.\n","\n","    Returns:\n","        List of (train_idx, val_idx) tuples.\n","    \"\"\"\n","    # Convert y from (7500, 500, 1) → (7500, 500)\n","    y_flat = y[:, :, 0]  # Remove last dimension\n","\n","    # Get **max class per sequence**\n","    y_max = np.max(y_flat, axis=1)  # Shape → (7500,)\n","\n","    # Stratified K-Fold splitting\n","    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n","    folds = list(skf.split(X, y_max))  # Get (train_idx, val_idx) splits\n","\n","    return folds\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"nXahq6ze2yQw","executionInfo":{"status":"ok","timestamp":1744732818630,"user_tz":-120,"elapsed":35,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["from collections import defaultdict\n","\n","# Train and Evaluate Function\n","def train_and_evaluate(models, X, y, k_folds=5, batch_size=16, epochs=10,\n","                       basePath=\"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Model Comparisons/LSTM Models/\",\n","                       dir_name=\"signal_binary\"):\n","\n","    skf_folds = stratified_kfold_split(X,y,num_folds=k_folds)\n","\n","    # Determines whether we work with binary data or multi data\n","    binary = len(np.unique(y)) == 2  # Check if only 2 unique values in y\n","\n","    model_results = {model_name: {} for model_name in models.keys()}\n","    trained_models = {model_name: [] for model_name in models.keys()}\n","\n","    for model_name, model_fn in models.items():\n","        acc_scores, mcc_scores, train_times, infer_times = [], [], [], []\n","\n","        # *** Initialize a list to store history for each fold ***\n","        val_mcc = []\n","        val_accuracy = []\n","        val_losses = []\n","        training_losses = []\n","        training_accuracy = []\n","\n","\n","        for fold, (train_idx, val_idx) in enumerate(skf_folds):\n","\n","            print(f\"Training Model: {model_name}, Fold: {fold+1}\")\n","\n","            X_train, X_val = X[train_idx], X[val_idx]\n","            y_train, y_val = y[train_idx], y[val_idx]\n","\n","\n","\n","            model = model_fn(input_shape=X_train.shape[1:], binary=binary)\n","\n","            mcc_callback = MCCCallback(validation_data=(X_val, y_val), is_binary=binary)\n","\n","            if model_name == \"CNN\":\n","                train_gen_100 = SlidingWindowGenerator(X_train, y_train, window_size=100, batch_size=batch_size*32, shuffle=False)\n","                val_gen_100 = SlidingWindowGenerator(X_val, y_val, window_size=100, batch_size=batch_size*32, shuffle=False)\n","\n","                # start_train = time.time()\n","                # history = model.fit(train_gen_100, epochs=epochs, batch_size=batch_size*32, validation_data=val_gen_100, verbose=1)\n","                # train_time = time.time() - start_train\n","\n","                # start_time = time.time()\n","                # y_true = []\n","                # y_pred = []\n","                # for i in range(len(val_gen_100)):\n","                #     X_batch, y_batch = val_gen_100[i]  # Get a batch of data\n","                #     y_true.extend(y_batch)\n","                #     y_pred_batch = model.predict(X_batch).argmax(axis=-1) if not binary else (model.predict(X_batch) > 0.5).astype(int)\n","                #     y_pred.extend(y_pred_batch)\n","                # inference_time = time.time() - start_time\n","\n","            else:\n","                start_train = time.time()\n","                history = model.fit(X_train, y_train, epochs=epochs,\n","                                    batch_size=batch_size, validation_data=(X_val, y_val),\n","                                    verbose=1, callbacks=[mcc_callback])\n","                train_time = time.time() - start_train\n","\n","                val_mccs = history.history['mcc']  # Get the validation MCC values\n","                training_accs = history.history['accuracy']\n","                val_accs = history.history['val_accuracy']\n","                val_loss = history.history['val_loss']\n","                training_loss = history.history['loss']\n","\n","\n","                start_infer = time.time()\n","                y_pred_probs = model.predict(X_val)\n","                infer_time = (time.time() - start_infer) / len(X_val)\n","\n","                y_pred = model.predict(X_val).argmax(axis=-1) if not binary else (model.predict(X_val) > 0.5).astype(int)\n","                y_true_flat = y_val.flatten()\n","                y_pred_flat = y_pred.flatten()\n","\n","            acc_scores.append(accuracy_score(y_true_flat, y_pred_flat))\n","            mcc_scores.append(matthews_corrcoef(y_true_flat, y_pred_flat))\n","            train_times.append(train_time)\n","            infer_times.append(infer_time)\n","\n","            val_mcc.append(val_mccs)\n","            val_accuracy.append(val_accs)\n","            val_losses.append(val_loss)\n","            training_accuracy.append(training_accs)\n","            training_losses.append(training_loss)\n","\n","\n","            num_params = model.count_params()\n","\n","            model_save_path = f\"{basePath}/{dir_name}/{model_name}/{model_name}_fold_{fold+1}.h5\"\n","            model.save(model_save_path)\n","            trained_models[model_name].append(model_save_path)\n","\n","        model_results[model_name] = {\n","            \"Accuracy\": {\"mean\": np.mean(acc_scores), \"std\": np.std(acc_scores), \"min\": np.min(acc_scores), \"max\": np.max(acc_scores)},\n","            \"MCC\": {\"mean\": np.mean(mcc_scores), \"std\": np.std(mcc_scores), \"min\": np.min(mcc_scores), \"max\": np.max(mcc_scores)},\n","            \"Train Time (s)\": {\"mean\": np.mean(train_times), \"std\": np.std(train_times), \"min\": np.min(train_times), \"max\": np.max(train_times)},\n","            \"Inference Time (s/sample)\": {\"mean\": np.mean(infer_times), \"std\": np.std(infer_times), \"min\": np.min(infer_times), \"max\": np.max(infer_times)},\n","            \"Training Accuracy\": training_accuracy,\n","            \"Training Loss\": training_losses,\n","            \"Validation MCC\": val_mcc,\n","            \"Validation Accuracy\": val_accuracy,\n","            \"Validation Loss\": val_losses,\n","            \"Parameters\": num_params\n","        }\n","\n","        pprint(model_results[model_name], width=1)\n","\n","        final_model_save_path = f\"{basePath}/{dir_name}/{model_name}/{model_name}_final.h5\"\n","        model.save(final_model_save_path)\n","\n","    return model_results, trained_models\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"hFTGPyDbvBsj","executionInfo":{"status":"ok","timestamp":1744732818654,"user_tz":-120,"elapsed":23,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from sklearn.metrics import matthews_corrcoef\n","\n","class MCCCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, validation_data, is_binary=True):\n","        super().__init__()\n","        self.validation_data = validation_data  # Tuple (x_val, y_val)\n","        self.is_binary = is_binary\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        logs = logs or {}\n","        x_val, y_val = self.validation_data\n","        y_pred = self.model.predict(x_val, verbose=0)\n","\n","        # Ensure y_val is flattened (shape: (N,))\n","        y_true = y_val.flatten()\n","\n","        if self.is_binary:\n","            # Binary classification: round predictions\n","            y_pred = np.round(y_pred).flatten()\n","        else:\n","            print(y_pred.shape)\n","            # Multiclass classification: use argmax (since predictions are (N, 5) softmax outputs)\n","            y_pred = np.argmax(y_pred, axis=-1).flatten()\n","\n","        # Compute MCC\n","        mcc = matthews_corrcoef(y_true, y_pred)\n","\n","        # Log MCC\n","        logs[\"mcc\"] = mcc\n","        print(f\"\\nEpoch {epoch + 1} - MCC: {mcc:.4f}\")\n"]},{"cell_type":"code","source":["simple_models_dict = model_funcs_simple()"],"metadata":{"id":"YqnNRP24ESDG","executionInfo":{"status":"ok","timestamp":1744732818728,"user_tz":-120,"elapsed":2,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["simple_models_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3smsJ6ky89p","executionInfo":{"status":"ok","timestamp":1744732818754,"user_tz":-120,"elapsed":25,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"a7fe3724-87cd-4292-d307-aca4f1068196"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'LSTM': <function __main__.create_lstm(input_shape, binary)>,\n"," 'LSTM_Dense': <function __main__.create_lstm_with_dense(input_shape, binary)>,\n"," 'LSTM_Deep': <function __main__.create_2_hidden_lstm_with_dense(input_shape, binary)>,\n"," 'BiLSTM': <function __main__.create_bidirectional_lstm(input_shape, binary)>,\n"," 'BiLSTM_Dense': <function __main__.create_bidirectional_lstm_with_dense(input_shape, binary)>,\n"," 'BiLSTM_Deep': <function __main__.create_2_hidden_bidirectional_lstm_with_dense(input_shape, binary)>}"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## Multi Models"],"metadata":{"id":"djZ6bveXl7T5"}},{"cell_type":"code","execution_count":28,"metadata":{"id":"_pTrbuaaRney","executionInfo":{"status":"ok","timestamp":1744732821161,"user_tz":-120,"elapsed":2406,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[],"source":["wavelet_remove_index = [1,2,3,4]\n","all_indices = np.arange(data_vec_reshaped.shape[-1])\n","indices_to_keep = np.delete(all_indices, wavelet_remove_index)\n","\n","hilbert_data_vec = data_vec_reshaped[:,:,:5]\n","signal_data_vec = data_vec_reshaped[:,:, :1]\n","wavelet_data_vec = data_vec_reshaped[:,:, indices_to_keep]\n","all_data_vec = data_vec_reshaped\n","\n","label_multi = label_vec_reshaped\n","label_binary = label_vec_reshaped.copy()\n","label_binary[label_binary >= 1] = 1  # Convert to binary labels\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1744732821225,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"},"user_tz":-120},"id":"ne-r5cUFRrLI","outputId":"c6be6bcb-c515-4bdb-8f3c-f7c8175d632e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(15000, 500, 1)\n","(15000, 500, 5)\n","(15000, 500, 21)\n","(15000, 500, 25)\n","(15000, 500, 1)\n","(15000, 500, 1)\n"]}],"source":["print(signal_data_vec.shape)\n","print(hilbert_data_vec.shape)\n","print(wavelet_data_vec.shape)\n","print(all_data_vec.shape)\n","print(label_multi.shape)\n","print(label_binary.shape)"]},{"cell_type":"code","source":["n_epochs = 20"],"metadata":{"id":"UrfZEPf5KO9x","executionInfo":{"status":"ok","timestamp":1744732821227,"user_tz":-120,"elapsed":1,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["All Features"],"metadata":{"id":"w4MS42w_nWhW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"r7ohm4QrQ7kb","outputId":"7f973b48-610b-42e3-e4f2-cf175ac6864e","executionInfo":{"status":"error","timestamp":1738873437484,"user_tz":-60,"elapsed":550237,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 1\n","Epoch 1/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.5312 - loss: 1.2774(200, 1500, 5)\n","\n","Epoch 1 - MCC: 0.6579\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 175ms/step - accuracy: 0.5333 - loss: 1.2729 - val_accuracy: 0.7850 - val_loss: 0.6623 - mcc: 0.6579\n","Epoch 2/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7942 - loss: 0.6200(200, 1500, 5)\n","\n","Epoch 2 - MCC: 0.7697\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.7947 - loss: 0.6188 - val_accuracy: 0.8530 - val_loss: 0.4381 - mcc: 0.7697\n","Epoch 3/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8458 - loss: 0.4552(200, 1500, 5)\n","\n","Epoch 3 - MCC: 0.7803\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.8458 - loss: 0.4550 - val_accuracy: 0.8597 - val_loss: 0.4004 - mcc: 0.7803\n","Epoch 4/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8525 - loss: 0.4256(200, 1500, 5)\n","\n","Epoch 4 - MCC: 0.7908\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 134ms/step - accuracy: 0.8527 - loss: 0.4252 - val_accuracy: 0.8662 - val_loss: 0.3666 - mcc: 0.7908\n","Epoch 5/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8612 - loss: 0.3918(200, 1500, 5)\n","\n","Epoch 5 - MCC: 0.7945\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.8613 - loss: 0.3915 - val_accuracy: 0.8680 - val_loss: 0.3531 - mcc: 0.7945\n","Epoch 6/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8710 - loss: 0.3637(200, 1500, 5)\n","\n","Epoch 6 - MCC: 0.7986\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 157ms/step - accuracy: 0.8710 - loss: 0.3636 - val_accuracy: 0.8715 - val_loss: 0.3497 - mcc: 0.7986\n","Epoch 7/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8773 - loss: 0.3429(200, 1500, 5)\n","\n","Epoch 7 - MCC: 0.8082\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 124ms/step - accuracy: 0.8772 - loss: 0.3431 - val_accuracy: 0.8766 - val_loss: 0.3270 - mcc: 0.8082\n","Epoch 8/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8776 - loss: 0.3373(200, 1500, 5)\n","\n","Epoch 8 - MCC: 0.8076\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - accuracy: 0.8776 - loss: 0.3373 - val_accuracy: 0.8767 - val_loss: 0.3338 - mcc: 0.8076\n","Epoch 9/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8772 - loss: 0.3389(200, 1500, 5)\n","\n","Epoch 9 - MCC: 0.8060\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - accuracy: 0.8772 - loss: 0.3387 - val_accuracy: 0.8760 - val_loss: 0.3264 - mcc: 0.8060\n","Epoch 10/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8866 - loss: 0.3096(200, 1500, 5)\n","\n","Epoch 10 - MCC: 0.7838\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 147ms/step - accuracy: 0.8863 - loss: 0.3102 - val_accuracy: 0.8590 - val_loss: 0.3725 - mcc: 0.7838\n","Epoch 11/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8711 - loss: 0.3606(200, 1500, 5)\n","\n","Epoch 11 - MCC: 0.8040\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.8711 - loss: 0.3604 - val_accuracy: 0.8752 - val_loss: 0.3404 - mcc: 0.8040\n","Epoch 12/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8743 - loss: 0.3433(200, 1500, 5)\n","\n","Epoch 12 - MCC: 0.8233\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - accuracy: 0.8745 - loss: 0.3428 - val_accuracy: 0.8862 - val_loss: 0.2927 - mcc: 0.8233\n","Epoch 13/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8966 - loss: 0.2782(200, 1500, 5)\n","\n","Epoch 13 - MCC: 0.8299\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - accuracy: 0.8965 - loss: 0.2785 - val_accuracy: 0.8910 - val_loss: 0.2830 - mcc: 0.8299\n","Epoch 14/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8966 - loss: 0.2728(200, 1500, 5)\n","\n","Epoch 14 - MCC: 0.8246\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 140ms/step - accuracy: 0.8965 - loss: 0.2730 - val_accuracy: 0.8875 - val_loss: 0.2868 - mcc: 0.8246\n","Epoch 15/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8971 - loss: 0.2743(200, 1500, 5)\n","\n","Epoch 15 - MCC: 0.8315\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - accuracy: 0.8971 - loss: 0.2743 - val_accuracy: 0.8914 - val_loss: 0.2812 - mcc: 0.8315\n","Epoch 16/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8959 - loss: 0.2773(200, 1500, 5)\n","\n","Epoch 16 - MCC: 0.8219\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 122ms/step - accuracy: 0.8958 - loss: 0.2776 - val_accuracy: 0.8861 - val_loss: 0.2967 - mcc: 0.8219\n","Epoch 17/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8876 - loss: 0.3001(200, 1500, 5)\n","\n","Epoch 17 - MCC: 0.8275\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.8877 - loss: 0.2999 - val_accuracy: 0.8897 - val_loss: 0.2862 - mcc: 0.8275\n","Epoch 18/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9016 - loss: 0.2598(200, 1500, 5)\n","\n","Epoch 18 - MCC: 0.8348\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 132ms/step - accuracy: 0.9016 - loss: 0.2600 - val_accuracy: 0.8934 - val_loss: 0.2720 - mcc: 0.8348\n","Epoch 19/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9027 - loss: 0.2554(200, 1500, 5)\n","\n","Epoch 19 - MCC: 0.8387\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 148ms/step - accuracy: 0.9027 - loss: 0.2555 - val_accuracy: 0.8960 - val_loss: 0.2669 - mcc: 0.8387\n","Epoch 20/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8973 - loss: 0.2716(200, 1500, 5)\n","\n","Epoch 20 - MCC: 0.8409\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 122ms/step - accuracy: 0.8973 - loss: 0.2716 - val_accuracy: 0.8973 - val_loss: 0.2632 - mcc: 0.8409\n","Epoch 21/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9020 - loss: 0.2550(200, 1500, 5)\n","\n","Epoch 21 - MCC: 0.8350\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - accuracy: 0.9021 - loss: 0.2550 - val_accuracy: 0.8939 - val_loss: 0.2797 - mcc: 0.8350\n","Epoch 22/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9032 - loss: 0.2582(200, 1500, 5)\n","\n","Epoch 22 - MCC: 0.7193\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.9028 - loss: 0.2596 - val_accuracy: 0.8223 - val_loss: 0.5260 - mcc: 0.7193\n","Epoch 23/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.8481 - loss: 0.4383(200, 1500, 5)\n","\n","Epoch 23 - MCC: 0.8168\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 141ms/step - accuracy: 0.8484 - loss: 0.4372 - val_accuracy: 0.8830 - val_loss: 0.3090 - mcc: 0.8168\n","Epoch 24/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8815 - loss: 0.3216(200, 1500, 5)\n","\n","Epoch 24 - MCC: 0.8164\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 143ms/step - accuracy: 0.8816 - loss: 0.3213 - val_accuracy: 0.8820 - val_loss: 0.3192 - mcc: 0.8164\n","Epoch 25/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8935 - loss: 0.2914(200, 1500, 5)\n","\n","Epoch 25 - MCC: 0.8313\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 123ms/step - accuracy: 0.8934 - loss: 0.2915 - val_accuracy: 0.8916 - val_loss: 0.2825 - mcc: 0.8313\n","Epoch 26/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8968 - loss: 0.2781(200, 1500, 5)\n","\n","Epoch 26 - MCC: 0.8340\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.8968 - loss: 0.2781 - val_accuracy: 0.8939 - val_loss: 0.2813 - mcc: 0.8340\n","Epoch 27/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8964 - loss: 0.2782(200, 1500, 5)\n","\n","Epoch 27 - MCC: 0.8381\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 135ms/step - accuracy: 0.8964 - loss: 0.2782 - val_accuracy: 0.8964 - val_loss: 0.2672 - mcc: 0.8381\n","Epoch 28/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9054 - loss: 0.2533(200, 1500, 5)\n","\n","Epoch 28 - MCC: 0.8364\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 148ms/step - accuracy: 0.9053 - loss: 0.2535 - val_accuracy: 0.8950 - val_loss: 0.2692 - mcc: 0.8364\n","Epoch 29/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8997 - loss: 0.2638(200, 1500, 5)\n","\n","Epoch 29 - MCC: 0.8180\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 124ms/step - accuracy: 0.8997 - loss: 0.2638 - val_accuracy: 0.8810 - val_loss: 0.3032 - mcc: 0.8180\n","Epoch 30/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8978 - loss: 0.2742(200, 1500, 5)\n","\n","Epoch 30 - MCC: 0.8450\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - accuracy: 0.8979 - loss: 0.2740 - val_accuracy: 0.9003 - val_loss: 0.2548 - mcc: 0.8450\n","Epoch 31/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9040 - loss: 0.2551(200, 1500, 5)\n","\n","Epoch 31 - MCC: 0.8485\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.9041 - loss: 0.2550 - val_accuracy: 0.9028 - val_loss: 0.2491 - mcc: 0.8485\n","Epoch 32/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9066 - loss: 0.2502(200, 1500, 5)\n","\n","Epoch 32 - MCC: 0.8500\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 143ms/step - accuracy: 0.9066 - loss: 0.2502 - val_accuracy: 0.9038 - val_loss: 0.2451 - mcc: 0.8500\n","Epoch 33/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9004 - loss: 0.2683(200, 1500, 5)\n","\n","Epoch 33 - MCC: 0.8383\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 149ms/step - accuracy: 0.9004 - loss: 0.2684 - val_accuracy: 0.8962 - val_loss: 0.2701 - mcc: 0.8383\n","Epoch 34/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8980 - loss: 0.2751(200, 1500, 5)\n","\n","Epoch 34 - MCC: 0.8371\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 124ms/step - accuracy: 0.8981 - loss: 0.2749 - val_accuracy: 0.8954 - val_loss: 0.2707 - mcc: 0.8371\n","Epoch 35/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8883 - loss: 0.3019(200, 1500, 5)\n","\n","Epoch 35 - MCC: 0.8403\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 122ms/step - accuracy: 0.8885 - loss: 0.3013 - val_accuracy: 0.8977 - val_loss: 0.2696 - mcc: 0.8403\n","Epoch 36/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8977 - loss: 0.2739 (200, 1500, 5)\n","\n","Epoch 36 - MCC: 0.8400\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 128ms/step - accuracy: 0.8979 - loss: 0.2734 - val_accuracy: 0.8971 - val_loss: 0.2719 - mcc: 0.8400\n","Epoch 37/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9067 - loss: 0.2479(200, 1500, 5)\n","\n","Epoch 37 - MCC: 0.7857\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 148ms/step - accuracy: 0.9065 - loss: 0.2486 - val_accuracy: 0.8630 - val_loss: 0.3977 - mcc: 0.7857\n","Epoch 38/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8760 - loss: 0.3473(200, 1500, 5)\n","\n","Epoch 38 - MCC: 0.8309\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - accuracy: 0.8763 - loss: 0.3464 - val_accuracy: 0.8917 - val_loss: 0.2811 - mcc: 0.8309\n","Epoch 39/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9039 - loss: 0.2585(200, 1500, 5)\n","\n","Epoch 39 - MCC: 0.8399\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 125ms/step - accuracy: 0.9038 - loss: 0.2587 - val_accuracy: 0.8971 - val_loss: 0.2615 - mcc: 0.8399\n","Epoch 40/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8401 - loss: 0.4992(200, 1500, 5)\n","\n","Epoch 40 - MCC: 0.7439\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 124ms/step - accuracy: 0.8396 - loss: 0.5007 - val_accuracy: 0.8335 - val_loss: 0.4705 - mcc: 0.7439\n","Epoch 41/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8551 - loss: 0.4277(200, 1500, 5)\n","\n","Epoch 41 - MCC: 0.8078\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 136ms/step - accuracy: 0.8552 - loss: 0.4272 - val_accuracy: 0.8754 - val_loss: 0.3431 - mcc: 0.8078\n","Epoch 42/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8838 - loss: 0.3321(200, 1500, 5)\n","\n","Epoch 42 - MCC: 0.8175\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 148ms/step - accuracy: 0.8838 - loss: 0.3321 - val_accuracy: 0.8828 - val_loss: 0.3174 - mcc: 0.8175\n","Epoch 43/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8846 - loss: 0.3206(200, 1500, 5)\n","\n","Epoch 43 - MCC: 0.8214\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 124ms/step - accuracy: 0.8846 - loss: 0.3206 - val_accuracy: 0.8856 - val_loss: 0.3213 - mcc: 0.8214\n","Epoch 44/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8830 - loss: 0.3211(200, 1500, 5)\n","\n","Epoch 44 - MCC: 0.8102\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - accuracy: 0.8830 - loss: 0.3210 - val_accuracy: 0.8783 - val_loss: 0.3265 - mcc: 0.8102\n","Epoch 45/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8833 - loss: 0.3253(200, 1500, 5)\n","\n","Epoch 45 - MCC: 0.8265\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.8833 - loss: 0.3251 - val_accuracy: 0.8887 - val_loss: 0.3013 - mcc: 0.8265\n","Epoch 46/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8913 - loss: 0.2968(200, 1500, 5)\n","\n","Epoch 46 - MCC: 0.8300\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 120ms/step - accuracy: 0.8913 - loss: 0.2968 - val_accuracy: 0.8909 - val_loss: 0.2912 - mcc: 0.8300\n","Epoch 47/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8870 - loss: 0.3056(200, 1500, 5)\n","\n","Epoch 47 - MCC: 0.8322\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.8871 - loss: 0.3053 - val_accuracy: 0.8924 - val_loss: 0.2870 - mcc: 0.8322\n","Epoch 48/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8977 - loss: 0.2728(200, 1500, 5)\n","\n","Epoch 48 - MCC: 0.8333\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 144ms/step - accuracy: 0.8976 - loss: 0.2730 - val_accuracy: 0.8933 - val_loss: 0.2833 - mcc: 0.8333\n","Epoch 49/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9011 - loss: 0.2615(200, 1500, 5)\n","\n","Epoch 49 - MCC: 0.8324\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.9010 - loss: 0.2618 - val_accuracy: 0.8926 - val_loss: 0.2868 - mcc: 0.8324\n","Epoch 50/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8918 - loss: 0.2921(200, 1500, 5)\n","\n","Epoch 50 - MCC: 0.8402\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 143ms/step - accuracy: 0.8919 - loss: 0.2918 - val_accuracy: 0.8973 - val_loss: 0.2685 - mcc: 0.8402\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step\n","\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 2\n","Epoch 1/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.4892 - loss: 1.3266(200, 1500, 5)\n","\n","Epoch 1 - MCC: 0.6045\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 157ms/step - accuracy: 0.4915 - loss: 1.3217 - val_accuracy: 0.7542 - val_loss: 0.6656 - mcc: 0.6045\n","Epoch 2/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.7578 - loss: 0.6563(200, 1500, 5)\n","\n","Epoch 2 - MCC: 0.7772\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.7584 - loss: 0.6550 - val_accuracy: 0.8561 - val_loss: 0.4267 - mcc: 0.7772\n","Epoch 3/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8381 - loss: 0.4807(200, 1500, 5)\n","\n","Epoch 3 - MCC: 0.8002\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.8382 - loss: 0.4803 - val_accuracy: 0.8721 - val_loss: 0.3679 - mcc: 0.8002\n","Epoch 4/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8653 - loss: 0.3908(200, 1500, 5)\n","\n","Epoch 4 - MCC: 0.8223\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 144ms/step - accuracy: 0.8653 - loss: 0.3910 - val_accuracy: 0.8858 - val_loss: 0.3233 - mcc: 0.8223\n","Epoch 5/50\n","\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8717 - loss: 0.3641"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-504e8e55c6fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_models_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_data_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_multi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"100epoch_DeepLSTM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-8d9196b0f124>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(models, X, y, k_folds, batch_size, epochs, basePath, dir_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mstart_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 history = model.fit(X_train, y_train, epochs=epochs,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                     verbose=1, callbacks=[mcc_callback])\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-819175ea534e>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Ensure y_val is flattened (shape: (N,))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_to_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":[]},{"cell_type":"code","execution_count":60,"metadata":{"id":"nTQrwqXlnonj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"11e72ed9-b4c3-43d0-af42-7e93fab80e06","collapsed":true,"executionInfo":{"status":"ok","timestamp":1744687003788,"user_tz":-120,"elapsed":8536139,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7345 - loss: 0.7342(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7823\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 28ms/step - accuracy: 0.7347 - loss: 0.7337 - val_accuracy: 0.8574 - val_loss: 0.3807 - mcc: 0.7823\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8545 - loss: 0.3916(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8018\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8545 - loss: 0.3915 - val_accuracy: 0.8709 - val_loss: 0.3484 - mcc: 0.8018\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8716 - loss: 0.3431(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8180\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8716 - loss: 0.3431 - val_accuracy: 0.8810 - val_loss: 0.3107 - mcc: 0.8180\n","Epoch 4/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8815 - loss: 0.3090(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8274\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8816 - loss: 0.3090 - val_accuracy: 0.8873 - val_loss: 0.2903 - mcc: 0.8274\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8878 - loss: 0.2890(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8356\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8878 - loss: 0.2890 - val_accuracy: 0.8923 - val_loss: 0.2725 - mcc: 0.8356\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8620 - loss: 0.3869(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8330\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.8621 - loss: 0.3867 - val_accuracy: 0.8911 - val_loss: 0.2849 - mcc: 0.8330\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8884 - loss: 0.2897(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8396\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8884 - loss: 0.2897 - val_accuracy: 0.8955 - val_loss: 0.2676 - mcc: 0.8396\n","Epoch 8/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8938 - loss: 0.2698(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8441\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8938 - loss: 0.2698 - val_accuracy: 0.8980 - val_loss: 0.2606 - mcc: 0.8441\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8957 - loss: 0.2652(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8477\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8957 - loss: 0.2652 - val_accuracy: 0.9001 - val_loss: 0.2548 - mcc: 0.8477\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8989 - loss: 0.2564(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8455\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8989 - loss: 0.2564 - val_accuracy: 0.8973 - val_loss: 0.2534 - mcc: 0.8455\n","Epoch 11/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8984 - loss: 0.2551(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8527\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8984 - loss: 0.2551 - val_accuracy: 0.9035 - val_loss: 0.2433 - mcc: 0.8527\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9009 - loss: 0.2500(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8516\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9009 - loss: 0.2499 - val_accuracy: 0.9027 - val_loss: 0.2430 - mcc: 0.8516\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9042 - loss: 0.2415(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8529\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9042 - loss: 0.2415 - val_accuracy: 0.9033 - val_loss: 0.2413 - mcc: 0.8529\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9038 - loss: 0.2419(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8560\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9038 - loss: 0.2419 - val_accuracy: 0.9060 - val_loss: 0.2395 - mcc: 0.8560\n","Epoch 15/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9058 - loss: 0.2373(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8561\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9058 - loss: 0.2373 - val_accuracy: 0.9060 - val_loss: 0.2360 - mcc: 0.8561\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9077 - loss: 0.2319(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8600\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9077 - loss: 0.2319 - val_accuracy: 0.9077 - val_loss: 0.2301 - mcc: 0.8600\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9063 - loss: 0.2357(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8584\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9063 - loss: 0.2357 - val_accuracy: 0.9075 - val_loss: 0.2312 - mcc: 0.8584\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9064 - loss: 0.2341(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8625\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.9064 - loss: 0.2341 - val_accuracy: 0.9099 - val_loss: 0.2244 - mcc: 0.8625\n","Epoch 19/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9095 - loss: 0.2257(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8646\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9095 - loss: 0.2257 - val_accuracy: 0.9114 - val_loss: 0.2225 - mcc: 0.8646\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9078 - loss: 0.2306(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8632\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9078 - loss: 0.2306 - val_accuracy: 0.9106 - val_loss: 0.2232 - mcc: 0.8632\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7196 - loss: 0.7531(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7834\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.7198 - loss: 0.7528 - val_accuracy: 0.8590 - val_loss: 0.3755 - mcc: 0.7834\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8594 - loss: 0.3747(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8056\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8594 - loss: 0.3747 - val_accuracy: 0.8730 - val_loss: 0.3390 - mcc: 0.8056\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8722 - loss: 0.3330(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7541\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8722 - loss: 0.3330 - val_accuracy: 0.8407 - val_loss: 0.4334 - mcc: 0.7541\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8729 - loss: 0.3360(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8247\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8730 - loss: 0.3360 - val_accuracy: 0.8857 - val_loss: 0.2959 - mcc: 0.8247\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8819 - loss: 0.3058(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8262\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8819 - loss: 0.3058 - val_accuracy: 0.8873 - val_loss: 0.2960 - mcc: 0.8262\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8865 - loss: 0.2945(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8339\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8865 - loss: 0.2945 - val_accuracy: 0.8919 - val_loss: 0.2780 - mcc: 0.8339\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8905 - loss: 0.2804(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8381\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8905 - loss: 0.2804 - val_accuracy: 0.8948 - val_loss: 0.2755 - mcc: 0.8381\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8932 - loss: 0.2735(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8448\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8932 - loss: 0.2735 - val_accuracy: 0.8987 - val_loss: 0.2589 - mcc: 0.8448\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8940 - loss: 0.2732(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8421\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8940 - loss: 0.2732 - val_accuracy: 0.8973 - val_loss: 0.2655 - mcc: 0.8421\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8979 - loss: 0.2606(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8457\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.8979 - loss: 0.2606 - val_accuracy: 0.8991 - val_loss: 0.2587 - mcc: 0.8457\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8981 - loss: 0.2604(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8480\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8981 - loss: 0.2604 - val_accuracy: 0.9008 - val_loss: 0.2503 - mcc: 0.8480\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9004 - loss: 0.2541(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8501\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9004 - loss: 0.2541 - val_accuracy: 0.9019 - val_loss: 0.2477 - mcc: 0.8501\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9022 - loss: 0.2479(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8529\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9022 - loss: 0.2479 - val_accuracy: 0.9033 - val_loss: 0.2426 - mcc: 0.8529\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9028 - loss: 0.2441(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8559\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9028 - loss: 0.2441 - val_accuracy: 0.9060 - val_loss: 0.2391 - mcc: 0.8559\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9033 - loss: 0.2453(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8542\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9033 - loss: 0.2454 - val_accuracy: 0.9049 - val_loss: 0.2429 - mcc: 0.8542\n","Epoch 16/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9055 - loss: 0.2385(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8508\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9055 - loss: 0.2385 - val_accuracy: 0.9027 - val_loss: 0.2472 - mcc: 0.8508\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9042 - loss: 0.2407(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8600\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9042 - loss: 0.2407 - val_accuracy: 0.9087 - val_loss: 0.2294 - mcc: 0.8600\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9061 - loss: 0.2362(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8598\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9061 - loss: 0.2362 - val_accuracy: 0.9088 - val_loss: 0.2303 - mcc: 0.8598\n","Epoch 19/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9082 - loss: 0.2294(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8616\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9082 - loss: 0.2294 - val_accuracy: 0.9099 - val_loss: 0.2282 - mcc: 0.8616\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9078 - loss: 0.2297(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8600\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.9078 - loss: 0.2297 - val_accuracy: 0.9089 - val_loss: 0.2295 - mcc: 0.8600\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7170 - loss: 0.7694(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7858\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.7172 - loss: 0.7688 - val_accuracy: 0.8609 - val_loss: 0.3762 - mcc: 0.7858\n","Epoch 2/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8657 - loss: 0.3621(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8074\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8657 - loss: 0.3621 - val_accuracy: 0.8735 - val_loss: 0.3342 - mcc: 0.8074\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8714 - loss: 0.3468(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8148\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8714 - loss: 0.3468 - val_accuracy: 0.8796 - val_loss: 0.3188 - mcc: 0.8148\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8792 - loss: 0.3224(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8164\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8792 - loss: 0.3224 - val_accuracy: 0.8797 - val_loss: 0.3222 - mcc: 0.8164\n","Epoch 5/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8823 - loss: 0.3141(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8287\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8823 - loss: 0.3141 - val_accuracy: 0.8872 - val_loss: 0.2929 - mcc: 0.8287\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8877 - loss: 0.2937(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8294\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8877 - loss: 0.2937 - val_accuracy: 0.8889 - val_loss: 0.2870 - mcc: 0.8294\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8913 - loss: 0.2805(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8230\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8913 - loss: 0.2805 - val_accuracy: 0.8827 - val_loss: 0.3095 - mcc: 0.8230\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8955 - loss: 0.2703(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8443\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8955 - loss: 0.2703 - val_accuracy: 0.8977 - val_loss: 0.2587 - mcc: 0.8443\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8976 - loss: 0.2619(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8279\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8976 - loss: 0.2619 - val_accuracy: 0.8881 - val_loss: 0.2893 - mcc: 0.8279\n","Epoch 10/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8967 - loss: 0.2659(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8463\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8967 - loss: 0.2658 - val_accuracy: 0.8996 - val_loss: 0.2545 - mcc: 0.8463\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8989 - loss: 0.2574(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8468\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8989 - loss: 0.2574 - val_accuracy: 0.8994 - val_loss: 0.2557 - mcc: 0.8468\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9022 - loss: 0.2479(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8514\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9022 - loss: 0.2479 - val_accuracy: 0.9028 - val_loss: 0.2442 - mcc: 0.8514\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9043 - loss: 0.2412(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8518\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9043 - loss: 0.2412 - val_accuracy: 0.9033 - val_loss: 0.2435 - mcc: 0.8518\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9052 - loss: 0.2394(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8433\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9052 - loss: 0.2394 - val_accuracy: 0.8978 - val_loss: 0.2588 - mcc: 0.8433\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9045 - loss: 0.2409(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8548\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9045 - loss: 0.2409 - val_accuracy: 0.9052 - val_loss: 0.2368 - mcc: 0.8548\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9062 - loss: 0.2373(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8416\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9062 - loss: 0.2373 - val_accuracy: 0.8967 - val_loss: 0.2739 - mcc: 0.8416\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2371(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8525\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9062 - loss: 0.2371 - val_accuracy: 0.9037 - val_loss: 0.2411 - mcc: 0.8525\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9095 - loss: 0.2285(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8584\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.9095 - loss: 0.2285 - val_accuracy: 0.9071 - val_loss: 0.2312 - mcc: 0.8584\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9088 - loss: 0.2279(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8576\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9088 - loss: 0.2279 - val_accuracy: 0.9070 - val_loss: 0.2320 - mcc: 0.8576\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9089 - loss: 0.2270(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8624\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9089 - loss: 0.2270 - val_accuracy: 0.9098 - val_loss: 0.2232 - mcc: 0.8624\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM, Fold: 4\n","Epoch 1/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7013 - loss: 0.7909(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7867\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.7017 - loss: 0.7899 - val_accuracy: 0.8607 - val_loss: 0.3806 - mcc: 0.7867\n","Epoch 2/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8666 - loss: 0.3614(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8073\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8666 - loss: 0.3614 - val_accuracy: 0.8756 - val_loss: 0.3322 - mcc: 0.8073\n","Epoch 3/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8773 - loss: 0.3283(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8148\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8773 - loss: 0.3283 - val_accuracy: 0.8802 - val_loss: 0.3216 - mcc: 0.8148\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8847 - loss: 0.3066(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8256\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8847 - loss: 0.3066 - val_accuracy: 0.8862 - val_loss: 0.3000 - mcc: 0.8256\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8886 - loss: 0.2919(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8357\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8886 - loss: 0.2919 - val_accuracy: 0.8931 - val_loss: 0.2800 - mcc: 0.8357\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8833 - loss: 0.3095(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8401\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8833 - loss: 0.3094 - val_accuracy: 0.8957 - val_loss: 0.2708 - mcc: 0.8401\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8934 - loss: 0.2749(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8441\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8935 - loss: 0.2748 - val_accuracy: 0.8987 - val_loss: 0.2610 - mcc: 0.8441\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8958 - loss: 0.2668(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8427\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8958 - loss: 0.2668 - val_accuracy: 0.8972 - val_loss: 0.2622 - mcc: 0.8427\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9006 - loss: 0.2518(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8444\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9006 - loss: 0.2518 - val_accuracy: 0.8985 - val_loss: 0.2617 - mcc: 0.8444\n","Epoch 10/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9025 - loss: 0.2488(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8494\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9025 - loss: 0.2488 - val_accuracy: 0.9019 - val_loss: 0.2520 - mcc: 0.8494\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9041 - loss: 0.2426(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8538\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9041 - loss: 0.2426 - val_accuracy: 0.9050 - val_loss: 0.2410 - mcc: 0.8538\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9047 - loss: 0.2405(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8555\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.9047 - loss: 0.2405 - val_accuracy: 0.9057 - val_loss: 0.2385 - mcc: 0.8555\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9064 - loss: 0.2353(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8576\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9064 - loss: 0.2353 - val_accuracy: 0.9071 - val_loss: 0.2344 - mcc: 0.8576\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9069 - loss: 0.2339(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8570\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9069 - loss: 0.2339 - val_accuracy: 0.9068 - val_loss: 0.2360 - mcc: 0.8570\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9062 - loss: 0.2343(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8569\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9062 - loss: 0.2343 - val_accuracy: 0.9070 - val_loss: 0.2386 - mcc: 0.8569\n","Epoch 16/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9086 - loss: 0.2291(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8584\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9086 - loss: 0.2291 - val_accuracy: 0.9076 - val_loss: 0.2345 - mcc: 0.8584\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9088 - loss: 0.2288(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8608\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9088 - loss: 0.2288 - val_accuracy: 0.9093 - val_loss: 0.2285 - mcc: 0.8608\n","Epoch 18/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9089 - loss: 0.2269(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8595\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9089 - loss: 0.2268 - val_accuracy: 0.9087 - val_loss: 0.2312 - mcc: 0.8595\n","Epoch 19/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9117 - loss: 0.2195(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8625\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9117 - loss: 0.2195 - val_accuracy: 0.9104 - val_loss: 0.2287 - mcc: 0.8625\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9112 - loss: 0.2208(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8644\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9112 - loss: 0.2208 - val_accuracy: 0.9116 - val_loss: 0.2228 - mcc: 0.8644\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM, Fold: 5\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7292 - loss: 0.7446(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7650\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 94ms/step - accuracy: 0.7294 - loss: 0.7441 - val_accuracy: 0.8432 - val_loss: 0.4250 - mcc: 0.7650\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8577 - loss: 0.3827(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7965\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8577 - loss: 0.3827 - val_accuracy: 0.8671 - val_loss: 0.3496 - mcc: 0.7965\n","Epoch 3/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8710 - loss: 0.3423(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8031\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8710 - loss: 0.3423 - val_accuracy: 0.8710 - val_loss: 0.3343 - mcc: 0.8031\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8802 - loss: 0.3162(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7555\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.8802 - loss: 0.3162 - val_accuracy: 0.8421 - val_loss: 0.4397 - mcc: 0.7555\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8757 - loss: 0.3375(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8160\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8757 - loss: 0.3374 - val_accuracy: 0.8803 - val_loss: 0.3144 - mcc: 0.8160\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8875 - loss: 0.2944(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8248\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8875 - loss: 0.2944 - val_accuracy: 0.8857 - val_loss: 0.2923 - mcc: 0.8248\n","Epoch 7/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8877 - loss: 0.2912(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8281\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8877 - loss: 0.2912 - val_accuracy: 0.8881 - val_loss: 0.2847 - mcc: 0.8281\n","Epoch 8/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8941 - loss: 0.2716(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8348\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8941 - loss: 0.2716 - val_accuracy: 0.8923 - val_loss: 0.2760 - mcc: 0.8348\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8917 - loss: 0.2783(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8389\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8918 - loss: 0.2783 - val_accuracy: 0.8940 - val_loss: 0.2668 - mcc: 0.8389\n","Epoch 10/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8977 - loss: 0.2609(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8397\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8977 - loss: 0.2609 - val_accuracy: 0.8949 - val_loss: 0.2649 - mcc: 0.8397\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9005 - loss: 0.2518(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8428\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9005 - loss: 0.2518 - val_accuracy: 0.8971 - val_loss: 0.2589 - mcc: 0.8428\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8992 - loss: 0.2560(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8447\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8992 - loss: 0.2560 - val_accuracy: 0.8988 - val_loss: 0.2561 - mcc: 0.8447\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9036 - loss: 0.2442(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8456\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9036 - loss: 0.2442 - val_accuracy: 0.8993 - val_loss: 0.2541 - mcc: 0.8456\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9036 - loss: 0.2445(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8491\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9036 - loss: 0.2445 - val_accuracy: 0.9012 - val_loss: 0.2467 - mcc: 0.8491\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9051 - loss: 0.2390(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8364\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9051 - loss: 0.2390 - val_accuracy: 0.8932 - val_loss: 0.2724 - mcc: 0.8364\n","Epoch 16/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9056 - loss: 0.2391(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8541\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.9056 - loss: 0.2391 - val_accuracy: 0.9044 - val_loss: 0.2380 - mcc: 0.8541\n","Epoch 17/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9074 - loss: 0.2327(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8558\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.9074 - loss: 0.2327 - val_accuracy: 0.9058 - val_loss: 0.2351 - mcc: 0.8558\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9082 - loss: 0.2300(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8528\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9082 - loss: 0.2300 - val_accuracy: 0.9037 - val_loss: 0.2400 - mcc: 0.8528\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9096 - loss: 0.2265(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8556\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9096 - loss: 0.2265 - val_accuracy: 0.9053 - val_loss: 0.2328 - mcc: 0.8556\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9101 - loss: 0.2251(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8574\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.9101 - loss: 0.2251 - val_accuracy: 0.9067 - val_loss: 0.2317 - mcc: 0.8574\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'Accuracy': {'max': np.float64(0.9115606666666667),\n","              'mean': np.float64(0.9094962666666666),\n","              'min': np.float64(0.9066606666666667),\n","              'std': np.float64(0.0016706934608120057)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.000545881191889445),\n","                               'mean': np.float64(0.00045072229703267414),\n","                               'min': np.float64(0.0003652470111846924),\n","                               'std': np.float64(7.717516264741174e-05)},\n"," 'MCC': {'max': np.float64(0.8643566245816173),\n","         'mean': np.float64(0.8614838285790757),\n","         'min': np.float64(0.8574438733614156),\n","         'std': np.float64(0.002474916895642593)},\n"," 'Parameters': 7589,\n"," 'Train Time (s)': {'max': np.float64(416.66885685920715),\n","                    'mean': np.float64(373.2593544006348),\n","                    'min': np.float64(344.21003103256226),\n","                    'std': np.float64(25.507431027521996)},\n"," 'Training Accuracy': [[0.8083259463310242,\n","                        0.8569443225860596,\n","                        0.8741322159767151,\n","                        0.882564127445221,\n","                        0.888708233833313,\n","                        0.8742624521255493,\n","                        0.8903961777687073,\n","                        0.8944764137268066,\n","                        0.8969225883483887,\n","                        0.8992334604263306,\n","                        0.900164783000946,\n","                        0.9017797708511353,\n","                        0.9029394388198853,\n","                        0.904264509677887,\n","                        0.904920220375061,\n","                        0.9062756896018982,\n","                        0.9064161777496338,\n","                        0.9079926609992981,\n","                        0.9087862968444824,\n","                        0.9097862839698792],\n","                       [0.8031251430511475,\n","                        0.8623220324516296,\n","                        0.8727160692214966,\n","                        0.8784072399139404,\n","                        0.8835808634757996,\n","                        0.8881252408027649,\n","                        0.8906081318855286,\n","                        0.894316554069519,\n","                        0.8943802714347839,\n","                        0.8956266045570374,\n","                        0.8994238376617432,\n","                        0.8999286890029907,\n","                        0.9013627171516418,\n","                        0.9033396244049072,\n","                        0.9006033539772034,\n","                        0.9053289294242859,\n","                        0.9052255153656006,\n","                        0.906929612159729,\n","                        0.9080333113670349,\n","                        0.908724844455719],\n","                       [0.8048464059829712,\n","                        0.8685166239738464,\n","                        0.8726837038993835,\n","                        0.8794434666633606,\n","                        0.8852015733718872,\n","                        0.8895885348320007,\n","                        0.8929247260093689,\n","                        0.8955656886100769,\n","                        0.8957310318946838,\n","                        0.8980337977409363,\n","                        0.8983882665634155,\n","                        0.9018205404281616,\n","                        0.9032674431800842,\n","                        0.9044498801231384,\n","                        0.9060570597648621,\n","                        0.9068455696105957,\n","                        0.9035615921020508,\n","                        0.9079384207725525,\n","                        0.9090484976768494,\n","                        0.9097568392753601],\n","                       [0.8004466891288757,\n","                        0.8707870841026306,\n","                        0.8782426714897156,\n","                        0.8855023384094238,\n","                        0.8904317021369934,\n","                        0.8901679515838623,\n","                        0.8953781127929688,\n","                        0.8966841101646423,\n","                        0.9001089930534363,\n","                        0.9011570811271667,\n","                        0.9034767150878906,\n","                        0.9047726392745972,\n","                        0.9057040214538574,\n","                        0.906978189945221,\n","                        0.9079368710517883,\n","                        0.9085665941238403,\n","                        0.9095920324325562,\n","                        0.9105284810066223,\n","                        0.9107562899589539,\n","                        0.9112043380737305],\n","                       [0.8003087043762207,\n","                        0.8616729378700256,\n","                        0.872890055179596,\n","                        0.8768792152404785,\n","                        0.8808377385139465,\n","                        0.8874995708465576,\n","                        0.8898698091506958,\n","                        0.8942328095436096,\n","                        0.895356297492981,\n","                        0.899010419845581,\n","                        0.9000501036643982,\n","                        0.9023628830909729,\n","                        0.9028008580207825,\n","                        0.9044541120529175,\n","                        0.9039143919944763,\n","                        0.9067990779876709,\n","                        0.9077522158622742,\n","                        0.9088640213012695,\n","                        0.9094743728637695,\n","                        0.9099176526069641]],\n"," 'Training Loss': [[0.5269577503204346,\n","                    0.38686302304267883,\n","                    0.33285897970199585,\n","                    0.3052554130554199,\n","                    0.28587281703948975,\n","                    0.34090664982795715,\n","                    0.283141165971756,\n","                    0.26905184984207153,\n","                    0.26119592785835266,\n","                    0.2541534900665283,\n","                    0.25195327401161194,\n","                    0.2469194531440735,\n","                    0.24401892721652985,\n","                    0.2409433126449585,\n","                    0.23926673829555511,\n","                    0.23513002693653107,\n","                    0.23512400686740875,\n","                    0.230418860912323,\n","                    0.2284383326768875,\n","                    0.2258625626564026],\n","                   [0.5386708378791809,\n","                    0.3660763204097748,\n","                    0.3364142179489136,\n","                    0.31869956851005554,\n","                    0.30137160420417786,\n","                    0.2884444296360016,\n","                    0.2810991406440735,\n","                    0.27038589119911194,\n","                    0.2726837694644928,\n","                    0.26800501346588135,\n","                    0.25604844093322754,\n","                    0.25432881712913513,\n","                    0.24998223781585693,\n","                    0.24374833703041077,\n","                    0.2542443573474884,\n","                    0.23811233043670654,\n","                    0.2391577810049057,\n","                    0.23373401165008545,\n","                    0.23025882244110107,\n","                    0.22808408737182617],\n","                   [0.5369949340820312,\n","                    0.353083997964859,\n","                    0.34422802925109863,\n","                    0.3219708800315857,\n","                    0.30550235509872437,\n","                    0.28777286410331726,\n","                    0.27610987424850464,\n","                    0.26834356784820557,\n","                    0.2681906819343567,\n","                    0.26168903708457947,\n","                    0.2622755765914917,\n","                    0.24927592277526855,\n","                    0.24424798786640167,\n","                    0.24177633225917816,\n","                    0.23699568212032318,\n","                    0.23449215292930603,\n","                    0.24604080617427826,\n","                    0.23209930956363678,\n","                    0.2279917448759079,\n","                    0.2258576899766922],\n","                   [0.5451118350028992,\n","                    0.3491095006465912,\n","                    0.3279488980770111,\n","                    0.3037775456905365,\n","                    0.28698113560676575,\n","                    0.28777262568473816,\n","                    0.27029892802238464,\n","                    0.2657318413257599,\n","                    0.2540164589881897,\n","                    0.2519492506980896,\n","                    0.2435435950756073,\n","                    0.2402808666229248,\n","                    0.23713243007659912,\n","                    0.23332539200782776,\n","                    0.23057782649993896,\n","                    0.22876568138599396,\n","                    0.22620408236980438,\n","                    0.22318434715270996,\n","                    0.22162272036075592,\n","                    0.22106775641441345],\n","                   [0.5444414615631104,\n","                    0.3696885108947754,\n","                    0.3389783203601837,\n","                    0.32563507556915283,\n","                    0.3185802102088928,\n","                    0.2917659878730774,\n","                    0.28420940041542053,\n","                    0.27063992619514465,\n","                    0.267720103263855,\n","                    0.25700804591178894,\n","                    0.2535616159439087,\n","                    0.24740903079509735,\n","                    0.24643272161483765,\n","                    0.241667702794075,\n","                    0.24359330534934998,\n","                    0.23530903458595276,\n","                    0.23201502859592438,\n","                    0.2282455861568451,\n","                    0.22694291174411774,\n","                    0.22588035464286804]],\n"," 'Validation Accuracy': [[0.8573688864707947,\n","                          0.8708892464637756,\n","                          0.8810072541236877,\n","                          0.8873466849327087,\n","                          0.8922954797744751,\n","                          0.8910527229309082,\n","                          0.8954726457595825,\n","                          0.8979924321174622,\n","                          0.9000745415687561,\n","                          0.8973375558853149,\n","                          0.903461754322052,\n","                          0.9027266502380371,\n","                          0.9032886028289795,\n","                          0.9059879779815674,\n","                          0.9060064554214478,\n","                          0.9076676368713379,\n","                          0.9074611067771912,\n","                          0.9098619818687439,\n","                          0.9113745093345642,\n","                          0.9106000065803528],\n","                         [0.8590338826179504,\n","                          0.8729666471481323,\n","                          0.8406586647033691,\n","                          0.8857426047325134,\n","                          0.8872871994972229,\n","                          0.8919101357460022,\n","                          0.8947746753692627,\n","                          0.8987096548080444,\n","                          0.8972834348678589,\n","                          0.8990818858146667,\n","                          0.9008492827415466,\n","                          0.9018782377243042,\n","                          0.9033399820327759,\n","                          0.9060086011886597,\n","                          0.9049241542816162,\n","                          0.9026934504508972,\n","                          0.9086626172065735,\n","                          0.9088425040245056,\n","                          0.9099075794219971,\n","                          0.9088939428329468],\n","                         [0.8608550429344177,\n","                          0.8734819293022156,\n","                          0.8796136975288391,\n","                          0.8797064423561096,\n","                          0.8871751427650452,\n","                          0.8889258503913879,\n","                          0.8826948404312134,\n","                          0.8977254033088684,\n","                          0.8880630135536194,\n","                          0.8995800018310547,\n","                          0.8994240760803223,\n","                          0.9027563333511353,\n","                          0.9033327102661133,\n","                          0.897849440574646,\n","                          0.9052347540855408,\n","                          0.8967325091362,\n","                          0.9036533236503601,\n","                          0.9070976376533508,\n","                          0.9070335030555725,\n","                          0.9097659587860107],\n","                         [0.8606857657432556,\n","                          0.8756259083747864,\n","                          0.8802018761634827,\n","                          0.8862197995185852,\n","                          0.8931012153625488,\n","                          0.8956788778305054,\n","                          0.8986535668373108,\n","                          0.8972353339195251,\n","                          0.8985413312911987,\n","                          0.9019305109977722,\n","                          0.9049537777900696,\n","                          0.9057387709617615,\n","                          0.9070614576339722,\n","                          0.9068235158920288,\n","                          0.9069513082504272,\n","                          0.9076459407806396,\n","                          0.9092673063278198,\n","                          0.9087344408035278,\n","                          0.9103703498840332,\n","                          0.9115605354309082],\n","                         [0.8432233333587646,\n","                          0.867116391658783,\n","                          0.8709881901741028,\n","                          0.8421332240104675,\n","                          0.8803321123123169,\n","                          0.885684072971344,\n","                          0.8881465792655945,\n","                          0.8923346400260925,\n","                          0.893953263759613,\n","                          0.8948646783828735,\n","                          0.8970551490783691,\n","                          0.8987907767295837,\n","                          0.8993345499038696,\n","                          0.9011580944061279,\n","                          0.8931853175163269,\n","                          0.9044250249862671,\n","                          0.9058495163917542,\n","                          0.9036965370178223,\n","                          0.9053328633308411,\n","                          0.9066603779792786]],\n"," 'Validation Loss': [[0.38070157170295715,\n","                      0.3483860194683075,\n","                      0.31069615483283997,\n","                      0.29032599925994873,\n","                      0.2725391387939453,\n","                      0.28486368060112,\n","                      0.2675810158252716,\n","                      0.26056140661239624,\n","                      0.2548469603061676,\n","                      0.2533937394618988,\n","                      0.24329370260238647,\n","                      0.2430010437965393,\n","                      0.24133597314357758,\n","                      0.23950140178203583,\n","                      0.2360043078660965,\n","                      0.23007775843143463,\n","                      0.23118147253990173,\n","                      0.22440949082374573,\n","                      0.2224961817264557,\n","                      0.22320015728473663],\n","                     [0.37549811601638794,\n","                      0.33904847502708435,\n","                      0.4334263205528259,\n","                      0.29587674140930176,\n","                      0.2960485816001892,\n","                      0.2780018448829651,\n","                      0.2755413055419922,\n","                      0.2588907778263092,\n","                      0.26546740531921387,\n","                      0.2586635649204254,\n","                      0.2503132224082947,\n","                      0.2477307915687561,\n","                      0.24260534346103668,\n","                      0.23910386860370636,\n","                      0.24288804829120636,\n","                      0.24724017083644867,\n","                      0.22936512529850006,\n","                      0.23027758300304413,\n","                      0.22820812463760376,\n","                      0.22946874797344208],\n","                     [0.3762127161026001,\n","                      0.3342004418373108,\n","                      0.3187730610370636,\n","                      0.32224494218826294,\n","                      0.29288893938064575,\n","                      0.2869788706302643,\n","                      0.30948910117149353,\n","                      0.2587277889251709,\n","                      0.28930822014808655,\n","                      0.2545064389705658,\n","                      0.25570476055145264,\n","                      0.24422281980514526,\n","                      0.24352148175239563,\n","                      0.2587762773036957,\n","                      0.23677775263786316,\n","                      0.27392229437828064,\n","                      0.2411341816186905,\n","                      0.23120428621768951,\n","                      0.231988325715065,\n","                      0.22317548096179962],\n","                     [0.3806283473968506,\n","                      0.3322417140007019,\n","                      0.3215555250644684,\n","                      0.29998132586479187,\n","                      0.2800137400627136,\n","                      0.2708301246166229,\n","                      0.26098668575286865,\n","                      0.26215678453445435,\n","                      0.2616903483867645,\n","                      0.25197380781173706,\n","                      0.2410016655921936,\n","                      0.23850293457508087,\n","                      0.23436091840267181,\n","                      0.2360115498304367,\n","                      0.23856794834136963,\n","                      0.23454351723194122,\n","                      0.22848917543888092,\n","                      0.23118473589420319,\n","                      0.22868363559246063,\n","                      0.22282636165618896],\n","                     [0.4249781370162964,\n","                      0.3495708405971527,\n","                      0.33430227637290955,\n","                      0.4396623969078064,\n","                      0.31437307596206665,\n","                      0.29228338599205017,\n","                      0.2846737504005432,\n","                      0.27597159147262573,\n","                      0.2667730450630188,\n","                      0.2649412155151367,\n","                      0.2588830292224884,\n","                      0.2561379373073578,\n","                      0.25408998131752014,\n","                      0.2466968148946762,\n","                      0.27236461639404297,\n","                      0.2380101978778839,\n","                      0.2351464033126831,\n","                      0.23997527360916138,\n","                      0.23277467489242554,\n","                      0.2316855639219284]],\n"," 'Validation MCC': [[np.float64(0.7822708352757888),\n","                     np.float64(0.8018259077805152),\n","                     np.float64(0.8179936050404281),\n","                     np.float64(0.8274175667665775),\n","                     np.float64(0.8355971794494897),\n","                     np.float64(0.8330343750519297),\n","                     np.float64(0.8395788074150055),\n","                     np.float64(0.8440926139162932),\n","                     np.float64(0.8476536597573509),\n","                     np.float64(0.8454866119192385),\n","                     np.float64(0.8526804141701505),\n","                     np.float64(0.8516379858188601),\n","                     np.float64(0.8529060701790749),\n","                     np.float64(0.8559654214463942),\n","                     np.float64(0.8561181889768285),\n","                     np.float64(0.8599653494603556),\n","                     np.float64(0.8583640996570232),\n","                     np.float64(0.8625313100642236),\n","                     np.float64(0.8646125694586219),\n","                     np.float64(0.8632139330872031)],\n","                    [np.float64(0.7834329213132303),\n","                     np.float64(0.8056220384971992),\n","                     np.float64(0.7540590218755124),\n","                     np.float64(0.8247499633491384),\n","                     np.float64(0.8262275238132512),\n","                     np.float64(0.8338914615542359),\n","                     np.float64(0.8380590621805885),\n","                     np.float64(0.8447559883919757),\n","                     np.float64(0.8421083831617302),\n","                     np.float64(0.8457325619188552),\n","                     np.float64(0.8479877627304219),\n","                     np.float64(0.8501280874658138),\n","                     np.float64(0.8529214044371084),\n","                     np.float64(0.8559070110206288),\n","                     np.float64(0.8541742667177029),\n","                     np.float64(0.8508063718439898),\n","                     np.float64(0.8600102081197367),\n","                     np.float64(0.8597944976887985),\n","                     np.float64(0.8616451216339908),\n","                     np.float64(0.8599981637015994)],\n","                    [np.float64(0.7857873695811408),\n","                     np.float64(0.807391824358694),\n","                     np.float64(0.814808469598708),\n","                     np.float64(0.8164423815485097),\n","                     np.float64(0.8287402046624114),\n","                     np.float64(0.8294213958549359),\n","                     np.float64(0.8229881265309392),\n","                     np.float64(0.8442745322842812),\n","                     np.float64(0.8278974712857177),\n","                     np.float64(0.8463139985975302),\n","                     np.float64(0.8468006530579947),\n","                     np.float64(0.8514098964539238),\n","                     np.float64(0.8518409054826271),\n","                     np.float64(0.843268207998181),\n","                     np.float64(0.8548404536695577),\n","                     np.float64(0.8415684074877778),\n","                     np.float64(0.852512888161915),\n","                     np.float64(0.8584125311537923),\n","                     np.float64(0.8576009856746514),\n","                     np.float64(0.8624065481635423)],\n","                    [np.float64(0.7866819630150212),\n","                     np.float64(0.8073468487476883),\n","                     np.float64(0.8147878772734242),\n","                     np.float64(0.8256410178324746),\n","                     np.float64(0.835698275920067),\n","                     np.float64(0.8401372693736062),\n","                     np.float64(0.8440551792484335),\n","                     np.float64(0.8427260634238006),\n","                     np.float64(0.8444128333704283),\n","                     np.float64(0.8493639628708493),\n","                     np.float64(0.8538018385088681),\n","                     np.float64(0.8555119403162182),\n","                     np.float64(0.8576271199194521),\n","                     np.float64(0.8569566760463815),\n","                     np.float64(0.8568587078243415),\n","                     np.float64(0.858353311501802),\n","                     np.float64(0.8608100963957338),\n","                     np.float64(0.8595104332574675),\n","                     np.float64(0.8625223958418425),\n","                     np.float64(0.8643566245816173)],\n","                    [np.float64(0.7649646253629343),\n","                     np.float64(0.796547737880064),\n","                     np.float64(0.8031206529918021),\n","                     np.float64(0.7554571889205739),\n","                     np.float64(0.8160043434720464),\n","                     np.float64(0.8247930075924783),\n","                     np.float64(0.8280688360491162),\n","                     np.float64(0.8348190261913621),\n","                     np.float64(0.8389259926808161),\n","                     np.float64(0.8397173995475438),\n","                     np.float64(0.8427864674113532),\n","                     np.float64(0.8447082872887957),\n","                     np.float64(0.8456212958466854),\n","                     np.float64(0.8491384175995007),\n","                     np.float64(0.8364184044622416),\n","                     np.float64(0.8541196618944307),\n","                     np.float64(0.8558199547743502),\n","                     np.float64(0.8527957254965952),\n","                     np.float64(0.8556096105839341),\n","                     np.float64(0.8574438733614156)]]}\n","Training Model: LSTM_Dense, Fold: 1\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7218 - loss: 0.7544(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7922\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.7220 - loss: 0.7541 - val_accuracy: 0.8643 - val_loss: 0.3613 - mcc: 0.7922\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8660 - loss: 0.3573(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8107\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8660 - loss: 0.3573 - val_accuracy: 0.8768 - val_loss: 0.3252 - mcc: 0.8107\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8760 - loss: 0.3274(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8254\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8760 - loss: 0.3274 - val_accuracy: 0.8864 - val_loss: 0.2977 - mcc: 0.8254\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8831 - loss: 0.3075(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8296\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8831 - loss: 0.3075 - val_accuracy: 0.8893 - val_loss: 0.2960 - mcc: 0.8296\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8855 - loss: 0.3037(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8297\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.8855 - loss: 0.3037 - val_accuracy: 0.8887 - val_loss: 0.2924 - mcc: 0.8297\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8903 - loss: 0.2903(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8386\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.8903 - loss: 0.2903 - val_accuracy: 0.8944 - val_loss: 0.2756 - mcc: 0.8386\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8911 - loss: 0.2857(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8415\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.8911 - loss: 0.2857 - val_accuracy: 0.8966 - val_loss: 0.2711 - mcc: 0.8415\n","Epoch 8/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8942 - loss: 0.2761(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8447\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8942 - loss: 0.2762 - val_accuracy: 0.8988 - val_loss: 0.2620 - mcc: 0.8447\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8947 - loss: 0.2724(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8465\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 23ms/step - accuracy: 0.8947 - loss: 0.2724 - val_accuracy: 0.8996 - val_loss: 0.2585 - mcc: 0.8465\n","Epoch 10/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8983 - loss: 0.2622(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8532\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.8983 - loss: 0.2622 - val_accuracy: 0.9035 - val_loss: 0.2454 - mcc: 0.8532\n","Epoch 11/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9008 - loss: 0.2555(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8557\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.9008 - loss: 0.2555 - val_accuracy: 0.9056 - val_loss: 0.2415 - mcc: 0.8557\n","Epoch 12/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9001 - loss: 0.2542(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8583\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.9002 - loss: 0.2542 - val_accuracy: 0.9074 - val_loss: 0.2358 - mcc: 0.8583\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9023 - loss: 0.2492(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8558\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9023 - loss: 0.2491 - val_accuracy: 0.9057 - val_loss: 0.2388 - mcc: 0.8558\n","Epoch 14/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9050 - loss: 0.2389(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8566\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9050 - loss: 0.2389 - val_accuracy: 0.9056 - val_loss: 0.2371 - mcc: 0.8566\n","Epoch 15/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9040 - loss: 0.2407(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8617\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9040 - loss: 0.2406 - val_accuracy: 0.9097 - val_loss: 0.2276 - mcc: 0.8617\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9054 - loss: 0.2378(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8627\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9054 - loss: 0.2378 - val_accuracy: 0.9098 - val_loss: 0.2250 - mcc: 0.8627\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9094 - loss: 0.2263(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8654\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.9094 - loss: 0.2263 - val_accuracy: 0.9121 - val_loss: 0.2196 - mcc: 0.8654\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9092 - loss: 0.2247(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8571\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9092 - loss: 0.2247 - val_accuracy: 0.9066 - val_loss: 0.2387 - mcc: 0.8571\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9116 - loss: 0.2208(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8658\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9116 - loss: 0.2208 - val_accuracy: 0.9122 - val_loss: 0.2195 - mcc: 0.8658\n","Epoch 20/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9087 - loss: 0.2294(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8698\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.9087 - loss: 0.2294 - val_accuracy: 0.9148 - val_loss: 0.2154 - mcc: 0.8698\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM_Dense, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7182 - loss: 0.7617(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7998\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.7183 - loss: 0.7614 - val_accuracy: 0.8702 - val_loss: 0.3515 - mcc: 0.7998\n","Epoch 2/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8677 - loss: 0.3559(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8138\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8677 - loss: 0.3559 - val_accuracy: 0.8790 - val_loss: 0.3207 - mcc: 0.8138\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8784 - loss: 0.3205(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8205\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8784 - loss: 0.3205 - val_accuracy: 0.8837 - val_loss: 0.3041 - mcc: 0.8205\n","Epoch 4/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8840 - loss: 0.3042(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8243\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8840 - loss: 0.3042 - val_accuracy: 0.8860 - val_loss: 0.3011 - mcc: 0.8243\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8897 - loss: 0.2873(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8352\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8897 - loss: 0.2873 - val_accuracy: 0.8915 - val_loss: 0.2776 - mcc: 0.8352\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8927 - loss: 0.2777(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8409\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8927 - loss: 0.2777 - val_accuracy: 0.8968 - val_loss: 0.2694 - mcc: 0.8409\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8949 - loss: 0.2718(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8479\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.8949 - loss: 0.2718 - val_accuracy: 0.9008 - val_loss: 0.2573 - mcc: 0.8479\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8956 - loss: 0.2666(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8440\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8956 - loss: 0.2666 - val_accuracy: 0.8987 - val_loss: 0.2629 - mcc: 0.8440\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8978 - loss: 0.2624(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8463\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8978 - loss: 0.2623 - val_accuracy: 0.8993 - val_loss: 0.2587 - mcc: 0.8463\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9016 - loss: 0.2482(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8546\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9016 - loss: 0.2482 - val_accuracy: 0.9053 - val_loss: 0.2435 - mcc: 0.8546\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9034 - loss: 0.2448(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8476\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9034 - loss: 0.2448 - val_accuracy: 0.9007 - val_loss: 0.2573 - mcc: 0.8476\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9055 - loss: 0.2390(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8593\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9055 - loss: 0.2390 - val_accuracy: 0.9084 - val_loss: 0.2325 - mcc: 0.8593\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9028 - loss: 0.2451(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8547\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9027 - loss: 0.2451 - val_accuracy: 0.9056 - val_loss: 0.2425 - mcc: 0.8547\n","Epoch 14/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9041 - loss: 0.2424(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8572\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9041 - loss: 0.2424 - val_accuracy: 0.9067 - val_loss: 0.2360 - mcc: 0.8572\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9073 - loss: 0.2334(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8547\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9073 - loss: 0.2334 - val_accuracy: 0.9038 - val_loss: 0.2441 - mcc: 0.8547\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9083 - loss: 0.2303(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8624\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.9083 - loss: 0.2303 - val_accuracy: 0.9105 - val_loss: 0.2274 - mcc: 0.8624\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9084 - loss: 0.2282(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8602\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9084 - loss: 0.2282 - val_accuracy: 0.9089 - val_loss: 0.2335 - mcc: 0.8602\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9099 - loss: 0.2252(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8642\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9099 - loss: 0.2252 - val_accuracy: 0.9117 - val_loss: 0.2239 - mcc: 0.8642\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9103 - loss: 0.2236(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8566\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.9103 - loss: 0.2236 - val_accuracy: 0.9061 - val_loss: 0.2391 - mcc: 0.8566\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9114 - loss: 0.2205(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8662\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9114 - loss: 0.2205 - val_accuracy: 0.9129 - val_loss: 0.2196 - mcc: 0.8662\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM_Dense, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7097 - loss: 0.7777(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7952\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.7099 - loss: 0.7771 - val_accuracy: 0.8659 - val_loss: 0.3605 - mcc: 0.7952\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8671 - loss: 0.3545(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8104\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8671 - loss: 0.3544 - val_accuracy: 0.8763 - val_loss: 0.3267 - mcc: 0.8104\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8779 - loss: 0.3213(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8170\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.8779 - loss: 0.3213 - val_accuracy: 0.8804 - val_loss: 0.3111 - mcc: 0.8170\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8835 - loss: 0.3030(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8247\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8835 - loss: 0.3030 - val_accuracy: 0.8860 - val_loss: 0.2921 - mcc: 0.8247\n","Epoch 5/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8895 - loss: 0.2835(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8347\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.8895 - loss: 0.2835 - val_accuracy: 0.8910 - val_loss: 0.2755 - mcc: 0.8347\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8948 - loss: 0.2676(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8430\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8948 - loss: 0.2677 - val_accuracy: 0.8974 - val_loss: 0.2597 - mcc: 0.8430\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8976 - loss: 0.2611(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8406\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8976 - loss: 0.2611 - val_accuracy: 0.8961 - val_loss: 0.2642 - mcc: 0.8406\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8997 - loss: 0.2548(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8478\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8997 - loss: 0.2548 - val_accuracy: 0.9003 - val_loss: 0.2496 - mcc: 0.8478\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8987 - loss: 0.2545(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8458\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8987 - loss: 0.2545 - val_accuracy: 0.8988 - val_loss: 0.2519 - mcc: 0.8458\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9030 - loss: 0.2431(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8473\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9030 - loss: 0.2431 - val_accuracy: 0.9004 - val_loss: 0.2500 - mcc: 0.8473\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9046 - loss: 0.2391(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8566\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9046 - loss: 0.2391 - val_accuracy: 0.9060 - val_loss: 0.2336 - mcc: 0.8566\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9069 - loss: 0.2323(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8508\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9069 - loss: 0.2323 - val_accuracy: 0.9021 - val_loss: 0.2409 - mcc: 0.8508\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9061 - loss: 0.2349(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8581\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.9061 - loss: 0.2349 - val_accuracy: 0.9070 - val_loss: 0.2301 - mcc: 0.8581\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9094 - loss: 0.2267(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8217\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9094 - loss: 0.2267 - val_accuracy: 0.8838 - val_loss: 0.3263 - mcc: 0.8217\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9075 - loss: 0.2319(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8578\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9075 - loss: 0.2319 - val_accuracy: 0.9063 - val_loss: 0.2325 - mcc: 0.8578\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9089 - loss: 0.2280(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8605\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9089 - loss: 0.2280 - val_accuracy: 0.9088 - val_loss: 0.2275 - mcc: 0.8605\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9076 - loss: 0.2297(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8613\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9076 - loss: 0.2297 - val_accuracy: 0.9088 - val_loss: 0.2256 - mcc: 0.8613\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9108 - loss: 0.2217(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8620\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9108 - loss: 0.2217 - val_accuracy: 0.9094 - val_loss: 0.2246 - mcc: 0.8620\n","Epoch 19/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9113 - loss: 0.2208(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8646\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9113 - loss: 0.2208 - val_accuracy: 0.9113 - val_loss: 0.2202 - mcc: 0.8646\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9129 - loss: 0.2163(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8585\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9129 - loss: 0.2163 - val_accuracy: 0.9076 - val_loss: 0.2303 - mcc: 0.8585\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM_Dense, Fold: 4\n","Epoch 1/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7155 - loss: 0.7828(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7774\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 21ms/step - accuracy: 0.7160 - loss: 0.7815 - val_accuracy: 0.8568 - val_loss: 0.3816 - mcc: 0.7774\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8600 - loss: 0.3737(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8038\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.8601 - loss: 0.3737 - val_accuracy: 0.8729 - val_loss: 0.3364 - mcc: 0.8038\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8713 - loss: 0.3389(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8118\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.8713 - loss: 0.3389 - val_accuracy: 0.8778 - val_loss: 0.3230 - mcc: 0.8118\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8754 - loss: 0.3271(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8212\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.8754 - loss: 0.3271 - val_accuracy: 0.8830 - val_loss: 0.3043 - mcc: 0.8212\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8861 - loss: 0.2964(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8306\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8861 - loss: 0.2964 - val_accuracy: 0.8899 - val_loss: 0.2861 - mcc: 0.8306\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8820 - loss: 0.3107(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8153\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.8821 - loss: 0.3107 - val_accuracy: 0.8806 - val_loss: 0.3185 - mcc: 0.8153\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8872 - loss: 0.2932(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8214\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8872 - loss: 0.2931 - val_accuracy: 0.8834 - val_loss: 0.3104 - mcc: 0.8214\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8925 - loss: 0.2807(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8398\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.8925 - loss: 0.2807 - val_accuracy: 0.8957 - val_loss: 0.2699 - mcc: 0.8398\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8984 - loss: 0.2602(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8474\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.8984 - loss: 0.2602 - val_accuracy: 0.9009 - val_loss: 0.2553 - mcc: 0.8474\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9013 - loss: 0.2506(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8486\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9013 - loss: 0.2506 - val_accuracy: 0.9013 - val_loss: 0.2496 - mcc: 0.8486\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9022 - loss: 0.2484(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8530\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.9022 - loss: 0.2485 - val_accuracy: 0.9043 - val_loss: 0.2461 - mcc: 0.8530\n","Epoch 12/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9048 - loss: 0.2412(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8544\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.9048 - loss: 0.2412 - val_accuracy: 0.9053 - val_loss: 0.2407 - mcc: 0.8544\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9040 - loss: 0.2451(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8571\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9040 - loss: 0.2450 - val_accuracy: 0.9069 - val_loss: 0.2368 - mcc: 0.8571\n","Epoch 14/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9056 - loss: 0.2381(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8551\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9056 - loss: 0.2381 - val_accuracy: 0.9056 - val_loss: 0.2401 - mcc: 0.8551\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9061 - loss: 0.2372(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8546\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9061 - loss: 0.2372 - val_accuracy: 0.9056 - val_loss: 0.2413 - mcc: 0.8546\n","Epoch 16/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9092 - loss: 0.2286(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8546\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9092 - loss: 0.2286 - val_accuracy: 0.9055 - val_loss: 0.2410 - mcc: 0.8546\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9077 - loss: 0.2315(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8170\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9077 - loss: 0.2315 - val_accuracy: 0.8784 - val_loss: 0.3099 - mcc: 0.8170\n","Epoch 18/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9019 - loss: 0.2511(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8596\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9020 - loss: 0.2510 - val_accuracy: 0.9084 - val_loss: 0.2312 - mcc: 0.8596\n","Epoch 19/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9120 - loss: 0.2214(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8623\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9120 - loss: 0.2214 - val_accuracy: 0.9102 - val_loss: 0.2258 - mcc: 0.8623\n","Epoch 20/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9122 - loss: 0.2190(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8635\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9122 - loss: 0.2190 - val_accuracy: 0.9112 - val_loss: 0.2250 - mcc: 0.8635\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM_Dense, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7070 - loss: 0.8082(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7914\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.7071 - loss: 0.8078 - val_accuracy: 0.8644 - val_loss: 0.3625 - mcc: 0.7914\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8743 - loss: 0.3349(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8005\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.8743 - loss: 0.3349 - val_accuracy: 0.8697 - val_loss: 0.3473 - mcc: 0.8005\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8811 - loss: 0.3131(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8163\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.8812 - loss: 0.3131 - val_accuracy: 0.8799 - val_loss: 0.3146 - mcc: 0.8163\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8876 - loss: 0.2947(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8298\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.8876 - loss: 0.2946 - val_accuracy: 0.8891 - val_loss: 0.2872 - mcc: 0.8298\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8924 - loss: 0.2786(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8328\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.8924 - loss: 0.2786 - val_accuracy: 0.8910 - val_loss: 0.2797 - mcc: 0.8328\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8952 - loss: 0.2717(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8334\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8952 - loss: 0.2717 - val_accuracy: 0.8913 - val_loss: 0.2796 - mcc: 0.8334\n","Epoch 7/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8953 - loss: 0.2706(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8381\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8953 - loss: 0.2706 - val_accuracy: 0.8946 - val_loss: 0.2713 - mcc: 0.8381\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8996 - loss: 0.2582(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8395\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8996 - loss: 0.2582 - val_accuracy: 0.8949 - val_loss: 0.2681 - mcc: 0.8395\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9003 - loss: 0.2552(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8351\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.9003 - loss: 0.2552 - val_accuracy: 0.8919 - val_loss: 0.2764 - mcc: 0.8351\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8996 - loss: 0.2587(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8431\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.8996 - loss: 0.2587 - val_accuracy: 0.8978 - val_loss: 0.2617 - mcc: 0.8431\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9009 - loss: 0.2514(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8422\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9009 - loss: 0.2513 - val_accuracy: 0.8969 - val_loss: 0.2686 - mcc: 0.8422\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9051 - loss: 0.2414(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8484\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.9051 - loss: 0.2414 - val_accuracy: 0.9008 - val_loss: 0.2487 - mcc: 0.8484\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9059 - loss: 0.2397(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8512\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9059 - loss: 0.2397 - val_accuracy: 0.9027 - val_loss: 0.2450 - mcc: 0.8512\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9073 - loss: 0.2343(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8504\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9073 - loss: 0.2343 - val_accuracy: 0.9024 - val_loss: 0.2463 - mcc: 0.8504\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9070 - loss: 0.2348(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8519\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9070 - loss: 0.2348 - val_accuracy: 0.9030 - val_loss: 0.2432 - mcc: 0.8519\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9088 - loss: 0.2302(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8548\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.9088 - loss: 0.2302 - val_accuracy: 0.9053 - val_loss: 0.2387 - mcc: 0.8548\n","Epoch 17/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9093 - loss: 0.2288(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8566\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 22ms/step - accuracy: 0.9093 - loss: 0.2288 - val_accuracy: 0.9060 - val_loss: 0.2336 - mcc: 0.8566\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9112 - loss: 0.2227(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8583\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9112 - loss: 0.2227 - val_accuracy: 0.9075 - val_loss: 0.2327 - mcc: 0.8583\n","Epoch 19/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9107 - loss: 0.2237(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8585\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9107 - loss: 0.2237 - val_accuracy: 0.9073 - val_loss: 0.2306 - mcc: 0.8585\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9116 - loss: 0.2210(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8600\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.9116 - loss: 0.2210 - val_accuracy: 0.9081 - val_loss: 0.2275 - mcc: 0.8600\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'Accuracy': {'max': np.float64(0.914814),\n","              'mean': np.float64(0.9109288),\n","              'min': np.float64(0.907606),\n","              'std': np.float64(0.0027531744716074898)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0005701730251312255),\n","                               'mean': np.float64(0.0005142697016398112),\n","                               'min': np.float64(0.00037795646985371907),\n","                               'std': np.float64(6.908408378223667e-05)},\n"," 'MCC': {'max': np.float64(0.8698338287709787),\n","         'mean': np.float64(0.8636210383695643),\n","         'min': np.float64(0.8585259744411103),\n","         'std': np.float64(0.004111918097038068)},\n"," 'Parameters': 8037,\n"," 'Train Time (s)': {'max': np.float64(373.7579791545868),\n","                    'mean': np.float64(363.85867524147034),\n","                    'min': np.float64(355.38264751434326),\n","                    'std': np.float64(7.417127346749731)},\n"," 'Training Accuracy': [[0.8098497986793518,\n","                        0.8680099248886108,\n","                        0.8788726925849915,\n","                        0.8836743235588074,\n","                        0.8842329382896423,\n","                        0.8895115852355957,\n","                        0.8919174671173096,\n","                        0.8938518166542053,\n","                        0.8951988220214844,\n","                        0.8983756303787231,\n","                        0.8997384309768677,\n","                        0.9021032452583313,\n","                        0.903359055519104,\n","                        0.9050632119178772,\n","                        0.9064898490905762,\n","                        0.9073591232299805,\n","                        0.9090651869773865,\n","                        0.9093509316444397,\n","                        0.9108119010925293,\n","                        0.9101569652557373],\n","                       [0.8090230226516724,\n","                        0.8690704703330994,\n","                        0.879179835319519,\n","                        0.8849951028823853,\n","                        0.8891069889068604,\n","                        0.8923506140708923,\n","                        0.8946517109870911,\n","                        0.8954071998596191,\n","                        0.8994156122207642,\n","                        0.9021318554878235,\n","                        0.9030689001083374,\n","                        0.9049214720726013,\n","                        0.9018059372901917,\n","                        0.906567394733429,\n","                        0.9070255756378174,\n","                        0.9080021977424622,\n","                        0.9090753793716431,\n","                        0.9094155430793762,\n","                        0.9096929430961609,\n","                        0.9107635021209717],\n","                       [0.8061138391494751,\n","                        0.870704174041748,\n","                        0.879443347454071,\n","                        0.8844861388206482,\n","                        0.8909083604812622,\n","                        0.8941521048545837,\n","                        0.8963947892189026,\n","                        0.8988328576087952,\n","                        0.9012867212295532,\n","                        0.9026869535446167,\n","                        0.9040507078170776,\n","                        0.9058095812797546,\n","                        0.9062469005584717,\n","                        0.9077435731887817,\n","                        0.908578097820282,\n","                        0.9096567630767822,\n","                        0.9103302955627441,\n","                        0.9107753038406372,\n","                        0.9115879535675049,\n","                        0.9126105904579163],\n","                       [0.7997110486030579,\n","                        0.8652399778366089,\n","                        0.8721930384635925,\n","                        0.8784756660461426,\n","                        0.8857016563415527,\n","                        0.8855020403862,\n","                        0.8918991088867188,\n","                        0.8911115527153015,\n","                        0.8987082242965698,\n","                        0.9013251662254333,\n","                        0.9015787243843079,\n","                        0.904189944267273,\n","                        0.9048640131950378,\n","                        0.9047093987464905,\n","                        0.9070090055465698,\n","                        0.9085437059402466,\n","                        0.9076122045516968,\n","                        0.9062830209732056,\n","                        0.9108849167823792,\n","                        0.9117746353149414],\n","                       [0.8069047927856445,\n","                        0.8762189149856567,\n","                        0.8850330114364624,\n","                        0.889845073223114,\n","                        0.8937174677848816,\n","                        0.895346999168396,\n","                        0.8972944617271423,\n","                        0.8994925022125244,\n","                        0.8988816738128662,\n","                        0.9005141854286194,\n","                        0.902704656124115,\n","                        0.9040063619613647,\n","                        0.9047008752822876,\n","                        0.9062981605529785,\n","                        0.9075174927711487,\n","                        0.9087756872177124,\n","                        0.9092900156974792,\n","                        0.9109285473823547,\n","                        0.9109665751457214,\n","                        0.9127221703529358]],\n"," 'Training Loss': [[0.5198460817337036,\n","                    0.35036909580230713,\n","                    0.3197076916694641,\n","                    0.3066273629665375,\n","                    0.30765244364738464,\n","                    0.2907663881778717,\n","                    0.2833726704120636,\n","                    0.27700290083885193,\n","                    0.27120205760002136,\n","                    0.26147228479385376,\n","                    0.25614845752716064,\n","                    0.24872596561908722,\n","                    0.24511174857616425,\n","                    0.2393636703491211,\n","                    0.23450635373592377,\n","                    0.23223336040973663,\n","                    0.22793783247470856,\n","                    0.22599147260189056,\n","                    0.2219497114419937,\n","                    0.22491277754306793],\n","                   [0.5223318934440613,\n","                    0.3502114713191986,\n","                    0.31738004088401794,\n","                    0.30046138167381287,\n","                    0.28765422105789185,\n","                    0.27877622842788696,\n","                    0.2720702886581421,\n","                    0.26947787404060364,\n","                    0.2573730945587158,\n","                    0.2479102611541748,\n","                    0.24557267129421234,\n","                    0.2396051436662674,\n","                    0.24942578375339508,\n","                    0.23540063202381134,\n","                    0.234361931681633,\n","                    0.23034901916980743,\n","                    0.22695773839950562,\n","                    0.22644977271556854,\n","                    0.22645941376686096,\n","                    0.2214636206626892],\n","                   [0.5285805463790894,\n","                    0.34379664063453674,\n","                    0.3164377510547638,\n","                    0.29944664239883423,\n","                    0.2795369327068329,\n","                    0.2698323130607605,\n","                    0.26307764649391174,\n","                    0.2564372718334198,\n","                    0.2488248646259308,\n","                    0.24363546073436737,\n","                    0.24053658545017242,\n","                    0.23621401190757751,\n","                    0.23448820412158966,\n","                    0.23069319128990173,\n","                    0.228877454996109,\n","                    0.22593922913074493,\n","                    0.22341907024383545,\n","                    0.2225872427225113,\n","                    0.22067739069461823,\n","                    0.2180495411157608],\n","                   [0.5423906445503235,\n","                    0.3588027358055115,\n","                    0.33797112107276917,\n","                    0.31854748725891113,\n","                    0.2978842854499817,\n","                    0.30006733536720276,\n","                    0.27919408679008484,\n","                    0.28499165177345276,\n","                    0.2587949335575104,\n","                    0.25039806962013245,\n","                    0.25041791796684265,\n","                    0.2427094727754593,\n","                    0.24073803424835205,\n","                    0.24189892411231995,\n","                    0.234503373503685,\n","                    0.2299157679080963,\n","                    0.23315903544425964,\n","                    0.23713767528533936,\n","                    0.22339296340942383,\n","                    0.22073154151439667],\n","                   [0.532384991645813,\n","                    0.32826492190361023,\n","                    0.30086225271224976,\n","                    0.2867869734764099,\n","                    0.2754567265510559,\n","                    0.2702617943286896,\n","                    0.2648715674877167,\n","                    0.25840845704078674,\n","                    0.2597965598106384,\n","                    0.2555396556854248,\n","                    0.2476261407136917,\n","                    0.24402059614658356,\n","                    0.2425350397825241,\n","                    0.2367265820503235,\n","                    0.23334664106369019,\n","                    0.2298005223274231,\n","                    0.22852186858654022,\n","                    0.223745197057724,\n","                    0.2232663929462433,\n","                    0.21812520921230316]],\n"," 'Validation Accuracy': [[0.8643453121185303,\n","                          0.8767869472503662,\n","                          0.8863531947135925,\n","                          0.8892503976821899,\n","                          0.8887351751327515,\n","                          0.8944284915924072,\n","                          0.8966214060783386,\n","                          0.8987749218940735,\n","                          0.8995941281318665,\n","                          0.9035336375236511,\n","                          0.905580997467041,\n","                          0.9073595404624939,\n","                          0.9056781530380249,\n","                          0.905629575252533,\n","                          0.9096546173095703,\n","                          0.9098194241523743,\n","                          0.9121243357658386,\n","                          0.9066131711006165,\n","                          0.9122149348258972,\n","                          0.9148143529891968],\n","                         [0.870240330696106,\n","                          0.8789710402488708,\n","                          0.8836852312088013,\n","                          0.8860478401184082,\n","                          0.8914981484413147,\n","                          0.8967695832252502,\n","                          0.9008272886276245,\n","                          0.8986859321594238,\n","                          0.8993033170700073,\n","                          0.9052972793579102,\n","                          0.900733470916748,\n","                          0.908379316329956,\n","                          0.9056060910224915,\n","                          0.9067438840866089,\n","                          0.9037818312644958,\n","                          0.9105389714241028,\n","                          0.9088678956031799,\n","                          0.9116938710212708,\n","                          0.9061419367790222,\n","                          0.9129173755645752],\n","                         [0.8659017086029053,\n","                          0.8763055801391602,\n","                          0.8803644776344299,\n","                          0.8859907388687134,\n","                          0.8910425901412964,\n","                          0.897389829158783,\n","                          0.8961352705955505,\n","                          0.9002812504768372,\n","                          0.898796796798706,\n","                          0.9003968238830566,\n","                          0.9059541821479797,\n","                          0.9021000862121582,\n","                          0.9070307016372681,\n","                          0.8838043808937073,\n","                          0.9063013792037964,\n","                          0.908808171749115,\n","                          0.908817708492279,\n","                          0.9094314575195312,\n","                          0.9113380312919617,\n","                          0.9076058864593506],\n","                         [0.8568326830863953,\n","                          0.8729000687599182,\n","                          0.877760648727417,\n","                          0.8829631209373474,\n","                          0.8899351358413696,\n","                          0.8805748820304871,\n","                          0.8834284543991089,\n","                          0.8956763744354248,\n","                          0.9008692502975464,\n","                          0.9013153314590454,\n","                          0.9043153524398804,\n","                          0.9052860736846924,\n","                          0.9069259166717529,\n","                          0.9056311249732971,\n","                          0.9055549502372742,\n","                          0.9055374264717102,\n","                          0.8784241676330566,\n","                          0.908431887626648,\n","                          0.9102475047111511,\n","                          0.9111628532409668],\n","                         [0.8643595576286316,\n","                          0.8696892261505127,\n","                          0.8798973560333252,\n","                          0.8891427516937256,\n","                          0.8909653425216675,\n","                          0.8912572264671326,\n","                          0.8945868611335754,\n","                          0.8949192762374878,\n","                          0.8918860554695129,\n","                          0.8977987170219421,\n","                          0.8969354033470154,\n","                          0.9008234143257141,\n","                          0.9027389883995056,\n","                          0.9023737907409668,\n","                          0.9029926061630249,\n","                          0.9053005576133728,\n","                          0.9059853553771973,\n","                          0.9074511528015137,\n","                          0.9072960019111633,\n","                          0.9081441164016724]],\n"," 'Validation Loss': [[0.3612937927246094,\n","                      0.3251675069332123,\n","                      0.29771971702575684,\n","                      0.2960171103477478,\n","                      0.29244187474250793,\n","                      0.27557408809661865,\n","                      0.27114394307136536,\n","                      0.26198360323905945,\n","                      0.25853821635246277,\n","                      0.24542348086833954,\n","                      0.24154745042324066,\n","                      0.23584334552288055,\n","                      0.23884545266628265,\n","                      0.23705033957958221,\n","                      0.22762815654277802,\n","                      0.22501717507839203,\n","                      0.21956171095371246,\n","                      0.2386527955532074,\n","                      0.219485804438591,\n","                      0.21537435054779053],\n","                     [0.35149699449539185,\n","                      0.32073208689689636,\n","                      0.30408939719200134,\n","                      0.3010922372341156,\n","                      0.2776326835155487,\n","                      0.26943454146385193,\n","                      0.25731006264686584,\n","                      0.2629100978374481,\n","                      0.2587103545665741,\n","                      0.24351824820041656,\n","                      0.25728315114974976,\n","                      0.23247747123241425,\n","                      0.24246034026145935,\n","                      0.23596663773059845,\n","                      0.24406495690345764,\n","                      0.22741203010082245,\n","                      0.23347920179367065,\n","                      0.22393810749053955,\n","                      0.23909173905849457,\n","                      0.2195780873298645],\n","                     [0.36052706837654114,\n","                      0.3267052471637726,\n","                      0.31113821268081665,\n","                      0.2921410799026489,\n","                      0.275465726852417,\n","                      0.2596760094165802,\n","                      0.26415950059890747,\n","                      0.24956662952899933,\n","                      0.25190046429634094,\n","                      0.249984011054039,\n","                      0.2335871160030365,\n","                      0.24092988669872284,\n","                      0.23006276786327362,\n","                      0.3262624442577362,\n","                      0.2325448840856552,\n","                      0.22750107944011688,\n","                      0.22555746138095856,\n","                      0.22459551692008972,\n","                      0.22020915150642395,\n","                      0.23030748963356018],\n","                     [0.38156792521476746,\n","                      0.3363707363605499,\n","                      0.3230366110801697,\n","                      0.3042837977409363,\n","                      0.28605416417121887,\n","                      0.31850579380989075,\n","                      0.3104369640350342,\n","                      0.26988354325294495,\n","                      0.25533998012542725,\n","                      0.24962764978408813,\n","                      0.24609315395355225,\n","                      0.2406902313232422,\n","                      0.2367730289697647,\n","                      0.24009330570697784,\n","                      0.24131380021572113,\n","                      0.24101108312606812,\n","                      0.30985739827156067,\n","                      0.23120273649692535,\n","                      0.22575628757476807,\n","                      0.22498181462287903],\n","                     [0.3624860942363739,\n","                      0.3472554087638855,\n","                      0.3145694434642792,\n","                      0.28717121481895447,\n","                      0.2796887159347534,\n","                      0.27955764532089233,\n","                      0.2712745666503906,\n","                      0.26813584566116333,\n","                      0.27642351388931274,\n","                      0.26170557737350464,\n","                      0.2685690224170685,\n","                      0.24870730936527252,\n","                      0.24495741724967957,\n","                      0.24634088575839996,\n","                      0.2432439923286438,\n","                      0.23866741359233856,\n","                      0.2336122989654541,\n","                      0.23269787430763245,\n","                      0.23061363399028778,\n","                      0.2275378406047821]],\n"," 'Validation MCC': [[np.float64(0.7921777498391833),\n","                     np.float64(0.8106839430319863),\n","                     np.float64(0.8253600955137765),\n","                     np.float64(0.8296487785580381),\n","                     np.float64(0.8297110155271747),\n","                     np.float64(0.8386109644849065),\n","                     np.float64(0.8415118865610106),\n","                     np.float64(0.8447334210580388),\n","                     np.float64(0.8465309555159412),\n","                     np.float64(0.8531765774004316),\n","                     np.float64(0.8556843133805304),\n","                     np.float64(0.8583353028589985),\n","                     np.float64(0.8557556406058152),\n","                     np.float64(0.856591315912072),\n","                     np.float64(0.8616506019011022),\n","                     np.float64(0.8626897719171988),\n","                     np.float64(0.8653901077595924),\n","                     np.float64(0.8570714171488854),\n","                     np.float64(0.8657869092165441),\n","                     np.float64(0.8698338287709787)],\n","                    [np.float64(0.7997862444561477),\n","                     np.float64(0.8137906595323515),\n","                     np.float64(0.8204655983254449),\n","                     np.float64(0.8243237431840013),\n","                     np.float64(0.8351857885045506),\n","                     np.float64(0.8409452861938261),\n","                     np.float64(0.8479032380899035),\n","                     np.float64(0.8439651095879055),\n","                     np.float64(0.8463080746234476),\n","                     np.float64(0.854607002499433),\n","                     np.float64(0.847566523736188),\n","                     np.float64(0.859254282019815),\n","                     np.float64(0.8547341326655814),\n","                     np.float64(0.8571517524330763),\n","                     np.float64(0.8546880536751059),\n","                     np.float64(0.8624098477695032),\n","                     np.float64(0.8601905244244611),\n","                     np.float64(0.8642259330279123),\n","                     np.float64(0.8565805445581466),\n","                     np.float64(0.8662461988599237)],\n","                    [np.float64(0.7951599418355659),\n","                     np.float64(0.8104108195303458),\n","                     np.float64(0.81695086110423),\n","                     np.float64(0.8247089620001199),\n","                     np.float64(0.8347260258574993),\n","                     np.float64(0.8429528995446453),\n","                     np.float64(0.8406392178460046),\n","                     np.float64(0.8478397746913389),\n","                     np.float64(0.8458192728948029),\n","                     np.float64(0.8473179000404193),\n","                     np.float64(0.8566232242790365),\n","                     np.float64(0.8507623618007751),\n","                     np.float64(0.8581449181228296),\n","                     np.float64(0.8216676095248878),\n","                     np.float64(0.8578283911658237),\n","                     np.float64(0.8605273215084392),\n","                     np.float64(0.8613025305563865),\n","                     np.float64(0.8620208535423683),\n","                     np.float64(0.8645595656976421),\n","                     np.float64(0.8585259744411103)],\n","                    [np.float64(0.7774127253522144),\n","                     np.float64(0.803843616898772),\n","                     np.float64(0.8117545133252696),\n","                     np.float64(0.8211707513401615),\n","                     np.float64(0.8306444870887009),\n","                     np.float64(0.8153082649472941),\n","                     np.float64(0.8214117921765975),\n","                     np.float64(0.839761021623382),\n","                     np.float64(0.8474468887258286),\n","                     np.float64(0.848641487897419),\n","                     np.float64(0.8529597847527666),\n","                     np.float64(0.8544051786296111),\n","                     np.float64(0.857086219146033),\n","                     np.float64(0.8550580534711915),\n","                     np.float64(0.8545804678895995),\n","                     np.float64(0.8545651963832646),\n","                     np.float64(0.8169632836319418),\n","                     np.float64(0.8596344116465191),\n","                     np.float64(0.8623218690718336),\n","                     np.float64(0.8634938486203877)],\n","                    [np.float64(0.7914194796983349),\n","                     np.float64(0.8005442851822941),\n","                     np.float64(0.8162744797733222),\n","                     np.float64(0.8297678306306647),\n","                     np.float64(0.8327618212852902),\n","                     np.float64(0.8334002293188584),\n","                     np.float64(0.8380550158432893),\n","                     np.float64(0.8394582620549527),\n","                     np.float64(0.8350587606394125),\n","                     np.float64(0.8431299616543964),\n","                     np.float64(0.8422443483572655),\n","                     np.float64(0.848357884447753),\n","                     np.float64(0.8511601087997288),\n","                     np.float64(0.8504485843576304),\n","                     np.float64(0.8518790375962917),\n","                     np.float64(0.8548317599874712),\n","                     np.float64(0.8565909632297083),\n","                     np.float64(0.8583183900735394),\n","                     np.float64(0.8585014108492921),\n","                     np.float64(0.8600053411554206)]]}\n","Training Model: LSTM_Deep, Fold: 1\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7502 - loss: 0.6683(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8082\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 38ms/step - accuracy: 0.7503 - loss: 0.6681 - val_accuracy: 0.8754 - val_loss: 0.3305 - mcc: 0.8082\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8719 - loss: 0.3386(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8270\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 36ms/step - accuracy: 0.8719 - loss: 0.3386 - val_accuracy: 0.8875 - val_loss: 0.2922 - mcc: 0.8270\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8889 - loss: 0.2898(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8346\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.8889 - loss: 0.2898 - val_accuracy: 0.8911 - val_loss: 0.2791 - mcc: 0.8346\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8945 - loss: 0.2737(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8468\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8945 - loss: 0.2737 - val_accuracy: 0.8999 - val_loss: 0.2556 - mcc: 0.8468\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8987 - loss: 0.2588(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8552\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8987 - loss: 0.2588 - val_accuracy: 0.9053 - val_loss: 0.2424 - mcc: 0.8552\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9018 - loss: 0.2512(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8547\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9018 - loss: 0.2512 - val_accuracy: 0.9050 - val_loss: 0.2424 - mcc: 0.8547\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9014 - loss: 0.2505(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8600\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.9014 - loss: 0.2505 - val_accuracy: 0.9085 - val_loss: 0.2310 - mcc: 0.8600\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9077 - loss: 0.2324(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8633\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 36ms/step - accuracy: 0.9077 - loss: 0.2324 - val_accuracy: 0.9108 - val_loss: 0.2256 - mcc: 0.8633\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9102 - loss: 0.2256(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8706\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9102 - loss: 0.2256 - val_accuracy: 0.9153 - val_loss: 0.2129 - mcc: 0.8706\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9140 - loss: 0.2156(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8706\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9140 - loss: 0.2156 - val_accuracy: 0.9153 - val_loss: 0.2120 - mcc: 0.8706\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9137 - loss: 0.2151(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8736\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9137 - loss: 0.2151 - val_accuracy: 0.9172 - val_loss: 0.2091 - mcc: 0.8736\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9167 - loss: 0.2090(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8743\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.2090 - val_accuracy: 0.9178 - val_loss: 0.2068 - mcc: 0.8743\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9173 - loss: 0.2070(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8759\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9173 - loss: 0.2070 - val_accuracy: 0.9186 - val_loss: 0.2065 - mcc: 0.8759\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9185 - loss: 0.2030(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8780\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9185 - loss: 0.2030 - val_accuracy: 0.9202 - val_loss: 0.2002 - mcc: 0.8780\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9202 - loss: 0.1986(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8785\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9202 - loss: 0.1986 - val_accuracy: 0.9204 - val_loss: 0.1992 - mcc: 0.8785\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9209 - loss: 0.1975(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8690\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9209 - loss: 0.1975 - val_accuracy: 0.9139 - val_loss: 0.2187 - mcc: 0.8690\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9185 - loss: 0.2037(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8777\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9185 - loss: 0.2037 - val_accuracy: 0.9201 - val_loss: 0.2024 - mcc: 0.8777\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9208 - loss: 0.1971(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8834\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9208 - loss: 0.1971 - val_accuracy: 0.9237 - val_loss: 0.1915 - mcc: 0.8834\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9236 - loss: 0.1888(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8838\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9236 - loss: 0.1888 - val_accuracy: 0.9236 - val_loss: 0.1906 - mcc: 0.8838\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9235 - loss: 0.1898(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8812\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9235 - loss: 0.1898 - val_accuracy: 0.9223 - val_loss: 0.1935 - mcc: 0.8812\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM_Deep, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7553 - loss: 0.6762(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8074\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.7554 - loss: 0.6760 - val_accuracy: 0.8748 - val_loss: 0.3278 - mcc: 0.8074\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8767 - loss: 0.3258(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8357\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8767 - loss: 0.3258 - val_accuracy: 0.8930 - val_loss: 0.2774 - mcc: 0.8357\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8892 - loss: 0.2860(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8458\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.8892 - loss: 0.2860 - val_accuracy: 0.8998 - val_loss: 0.2564 - mcc: 0.8458\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8901 - loss: 0.2885(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8444\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.8901 - loss: 0.2884 - val_accuracy: 0.8987 - val_loss: 0.2633 - mcc: 0.8444\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8979 - loss: 0.2622(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8460\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8979 - loss: 0.2622 - val_accuracy: 0.8999 - val_loss: 0.2607 - mcc: 0.8460\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9010 - loss: 0.2568(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8586\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9010 - loss: 0.2568 - val_accuracy: 0.9081 - val_loss: 0.2353 - mcc: 0.8586\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9058 - loss: 0.2413(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8637\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 39ms/step - accuracy: 0.9058 - loss: 0.2413 - val_accuracy: 0.9107 - val_loss: 0.2273 - mcc: 0.8637\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9083 - loss: 0.2327(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8649\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.9083 - loss: 0.2327 - val_accuracy: 0.9115 - val_loss: 0.2209 - mcc: 0.8649\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9121 - loss: 0.2204(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8687\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9121 - loss: 0.2204 - val_accuracy: 0.9140 - val_loss: 0.2153 - mcc: 0.8687\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9153 - loss: 0.2126(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8697\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9153 - loss: 0.2126 - val_accuracy: 0.9147 - val_loss: 0.2154 - mcc: 0.8697\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9159 - loss: 0.2082(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8748\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9159 - loss: 0.2082 - val_accuracy: 0.9185 - val_loss: 0.2050 - mcc: 0.8748\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9169 - loss: 0.2078(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8730\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9169 - loss: 0.2078 - val_accuracy: 0.9164 - val_loss: 0.2088 - mcc: 0.8730\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9176 - loss: 0.2051(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8770\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9176 - loss: 0.2051 - val_accuracy: 0.9197 - val_loss: 0.2013 - mcc: 0.8770\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9194 - loss: 0.2007(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8789\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9194 - loss: 0.2007 - val_accuracy: 0.9208 - val_loss: 0.1977 - mcc: 0.8789\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9195 - loss: 0.2004(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8787\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9195 - loss: 0.2004 - val_accuracy: 0.9207 - val_loss: 0.1987 - mcc: 0.8787\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9203 - loss: 0.1969(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8828\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9203 - loss: 0.1969 - val_accuracy: 0.9236 - val_loss: 0.1924 - mcc: 0.8828\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9216 - loss: 0.1958(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8824\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 39ms/step - accuracy: 0.9216 - loss: 0.1958 - val_accuracy: 0.9232 - val_loss: 0.1920 - mcc: 0.8824\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9194 - loss: 0.2000(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8789\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.9194 - loss: 0.2000 - val_accuracy: 0.9209 - val_loss: 0.1970 - mcc: 0.8789\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9231 - loss: 0.1920(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8776\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9231 - loss: 0.1920 - val_accuracy: 0.9200 - val_loss: 0.2003 - mcc: 0.8776\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9236 - loss: 0.1893(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8826\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9236 - loss: 0.1893 - val_accuracy: 0.9231 - val_loss: 0.1930 - mcc: 0.8826\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM_Deep, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7392 - loss: 0.6920(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7993\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 36ms/step - accuracy: 0.7394 - loss: 0.6914 - val_accuracy: 0.8688 - val_loss: 0.3470 - mcc: 0.7993\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8736 - loss: 0.3374(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8168\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8736 - loss: 0.3374 - val_accuracy: 0.8808 - val_loss: 0.3117 - mcc: 0.8168\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8851 - loss: 0.3032(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8305\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8851 - loss: 0.3032 - val_accuracy: 0.8895 - val_loss: 0.2903 - mcc: 0.8305\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8921 - loss: 0.2840(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8359\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8921 - loss: 0.2840 - val_accuracy: 0.8928 - val_loss: 0.2783 - mcc: 0.8359\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8948 - loss: 0.2796(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8456\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.8948 - loss: 0.2796 - val_accuracy: 0.8989 - val_loss: 0.2610 - mcc: 0.8456\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8999 - loss: 0.2620(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8458\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8999 - loss: 0.2620 - val_accuracy: 0.8994 - val_loss: 0.2608 - mcc: 0.8458\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9004 - loss: 0.2598(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8485\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.9004 - loss: 0.2598 - val_accuracy: 0.9003 - val_loss: 0.2517 - mcc: 0.8485\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9002 - loss: 0.2580(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8514\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9002 - loss: 0.2580 - val_accuracy: 0.9030 - val_loss: 0.2457 - mcc: 0.8514\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9086 - loss: 0.2339(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8631\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9086 - loss: 0.2339 - val_accuracy: 0.9104 - val_loss: 0.2258 - mcc: 0.8631\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9100 - loss: 0.2296(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8593\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9100 - loss: 0.2296 - val_accuracy: 0.9078 - val_loss: 0.2315 - mcc: 0.8593\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9129 - loss: 0.2217(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8424\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 36ms/step - accuracy: 0.9129 - loss: 0.2217 - val_accuracy: 0.8967 - val_loss: 0.2610 - mcc: 0.8424\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9080 - loss: 0.2375(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8639\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.9080 - loss: 0.2375 - val_accuracy: 0.9109 - val_loss: 0.2262 - mcc: 0.8639\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9152 - loss: 0.2155(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8630\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.9152 - loss: 0.2155 - val_accuracy: 0.9104 - val_loss: 0.2240 - mcc: 0.8630\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9173 - loss: 0.2101(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8709\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 37ms/step - accuracy: 0.9173 - loss: 0.2101 - val_accuracy: 0.9152 - val_loss: 0.2111 - mcc: 0.8709\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9193 - loss: 0.2035(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8736\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9193 - loss: 0.2035 - val_accuracy: 0.9174 - val_loss: 0.2058 - mcc: 0.8736\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9199 - loss: 0.2018(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8763\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9199 - loss: 0.2018 - val_accuracy: 0.9188 - val_loss: 0.2016 - mcc: 0.8763\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9205 - loss: 0.1986(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8766\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9205 - loss: 0.1986 - val_accuracy: 0.9190 - val_loss: 0.2001 - mcc: 0.8766\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9225 - loss: 0.1949(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8742\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9225 - loss: 0.1949 - val_accuracy: 0.9176 - val_loss: 0.2059 - mcc: 0.8742\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9236 - loss: 0.1914(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8773\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.9236 - loss: 0.1914 - val_accuracy: 0.9197 - val_loss: 0.1998 - mcc: 0.8773\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9246 - loss: 0.1878(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8808\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9246 - loss: 0.1878 - val_accuracy: 0.9216 - val_loss: 0.1949 - mcc: 0.8808\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM_Deep, Fold: 4\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7492 - loss: 0.6726(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8044\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.7494 - loss: 0.6721 - val_accuracy: 0.8737 - val_loss: 0.3350 - mcc: 0.8044\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8761 - loss: 0.3255(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8147\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8761 - loss: 0.3255 - val_accuracy: 0.8791 - val_loss: 0.3120 - mcc: 0.8147\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8880 - loss: 0.2900(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8411\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.8880 - loss: 0.2900 - val_accuracy: 0.8970 - val_loss: 0.2689 - mcc: 0.8411\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8957 - loss: 0.2690(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8484\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.8957 - loss: 0.2690 - val_accuracy: 0.9016 - val_loss: 0.2560 - mcc: 0.8484\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9020 - loss: 0.2504(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8551\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.9020 - loss: 0.2504 - val_accuracy: 0.9056 - val_loss: 0.2431 - mcc: 0.8551\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9059 - loss: 0.2385(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8567\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9059 - loss: 0.2385 - val_accuracy: 0.9064 - val_loss: 0.2396 - mcc: 0.8567\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9086 - loss: 0.2316(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8569\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9086 - loss: 0.2316 - val_accuracy: 0.9069 - val_loss: 0.2366 - mcc: 0.8569\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9107 - loss: 0.2257(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8595\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9107 - loss: 0.2257 - val_accuracy: 0.9086 - val_loss: 0.2349 - mcc: 0.8595\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9131 - loss: 0.2215(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8666\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9131 - loss: 0.2215 - val_accuracy: 0.9131 - val_loss: 0.2224 - mcc: 0.8666\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9134 - loss: 0.2181(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8649\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9134 - loss: 0.2181 - val_accuracy: 0.9120 - val_loss: 0.2283 - mcc: 0.8649\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9047 - loss: 0.2445(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8708\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9047 - loss: 0.2445 - val_accuracy: 0.9159 - val_loss: 0.2163 - mcc: 0.8708\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9169 - loss: 0.2108(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8699\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.9169 - loss: 0.2108 - val_accuracy: 0.9153 - val_loss: 0.2174 - mcc: 0.8699\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9198 - loss: 0.1989(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8729\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9198 - loss: 0.1989 - val_accuracy: 0.9173 - val_loss: 0.2094 - mcc: 0.8729\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9206 - loss: 0.1970(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8696\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9206 - loss: 0.1970 - val_accuracy: 0.9148 - val_loss: 0.2149 - mcc: 0.8696\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9215 - loss: 0.1963(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8727\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9215 - loss: 0.1963 - val_accuracy: 0.9168 - val_loss: 0.2108 - mcc: 0.8727\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9216 - loss: 0.1940(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8757\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9216 - loss: 0.1940 - val_accuracy: 0.9191 - val_loss: 0.2047 - mcc: 0.8757\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9224 - loss: 0.1924(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8770\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9224 - loss: 0.1924 - val_accuracy: 0.9198 - val_loss: 0.2024 - mcc: 0.8770\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9226 - loss: 0.1930(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8777\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9226 - loss: 0.1930 - val_accuracy: 0.9204 - val_loss: 0.2014 - mcc: 0.8777\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9226 - loss: 0.1912(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8798\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 40ms/step - accuracy: 0.9226 - loss: 0.1912 - val_accuracy: 0.9217 - val_loss: 0.1978 - mcc: 0.8798\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9248 - loss: 0.1870(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8819\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 35ms/step - accuracy: 0.9248 - loss: 0.1870 - val_accuracy: 0.9228 - val_loss: 0.1953 - mcc: 0.8819\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: LSTM_Deep, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7456 - loss: 0.7114(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8012\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.7457 - loss: 0.7111 - val_accuracy: 0.8703 - val_loss: 0.3465 - mcc: 0.8012\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8786 - loss: 0.3245(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8217\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8786 - loss: 0.3245 - val_accuracy: 0.8841 - val_loss: 0.3050 - mcc: 0.8217\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8877 - loss: 0.2944(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8264\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8877 - loss: 0.2944 - val_accuracy: 0.8857 - val_loss: 0.2988 - mcc: 0.8264\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8935 - loss: 0.2812(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8383\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8935 - loss: 0.2812 - val_accuracy: 0.8945 - val_loss: 0.2768 - mcc: 0.8383\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8972 - loss: 0.2683(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8379\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8972 - loss: 0.2682 - val_accuracy: 0.8941 - val_loss: 0.2793 - mcc: 0.8379\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8990 - loss: 0.2644(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8478\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8990 - loss: 0.2643 - val_accuracy: 0.8999 - val_loss: 0.2590 - mcc: 0.8478\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9029 - loss: 0.2532(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8539\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9029 - loss: 0.2532 - val_accuracy: 0.9047 - val_loss: 0.2490 - mcc: 0.8539\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9059 - loss: 0.2437(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8550\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9059 - loss: 0.2437 - val_accuracy: 0.9053 - val_loss: 0.2460 - mcc: 0.8550\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9082 - loss: 0.2375(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8588\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9082 - loss: 0.2375 - val_accuracy: 0.9078 - val_loss: 0.2397 - mcc: 0.8588\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9109 - loss: 0.2297(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8530\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9109 - loss: 0.2297 - val_accuracy: 0.9041 - val_loss: 0.2450 - mcc: 0.8530\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9150 - loss: 0.2169(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8653\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9150 - loss: 0.2169 - val_accuracy: 0.9120 - val_loss: 0.2234 - mcc: 0.8653\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9168 - loss: 0.2120(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8591\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9168 - loss: 0.2120 - val_accuracy: 0.9079 - val_loss: 0.2418 - mcc: 0.8591\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9174 - loss: 0.2097(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8683\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9174 - loss: 0.2097 - val_accuracy: 0.9139 - val_loss: 0.2167 - mcc: 0.8683\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9186 - loss: 0.2056(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8694\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9186 - loss: 0.2056 - val_accuracy: 0.9145 - val_loss: 0.2169 - mcc: 0.8694\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9211 - loss: 0.1995(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8712\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9211 - loss: 0.1995 - val_accuracy: 0.9157 - val_loss: 0.2119 - mcc: 0.8712\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9222 - loss: 0.1980(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8740\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9222 - loss: 0.1980 - val_accuracy: 0.9176 - val_loss: 0.2084 - mcc: 0.8740\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9220 - loss: 0.1958(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8702\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9220 - loss: 0.1958 - val_accuracy: 0.9150 - val_loss: 0.2144 - mcc: 0.8702\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9221 - loss: 0.1957(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8768\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9221 - loss: 0.1957 - val_accuracy: 0.9194 - val_loss: 0.2025 - mcc: 0.8768\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9265 - loss: 0.1854(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8806\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9265 - loss: 0.1854 - val_accuracy: 0.9216 - val_loss: 0.1956 - mcc: 0.8806\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9253 - loss: 0.1862(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8802\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.9253 - loss: 0.1862 - val_accuracy: 0.9214 - val_loss: 0.1954 - mcc: 0.8802\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'Accuracy': {'max': np.float64(0.92312),\n","              'mean': np.float64(0.9222506666666668),\n","              'min': np.float64(0.9213953333333333),\n","              'std': np.float64(0.0006575825085535542)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0010013848940531413),\n","                               'mean': np.float64(0.0007793351491292317),\n","                               'min': np.float64(0.0005413823922475179),\n","                               'std': np.float64(0.00018754764612418837)},\n"," 'MCC': {'max': np.float64(0.8826276193145361),\n","         'mean': np.float64(0.8813392268798907),\n","         'min': np.float64(0.8801945354006611),\n","         'std': np.float64(0.0008446351740348602)},\n"," 'Parameters': 36069,\n"," 'Train Time (s)': {'max': np.float64(755.4301896095276),\n","                    'mean': np.float64(710.2303687095642),\n","                    'min': np.float64(665.3912403583527),\n","                    'std': np.float64(34.23030243241782)},\n"," 'Training Accuracy': [[0.8238983750343323,\n","                        0.8761965036392212,\n","                        0.8896418213844299,\n","                        0.8955038189888,\n","                        0.8991687297821045,\n","                        0.9022654891014099,\n","                        0.9024484157562256,\n","                        0.9070205092430115,\n","                        0.910722017288208,\n","                        0.9131357669830322,\n","                        0.9147629141807556,\n","                        0.9161171913146973,\n","                        0.9170146584510803,\n","                        0.9186212420463562,\n","                        0.9193989038467407,\n","                        0.9180285930633545,\n","                        0.9190717935562134,\n","                        0.9211116433143616,\n","                        0.9222276210784912,\n","                        0.9215905666351318],\n","                       [0.826632559299469,\n","                        0.8807302117347717,\n","                        0.89028000831604,\n","                        0.8922094106674194,\n","                        0.8973564505577087,\n","                        0.9017736911773682,\n","                        0.906390368938446,\n","                        0.909044086933136,\n","                        0.9117738604545593,\n","                        0.9142045974731445,\n","                        0.9157418012619019,\n","                        0.9167342782020569,\n","                        0.9181551933288574,\n","                        0.9192779064178467,\n","                        0.919843316078186,\n","                        0.9211369752883911,\n","                        0.9222010374069214,\n","                        0.921937108039856,\n","                        0.9220698475837708,\n","                        0.9232642650604248],\n","                       [0.8161866068840027,\n","                        0.876853346824646,\n","                        0.8884235620498657,\n","                        0.8933190703392029,\n","                        0.894524097442627,\n","                        0.8987190127372742,\n","                        0.8975657820701599,\n","                        0.9025635719299316,\n","                        0.9088023900985718,\n","                        0.9093331694602966,\n","                        0.9097256064414978,\n","                        0.9088197350502014,\n","                        0.9152455925941467,\n","                        0.9173265099525452,\n","                        0.9187278747558594,\n","                        0.9204404950141907,\n","                        0.9203094244003296,\n","                        0.9215746521949768,\n","                        0.9229701161384583,\n","                        0.9231756329536438],\n","                       [0.8227059841156006,\n","                        0.8798268437385559,\n","                        0.8892791271209717,\n","                        0.8968129754066467,\n","                        0.9017046093940735,\n","                        0.9055376052856445,\n","                        0.9081043004989624,\n","                        0.9103191494941711,\n","                        0.9117697477340698,\n","                        0.9129338264465332,\n","                        0.9109823107719421,\n","                        0.9168765544891357,\n","                        0.9179894328117371,\n","                        0.9192439913749695,\n","                        0.9202865958213806,\n","                        0.9215170741081238,\n","                        0.9220810532569885,\n","                        0.923223078250885,\n","                        0.9231155514717102,\n","                        0.9242925047874451],\n","                       [0.8201251029968262,\n","                        0.8811818957328796,\n","                        0.8902717232704163,\n","                        0.8935078382492065,\n","                        0.8988581299781799,\n","                        0.9014344811439514,\n","                        0.9042115211486816,\n","                        0.9073540568351746,\n","                        0.9088086485862732,\n","                        0.9108978509902954,\n","                        0.9145346879959106,\n","                        0.9167137742042542,\n","                        0.917466402053833,\n","                        0.9184474945068359,\n","                        0.9202892184257507,\n","                        0.9212022423744202,\n","                        0.9224991202354431,\n","                        0.923173189163208,\n","                        0.9248458743095398,\n","                        0.9257774949073792]],\n"," 'Training Loss': [[0.4756179749965668,\n","                    0.3280865252017975,\n","                    0.2871030867099762,\n","                    0.2698242664337158,\n","                    0.25938400626182556,\n","                    0.2503920793533325,\n","                    0.24901053309440613,\n","                    0.23517726361751556,\n","                    0.22425870597362518,\n","                    0.21738916635513306,\n","                    0.21260665357112885,\n","                    0.20928794145584106,\n","                    0.20757153630256653,\n","                    0.2028283029794693,\n","                    0.19981607794761658,\n","                    0.2045648992061615,\n","                    0.20235410332679749,\n","                    0.1959977149963379,\n","                    0.19267211854457855,\n","                    0.19452454149723053],\n","                   [0.47258105874061584,\n","                    0.31322813034057617,\n","                    0.2839610278606415,\n","                    0.2812359631061554,\n","                    0.26512500643730164,\n","                    0.2525821030139923,\n","                    0.23817595839500427,\n","                    0.23010341823101044,\n","                    0.22113405168056488,\n","                    0.21432919800281525,\n","                    0.20988203585147858,\n","                    0.2082093060016632,\n","                    0.20395712554454803,\n","                    0.20141032338142395,\n","                    0.19954702258110046,\n","                    0.19530098140239716,\n","                    0.19378100335597992,\n","                    0.1936277449131012,\n","                    0.19500449299812317,\n","                    0.19057363271713257],\n","                   [0.4924755096435547,\n","                    0.32705146074295044,\n","                    0.29495349526405334,\n","                    0.2806012034416199,\n","                    0.2783670723438263,\n","                    0.265716016292572,\n","                    0.26763716340065,\n","                    0.2509520947933197,\n","                    0.2324148416519165,\n","                    0.23161745071411133,\n","                    0.23071767389774323,\n","                    0.23556536436080933,\n","                    0.215619757771492,\n","                    0.20865075290203094,\n","                    0.2046239823102951,\n","                    0.20043927431106567,\n","                    0.19959230720996857,\n","                    0.19656699895858765,\n","                    0.19371266663074493,\n","                    0.1918044239282608],\n","                   [0.47451528906822205,\n","                    0.31466031074523926,\n","                    0.28741464018821716,\n","                    0.2658340632915497,\n","                    0.25144436955451965,\n","                    0.23995380103588104,\n","                    0.232749342918396,\n","                    0.2265394628047943,\n","                    0.22391977906227112,\n","                    0.21997393667697906,\n","                    0.22544842958450317,\n","                    0.20845796167850494,\n","                    0.20423050224781036,\n","                    0.20102258026599884,\n","                    0.1985640823841095,\n","                    0.19502583146095276,\n","                    0.19387181103229523,\n","                    0.19107942283153534,\n","                    0.19063271582126617,\n","                    0.18788379430770874],\n","                   [0.4929827153682709,\n","                    0.31686991453170776,\n","                    0.2886684834957123,\n","                    0.2820412218570709,\n","                    0.2652597725391388,\n","                    0.2569660246372223,\n","                    0.24928294122219086,\n","                    0.23967769742012024,\n","                    0.2355756014585495,\n","                    0.22970668971538544,\n","                    0.21764512360095978,\n","                    0.21171151101589203,\n","                    0.20943526923656464,\n","                    0.20666784048080444,\n","                    0.20149827003479004,\n","                    0.199555903673172,\n","                    0.1946205496788025,\n","                    0.19353358447551727,\n","                    0.1886775642633438,\n","                    0.18591073155403137]],\n"," 'Validation Accuracy': [[0.8753983378410339,\n","                          0.8874754905700684,\n","                          0.8910524845123291,\n","                          0.8998813629150391,\n","                          0.9053494334220886,\n","                          0.9050019383430481,\n","                          0.9084903001785278,\n","                          0.9107721447944641,\n","                          0.9153436422348022,\n","                          0.9152724742889404,\n","                          0.917242169380188,\n","                          0.9177859425544739,\n","                          0.9186499714851379,\n","                          0.920157790184021,\n","                          0.9204325079917908,\n","                          0.9139328002929688,\n","                          0.920065701007843,\n","                          0.9236754775047302,\n","                          0.9236260652542114,\n","                          0.9222884774208069],\n","                         [0.8748165965080261,\n","                          0.8929804563522339,\n","                          0.8997600674629211,\n","                          0.8986860513687134,\n","                          0.8998717665672302,\n","                          0.9080713987350464,\n","                          0.9107139706611633,\n","                          0.9115484356880188,\n","                          0.9139765501022339,\n","                          0.9147131443023682,\n","                          0.9184746742248535,\n","                          0.9164352416992188,\n","                          0.9197468161582947,\n","                          0.9208491444587708,\n","                          0.9207061529159546,\n","                          0.9235803484916687,\n","                          0.9231760501861572,\n","                          0.9209455847740173,\n","                          0.9200239181518555,\n","                          0.9231202006340027],\n","                         [0.868788480758667,\n","                          0.8807604312896729,\n","                          0.8895100951194763,\n","                          0.8927705883979797,\n","                          0.8989364504814148,\n","                          0.8994037508964539,\n","                          0.9003387093544006,\n","                          0.9029797315597534,\n","                          0.910398542881012,\n","                          0.9078211188316345,\n","                          0.8966766595840454,\n","                          0.9109330773353577,\n","                          0.9104230999946594,\n","                          0.9151825904846191,\n","                          0.9173718094825745,\n","                          0.9188405871391296,\n","                          0.9190270304679871,\n","                          0.9176074862480164,\n","                          0.9197219014167786,\n","                          0.9216454029083252],\n","                         [0.8736571669578552,\n","                          0.8791114687919617,\n","                          0.8969694972038269,\n","                          0.9016252160072327,\n","                          0.9056466221809387,\n","                          0.9064319729804993,\n","                          0.9068906903266907,\n","                          0.9085533618927002,\n","                          0.9130868315696716,\n","                          0.9120405316352844,\n","                          0.9158554673194885,\n","                          0.9153264760971069,\n","                          0.9172926545143127,\n","                          0.9147577285766602,\n","                          0.9168016910552979,\n","                          0.9190757870674133,\n","                          0.9198354482650757,\n","                          0.9203879237174988,\n","                          0.9216955304145813,\n","                          0.9228048324584961],\n","                         [0.87029629945755,\n","                          0.8840797543525696,\n","                          0.8857408761978149,\n","                          0.8944854140281677,\n","                          0.894121527671814,\n","                          0.899934709072113,\n","                          0.9046691060066223,\n","                          0.9052663445472717,\n","                          0.9078306555747986,\n","                          0.9041333198547363,\n","                          0.9119524955749512,\n","                          0.9079368710517883,\n","                          0.9138515591621399,\n","                          0.9144840240478516,\n","                          0.9157059788703918,\n","                          0.9175518155097961,\n","                          0.9150159358978271,\n","                          0.9193940162658691,\n","                          0.9216465950012207,\n","                          0.9213953614234924]],\n"," 'Validation Loss': [[0.3305113613605499,\n","                      0.2922179400920868,\n","                      0.279087632894516,\n","                      0.25563323497772217,\n","                      0.24241392314434052,\n","                      0.24241748452186584,\n","                      0.23104135692119598,\n","                      0.22560271620750427,\n","                      0.21294571459293365,\n","                      0.21199874579906464,\n","                      0.20905113220214844,\n","                      0.20682621002197266,\n","                      0.2064586579799652,\n","                      0.2001982033252716,\n","                      0.1992013305425644,\n","                      0.21867488324642181,\n","                      0.20241795480251312,\n","                      0.1915007382631302,\n","                      0.19058991968631744,\n","                      0.19349133968353271],\n","                     [0.3278363049030304,\n","                      0.2773928940296173,\n","                      0.2563781440258026,\n","                      0.2633295953273773,\n","                      0.2606542110443115,\n","                      0.23533634841442108,\n","                      0.2272770255804062,\n","                      0.2208983600139618,\n","                      0.21525652706623077,\n","                      0.2153635323047638,\n","                      0.20496173202991486,\n","                      0.2087688148021698,\n","                      0.20133432745933533,\n","                      0.19773437082767487,\n","                      0.19872677326202393,\n","                      0.19237206876277924,\n","                      0.19203434884548187,\n","                      0.19699078798294067,\n","                      0.2002517729997635,\n","                      0.19295015931129456],\n","                     [0.3470238447189331,\n","                      0.3117077648639679,\n","                      0.29033923149108887,\n","                      0.27828162908554077,\n","                      0.2609819173812866,\n","                      0.26082196831703186,\n","                      0.25165170431137085,\n","                      0.2457253485918045,\n","                      0.22579754889011383,\n","                      0.23153920471668243,\n","                      0.2609553337097168,\n","                      0.22624485194683075,\n","                      0.22399289906024933,\n","                      0.21107687056064606,\n","                      0.20580893754959106,\n","                      0.20164109766483307,\n","                      0.20013409852981567,\n","                      0.2058783769607544,\n","                      0.19983665645122528,\n","                      0.1949453502893448],\n","                     [0.3350255787372589,\n","                      0.3119557797908783,\n","                      0.26892194151878357,\n","                      0.25595182180404663,\n","                      0.24310451745986938,\n","                      0.2395530641078949,\n","                      0.2365560233592987,\n","                      0.23485369980335236,\n","                      0.22240488231182098,\n","                      0.22830656170845032,\n","                      0.21627084910869598,\n","                      0.21741247177124023,\n","                      0.2094232141971588,\n","                      0.2149292379617691,\n","                      0.2108398973941803,\n","                      0.2046569436788559,\n","                      0.20241469144821167,\n","                      0.20141391456127167,\n","                      0.19778142869472504,\n","                      0.1953219175338745],\n","                     [0.3465191721916199,\n","                      0.3049868941307068,\n","                      0.29881349205970764,\n","                      0.2767832577228546,\n","                      0.2792609632015228,\n","                      0.25898221135139465,\n","                      0.2490481734275818,\n","                      0.2460358440876007,\n","                      0.23968860507011414,\n","                      0.2450195550918579,\n","                      0.22341874241828918,\n","                      0.24181069433689117,\n","                      0.21669228374958038,\n","                      0.21691688895225525,\n","                      0.21186304092407227,\n","                      0.20836088061332703,\n","                      0.21439684927463531,\n","                      0.20248357951641083,\n","                      0.19564013183116913,\n","                      0.19542422890663147]],\n"," 'Validation MCC': [[np.float64(0.8082365471985826),\n","                     np.float64(0.8270313642532323),\n","                     np.float64(0.8346423012335927),\n","                     np.float64(0.8468110203869194),\n","                     np.float64(0.8552079885899067),\n","                     np.float64(0.8546521753319698),\n","                     np.float64(0.8600084429258433),\n","                     np.float64(0.8632966151671636),\n","                     np.float64(0.8706090404724319),\n","                     np.float64(0.870640879332092),\n","                     np.float64(0.8735524148355036),\n","                     np.float64(0.8743308031307014),\n","                     np.float64(0.8758957103240054),\n","                     np.float64(0.8780321287084075),\n","                     np.float64(0.8784514645890541),\n","                     np.float64(0.8690403587154255),\n","                     np.float64(0.8776921951909602),\n","                     np.float64(0.8833703373926913),\n","                     np.float64(0.8837651328757936),\n","                     np.float64(0.8812400343058551)],\n","                    [np.float64(0.807406293572974),\n","                     np.float64(0.8356928326798231),\n","                     np.float64(0.8457594290567791),\n","                     np.float64(0.8443590787403663),\n","                     np.float64(0.846026736336286),\n","                     np.float64(0.8586159909978986),\n","                     np.float64(0.8637345750479146),\n","                     np.float64(0.8649496955540349),\n","                     np.float64(0.868709937664263),\n","                     np.float64(0.8696844177552924),\n","                     np.float64(0.8748088034821996),\n","                     np.float64(0.8729670728110047),\n","                     np.float64(0.8770376115187262),\n","                     np.float64(0.8788949377909018),\n","                     np.float64(0.8787001373601692),\n","                     np.float64(0.882807480763775),\n","                     np.float64(0.8824048430038302),\n","                     np.float64(0.8788690013339228),\n","                     np.float64(0.8776357140047162),\n","                     np.float64(0.8826276193145361)],\n","                    [np.float64(0.7992842167100802),\n","                     np.float64(0.8167738885262958),\n","                     np.float64(0.8305190095568085),\n","                     np.float64(0.8358894806675246),\n","                     np.float64(0.8456298236390924),\n","                     np.float64(0.8457688612240418),\n","                     np.float64(0.8485184354397431),\n","                     np.float64(0.8514020901492702),\n","                     np.float64(0.8630584018401142),\n","                     np.float64(0.8593191563528619),\n","                     np.float64(0.8423808348942246),\n","                     np.float64(0.8638826563021877),\n","                     np.float64(0.863027878975521),\n","                     np.float64(0.8709298290863086),\n","                     np.float64(0.8736048842594324),\n","                     np.float64(0.8763214737790028),\n","                     np.float64(0.8765531987957895),\n","                     np.float64(0.8741945491146884),\n","                     np.float64(0.877254014804989),\n","                     np.float64(0.88077570585433)],\n","                    [np.float64(0.8044202618367303),\n","                     np.float64(0.8147175954528798),\n","                     np.float64(0.8411180733750546),\n","                     np.float64(0.8483978955292684),\n","                     np.float64(0.8550800961974153),\n","                     np.float64(0.8566978696252489),\n","                     np.float64(0.856881105014298),\n","                     np.float64(0.8595436291446171),\n","                     np.float64(0.8665629943597389),\n","                     np.float64(0.8648951743617886),\n","                     np.float64(0.8708465212528533),\n","                     np.float64(0.869914454073945),\n","                     np.float64(0.8729098177133354),\n","                     np.float64(0.8695552904028802),\n","                     np.float64(0.8726902338778229),\n","                     np.float64(0.8756798915690683),\n","                     np.float64(0.877011514666482),\n","                     np.float64(0.8777398872374469),\n","                     np.float64(0.8798297351104742),\n","                     np.float64(0.881858239524071)],\n","                    [np.float64(0.8011903157889653),\n","                     np.float64(0.82173224429984),\n","                     np.float64(0.8263708938062286),\n","                     np.float64(0.8383146186182262),\n","                     np.float64(0.8379350963173757),\n","                     np.float64(0.8478265741518588),\n","                     np.float64(0.8538775266216532),\n","                     np.float64(0.8549503052943735),\n","                     np.float64(0.8588143786642625),\n","                     np.float64(0.8530122095813502),\n","                     np.float64(0.8652591511691591),\n","                     np.float64(0.859112935162223),\n","                     np.float64(0.8683051290374343),\n","                     np.float64(0.8694404385536497),\n","                     np.float64(0.8712352272163307),\n","                     np.float64(0.8740001568696653),\n","                     np.float64(0.8701867877098842),\n","                     np.float64(0.8768093506150286),\n","                     np.float64(0.8805677317038825),\n","                     np.float64(0.8801945354006611)]]}\n","Training Model: BiLSTM, Fold: 1\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7690 - loss: 0.6378(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8206\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 37ms/step - accuracy: 0.7691 - loss: 0.6375 - val_accuracy: 0.8829 - val_loss: 0.3057 - mcc: 0.8206\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8884 - loss: 0.2945(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8365\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8884 - loss: 0.2944 - val_accuracy: 0.8925 - val_loss: 0.2939 - mcc: 0.8365\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8993 - loss: 0.2665(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8651\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.8993 - loss: 0.2665 - val_accuracy: 0.9115 - val_loss: 0.2320 - mcc: 0.8651\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9073 - loss: 0.2432(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8601\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9073 - loss: 0.2432 - val_accuracy: 0.9079 - val_loss: 0.2476 - mcc: 0.8601\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8979 - loss: 0.2821(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8689\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8979 - loss: 0.2821 - val_accuracy: 0.9142 - val_loss: 0.2248 - mcc: 0.8689\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9106 - loss: 0.2337(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8728\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9106 - loss: 0.2337 - val_accuracy: 0.9170 - val_loss: 0.2182 - mcc: 0.8728\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9157 - loss: 0.2177(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8754\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9157 - loss: 0.2176 - val_accuracy: 0.9182 - val_loss: 0.2145 - mcc: 0.8754\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9187 - loss: 0.2099(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8769\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.9187 - loss: 0.2100 - val_accuracy: 0.9194 - val_loss: 0.2105 - mcc: 0.8769\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9199 - loss: 0.2070(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8825\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9199 - loss: 0.2070 - val_accuracy: 0.9227 - val_loss: 0.1979 - mcc: 0.8825\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9194 - loss: 0.2075(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8814\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9194 - loss: 0.2075 - val_accuracy: 0.9219 - val_loss: 0.2005 - mcc: 0.8814\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9217 - loss: 0.1997(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8847\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9217 - loss: 0.1997 - val_accuracy: 0.9237 - val_loss: 0.1951 - mcc: 0.8847\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9241 - loss: 0.1940(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8915\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9241 - loss: 0.1940 - val_accuracy: 0.9289 - val_loss: 0.1802 - mcc: 0.8915\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9273 - loss: 0.1841(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8924\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9273 - loss: 0.1841 - val_accuracy: 0.9295 - val_loss: 0.1796 - mcc: 0.8924\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9262 - loss: 0.1870(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8932\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9262 - loss: 0.1870 - val_accuracy: 0.9299 - val_loss: 0.1775 - mcc: 0.8932\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9301 - loss: 0.1767(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8929\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9301 - loss: 0.1767 - val_accuracy: 0.9298 - val_loss: 0.1753 - mcc: 0.8929\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9298 - loss: 0.1775(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8954\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9298 - loss: 0.1775 - val_accuracy: 0.9310 - val_loss: 0.1726 - mcc: 0.8954\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9298 - loss: 0.1752(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8967\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9298 - loss: 0.1752 - val_accuracy: 0.9320 - val_loss: 0.1700 - mcc: 0.8967\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9325 - loss: 0.1681(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8992\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9325 - loss: 0.1681 - val_accuracy: 0.9337 - val_loss: 0.1658 - mcc: 0.8992\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9324 - loss: 0.1679(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8987\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9324 - loss: 0.1679 - val_accuracy: 0.9335 - val_loss: 0.1647 - mcc: 0.8987\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9320 - loss: 0.1702(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9007\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9320 - loss: 0.1702 - val_accuracy: 0.9346 - val_loss: 0.1639 - mcc: 0.9007\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: BiLSTM, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7610 - loss: 0.6549(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8348\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.7611 - loss: 0.6547 - val_accuracy: 0.8926 - val_loss: 0.2867 - mcc: 0.8348\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8903 - loss: 0.2915(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8520\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.8903 - loss: 0.2915 - val_accuracy: 0.9033 - val_loss: 0.2598 - mcc: 0.8520\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9000 - loss: 0.2668(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8641\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9000 - loss: 0.2668 - val_accuracy: 0.9111 - val_loss: 0.2331 - mcc: 0.8641\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9108 - loss: 0.2333(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8591\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9108 - loss: 0.2333 - val_accuracy: 0.9084 - val_loss: 0.2455 - mcc: 0.8591\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9110 - loss: 0.2347(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8636\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.9110 - loss: 0.2347 - val_accuracy: 0.9110 - val_loss: 0.2364 - mcc: 0.8636\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9119 - loss: 0.2331(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8746\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9119 - loss: 0.2331 - val_accuracy: 0.9181 - val_loss: 0.2118 - mcc: 0.8746\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9155 - loss: 0.2184(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8839\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9155 - loss: 0.2184 - val_accuracy: 0.9241 - val_loss: 0.1965 - mcc: 0.8839\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9218 - loss: 0.2018(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8816\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9218 - loss: 0.2019 - val_accuracy: 0.9226 - val_loss: 0.2012 - mcc: 0.8816\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9232 - loss: 0.1976(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8901\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9232 - loss: 0.1976 - val_accuracy: 0.9281 - val_loss: 0.1819 - mcc: 0.8901\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9207 - loss: 0.2047(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8928\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9207 - loss: 0.2046 - val_accuracy: 0.9297 - val_loss: 0.1793 - mcc: 0.8928\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9272 - loss: 0.1853(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8945\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9272 - loss: 0.1853 - val_accuracy: 0.9311 - val_loss: 0.1749 - mcc: 0.8945\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9276 - loss: 0.1834(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8930\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.9276 - loss: 0.1834 - val_accuracy: 0.9298 - val_loss: 0.1771 - mcc: 0.8930\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9281 - loss: 0.1819(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8875\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9281 - loss: 0.1819 - val_accuracy: 0.9267 - val_loss: 0.1891 - mcc: 0.8875\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9297 - loss: 0.1760(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8976\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9297 - loss: 0.1760 - val_accuracy: 0.9331 - val_loss: 0.1663 - mcc: 0.8976\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9321 - loss: 0.1696(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8924\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9321 - loss: 0.1696 - val_accuracy: 0.9291 - val_loss: 0.1796 - mcc: 0.8924\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9335 - loss: 0.1657(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8988\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9335 - loss: 0.1657 - val_accuracy: 0.9340 - val_loss: 0.1650 - mcc: 0.8988\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9347 - loss: 0.1636(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8870\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.9347 - loss: 0.1636 - val_accuracy: 0.9260 - val_loss: 0.1915 - mcc: 0.8870\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9316 - loss: 0.1732(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9005\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9316 - loss: 0.1732 - val_accuracy: 0.9349 - val_loss: 0.1618 - mcc: 0.9005\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9348 - loss: 0.1624(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9027\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9348 - loss: 0.1624 - val_accuracy: 0.9364 - val_loss: 0.1581 - mcc: 0.9027\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9362 - loss: 0.1582(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9029\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9362 - loss: 0.1582 - val_accuracy: 0.9366 - val_loss: 0.1581 - mcc: 0.9029\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: BiLSTM, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7519 - loss: 0.6477(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8121\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 39ms/step - accuracy: 0.7521 - loss: 0.6472 - val_accuracy: 0.8780 - val_loss: 0.3226 - mcc: 0.8121\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8872 - loss: 0.2976(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8473\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8872 - loss: 0.2975 - val_accuracy: 0.9000 - val_loss: 0.2625 - mcc: 0.8473\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9034 - loss: 0.2549(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8631\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9034 - loss: 0.2549 - val_accuracy: 0.9101 - val_loss: 0.2364 - mcc: 0.8631\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9125 - loss: 0.2304(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8650\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9125 - loss: 0.2304 - val_accuracy: 0.9108 - val_loss: 0.2292 - mcc: 0.8650\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9161 - loss: 0.2190(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8713\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9161 - loss: 0.2190 - val_accuracy: 0.9156 - val_loss: 0.2182 - mcc: 0.8713\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9153 - loss: 0.2206(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8749\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9153 - loss: 0.2206 - val_accuracy: 0.9181 - val_loss: 0.2122 - mcc: 0.8749\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9149 - loss: 0.2218(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8747\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9149 - loss: 0.2218 - val_accuracy: 0.9179 - val_loss: 0.2107 - mcc: 0.8747\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9160 - loss: 0.2192(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8784\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9160 - loss: 0.2192 - val_accuracy: 0.9203 - val_loss: 0.2046 - mcc: 0.8784\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9210 - loss: 0.2048(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8857\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 39ms/step - accuracy: 0.9210 - loss: 0.2048 - val_accuracy: 0.9252 - val_loss: 0.1910 - mcc: 0.8857\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9254 - loss: 0.1901(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8191\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9254 - loss: 0.1901 - val_accuracy: 0.8822 - val_loss: 0.3087 - mcc: 0.8191\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9044 - loss: 0.2506(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8730\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9044 - loss: 0.2505 - val_accuracy: 0.9168 - val_loss: 0.2138 - mcc: 0.8730\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9209 - loss: 0.2012(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8825\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9209 - loss: 0.2012 - val_accuracy: 0.9228 - val_loss: 0.1935 - mcc: 0.8825\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9232 - loss: 0.1964(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8829\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9232 - loss: 0.1964 - val_accuracy: 0.9230 - val_loss: 0.1959 - mcc: 0.8829\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9253 - loss: 0.1908(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8740\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9253 - loss: 0.1908 - val_accuracy: 0.9175 - val_loss: 0.2128 - mcc: 0.8740\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9276 - loss: 0.1849(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8924\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9276 - loss: 0.1849 - val_accuracy: 0.9293 - val_loss: 0.1774 - mcc: 0.8924\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9308 - loss: 0.1730(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8951\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9308 - loss: 0.1730 - val_accuracy: 0.9311 - val_loss: 0.1716 - mcc: 0.8951\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9322 - loss: 0.1691(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8973\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9322 - loss: 0.1691 - val_accuracy: 0.9326 - val_loss: 0.1686 - mcc: 0.8973\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9303 - loss: 0.1768(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8932\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9303 - loss: 0.1768 - val_accuracy: 0.9300 - val_loss: 0.1775 - mcc: 0.8932\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9327 - loss: 0.1694(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8980\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 40ms/step - accuracy: 0.9327 - loss: 0.1694 - val_accuracy: 0.9328 - val_loss: 0.1654 - mcc: 0.8980\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9357 - loss: 0.1599(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8989\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 36ms/step - accuracy: 0.9357 - loss: 0.1599 - val_accuracy: 0.9335 - val_loss: 0.1649 - mcc: 0.8989\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: BiLSTM, Fold: 4\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7651 - loss: 0.6421(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8182\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 40ms/step - accuracy: 0.7652 - loss: 0.6418 - val_accuracy: 0.8818 - val_loss: 0.3075 - mcc: 0.8182\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8883 - loss: 0.2915(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8346\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 34ms/step - accuracy: 0.8883 - loss: 0.2915 - val_accuracy: 0.8922 - val_loss: 0.2915 - mcc: 0.8346\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8994 - loss: 0.2657(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8531\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8994 - loss: 0.2657 - val_accuracy: 0.9042 - val_loss: 0.2494 - mcc: 0.8531\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9071 - loss: 0.2425(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8696\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9071 - loss: 0.2425 - val_accuracy: 0.9147 - val_loss: 0.2213 - mcc: 0.8696\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9066 - loss: 0.2439(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8627\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9065 - loss: 0.2439 - val_accuracy: 0.9105 - val_loss: 0.2313 - mcc: 0.8627\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9122 - loss: 0.2271(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8621\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9122 - loss: 0.2271 - val_accuracy: 0.9101 - val_loss: 0.2400 - mcc: 0.8621\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9157 - loss: 0.2187(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8783\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9157 - loss: 0.2187 - val_accuracy: 0.9201 - val_loss: 0.2050 - mcc: 0.8783\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9098 - loss: 0.2354(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8731\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.9098 - loss: 0.2354 - val_accuracy: 0.9174 - val_loss: 0.2153 - mcc: 0.8731\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9194 - loss: 0.2082(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8802\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9194 - loss: 0.2082 - val_accuracy: 0.9219 - val_loss: 0.1992 - mcc: 0.8802\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9224 - loss: 0.1977(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8812\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9224 - loss: 0.1977 - val_accuracy: 0.9224 - val_loss: 0.1994 - mcc: 0.8812\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9248 - loss: 0.1889(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8882\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9248 - loss: 0.1889 - val_accuracy: 0.9267 - val_loss: 0.1854 - mcc: 0.8882\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9235 - loss: 0.1945(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8895\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9235 - loss: 0.1945 - val_accuracy: 0.9278 - val_loss: 0.1841 - mcc: 0.8895\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9257 - loss: 0.1903(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8870\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9257 - loss: 0.1903 - val_accuracy: 0.9261 - val_loss: 0.1888 - mcc: 0.8870\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9245 - loss: 0.1920(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8835\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.9245 - loss: 0.1920 - val_accuracy: 0.9232 - val_loss: 0.1974 - mcc: 0.8835\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9266 - loss: 0.1855(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8822\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 36ms/step - accuracy: 0.9266 - loss: 0.1856 - val_accuracy: 0.9228 - val_loss: 0.1986 - mcc: 0.8822\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9250 - loss: 0.1920(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8901\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9250 - loss: 0.1920 - val_accuracy: 0.9281 - val_loss: 0.1836 - mcc: 0.8901\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9309 - loss: 0.1734(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8877\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9309 - loss: 0.1734 - val_accuracy: 0.9265 - val_loss: 0.1853 - mcc: 0.8877\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9324 - loss: 0.1694(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8860\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9324 - loss: 0.1694 - val_accuracy: 0.9255 - val_loss: 0.1873 - mcc: 0.8860\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9331 - loss: 0.1668(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8961\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 39ms/step - accuracy: 0.9331 - loss: 0.1668 - val_accuracy: 0.9319 - val_loss: 0.1705 - mcc: 0.8961\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9344 - loss: 0.1636(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8974\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.9344 - loss: 0.1636 - val_accuracy: 0.9326 - val_loss: 0.1714 - mcc: 0.8974\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: BiLSTM, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7704 - loss: 0.6354(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8228\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - accuracy: 0.7705 - loss: 0.6351 - val_accuracy: 0.8848 - val_loss: 0.3066 - mcc: 0.8228\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8936 - loss: 0.2812(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8411\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.8936 - loss: 0.2811 - val_accuracy: 0.8964 - val_loss: 0.2765 - mcc: 0.8411\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9063 - loss: 0.2485(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8629\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9063 - loss: 0.2485 - val_accuracy: 0.9102 - val_loss: 0.2344 - mcc: 0.8629\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9107 - loss: 0.2377(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8468\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9107 - loss: 0.2377 - val_accuracy: 0.8995 - val_loss: 0.2647 - mcc: 0.8468\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9114 - loss: 0.2399(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8620\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9114 - loss: 0.2399 - val_accuracy: 0.9098 - val_loss: 0.2397 - mcc: 0.8620\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9158 - loss: 0.2226(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8717\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9158 - loss: 0.2226 - val_accuracy: 0.9160 - val_loss: 0.2225 - mcc: 0.8717\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9205 - loss: 0.2088(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8729\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9205 - loss: 0.2088 - val_accuracy: 0.9168 - val_loss: 0.2149 - mcc: 0.8729\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9221 - loss: 0.2021(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8795\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9221 - loss: 0.2021 - val_accuracy: 0.9212 - val_loss: 0.2030 - mcc: 0.8795\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9214 - loss: 0.2054(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8784\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9214 - loss: 0.2054 - val_accuracy: 0.9203 - val_loss: 0.2082 - mcc: 0.8784\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9235 - loss: 0.1986(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8779\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.9235 - loss: 0.1986 - val_accuracy: 0.9198 - val_loss: 0.2082 - mcc: 0.8779\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9260 - loss: 0.1903(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8844\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9260 - loss: 0.1902 - val_accuracy: 0.9242 - val_loss: 0.1925 - mcc: 0.8844\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9278 - loss: 0.1855(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8859\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9278 - loss: 0.1856 - val_accuracy: 0.9248 - val_loss: 0.1902 - mcc: 0.8859\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9284 - loss: 0.1834(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8871\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9284 - loss: 0.1834 - val_accuracy: 0.9253 - val_loss: 0.1884 - mcc: 0.8871\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9300 - loss: 0.1784(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8882\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.9300 - loss: 0.1784 - val_accuracy: 0.9267 - val_loss: 0.1867 - mcc: 0.8882\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9324 - loss: 0.1711(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8897\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9324 - loss: 0.1711 - val_accuracy: 0.9278 - val_loss: 0.1839 - mcc: 0.8897\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9341 - loss: 0.1675(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8940\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9341 - loss: 0.1675 - val_accuracy: 0.9304 - val_loss: 0.1744 - mcc: 0.8940\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9327 - loss: 0.1698(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8916\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9327 - loss: 0.1698 - val_accuracy: 0.9285 - val_loss: 0.1808 - mcc: 0.8916\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9333 - loss: 0.1698(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8958\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9333 - loss: 0.1698 - val_accuracy: 0.9316 - val_loss: 0.1698 - mcc: 0.8958\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9327 - loss: 0.1702(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8921\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9327 - loss: 0.1702 - val_accuracy: 0.9292 - val_loss: 0.1799 - mcc: 0.8921\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9349 - loss: 0.1664(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8975\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9349 - loss: 0.1663 - val_accuracy: 0.9326 - val_loss: 0.1670 - mcc: 0.8975\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["{'Accuracy': {'max': np.float64(0.9365886666666666),\n","              'mean': np.float64(0.9339582666666667),\n","              'min': np.float64(0.932564),\n","              'std': np.float64(0.001512672548835349)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0010033814112345378),\n","                               'mean': np.float64(0.0007242165247599284),\n","                               'min': np.float64(0.0005463913281758626),\n","                               'std': np.float64(0.00021368509571106014)},\n"," 'MCC': {'max': np.float64(0.902906850488781),\n","         'mean': np.float64(0.8994807177786868),\n","         'min': np.float64(0.8973597843398651),\n","         'std': np.float64(0.0020965527595715482)},\n"," 'Parameters': 15173,\n"," 'Train Time (s)': {'max': np.float64(795.5878756046295),\n","                    'mean': np.float64(732.5600620746612),\n","                    'min': np.float64(707.3395938873291),\n","                    'std': np.float64(32.168503842329095)},\n"," 'Training Accuracy': [[0.8348616361618042,\n","                        0.8915862441062927,\n","                        0.9034662842750549,\n","                        0.9060332775115967,\n","                        0.9043388962745667,\n","                        0.9131035208702087,\n","                        0.9167132377624512,\n","                        0.9159115552902222,\n","                        0.9206769466400146,\n","                        0.9206416606903076,\n","                        0.9215141534805298,\n","                        0.9249281287193298,\n","                        0.926472008228302,\n","                        0.9270458221435547,\n","                        0.9289211630821228,\n","                        0.9304094910621643,\n","                        0.9307771921157837,\n","                        0.9326527714729309,\n","                        0.933392345905304,\n","                        0.9335601925849915],\n","                       [0.8395441174507141,\n","                        0.8930782079696655,\n","                        0.904432475566864,\n","                        0.9109171032905579,\n","                        0.9113021492958069,\n","                        0.9145748019218445,\n","                        0.9178402423858643,\n","                        0.9192160367965698,\n","                        0.9237457513809204,\n","                        0.9233734011650085,\n","                        0.9272627830505371,\n","                        0.9280807971954346,\n","                        0.9291048049926758,\n","                        0.9302722215652466,\n","                        0.9320492148399353,\n","                        0.9328508377075195,\n","                        0.9318913817405701,\n","                        0.9330598711967468,\n","                        0.9351150989532471,\n","                        0.9353212714195251],\n","                       [0.8347768783569336,\n","                        0.8939151167869568,\n","                        0.9059137105941772,\n","                        0.9120388031005859,\n","                        0.9134333729743958,\n","                        0.9141052961349487,\n","                        0.9159352779388428,\n","                        0.9148547649383545,\n","                        0.9225673079490662,\n","                        0.9199974536895752,\n","                        0.9103648066520691,\n","                        0.921692967414856,\n","                        0.923517644405365,\n","                        0.9251115918159485,\n","                        0.928612470626831,\n","                        0.9310063719749451,\n","                        0.9322928190231323,\n","                        0.9301509857177734,\n","                        0.9331033825874329,\n","                        0.9344379305839539],\n","                       [0.8404617309570312,\n","                        0.8901164531707764,\n","                        0.9005122184753418,\n","                        0.908136785030365,\n","                        0.9042192101478577,\n","                        0.9116988778114319,\n","                        0.9177444577217102,\n","                        0.9099417924880981,\n","                        0.9201368093490601,\n","                        0.9233766794204712,\n","                        0.9259899854660034,\n","                        0.9249924421310425,\n","                        0.9252521395683289,\n","                        0.925552487373352,\n","                        0.9252697229385376,\n","                        0.9250929951667786,\n","                        0.9304854273796082,\n","                        0.9313538670539856,\n","                        0.9319567084312439,\n","                        0.9333134889602661],\n","                       [0.843974769115448,\n","                        0.8971009254455566,\n","                        0.9084574580192566,\n","                        0.9077488780021667,\n","                        0.9101250767707825,\n","                        0.9152936935424805,\n","                        0.920434832572937,\n","                        0.922655463218689,\n","                        0.9194498658180237,\n","                        0.9249122142791748,\n","                        0.9269440770149231,\n","                        0.9270724058151245,\n","                        0.9286269545555115,\n","                        0.9295682907104492,\n","                        0.9320794343948364,\n","                        0.9331645369529724,\n","                        0.9320598840713501,\n","                        0.933393657207489,\n","                        0.9326878190040588,\n","                        0.9347859025001526]],\n"," 'Training Loss': [[0.44500964879989624,\n","                    0.28544777631759644,\n","                    0.2539207935333252,\n","                    0.24885514378547668,\n","                    0.2591286897659302,\n","                    0.22721950709819794,\n","                    0.21553796529769897,\n","                    0.21666550636291504,\n","                    0.20363137125968933,\n","                    0.20325689017772675,\n","                    0.20104433596134186,\n","                    0.19113792479038239,\n","                    0.18630224466323853,\n","                    0.1849130243062973,\n","                    0.17893730103969574,\n","                    0.17482508718967438,\n","                    0.17278780043125153,\n","                    0.16811132431030273,\n","                    0.16574618220329285,\n","                    0.16490626335144043],\n","                   [0.43495944142341614,\n","                    0.284837007522583,\n","                    0.2536388039588928,\n","                    0.2333938628435135,\n","                    0.23351295292377472,\n","                    0.2244013249874115,\n","                    0.2133401483297348,\n","                    0.21034954488277435,\n","                    0.19596928358078003,\n","                    0.1970060020685196,\n","                    0.18449924886226654,\n","                    0.18149054050445557,\n","                    0.178996741771698,\n","                    0.17472833395004272,\n","                    0.16954846680164337,\n","                    0.16684482991695404,\n","                    0.17197808623313904,\n","                    0.16782106459140778,\n","                    0.16160985827445984,\n","                    0.16061095893383026],\n","                   [0.4391034245491028,\n","                    0.28019216656684875,\n","                    0.2479201853275299,\n","                    0.2302122563123703,\n","                    0.22653250396251678,\n","                    0.22336816787719727,\n","                    0.21872633695602417,\n","                    0.22318151593208313,\n","                    0.2000812441110611,\n","                    0.20621690154075623,\n","                    0.23304681479930878,\n","                    0.20026934146881104,\n","                    0.19621247053146362,\n","                    0.19151455163955688,\n","                    0.18088431656360626,\n","                    0.17287655174732208,\n","                    0.16939865052700043,\n","                    0.17752742767333984,\n","                    0.16742244362831116,\n","                    0.1634349226951599],\n","                   [0.4297642111778259,\n","                    0.2882141172885895,\n","                    0.2623707354068756,\n","                    0.24010612070560455,\n","                    0.25191372632980347,\n","                    0.23024380207061768,\n","                    0.21237681806087494,\n","                    0.23495705425739288,\n","                    0.20570138096809387,\n","                    0.1952151656150818,\n","                    0.18663527071475983,\n","                    0.1911686211824417,\n","                    0.19179312884807587,\n","                    0.1901739239692688,\n","                    0.18997085094451904,\n","                    0.19125226140022278,\n","                    0.17408882081508636,\n","                    0.17191730439662933,\n","                    0.16918686032295227,\n","                    0.1661149263381958],\n","                   [0.4230903387069702,\n","                    0.2723237872123718,\n","                    0.24132071435451508,\n","                    0.2463921159505844,\n","                    0.242584228515625,\n","                    0.22347469627857208,\n","                    0.20821785926818848,\n","                    0.19983461499214172,\n","                    0.21083864569664001,\n","                    0.1943145990371704,\n","                    0.18836313486099243,\n","                    0.1883525550365448,\n","                    0.1826532632112503,\n","                    0.18017223477363586,\n","                    0.17223770916461945,\n","                    0.16904541850090027,\n","                    0.17304189503192902,\n","                    0.1688864529132843,\n","                    0.17134886980056763,\n","                    0.1654367595911026]],\n"," 'Validation Accuracy': [[0.8829463720321655,\n","                          0.8925487399101257,\n","                          0.9114636778831482,\n","                          0.9079354405403137,\n","                          0.9141912460327148,\n","                          0.916962742805481,\n","                          0.9181583523750305,\n","                          0.9193952679634094,\n","                          0.9227281808853149,\n","                          0.921855628490448,\n","                          0.9236980080604553,\n","                          0.9288846850395203,\n","                          0.9294573664665222,\n","                          0.9299252033233643,\n","                          0.9298006296157837,\n","                          0.9310460686683655,\n","                          0.9319642186164856,\n","                          0.9337000846862793,\n","                          0.9335200786590576,\n","                          0.9345983266830444],\n","                         [0.8925702571868896,\n","                          0.9032528400421143,\n","                          0.9111127257347107,\n","                          0.9083741307258606,\n","                          0.9109657406806946,\n","                          0.9180737137794495,\n","                          0.9240630865097046,\n","                          0.9225831031799316,\n","                          0.9280501008033752,\n","                          0.9297210574150085,\n","                          0.9310822486877441,\n","                          0.9297705292701721,\n","                          0.9266619682312012,\n","                          0.9331122040748596,\n","                          0.9290878176689148,\n","                          0.9339640140533447,\n","                          0.9260371327400208,\n","                          0.9349374771118164,\n","                          0.9364413022994995,\n","                          0.936588704586029],\n","                         [0.8780454397201538,\n","                          0.9000032544136047,\n","                          0.9101207852363586,\n","                          0.9108031988143921,\n","                          0.9156360030174255,\n","                          0.9181140065193176,\n","                          0.9179233312606812,\n","                          0.9202578067779541,\n","                          0.9251587986946106,\n","                          0.8822134733200073,\n","                          0.9168095588684082,\n","                          0.9227813482284546,\n","                          0.9230355024337769,\n","                          0.9175038933753967,\n","                          0.9292612075805664,\n","                          0.9310859441757202,\n","                          0.9325646162033081,\n","                          0.9299584031105042,\n","                          0.9328446388244629,\n","                          0.9334730505943298],\n","                         [0.8818238973617554,\n","                          0.8922482132911682,\n","                          0.9042004942893982,\n","                          0.9147343039512634,\n","                          0.9105188250541687,\n","                          0.9101303815841675,\n","                          0.9201087951660156,\n","                          0.9173955321311951,\n","                          0.9218831658363342,\n","                          0.922377347946167,\n","                          0.9267134070396423,\n","                          0.9277514815330505,\n","                          0.9261118769645691,\n","                          0.923240065574646,\n","                          0.9227998852729797,\n","                          0.9281275272369385,\n","                          0.926482081413269,\n","                          0.9254857301712036,\n","                          0.9318618178367615,\n","                          0.9325639605522156],\n","                         [0.8847780227661133,\n","                          0.8964114785194397,\n","                          0.9101992249488831,\n","                          0.8994659781455994,\n","                          0.9097887277603149,\n","                          0.9159739017486572,\n","                          0.9167900681495667,\n","                          0.9211781620979309,\n","                          0.9202961325645447,\n","                          0.9197584390640259,\n","                          0.9242085218429565,\n","                          0.9247705340385437,\n","                          0.9252665042877197,\n","                          0.926721453666687,\n","                          0.9277527928352356,\n","                          0.9303720593452454,\n","                          0.9285392165184021,\n","                          0.9316034317016602,\n","                          0.9291720390319824,\n","                          0.9325677752494812]],\n"," 'Validation Loss': [[0.3057374954223633,\n","                      0.29391714930534363,\n","                      0.2319725602865219,\n","                      0.24755367636680603,\n","                      0.2248149812221527,\n","                      0.2181805670261383,\n","                      0.21450094878673553,\n","                      0.21054646372795105,\n","                      0.19789807498455048,\n","                      0.20051869750022888,\n","                      0.1951221376657486,\n","                      0.18021883070468903,\n","                      0.1796250194311142,\n","                      0.1775413453578949,\n","                      0.1753462255001068,\n","                      0.17258121073246002,\n","                      0.16996780037879944,\n","                      0.1657743602991104,\n","                      0.16469113528728485,\n","                      0.1638762503862381],\n","                     [0.2866520881652832,\n","                      0.25984302163124084,\n","                      0.23305682837963104,\n","                      0.24552325904369354,\n","                      0.2363627701997757,\n","                      0.2117636352777481,\n","                      0.19649310410022736,\n","                      0.20117278397083282,\n","                      0.1819269359111786,\n","                      0.17933832108974457,\n","                      0.17492663860321045,\n","                      0.17708294093608856,\n","                      0.18912740051746368,\n","                      0.16627153754234314,\n","                      0.17962725460529327,\n","                      0.1649702787399292,\n","                      0.19147971272468567,\n","                      0.1617673635482788,\n","                      0.15808767080307007,\n","                      0.15805697441101074],\n","                     [0.3226073980331421,\n","                      0.26253917813301086,\n","                      0.2363572120666504,\n","                      0.22918573021888733,\n","                      0.21824724972248077,\n","                      0.21215079724788666,\n","                      0.21070744097232819,\n","                      0.2046072632074356,\n","                      0.19103024899959564,\n","                      0.30874544382095337,\n","                      0.2138008177280426,\n","                      0.193516343832016,\n","                      0.1959407776594162,\n","                      0.2128118872642517,\n","                      0.17735564708709717,\n","                      0.1715632677078247,\n","                      0.1686161309480667,\n","                      0.1774984896183014,\n","                      0.16542166471481323,\n","                      0.16488975286483765],\n","                     [0.30749258399009705,\n","                      0.2914523780345917,\n","                      0.24939210712909698,\n","                      0.2212790548801422,\n","                      0.23134341835975647,\n","                      0.23995602130889893,\n","                      0.20495036244392395,\n","                      0.2153465747833252,\n","                      0.19923463463783264,\n","                      0.199410542845726,\n","                      0.18542569875717163,\n","                      0.18413816392421722,\n","                      0.18884941935539246,\n","                      0.19740897417068481,\n","                      0.1986469328403473,\n","                      0.18356233835220337,\n","                      0.1853352189064026,\n","                      0.18728260695934296,\n","                      0.17052140831947327,\n","                      0.17142748832702637],\n","                     [0.30657070875167847,\n","                      0.2764992415904999,\n","                      0.2344089299440384,\n","                      0.26468566060066223,\n","                      0.23965176939964294,\n","                      0.22248762845993042,\n","                      0.21485355496406555,\n","                      0.2030048966407776,\n","                      0.2082226723432541,\n","                      0.20822136104106903,\n","                      0.19246385991573334,\n","                      0.19018226861953735,\n","                      0.1883721500635147,\n","                      0.186655193567276,\n","                      0.18390050530433655,\n","                      0.17436374723911285,\n","                      0.18080255389213562,\n","                      0.16978079080581665,\n","                      0.17990392446517944,\n","                      0.1669645458459854]],\n"," 'Validation MCC': [[np.float64(0.8205660345471402),\n","                     np.float64(0.8364538117318348),\n","                     np.float64(0.8651448852221236),\n","                     np.float64(0.8601439052613654),\n","                     np.float64(0.8689396835410098),\n","                     np.float64(0.872826808243866),\n","                     np.float64(0.8754188995554999),\n","                     np.float64(0.8768708635703477),\n","                     np.float64(0.8824942562291512),\n","                     np.float64(0.8813806156682142),\n","                     np.float64(0.8846680801581164),\n","                     np.float64(0.8915105748314991),\n","                     np.float64(0.8923700952540654),\n","                     np.float64(0.8932444336666528),\n","                     np.float64(0.892936518889245),\n","                     np.float64(0.8953625378212721),\n","                     np.float64(0.89665982074395),\n","                     np.float64(0.8992064153707597),\n","                     np.float64(0.8986603660890271),\n","                     np.float64(0.9007219595411058)],\n","                    [np.float64(0.8347786270245982),\n","                     np.float64(0.8520012813554252),\n","                     np.float64(0.8640796421777031),\n","                     np.float64(0.8591114415721895),\n","                     np.float64(0.8635713742486678),\n","                     np.float64(0.8745725810111806),\n","                     np.float64(0.8839288691774735),\n","                     np.float64(0.8816067074774289),\n","                     np.float64(0.8900619984452404),\n","                     np.float64(0.892765310323888),\n","                     np.float64(0.8944561976571201),\n","                     np.float64(0.892984306114358),\n","                     np.float64(0.8875289516852811),\n","                     np.float64(0.8976379638701617),\n","                     np.float64(0.8923619755052582),\n","                     np.float64(0.898766557472469),\n","                     np.float64(0.8869624604264791),\n","                     np.float64(0.9005002870381424),\n","                     np.float64(0.9027057919282467),\n","                     np.float64(0.902906850488781)],\n","                    [np.float64(0.812144247178058),\n","                     np.float64(0.8472820621820958),\n","                     np.float64(0.8631350079531016),\n","                     np.float64(0.8650279602496886),\n","                     np.float64(0.8713311946961786),\n","                     np.float64(0.8748756274507602),\n","                     np.float64(0.8747086946800323),\n","                     np.float64(0.8783608503702006),\n","                     np.float64(0.8857167170404784),\n","                     np.float64(0.8191345727475515),\n","                     np.float64(0.8729608505166441),\n","                     np.float64(0.8824823731934436),\n","                     np.float64(0.8828555034071933),\n","                     np.float64(0.8740324777301376),\n","                     np.float64(0.8924244689396887),\n","                     np.float64(0.895139410197164),\n","                     np.float64(0.8972827563665088),\n","                     np.float64(0.8932051026983099),\n","                     np.float64(0.8979797550047126),\n","                     np.float64(0.8988996176586782)],\n","                    [np.float64(0.8181804634548528),\n","                     np.float64(0.8345797982700686),\n","                     np.float64(0.8531316397749066),\n","                     np.float64(0.8695702565956107),\n","                     np.float64(0.8626620909763668),\n","                     np.float64(0.8620696756451763),\n","                     np.float64(0.8783228242889564),\n","                     np.float64(0.8730556581337539),\n","                     np.float64(0.8802367690478529),\n","                     np.float64(0.8811900881191583),\n","                     np.float64(0.8881528500996867),\n","                     np.float64(0.8895110057191767),\n","                     np.float64(0.8870104974097123),\n","                     np.float64(0.8835221379697616),\n","                     np.float64(0.8822497854697471),\n","                     np.float64(0.8901061692222322),\n","                     np.float64(0.8876811480778216),\n","                     np.float64(0.8859647574293905),\n","                     np.float64(0.8960776211146271),\n","                     np.float64(0.8973597843398651)],\n","                    [np.float64(0.8227951083353734),\n","                     np.float64(0.8411258822774716),\n","                     np.float64(0.8628611770875578),\n","                     np.float64(0.84677043311787),\n","                     np.float64(0.8620418481934149),\n","                     np.float64(0.8717047083653998),\n","                     np.float64(0.8728954700592835),\n","                     np.float64(0.8794848021526348),\n","                     np.float64(0.8783945417577366),\n","                     np.float64(0.8778700598631937),\n","                     np.float64(0.8844418060244563),\n","                     np.float64(0.885884890546869),\n","                     np.float64(0.8870837792996318),\n","                     np.float64(0.8881839781992581),\n","                     np.float64(0.8897155330707165),\n","                     np.float64(0.8939733088230768),\n","                     np.float64(0.8915813261666106),\n","                     np.float64(0.895827055561692),\n","                     np.float64(0.8921026817663777),\n","                     np.float64(0.8975153768650033)]]}\n","Training Model: BiLSTM_Dense, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7352 - loss: 0.6975(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8240\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - accuracy: 0.7355 - loss: 0.6969 - val_accuracy: 0.8852 - val_loss: 0.3004 - mcc: 0.8240\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8907 - loss: 0.2879(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8472\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8907 - loss: 0.2879 - val_accuracy: 0.9004 - val_loss: 0.2612 - mcc: 0.8472\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9050 - loss: 0.2506(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8513\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9050 - loss: 0.2506 - val_accuracy: 0.9026 - val_loss: 0.2644 - mcc: 0.8513\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9098 - loss: 0.2369(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8719\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.9098 - loss: 0.2369 - val_accuracy: 0.9162 - val_loss: 0.2204 - mcc: 0.8719\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9138 - loss: 0.2256(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8779\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9138 - loss: 0.2256 - val_accuracy: 0.9198 - val_loss: 0.2062 - mcc: 0.8779\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9192 - loss: 0.2084(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8679\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9192 - loss: 0.2084 - val_accuracy: 0.9134 - val_loss: 0.2302 - mcc: 0.8679\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9211 - loss: 0.2030(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8888\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9211 - loss: 0.2030 - val_accuracy: 0.9271 - val_loss: 0.1850 - mcc: 0.8888\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9278 - loss: 0.1818(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8657\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9278 - loss: 0.1818 - val_accuracy: 0.9121 - val_loss: 0.2315 - mcc: 0.8657\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9245 - loss: 0.1911(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8964\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9246 - loss: 0.1911 - val_accuracy: 0.9318 - val_loss: 0.1710 - mcc: 0.8964\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9319 - loss: 0.1693(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8915\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9319 - loss: 0.1694 - val_accuracy: 0.9290 - val_loss: 0.1785 - mcc: 0.8915\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9314 - loss: 0.1712(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8965\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 36ms/step - accuracy: 0.9314 - loss: 0.1712 - val_accuracy: 0.9322 - val_loss: 0.1701 - mcc: 0.8965\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9309 - loss: 0.1725(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9008\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9309 - loss: 0.1725 - val_accuracy: 0.9347 - val_loss: 0.1624 - mcc: 0.9008\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9346 - loss: 0.1614(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8811\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9346 - loss: 0.1614 - val_accuracy: 0.9213 - val_loss: 0.2076 - mcc: 0.8811\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9346 - loss: 0.1634(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9057\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.9346 - loss: 0.1634 - val_accuracy: 0.9381 - val_loss: 0.1536 - mcc: 0.9057\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9359 - loss: 0.1586(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9042\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9359 - loss: 0.1586 - val_accuracy: 0.9372 - val_loss: 0.1561 - mcc: 0.9042\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9379 - loss: 0.1541(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9016\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9379 - loss: 0.1541 - val_accuracy: 0.9350 - val_loss: 0.1610 - mcc: 0.9016\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9346 - loss: 0.1609(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9074\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9346 - loss: 0.1609 - val_accuracy: 0.9392 - val_loss: 0.1507 - mcc: 0.9074\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9363 - loss: 0.1573(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9058\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9363 - loss: 0.1573 - val_accuracy: 0.9381 - val_loss: 0.1524 - mcc: 0.9058\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9397 - loss: 0.1484(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9053\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9397 - loss: 0.1485 - val_accuracy: 0.9377 - val_loss: 0.1532 - mcc: 0.9053\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9391 - loss: 0.1503(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9073\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9391 - loss: 0.1502 - val_accuracy: 0.9390 - val_loss: 0.1500 - mcc: 0.9073\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: BiLSTM_Dense, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7580 - loss: 0.6467(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8355\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.7582 - loss: 0.6464 - val_accuracy: 0.8927 - val_loss: 0.2793 - mcc: 0.8355\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8945 - loss: 0.2775(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8606\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.8945 - loss: 0.2775 - val_accuracy: 0.9084 - val_loss: 0.2381 - mcc: 0.8606\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9046 - loss: 0.2487(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8669\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9046 - loss: 0.2487 - val_accuracy: 0.9130 - val_loss: 0.2269 - mcc: 0.8669\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9121 - loss: 0.2292(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8712\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9121 - loss: 0.2292 - val_accuracy: 0.9160 - val_loss: 0.2166 - mcc: 0.8712\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9144 - loss: 0.2208(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8828\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9144 - loss: 0.2208 - val_accuracy: 0.9233 - val_loss: 0.1973 - mcc: 0.8828\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9194 - loss: 0.2059(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8739\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.9194 - loss: 0.2059 - val_accuracy: 0.9175 - val_loss: 0.2088 - mcc: 0.8739\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9216 - loss: 0.1998(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8895\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9216 - loss: 0.1998 - val_accuracy: 0.9277 - val_loss: 0.1816 - mcc: 0.8895\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9250 - loss: 0.1874(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8857\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9250 - loss: 0.1874 - val_accuracy: 0.9252 - val_loss: 0.1891 - mcc: 0.8857\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9184 - loss: 0.2101(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8760\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9184 - loss: 0.2101 - val_accuracy: 0.9191 - val_loss: 0.2097 - mcc: 0.8760\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9206 - loss: 0.2047(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8891\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 39ms/step - accuracy: 0.9206 - loss: 0.2046 - val_accuracy: 0.9273 - val_loss: 0.1861 - mcc: 0.8891\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9259 - loss: 0.1877(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8893\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.9259 - loss: 0.1877 - val_accuracy: 0.9277 - val_loss: 0.1828 - mcc: 0.8893\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9282 - loss: 0.1792(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8981\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9282 - loss: 0.1792 - val_accuracy: 0.9332 - val_loss: 0.1675 - mcc: 0.8981\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9318 - loss: 0.1705(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8854\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9318 - loss: 0.1705 - val_accuracy: 0.9253 - val_loss: 0.1932 - mcc: 0.8854\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9278 - loss: 0.1825(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9005\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9278 - loss: 0.1824 - val_accuracy: 0.9345 - val_loss: 0.1633 - mcc: 0.9005\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9293 - loss: 0.1777(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8894\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9293 - loss: 0.1777 - val_accuracy: 0.9277 - val_loss: 0.1863 - mcc: 0.8894\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9260 - loss: 0.1881(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8957\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9260 - loss: 0.1881 - val_accuracy: 0.9313 - val_loss: 0.1722 - mcc: 0.8957\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9324 - loss: 0.1709(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8991\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9324 - loss: 0.1709 - val_accuracy: 0.9342 - val_loss: 0.1668 - mcc: 0.8991\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9346 - loss: 0.1645(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8974\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step - accuracy: 0.9346 - loss: 0.1645 - val_accuracy: 0.9327 - val_loss: 0.1699 - mcc: 0.8974\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9360 - loss: 0.1600(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9055\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 36ms/step - accuracy: 0.9360 - loss: 0.1600 - val_accuracy: 0.9380 - val_loss: 0.1560 - mcc: 0.9055\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9348 - loss: 0.1623(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9040\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9348 - loss: 0.1623 - val_accuracy: 0.9373 - val_loss: 0.1568 - mcc: 0.9040\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Model: BiLSTM_Dense, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7626 - loss: 0.6422(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8340\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - accuracy: 0.7628 - loss: 0.6416 - val_accuracy: 0.8914 - val_loss: 0.2907 - mcc: 0.8340\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8943 - loss: 0.2771(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8604\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.8943 - loss: 0.2771 - val_accuracy: 0.9084 - val_loss: 0.2383 - mcc: 0.8604\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9043 - loss: 0.2511(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8687\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9043 - loss: 0.2511 - val_accuracy: 0.9136 - val_loss: 0.2218 - mcc: 0.8687\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9140 - loss: 0.2222(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8726\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9140 - loss: 0.2222 - val_accuracy: 0.9160 - val_loss: 0.2173 - mcc: 0.8726\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9167 - loss: 0.2155(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8714\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9167 - loss: 0.2155 - val_accuracy: 0.9153 - val_loss: 0.2160 - mcc: 0.8714\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9194 - loss: 0.2055(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8792\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9194 - loss: 0.2055 - val_accuracy: 0.9207 - val_loss: 0.2043 - mcc: 0.8792\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9228 - loss: 0.1972(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8801\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9228 - loss: 0.1972 - val_accuracy: 0.9215 - val_loss: 0.2055 - mcc: 0.8801\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9248 - loss: 0.1932(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8874\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 37ms/step - accuracy: 0.9248 - loss: 0.1932 - val_accuracy: 0.9261 - val_loss: 0.1864 - mcc: 0.8874\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9287 - loss: 0.1809(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8898\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9287 - loss: 0.1809 - val_accuracy: 0.9273 - val_loss: 0.1811 - mcc: 0.8898\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9269 - loss: 0.1864(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8810\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9269 - loss: 0.1864 - val_accuracy: 0.9221 - val_loss: 0.2011 - mcc: 0.8810\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9256 - loss: 0.1918(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8875\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9256 - loss: 0.1918 - val_accuracy: 0.9263 - val_loss: 0.1879 - mcc: 0.8875\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9260 - loss: 0.1903(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8907\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9260 - loss: 0.1903 - val_accuracy: 0.9283 - val_loss: 0.1822 - mcc: 0.8907\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9296 - loss: 0.1790(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8861\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9296 - loss: 0.1790 - val_accuracy: 0.9250 - val_loss: 0.1923 - mcc: 0.8861\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9324 - loss: 0.1725(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8910\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9324 - loss: 0.1725 - val_accuracy: 0.9285 - val_loss: 0.1799 - mcc: 0.8910\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9323 - loss: 0.1712(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8935\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9323 - loss: 0.1712 - val_accuracy: 0.9301 - val_loss: 0.1744 - mcc: 0.8935\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9322 - loss: 0.1707(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8329\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.9322 - loss: 0.1707 - val_accuracy: 0.8910 - val_loss: 0.3661 - mcc: 0.8329\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9309 - loss: 0.1785(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8997\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 38ms/step - accuracy: 0.9309 - loss: 0.1785 - val_accuracy: 0.9341 - val_loss: 0.1641 - mcc: 0.8997\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9359 - loss: 0.1604(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8986\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.9359 - loss: 0.1604 - val_accuracy: 0.9336 - val_loss: 0.1679 - mcc: 0.8986\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9379 - loss: 0.1553(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8964\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9379 - loss: 0.1553 - val_accuracy: 0.9321 - val_loss: 0.1720 - mcc: 0.8964\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9355 - loss: 0.1625(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9041\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9355 - loss: 0.1625 - val_accuracy: 0.9368 - val_loss: 0.1572 - mcc: 0.9041\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 4\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7640 - loss: 0.6470(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8342\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.7642 - loss: 0.6464 - val_accuracy: 0.8908 - val_loss: 0.2935 - mcc: 0.8342\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8916 - loss: 0.2917(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8540\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8916 - loss: 0.2917 - val_accuracy: 0.9048 - val_loss: 0.2534 - mcc: 0.8540\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9042 - loss: 0.2508(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8541\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9042 - loss: 0.2508 - val_accuracy: 0.9036 - val_loss: 0.2497 - mcc: 0.8541\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9129 - loss: 0.2278(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8740\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9129 - loss: 0.2278 - val_accuracy: 0.9175 - val_loss: 0.2133 - mcc: 0.8740\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9174 - loss: 0.2131(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8773\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9174 - loss: 0.2131 - val_accuracy: 0.9199 - val_loss: 0.2064 - mcc: 0.8773\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9206 - loss: 0.2043(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8803\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9206 - loss: 0.2043 - val_accuracy: 0.9218 - val_loss: 0.2012 - mcc: 0.8803\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9207 - loss: 0.2037(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8795\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9207 - loss: 0.2037 - val_accuracy: 0.9206 - val_loss: 0.2036 - mcc: 0.8795\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9218 - loss: 0.2006(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8843\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9218 - loss: 0.2006 - val_accuracy: 0.9241 - val_loss: 0.1943 - mcc: 0.8843\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9263 - loss: 0.1867(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8909\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9263 - loss: 0.1867 - val_accuracy: 0.9287 - val_loss: 0.1807 - mcc: 0.8909\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9271 - loss: 0.1829(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8908\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9271 - loss: 0.1829 - val_accuracy: 0.9284 - val_loss: 0.1807 - mcc: 0.8908\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9290 - loss: 0.1769(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8824\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9290 - loss: 0.1770 - val_accuracy: 0.9229 - val_loss: 0.1994 - mcc: 0.8824\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9257 - loss: 0.1882(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8940\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9257 - loss: 0.1882 - val_accuracy: 0.9306 - val_loss: 0.1760 - mcc: 0.8940\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9320 - loss: 0.1688(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8943\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9320 - loss: 0.1688 - val_accuracy: 0.9309 - val_loss: 0.1737 - mcc: 0.8943\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9325 - loss: 0.1687(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8946\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9325 - loss: 0.1687 - val_accuracy: 0.9313 - val_loss: 0.1745 - mcc: 0.8946\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9354 - loss: 0.1601(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8981\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9354 - loss: 0.1601 - val_accuracy: 0.9332 - val_loss: 0.1665 - mcc: 0.8981\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9354 - loss: 0.1595(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8971\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9354 - loss: 0.1595 - val_accuracy: 0.9327 - val_loss: 0.1694 - mcc: 0.8971\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9362 - loss: 0.1573(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8971\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9362 - loss: 0.1573 - val_accuracy: 0.9327 - val_loss: 0.1697 - mcc: 0.8971\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9348 - loss: 0.1615(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8986\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9348 - loss: 0.1615 - val_accuracy: 0.9336 - val_loss: 0.1646 - mcc: 0.8986\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9364 - loss: 0.1570(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8966\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9364 - loss: 0.1570 - val_accuracy: 0.9321 - val_loss: 0.1733 - mcc: 0.8966\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9373 - loss: 0.1537(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9002\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9373 - loss: 0.1537 - val_accuracy: 0.9349 - val_loss: 0.1631 - mcc: 0.9002\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7643 - loss: 0.6483(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8166\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.7644 - loss: 0.6480 - val_accuracy: 0.8806 - val_loss: 0.3106 - mcc: 0.8166\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8953 - loss: 0.2735(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8543\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.8953 - loss: 0.2735 - val_accuracy: 0.9043 - val_loss: 0.2475 - mcc: 0.8543\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9085 - loss: 0.2395(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8657\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9086 - loss: 0.2395 - val_accuracy: 0.9121 - val_loss: 0.2290 - mcc: 0.8657\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9151 - loss: 0.2201(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8417\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9151 - loss: 0.2201 - val_accuracy: 0.8968 - val_loss: 0.2656 - mcc: 0.8417\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9105 - loss: 0.2320(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8712\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9105 - loss: 0.2320 - val_accuracy: 0.9156 - val_loss: 0.2173 - mcc: 0.8712\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9203 - loss: 0.2059(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8722\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9203 - loss: 0.2059 - val_accuracy: 0.9162 - val_loss: 0.2156 - mcc: 0.8722\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9231 - loss: 0.1973(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8722\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9231 - loss: 0.1973 - val_accuracy: 0.9164 - val_loss: 0.2130 - mcc: 0.8722\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9216 - loss: 0.2004(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8804\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9216 - loss: 0.2004 - val_accuracy: 0.9215 - val_loss: 0.1984 - mcc: 0.8804\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9257 - loss: 0.1898(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8837\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9257 - loss: 0.1898 - val_accuracy: 0.9235 - val_loss: 0.1924 - mcc: 0.8837\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9254 - loss: 0.1902(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8882\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9254 - loss: 0.1901 - val_accuracy: 0.9267 - val_loss: 0.1840 - mcc: 0.8882\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9294 - loss: 0.1785(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8893\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.9294 - loss: 0.1785 - val_accuracy: 0.9275 - val_loss: 0.1830 - mcc: 0.8893\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9321 - loss: 0.1707(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8926\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 36ms/step - accuracy: 0.9321 - loss: 0.1707 - val_accuracy: 0.9294 - val_loss: 0.1770 - mcc: 0.8926\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9322 - loss: 0.1707(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8928\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9322 - loss: 0.1707 - val_accuracy: 0.9297 - val_loss: 0.1759 - mcc: 0.8928\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9341 - loss: 0.1648(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8896\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9341 - loss: 0.1648 - val_accuracy: 0.9275 - val_loss: 0.1813 - mcc: 0.8896\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9330 - loss: 0.1692(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8932\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9330 - loss: 0.1692 - val_accuracy: 0.9298 - val_loss: 0.1770 - mcc: 0.8932\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9353 - loss: 0.1623(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8981\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9353 - loss: 0.1623 - val_accuracy: 0.9326 - val_loss: 0.1665 - mcc: 0.8981\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9370 - loss: 0.1572(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8991\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 38ms/step - accuracy: 0.9370 - loss: 0.1572 - val_accuracy: 0.9335 - val_loss: 0.1649 - mcc: 0.8991\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9356 - loss: 0.1620(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9012\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 38ms/step - accuracy: 0.9356 - loss: 0.1620 - val_accuracy: 0.9352 - val_loss: 0.1597 - mcc: 0.9012\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9395 - loss: 0.1516(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8997\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9395 - loss: 0.1516 - val_accuracy: 0.9342 - val_loss: 0.1635 - mcc: 0.8997\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9375 - loss: 0.1564(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9032\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9375 - loss: 0.1564 - val_accuracy: 0.9362 - val_loss: 0.1567 - mcc: 0.9032\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.9389946666666666),\n","              'mean': np.float64(0.9368153333333333),\n","              'min': np.float64(0.934868),\n","              'std': np.float64(0.0013504902155225555)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0009687744776407878),\n","                               'mean': np.float64(0.0007356533527374268),\n","                               'min': np.float64(0.0005689518451690674),\n","                               'std': np.float64(0.00018983935292963113)},\n"," 'MCC': {'max': np.float64(0.9072568906637712),\n","         'mean': np.float64(0.9037371518636375),\n","         'min': np.float64(0.9001895024610983),\n","         'std': np.float64(0.002253421671236948)},\n"," 'Parameters': 15973,\n"," 'Train Time (s)': {'max': np.float64(725.1093018054962),\n","                    'mean': np.float64(694.8140852928161),\n","                    'min': np.float64(659.2861914634705),\n","                    'std': np.float64(26.99197797493737)},\n"," 'Training Accuracy': [[0.8288954496383667,\n","                        0.8947476148605347,\n","                        0.9064168930053711,\n","                        0.9100167751312256,\n","                        0.9154394268989563,\n","                        0.9180887937545776,\n","                        0.9222801923751831,\n","                        0.9266523718833923,\n","                        0.927516758441925,\n","                        0.9306581020355225,\n","                        0.9308304190635681,\n","                        0.9331642389297485,\n","                        0.9343546628952026,\n","                        0.9347436428070068,\n","                        0.936072826385498,\n","                        0.9367427825927734,\n","                        0.9377100467681885,\n","                        0.9379945397377014,\n","                        0.9392427802085876,\n","                        0.9399347305297852],\n","                       [0.839772641658783,\n","                        0.8987439870834351,\n","                        0.9056463837623596,\n","                        0.9127604961395264,\n","                        0.9155681133270264,\n","                        0.9177196621894836,\n","                        0.9214353561401367,\n","                        0.9239851236343384,\n","                        0.9177789688110352,\n","                        0.921098530292511,\n","                        0.9258143901824951,\n","                        0.9292856454849243,\n","                        0.9315937757492065,\n","                        0.9301497340202332,\n","                        0.9268172979354858,\n","                        0.9291021227836609,\n","                        0.9317837953567505,\n","                        0.9335460066795349,\n","                        0.9349496364593506,\n","                        0.935276448726654],\n","                       [0.8420088887214661,\n","                        0.8983045816421509,\n","                        0.9072585701942444,\n","                        0.9138829708099365,\n","                        0.9172651171684265,\n","                        0.9213702082633972,\n","                        0.9231812357902527,\n","                        0.926144540309906,\n","                        0.9277481436729431,\n","                        0.9228352308273315,\n","                        0.9245298504829407,\n","                        0.9256749153137207,\n","                        0.9296280741691589,\n","                        0.9308071136474609,\n","                        0.9328797459602356,\n","                        0.9324818253517151,\n","                        0.9324159622192383,\n","                        0.9357269406318665,\n","                        0.9358941316604614,\n","                        0.936829686164856],\n","                       [0.8440132737159729,\n","                        0.8957213759422302,\n","                        0.9054732918739319,\n","                        0.9136012196540833,\n","                        0.9172267913818359,\n","                        0.9203494191169739,\n","                        0.9219760894775391,\n","                        0.921805202960968,\n","                        0.926419198513031,\n","                        0.9277178645133972,\n","                        0.926861584186554,\n","                        0.9275947213172913,\n","                        0.9317718148231506,\n","                        0.9332183599472046,\n","                        0.9343565106391907,\n","                        0.9347830414772034,\n","                        0.9361222386360168,\n","                        0.9347836971282959,\n","                        0.9371769428253174,\n","                        0.9377102851867676],\n","                       [0.8404197692871094,\n","                        0.8993291258811951,\n","                        0.9096294045448303,\n","                        0.9153745174407959,\n","                        0.9158933162689209,\n","                        0.9208936095237732,\n","                        0.9187813997268677,\n","                        0.9227760434150696,\n","                        0.9257929921150208,\n","                        0.9273955821990967,\n","                        0.9302666187286377,\n","                        0.9317602515220642,\n","                        0.9328065514564514,\n","                        0.9340716004371643,\n","                        0.9329348206520081,\n","                        0.9361172914505005,\n","                        0.9365016222000122,\n","                        0.9373552203178406,\n","                        0.9383420348167419,\n","                        0.9390028715133667]],\n"," 'Training Loss': [[0.4566582143306732,\n","                    0.2771809995174408,\n","                    0.24713236093521118,\n","                    0.23591426014900208,\n","                    0.22037044167518616,\n","                    0.21157674491405487,\n","                    0.1988612860441208,\n","                    0.18530689179897308,\n","                    0.18266458809375763,\n","                    0.17339415848255157,\n","                    0.17320556938648224,\n","                    0.16677364706993103,\n","                    0.16310612857341766,\n","                    0.1622016429901123,\n","                    0.15863166749477386,\n","                    0.15662775933742523,\n","                    0.1542322039604187,\n","                    0.15316367149353027,\n","                    0.14983923733234406,\n","                    0.1484948694705963],\n","                   [0.42678576707839966,\n","                    0.2647683918476105,\n","                    0.24790604412555695,\n","                    0.22686544060707092,\n","                    0.21760699152946472,\n","                    0.21004760265350342,\n","                    0.19963818788528442,\n","                    0.19048060476779938,\n","                    0.2113550454378128,\n","                    0.20268657803535461,\n","                    0.18774554133415222,\n","                    0.17729194462299347,\n","                    0.170602485537529,\n","                    0.17516334354877472,\n","                    0.1851673126220703,\n","                    0.180230513215065,\n","                    0.17163342237472534,\n","                    0.1669618785381317,\n","                    0.16244742274284363,\n","                    0.16149240732192993],\n","                   [0.4246855080127716,\n","                    0.2666795253753662,\n","                    0.24312758445739746,\n","                    0.2223730832338333,\n","                    0.21436265110969543,\n","                    0.20161974430084229,\n","                    0.1959804743528366,\n","                    0.1879263073205948,\n","                    0.18317970633506775,\n","                    0.19928283989429474,\n","                    0.19438350200653076,\n","                    0.1908879429101944,\n","                    0.17984600365161896,\n","                    0.17609043419361115,\n","                    0.1692328155040741,\n","                    0.17128752171993256,\n","                    0.17089319229125977,\n","                    0.16091930866241455,\n","                    0.16046446561813354,\n","                    0.15824897587299347],\n","                   [0.42328447103500366,\n","                    0.2777630686759949,\n","                    0.24719883501529694,\n","                    0.22456473112106323,\n","                    0.2137216180562973,\n","                    0.20437169075012207,\n","                    0.199660986661911,\n","                    0.20104221999645233,\n","                    0.18541750311851501,\n","                    0.1813773363828659,\n","                    0.18379229307174683,\n","                    0.18235033750534058,\n","                    0.16981413960456848,\n","                    0.16589121520519257,\n","                    0.1621495932340622,\n","                    0.16120101511478424,\n","                    0.1580508053302765,\n","                    0.1616300344467163,\n","                    0.15464560687541962,\n","                    0.1534021645784378],\n","                   [0.42647403478622437,\n","                    0.26334819197654724,\n","                    0.23623763024806976,\n","                    0.2193780541419983,\n","                    0.21820688247680664,\n","                    0.20408964157104492,\n","                    0.20761972665786743,\n","                    0.19691422581672668,\n","                    0.18923015892505646,\n","                    0.1846933811903,\n","                    0.17663422226905823,\n","                    0.17196528613567352,\n","                    0.16876640915870667,\n","                    0.16531704366207123,\n","                    0.1696886569261551,\n","                    0.15956151485443115,\n","                    0.15828222036361694,\n","                    0.15657036006450653,\n","                    0.1534803956747055,\n","                    0.15194611251354218]],\n"," 'Validation Accuracy': [[0.8852022886276245,\n","                          0.900368869304657,\n","                          0.9025886058807373,\n","                          0.916183352470398,\n","                          0.9198147058486938,\n","                          0.9133660197257996,\n","                          0.9270681738853455,\n","                          0.9120619297027588,\n","                          0.9318206906318665,\n","                          0.9290055632591248,\n","                          0.9321892857551575,\n","                          0.9346781969070435,\n","                          0.9212513566017151,\n","                          0.9380635023117065,\n","                          0.9371660947799683,\n","                          0.9349507093429565,\n","                          0.9391860365867615,\n","                          0.9381041526794434,\n","                          0.9376707077026367,\n","                          0.9389945268630981],\n","                         [0.8927013278007507,\n","                          0.9083572030067444,\n","                          0.9130025506019592,\n","                          0.9160186648368835,\n","                          0.9232701063156128,\n","                          0.9175341129302979,\n","                          0.9277162551879883,\n","                          0.9251911044120789,\n","                          0.9190511703491211,\n","                          0.927304744720459,\n","                          0.9276851415634155,\n","                          0.9331732392311096,\n","                          0.9253444671630859,\n","                          0.9345467686653137,\n","                          0.9277309775352478,\n","                          0.9313310384750366,\n","                          0.9341747760772705,\n","                          0.932723879814148,\n","                          0.9379671812057495,\n","                          0.9372565150260925],\n","                         [0.8913801908493042,\n","                          0.9083842039108276,\n","                          0.9136368036270142,\n","                          0.916029155254364,\n","                          0.9152726531028748,\n","                          0.9206534028053284,\n","                          0.9215150475502014,\n","                          0.9260734915733337,\n","                          0.9272552132606506,\n","                          0.922099769115448,\n","                          0.9262632727622986,\n","                          0.9282786250114441,\n","                          0.9249614477157593,\n","                          0.9285094141960144,\n","                          0.9301031827926636,\n","                          0.8910021185874939,\n","                          0.934080958366394,\n","                          0.9335585236549377,\n","                          0.9320934414863586,\n","                          0.9367579817771912],\n","                         [0.890805721282959,\n","                          0.9048423767089844,\n","                          0.9036246538162231,\n","                          0.9174668192863464,\n","                          0.919944167137146,\n","                          0.9217979907989502,\n","                          0.9205676913261414,\n","                          0.9241043925285339,\n","                          0.9287363290786743,\n","                          0.9283757209777832,\n","                          0.9228999018669128,\n","                          0.9306309223175049,\n","                          0.9308986067771912,\n","                          0.9312503933906555,\n","                          0.933197021484375,\n","                          0.9327494502067566,\n","                          0.9326512813568115,\n","                          0.9336333274841309,\n","                          0.9320536255836487,\n","                          0.9348680973052979],\n","                         [0.8806126117706299,\n","                          0.9042797088623047,\n","                          0.9121153354644775,\n","                          0.896803617477417,\n","                          0.9155765771865845,\n","                          0.9161689281463623,\n","                          0.9163976907730103,\n","                          0.921511173248291,\n","                          0.9234541654586792,\n","                          0.9266825318336487,\n","                          0.9274760484695435,\n","                          0.9293835163116455,\n","                          0.9297160506248474,\n","                          0.9275439977645874,\n","                          0.9297606348991394,\n","                          0.9326353073120117,\n","                          0.9335333704948425,\n","                          0.9351986646652222,\n","                          0.9342195391654968,\n","                          0.9361993074417114]],\n"," 'Validation Loss': [[0.3004058599472046,\n","                      0.26120665669441223,\n","                      0.2643612325191498,\n","                      0.22044070065021515,\n","                      0.20624633133411407,\n","                      0.2301727831363678,\n","                      0.18503403663635254,\n","                      0.23145273327827454,\n","                      0.17103001475334167,\n","                      0.1784699261188507,\n","                      0.17013044655323029,\n","                      0.1624472290277481,\n","                      0.2075834572315216,\n","                      0.1535928100347519,\n","                      0.15607380867004395,\n","                      0.16103821992874146,\n","                      0.15066272020339966,\n","                      0.15235014259815216,\n","                      0.15318189561367035,\n","                      0.1499987542629242],\n","                     [0.27931299805641174,\n","                      0.2381347268819809,\n","                      0.22693516314029694,\n","                      0.21663179993629456,\n","                      0.19727933406829834,\n","                      0.20883774757385254,\n","                      0.18159659206867218,\n","                      0.18908685445785522,\n","                      0.20973460376262665,\n","                      0.18612204492092133,\n","                      0.1827772557735443,\n","                      0.16752737760543823,\n","                      0.19317936897277832,\n","                      0.1632620245218277,\n","                      0.1863432079553604,\n","                      0.17217494547367096,\n","                      0.16675283014774323,\n","                      0.1699158251285553,\n","                      0.1560206264257431,\n","                      0.15678583085536957],\n","                     [0.2907465398311615,\n","                      0.23834306001663208,\n","                      0.2217726707458496,\n","                      0.21729207038879395,\n","                      0.21601109206676483,\n","                      0.2043343484401703,\n","                      0.20552948117256165,\n","                      0.18642683327198029,\n","                      0.18110455572605133,\n","                      0.20110765099525452,\n","                      0.1879405975341797,\n","                      0.1821720451116562,\n","                      0.19227643311023712,\n","                      0.17992067337036133,\n","                      0.17442913353443146,\n","                      0.3661419153213501,\n","                      0.1640622317790985,\n","                      0.167908176779747,\n","                      0.17198282480239868,\n","                      0.15719345211982727],\n","                     [0.2935066819190979,\n","                      0.25337138772010803,\n","                      0.24965044856071472,\n","                      0.21331356465816498,\n","                      0.20642100274562836,\n","                      0.2011966109275818,\n","                      0.20357228815555573,\n","                      0.19434842467308044,\n","                      0.1806909739971161,\n","                      0.18070030212402344,\n","                      0.19940006732940674,\n","                      0.17604531347751617,\n","                      0.17366166412830353,\n","                      0.1744546741247177,\n","                      0.16649620234966278,\n","                      0.16940639913082123,\n","                      0.16971127688884735,\n","                      0.16460077464580536,\n","                      0.17332063615322113,\n","                      0.16308629512786865],\n","                     [0.31056147813796997,\n","                      0.24753592908382416,\n","                      0.22899548709392548,\n","                      0.26561668515205383,\n","                      0.21732056140899658,\n","                      0.21555352210998535,\n","                      0.2130076140165329,\n","                      0.19839197397232056,\n","                      0.19243572652339935,\n","                      0.18398168683052063,\n","                      0.18296785652637482,\n","                      0.17699533700942993,\n","                      0.17592337727546692,\n","                      0.18125000596046448,\n","                      0.17703033983707428,\n","                      0.16645479202270508,\n","                      0.16486066579818726,\n","                      0.15974211692810059,\n","                      0.16352373361587524,\n","                      0.15665487945079803]],\n"," 'Validation MCC': [[np.float64(0.8240208793161073),\n","                     np.float64(0.8472474013866373),\n","                     np.float64(0.8512995238734238),\n","                     np.float64(0.8718911859863392),\n","                     np.float64(0.8779378023170564),\n","                     np.float64(0.8678634017906036),\n","                     np.float64(0.888778403840348),\n","                     np.float64(0.8657476552519758),\n","                     np.float64(0.8964157370873956),\n","                     np.float64(0.891517029464332),\n","                     np.float64(0.8965475979656682),\n","                     np.float64(0.9008013870989338),\n","                     np.float64(0.881139079731373),\n","                     np.float64(0.9057143424369566),\n","                     np.float64(0.9042468198424961),\n","                     np.float64(0.9015862818895516),\n","                     np.float64(0.9073956564165957),\n","                     np.float64(0.9058274434798766),\n","                     np.float64(0.9052948570147684),\n","                     np.float64(0.9072568906637712)],\n","                    [np.float64(0.8355050679211996),\n","                     np.float64(0.8605911895183659),\n","                     np.float64(0.8668822696634981),\n","                     np.float64(0.8712266000728345),\n","                     np.float64(0.8827700004007191),\n","                     np.float64(0.8738819892946815),\n","                     np.float64(0.8894707421906018),\n","                     np.float64(0.8856749228679713),\n","                     np.float64(0.8760106684588232),\n","                     np.float64(0.8891090396939945),\n","                     np.float64(0.8893221783534698),\n","                     np.float64(0.8981453475144874),\n","                     np.float64(0.885426910244225),\n","                     np.float64(0.9004942009799366),\n","                     np.float64(0.8893701709166028),\n","                     np.float64(0.8956843070579529),\n","                     np.float64(0.8991454650344417),\n","                     np.float64(0.8974276524692556),\n","                     np.float64(0.9054805113320972),\n","                     np.float64(0.9039513809075127)],\n","                    [np.float64(0.8340344954459701),\n","                     np.float64(0.8603744755120329),\n","                     np.float64(0.8687137368046042),\n","                     np.float64(0.8726261389273391),\n","                     np.float64(0.8713744592882515),\n","                     np.float64(0.8792272075692533),\n","                     np.float64(0.8800568122118706),\n","                     np.float64(0.8873724291930903),\n","                     np.float64(0.8897886959900919),\n","                     np.float64(0.8810355609975269),\n","                     np.float64(0.8874876043400836),\n","                     np.float64(0.8907381895111262),\n","                     np.float64(0.8861367071005185),\n","                     np.float64(0.8910112918121754),\n","                     np.float64(0.8935301889307513),\n","                     np.float64(0.832902901180466),\n","                     np.float64(0.8997143929465031),\n","                     np.float64(0.8986311501509285),\n","                     np.float64(0.8964159902052723),\n","                     np.float64(0.9040634668813503)],\n","                    [np.float64(0.8342195334330187),\n","                     np.float64(0.8539722204785767),\n","                     np.float64(0.8540568429809743),\n","                     np.float64(0.874000497234591),\n","                     np.float64(0.8772658734473251),\n","                     np.float64(0.8802578955855004),\n","                     np.float64(0.8794640362534087),\n","                     np.float64(0.8843423756859818),\n","                     np.float64(0.8908877589026839),\n","                     np.float64(0.890824989347102),\n","                     np.float64(0.8824060511865112),\n","                     np.float64(0.893997431896023),\n","                     np.float64(0.8943357950433329),\n","                     np.float64(0.8945942525562351),\n","                     np.float64(0.8981027129620995),\n","                     np.float64(0.8971028177135356),\n","                     np.float64(0.8971230479324328),\n","                     np.float64(0.8985637153417996),\n","                     np.float64(0.8966105920637134),\n","                     np.float64(0.9001895024610983)],\n","                    [np.float64(0.8165518385629021),\n","                     np.float64(0.8542651251384143),\n","                     np.float64(0.8657342790618848),\n","                     np.float64(0.8417341538143116),\n","                     np.float64(0.8711741363278295),\n","                     np.float64(0.872164905166943),\n","                     np.float64(0.8721954172791072),\n","                     np.float64(0.8804466662772967),\n","                     np.float64(0.8837437390894323),\n","                     np.float64(0.8881857416618031),\n","                     np.float64(0.8892701265138353),\n","                     np.float64(0.8926141439758065),\n","                     np.float64(0.8927590798487834),\n","                     np.float64(0.8895700489554044),\n","                     np.float64(0.8931511506882364),\n","                     np.float64(0.898107384911902),\n","                     np.float64(0.899112060927408),\n","                     np.float64(0.9012276886435756),\n","                     np.float64(0.8997102075176109),\n","                     np.float64(0.9032245184044551)]]}\n","Training Model: BiLSTM_Deep, Fold: 1\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7936 - loss: 0.5657(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8650\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 68ms/step - accuracy: 0.7937 - loss: 0.5655 - val_accuracy: 0.9115 - val_loss: 0.2333 - mcc: 0.8650\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9117 - loss: 0.2326(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8865\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - accuracy: 0.9117 - loss: 0.2326 - val_accuracy: 0.9255 - val_loss: 0.1939 - mcc: 0.8865\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9233 - loss: 0.1969(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8946\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 69ms/step - accuracy: 0.9233 - loss: 0.1969 - val_accuracy: 0.9309 - val_loss: 0.1763 - mcc: 0.8946\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9281 - loss: 0.1813(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8974\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - accuracy: 0.9281 - loss: 0.1813 - val_accuracy: 0.9327 - val_loss: 0.1707 - mcc: 0.8974\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9333 - loss: 0.1678(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.9000\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 67ms/step - accuracy: 0.9333 - loss: 0.1678 - val_accuracy: 0.9344 - val_loss: 0.1677 - mcc: 0.9000\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9347 - loss: 0.1626(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.9007\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9347 - loss: 0.1626 - val_accuracy: 0.9345 - val_loss: 0.1627 - mcc: 0.9007\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9374 - loss: 0.1565(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.9085\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 67ms/step - accuracy: 0.9374 - loss: 0.1565 - val_accuracy: 0.9398 - val_loss: 0.1519 - mcc: 0.9085\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9406 - loss: 0.1481(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9106\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 67ms/step - accuracy: 0.9406 - loss: 0.1481 - val_accuracy: 0.9410 - val_loss: 0.1460 - mcc: 0.9106\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9424 - loss: 0.1430(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9094\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 67ms/step - accuracy: 0.9424 - loss: 0.1430 - val_accuracy: 0.9403 - val_loss: 0.1480 - mcc: 0.9094\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9441 - loss: 0.1389(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9140\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - accuracy: 0.9441 - loss: 0.1389 - val_accuracy: 0.9430 - val_loss: 0.1411 - mcc: 0.9140\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9436 - loss: 0.1393(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9099\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 67ms/step - accuracy: 0.9436 - loss: 0.1393 - val_accuracy: 0.9405 - val_loss: 0.1498 - mcc: 0.9099\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9468 - loss: 0.1306(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9129\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 66ms/step - accuracy: 0.9468 - loss: 0.1306 - val_accuracy: 0.9427 - val_loss: 0.1396 - mcc: 0.9129\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9475 - loss: 0.1297(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9144\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 70ms/step - accuracy: 0.9475 - loss: 0.1297 - val_accuracy: 0.9436 - val_loss: 0.1393 - mcc: 0.9144\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9476 - loss: 0.1290(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9168\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 66ms/step - accuracy: 0.9476 - loss: 0.1290 - val_accuracy: 0.9450 - val_loss: 0.1351 - mcc: 0.9168\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9486 - loss: 0.1264(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9111\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 67ms/step - accuracy: 0.9486 - loss: 0.1264 - val_accuracy: 0.9416 - val_loss: 0.1479 - mcc: 0.9111\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9480 - loss: 0.1271(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9196\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 66ms/step - accuracy: 0.9480 - loss: 0.1271 - val_accuracy: 0.9471 - val_loss: 0.1311 - mcc: 0.9196\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9514 - loss: 0.1191(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9187\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9514 - loss: 0.1191 - val_accuracy: 0.9465 - val_loss: 0.1322 - mcc: 0.9187\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9511 - loss: 0.1196(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9193\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 70ms/step - accuracy: 0.9511 - loss: 0.1196 - val_accuracy: 0.9466 - val_loss: 0.1315 - mcc: 0.9193\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9505 - loss: 0.1215(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9207\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 67ms/step - accuracy: 0.9505 - loss: 0.1215 - val_accuracy: 0.9478 - val_loss: 0.1282 - mcc: 0.9207\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9535 - loss: 0.1136(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9185\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9535 - loss: 0.1136 - val_accuracy: 0.9463 - val_loss: 0.1352 - mcc: 0.9185\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8084 - loss: 0.5373(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8648\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 70ms/step - accuracy: 0.8084 - loss: 0.5371 - val_accuracy: 0.9121 - val_loss: 0.2311 - mcc: 0.8648\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9076 - loss: 0.2432(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8786\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 65ms/step - accuracy: 0.9076 - loss: 0.2431 - val_accuracy: 0.9209 - val_loss: 0.2022 - mcc: 0.8786\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9182 - loss: 0.2086(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8838\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 69ms/step - accuracy: 0.9182 - loss: 0.2086 - val_accuracy: 0.9242 - val_loss: 0.1897 - mcc: 0.8838\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9261 - loss: 0.1851(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8948\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 66ms/step - accuracy: 0.9261 - loss: 0.1851 - val_accuracy: 0.9313 - val_loss: 0.1724 - mcc: 0.8948\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9298 - loss: 0.1773(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8990\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9298 - loss: 0.1773 - val_accuracy: 0.9339 - val_loss: 0.1664 - mcc: 0.8990\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9338 - loss: 0.1664(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.9053\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 66ms/step - accuracy: 0.9338 - loss: 0.1664 - val_accuracy: 0.9374 - val_loss: 0.1548 - mcc: 0.9053\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9355 - loss: 0.1604(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.9057\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 68ms/step - accuracy: 0.9355 - loss: 0.1604 - val_accuracy: 0.9379 - val_loss: 0.1531 - mcc: 0.9057\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9371 - loss: 0.1564(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9027\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - accuracy: 0.9371 - loss: 0.1564 - val_accuracy: 0.9365 - val_loss: 0.1618 - mcc: 0.9027\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9384 - loss: 0.1535(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9091\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.9384 - loss: 0.1535 - val_accuracy: 0.9404 - val_loss: 0.1467 - mcc: 0.9091\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9425 - loss: 0.1422(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9073\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9425 - loss: 0.1422 - val_accuracy: 0.9389 - val_loss: 0.1515 - mcc: 0.9073\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9435 - loss: 0.1389(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9141\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 68ms/step - accuracy: 0.9435 - loss: 0.1389 - val_accuracy: 0.9438 - val_loss: 0.1386 - mcc: 0.9141\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9440 - loss: 0.1388(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9148\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9440 - loss: 0.1388 - val_accuracy: 0.9443 - val_loss: 0.1361 - mcc: 0.9148\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9452 - loss: 0.1350(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9074\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9452 - loss: 0.1350 - val_accuracy: 0.9391 - val_loss: 0.1514 - mcc: 0.9074\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9432 - loss: 0.1400(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9173\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9432 - loss: 0.1400 - val_accuracy: 0.9458 - val_loss: 0.1326 - mcc: 0.9173\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9485 - loss: 0.1266(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9177\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9485 - loss: 0.1266 - val_accuracy: 0.9460 - val_loss: 0.1333 - mcc: 0.9177\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9486 - loss: 0.1255(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9176\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 66ms/step - accuracy: 0.9486 - loss: 0.1255 - val_accuracy: 0.9456 - val_loss: 0.1374 - mcc: 0.9176\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9479 - loss: 0.1280(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9143\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 66ms/step - accuracy: 0.9479 - loss: 0.1280 - val_accuracy: 0.9438 - val_loss: 0.1377 - mcc: 0.9143\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9494 - loss: 0.1232(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9177\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9494 - loss: 0.1232 - val_accuracy: 0.9461 - val_loss: 0.1333 - mcc: 0.9177\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9494 - loss: 0.1229(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9200\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 67ms/step - accuracy: 0.9494 - loss: 0.1229 - val_accuracy: 0.9475 - val_loss: 0.1316 - mcc: 0.9200\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9514 - loss: 0.1187(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9210\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9514 - loss: 0.1187 - val_accuracy: 0.9483 - val_loss: 0.1300 - mcc: 0.9210\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 3\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7989 - loss: 0.5556(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8500\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 68ms/step - accuracy: 0.7990 - loss: 0.5554 - val_accuracy: 0.9019 - val_loss: 0.2590 - mcc: 0.8500\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9075 - loss: 0.2459(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8741\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9075 - loss: 0.2459 - val_accuracy: 0.9161 - val_loss: 0.2145 - mcc: 0.8741\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9186 - loss: 0.2104(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8783\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9186 - loss: 0.2104 - val_accuracy: 0.9203 - val_loss: 0.2047 - mcc: 0.8783\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9242 - loss: 0.1923(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8828\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9242 - loss: 0.1923 - val_accuracy: 0.9217 - val_loss: 0.1931 - mcc: 0.8828\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9286 - loss: 0.1790(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8960\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9286 - loss: 0.1790 - val_accuracy: 0.9315 - val_loss: 0.1698 - mcc: 0.8960\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9331 - loss: 0.1666(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.9039\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 66ms/step - accuracy: 0.9331 - loss: 0.1666 - val_accuracy: 0.9368 - val_loss: 0.1574 - mcc: 0.9039\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9354 - loss: 0.1602(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.9058\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9354 - loss: 0.1602 - val_accuracy: 0.9381 - val_loss: 0.1521 - mcc: 0.9058\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9373 - loss: 0.1548(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9006\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9373 - loss: 0.1548 - val_accuracy: 0.9347 - val_loss: 0.1625 - mcc: 0.9006\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9388 - loss: 0.1512(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9046\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9388 - loss: 0.1512 - val_accuracy: 0.9370 - val_loss: 0.1562 - mcc: 0.9046\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9414 - loss: 0.1435(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9112\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 68ms/step - accuracy: 0.9414 - loss: 0.1435 - val_accuracy: 0.9416 - val_loss: 0.1440 - mcc: 0.9112\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9422 - loss: 0.1424(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9114\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 66ms/step - accuracy: 0.9422 - loss: 0.1424 - val_accuracy: 0.9417 - val_loss: 0.1456 - mcc: 0.9114\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9455 - loss: 0.1338(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9140\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9455 - loss: 0.1338 - val_accuracy: 0.9435 - val_loss: 0.1399 - mcc: 0.9140\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9458 - loss: 0.1336(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9152\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 70ms/step - accuracy: 0.9458 - loss: 0.1336 - val_accuracy: 0.9443 - val_loss: 0.1370 - mcc: 0.9152\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9456 - loss: 0.1341(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9123\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 65ms/step - accuracy: 0.9456 - loss: 0.1341 - val_accuracy: 0.9421 - val_loss: 0.1408 - mcc: 0.9123\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9488 - loss: 0.1248(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9162\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9488 - loss: 0.1248 - val_accuracy: 0.9449 - val_loss: 0.1354 - mcc: 0.9162\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9480 - loss: 0.1276(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9163\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9480 - loss: 0.1276 - val_accuracy: 0.9447 - val_loss: 0.1353 - mcc: 0.9163\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9484 - loss: 0.1259(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9163\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9484 - loss: 0.1259 - val_accuracy: 0.9449 - val_loss: 0.1349 - mcc: 0.9163\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9500 - loss: 0.1222(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9164\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9500 - loss: 0.1222 - val_accuracy: 0.9450 - val_loss: 0.1365 - mcc: 0.9164\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9509 - loss: 0.1214(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9193\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9509 - loss: 0.1214 - val_accuracy: 0.9469 - val_loss: 0.1315 - mcc: 0.9193\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9499 - loss: 0.1227(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9173\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9499 - loss: 0.1227 - val_accuracy: 0.9457 - val_loss: 0.1395 - mcc: 0.9173\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 4\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8126 - loss: 0.5186(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8502\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 66ms/step - accuracy: 0.8127 - loss: 0.5183 - val_accuracy: 0.9011 - val_loss: 0.2643 - mcc: 0.8502\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9070 - loss: 0.2490(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8601\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 68ms/step - accuracy: 0.9070 - loss: 0.2490 - val_accuracy: 0.9077 - val_loss: 0.2400 - mcc: 0.8601\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9167 - loss: 0.2160(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8891\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 69ms/step - accuracy: 0.9167 - loss: 0.2160 - val_accuracy: 0.9277 - val_loss: 0.1846 - mcc: 0.8891\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9270 - loss: 0.1832(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8948\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 66ms/step - accuracy: 0.9270 - loss: 0.1832 - val_accuracy: 0.9311 - val_loss: 0.1739 - mcc: 0.8948\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9327 - loss: 0.1686(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8919\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 66ms/step - accuracy: 0.9327 - loss: 0.1686 - val_accuracy: 0.9295 - val_loss: 0.1787 - mcc: 0.8919\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9321 - loss: 0.1701(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.9007\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9321 - loss: 0.1701 - val_accuracy: 0.9346 - val_loss: 0.1657 - mcc: 0.9007\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9383 - loss: 0.1532(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8999\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9383 - loss: 0.1532 - val_accuracy: 0.9347 - val_loss: 0.1636 - mcc: 0.8999\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9402 - loss: 0.1484(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9087\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9402 - loss: 0.1484 - val_accuracy: 0.9403 - val_loss: 0.1515 - mcc: 0.9087\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9403 - loss: 0.1478(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9097\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9403 - loss: 0.1478 - val_accuracy: 0.9410 - val_loss: 0.1469 - mcc: 0.9097\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9415 - loss: 0.1441(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9113\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 66ms/step - accuracy: 0.9415 - loss: 0.1441 - val_accuracy: 0.9419 - val_loss: 0.1442 - mcc: 0.9113\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9402 - loss: 0.1525(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9101\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9402 - loss: 0.1525 - val_accuracy: 0.9409 - val_loss: 0.1485 - mcc: 0.9101\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9439 - loss: 0.1373(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9128\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9439 - loss: 0.1373 - val_accuracy: 0.9430 - val_loss: 0.1439 - mcc: 0.9128\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9463 - loss: 0.1312(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9108\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9463 - loss: 0.1312 - val_accuracy: 0.9417 - val_loss: 0.1443 - mcc: 0.9108\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9466 - loss: 0.1308(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9069\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9466 - loss: 0.1308 - val_accuracy: 0.9392 - val_loss: 0.1572 - mcc: 0.9069\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9487 - loss: 0.1251(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9162\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9487 - loss: 0.1251 - val_accuracy: 0.9451 - val_loss: 0.1392 - mcc: 0.9162\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9495 - loss: 0.1232(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9092\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9495 - loss: 0.1232 - val_accuracy: 0.9404 - val_loss: 0.1510 - mcc: 0.9092\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9473 - loss: 0.1302(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9146\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9473 - loss: 0.1302 - val_accuracy: 0.9442 - val_loss: 0.1414 - mcc: 0.9146\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9507 - loss: 0.1203(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9169\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9507 - loss: 0.1203 - val_accuracy: 0.9453 - val_loss: 0.1372 - mcc: 0.9169\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9487 - loss: 0.1240(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9096\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9487 - loss: 0.1240 - val_accuracy: 0.9406 - val_loss: 0.1467 - mcc: 0.9096\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9504 - loss: 0.1199(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9149\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9504 - loss: 0.1199 - val_accuracy: 0.9442 - val_loss: 0.1397 - mcc: 0.9149\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8009 - loss: 0.5612(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8615\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 68ms/step - accuracy: 0.8010 - loss: 0.5610 - val_accuracy: 0.9095 - val_loss: 0.2402 - mcc: 0.8615\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9144 - loss: 0.2246(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8735\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.9144 - loss: 0.2246 - val_accuracy: 0.9173 - val_loss: 0.2152 - mcc: 0.8735\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9233 - loss: 0.1980(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8838\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9233 - loss: 0.1980 - val_accuracy: 0.9233 - val_loss: 0.1932 - mcc: 0.8838\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9288 - loss: 0.1803(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8933\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9288 - loss: 0.1803 - val_accuracy: 0.9299 - val_loss: 0.1756 - mcc: 0.8933\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9311 - loss: 0.1756(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8981\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9311 - loss: 0.1756 - val_accuracy: 0.9327 - val_loss: 0.1679 - mcc: 0.8981\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9384 - loss: 0.1531(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8961\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9384 - loss: 0.1531 - val_accuracy: 0.9311 - val_loss: 0.1703 - mcc: 0.8961\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9360 - loss: 0.1623(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.9005\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9360 - loss: 0.1623 - val_accuracy: 0.9347 - val_loss: 0.1589 - mcc: 0.9005\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9397 - loss: 0.1508(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9040\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9397 - loss: 0.1508 - val_accuracy: 0.9369 - val_loss: 0.1561 - mcc: 0.9040\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9418 - loss: 0.1435(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9058\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - accuracy: 0.9418 - loss: 0.1435 - val_accuracy: 0.9383 - val_loss: 0.1520 - mcc: 0.9058\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9432 - loss: 0.1403(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8614\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 67ms/step - accuracy: 0.9432 - loss: 0.1403 - val_accuracy: 0.9078 - val_loss: 0.2473 - mcc: 0.8614\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9385 - loss: 0.1553(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9099\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9385 - loss: 0.1553 - val_accuracy: 0.9408 - val_loss: 0.1449 - mcc: 0.9099\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9457 - loss: 0.1338(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8963\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 67ms/step - accuracy: 0.9457 - loss: 0.1338 - val_accuracy: 0.9317 - val_loss: 0.1684 - mcc: 0.8963\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9420 - loss: 0.1437(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9051\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9420 - loss: 0.1437 - val_accuracy: 0.9375 - val_loss: 0.1552 - mcc: 0.9051\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9471 - loss: 0.1316(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9122\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9471 - loss: 0.1316 - val_accuracy: 0.9423 - val_loss: 0.1406 - mcc: 0.9122\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9486 - loss: 0.1273(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9127\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9486 - loss: 0.1273 - val_accuracy: 0.9425 - val_loss: 0.1415 - mcc: 0.9127\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9480 - loss: 0.1282(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9146\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9480 - loss: 0.1282 - val_accuracy: 0.9439 - val_loss: 0.1377 - mcc: 0.9146\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9496 - loss: 0.1239(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9129\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9496 - loss: 0.1239 - val_accuracy: 0.9428 - val_loss: 0.1412 - mcc: 0.9129\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9504 - loss: 0.1203(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9136\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9504 - loss: 0.1203 - val_accuracy: 0.9432 - val_loss: 0.1374 - mcc: 0.9136\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9506 - loss: 0.1212(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9124\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9506 - loss: 0.1212 - val_accuracy: 0.9423 - val_loss: 0.1424 - mcc: 0.9124\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9505 - loss: 0.1213(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9152\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - accuracy: 0.9505 - loss: 0.1213 - val_accuracy: 0.9442 - val_loss: 0.1355 - mcc: 0.9152\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.9482513333333333),\n","              'mean': np.float64(0.9457213333333334),\n","              'min': np.float64(0.9441613333333333),\n","              'std': np.float64(0.0015069353152821307)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0018218497435251871),\n","                               'mean': np.float64(0.001107266060511271),\n","                               'min': np.float64(0.0008641552130381267),\n","                               'std': np.float64(0.0003589272276197002)},\n"," 'MCC': {'max': np.float64(0.9210456857425257),\n","         'mean': np.float64(0.9173979803886871),\n","         'min': np.float64(0.9149237083816362),\n","         'std': np.float64(0.002258226938407201)},\n"," 'Parameters': 88421,\n"," 'Train Time (s)': {'max': np.float64(1480.8817269802094),\n","                    'mean': np.float64(1416.9027934551239),\n","                    'min': np.float64(1349.2553870677948),\n","                    'std': np.float64(47.11786699791029)},\n"," 'Training Accuracy': [[0.8637949228286743,\n","                        0.9147616624832153,\n","                        0.9254827499389648,\n","                        0.9289405345916748,\n","                        0.9332839250564575,\n","                        0.935674250125885,\n","                        0.9371925592422485,\n","                        0.939522922039032,\n","                        0.9414233565330505,\n","                        0.9432014226913452,\n","                        0.9441109299659729,\n","                        0.9450942277908325,\n","                        0.9460628032684326,\n","                        0.9468106031417847,\n","                        0.9479477405548096,\n","                        0.9487301707267761,\n","                        0.9502102732658386,\n","                        0.9507982730865479,\n","                        0.9508808851242065,\n","                        0.9513822197914124],\n","                       [0.8670895099639893,\n","                        0.9112807512283325,\n","                        0.9230262637138367,\n","                        0.9271572828292847,\n","                        0.9308778643608093,\n","                        0.9340202808380127,\n","                        0.9365034103393555,\n","                        0.9378861784934998,\n","                        0.9387697577476501,\n","                        0.9421080946922302,\n","                        0.9434317946434021,\n","                        0.9437135457992554,\n","                        0.9431384205818176,\n","                        0.9443625211715698,\n","                        0.9473387598991394,\n","                        0.9482364654541016,\n","                        0.9479514360427856,\n","                        0.9491490125656128,\n","                        0.9488482475280762,\n","                        0.950966477394104],\n","                       [0.8641130924224854,\n","                        0.9102473855018616,\n","                        0.919050931930542,\n","                        0.925971508026123,\n","                        0.9277635812759399,\n","                        0.933517575263977,\n","                        0.9366245865821838,\n","                        0.9386351108551025,\n","                        0.9391401410102844,\n","                        0.9420467019081116,\n","                        0.9435377717018127,\n","                        0.9451141953468323,\n","                        0.9448426961898804,\n","                        0.9463273286819458,\n","                        0.9481278657913208,\n","                        0.9479489922523499,\n","                        0.9491053223609924,\n","                        0.9495039582252502,\n","                        0.9501280188560486,\n","                        0.9505457282066345],\n","                       [0.8719426393508911,\n","                        0.9085693955421448,\n","                        0.9212809801101685,\n","                        0.9290136694908142,\n","                        0.9322518110275269,\n","                        0.93353670835495,\n","                        0.9387242197990417,\n","                        0.9405748844146729,\n","                        0.9412552118301392,\n","                        0.9413496255874634,\n","                        0.9405497312545776,\n","                        0.9447206258773804,\n","                        0.9453401565551758,\n","                        0.9472832083702087,\n","                        0.9479383826255798,\n","                        0.9485847353935242,\n","                        0.9479211568832397,\n","                        0.9501371383666992,\n","                        0.9489710330963135,\n","                        0.9501426815986633],\n","                       [0.8686989545822144,\n","                        0.9144707322120667,\n","                        0.9244321584701538,\n","                        0.9301031231880188,\n","                        0.9333086609840393,\n","                        0.9374552369117737,\n","                        0.9370498061180115,\n","                        0.9393845796585083,\n","                        0.9418153762817383,\n","                        0.9418337941169739,\n","                        0.9420871734619141,\n","                        0.9451918005943298,\n","                        0.9432703256607056,\n","                        0.9465245604515076,\n","                        0.9480676054954529,\n","                        0.9481573104858398,\n","                        0.9491857886314392,\n","                        0.9495081305503845,\n","                        0.9496966004371643,\n","                        0.9506793022155762]],\n"," 'Training Loss': [[0.3728664815425873,\n","                    0.22168925404548645,\n","                    0.19035744667053223,\n","                    0.17962202429771423,\n","                    0.1670704185962677,\n","                    0.1600571870803833,\n","                    0.15644052624702454,\n","                    0.15091058611869812,\n","                    0.1449367105960846,\n","                    0.14058923721313477,\n","                    0.1379193216562271,\n","                    0.13464440405368805,\n","                    0.13280819356441498,\n","                    0.13073381781578064,\n","                    0.12731672823429108,\n","                    0.12516872584819794,\n","                    0.12202882021665573,\n","                    0.1205192357301712,\n","                    0.12047658115625381,\n","                    0.1192522868514061],\n","                   [0.3649107813835144,\n","                    0.23121769726276398,\n","                    0.1955002099275589,\n","                    0.18305836617946625,\n","                    0.17372360825538635,\n","                    0.16507889330387115,\n","                    0.15688718855381012,\n","                    0.15377046167850494,\n","                    0.15279516577720642,\n","                    0.14221927523612976,\n","                    0.13968968391418457,\n","                    0.1387825310230255,\n","                    0.14014287292957306,\n","                    0.13638745248317719,\n","                    0.1289345920085907,\n","                    0.12670426070690155,\n","                    0.12787418067455292,\n","                    0.12435419857501984,\n","                    0.12479991465806961,\n","                    0.11952923238277435],\n","                   [0.37068498134613037,\n","                    0.23706962168216705,\n","                    0.20944122970104218,\n","                    0.1871923804283142,\n","                    0.18224909901618958,\n","                    0.16498494148254395,\n","                    0.15752726793289185,\n","                    0.1513027399778366,\n","                    0.15043187141418457,\n","                    0.14239664375782013,\n","                    0.13975436985492706,\n","                    0.13475194573402405,\n","                    0.13619643449783325,\n","                    0.13224399089813232,\n","                    0.12715847790241241,\n","                    0.12771880626678467,\n","                    0.12452082335948944,\n","                    0.12322644889354706,\n","                    0.12242530286312103,\n","                    0.12079093605279922],\n","                   [0.3527953326702118,\n","                    0.24466606974601746,\n","                    0.2022271305322647,\n","                    0.17929695546627045,\n","                    0.17025983333587646,\n","                    0.16629953682422638,\n","                    0.15182295441627502,\n","                    0.14629757404327393,\n","                    0.1448882520198822,\n","                    0.144744411110878,\n","                    0.14827744662761688,\n","                    0.13634373247623444,\n","                    0.13362525403499603,\n","                    0.12951327860355377,\n","                    0.12728799879550934,\n","                    0.12552410364151,\n","                    0.1291390210390091,\n","                    0.121534064412117,\n","                    0.12479712814092636,\n","                    0.1215028166770935],\n","                   [0.3635842800140381,\n","                    0.2244836986064911,\n","                    0.19365975260734558,\n","                    0.17765429615974426,\n","                    0.16831320524215698,\n","                    0.1555112600326538,\n","                    0.15851452946662903,\n","                    0.15130221843719482,\n","                    0.14362800121307373,\n","                    0.14529994130134583,\n","                    0.14447782933712006,\n","                    0.13505622744560242,\n","                    0.14087431132793427,\n","                    0.13218063116073608,\n","                    0.12753550708293915,\n","                    0.12678208947181702,\n","                    0.12471490353345871,\n","                    0.12332885712385178,\n","                    0.12344294041395187,\n","                    0.12105941772460938]],\n"," 'Validation Accuracy': [[0.9114980697631836,\n","                          0.9255486130714417,\n","                          0.9309003949165344,\n","                          0.9326860308647156,\n","                          0.934366762638092,\n","                          0.9344736933708191,\n","                          0.9398482441902161,\n","                          0.941024899482727,\n","                          0.9403437376022339,\n","                          0.9430379271507263,\n","                          0.9405477643013,\n","                          0.9426692724227905,\n","                          0.9435747265815735,\n","                          0.9450259804725647,\n","                          0.9416278600692749,\n","                          0.9470629692077637,\n","                          0.9465241432189941,\n","                          0.9466472268104553,\n","                          0.9477747678756714,\n","                          0.946264386177063],\n","                         [0.9121264815330505,\n","                          0.9209136366844177,\n","                          0.9242399334907532,\n","                          0.931260883808136,\n","                          0.9338621497154236,\n","                          0.93740314245224,\n","                          0.9379034638404846,\n","                          0.9364902973175049,\n","                          0.9404087662696838,\n","                          0.9388601183891296,\n","                          0.9438009262084961,\n","                          0.9442548751831055,\n","                          0.9391002058982849,\n","                          0.945793628692627,\n","                          0.9460374116897583,\n","                          0.9455982446670532,\n","                          0.9437665343284607,\n","                          0.9461139440536499,\n","                          0.9475008249282837,\n","                          0.9482510685920715],\n","                         [0.9019368290901184,\n","                          0.9161126017570496,\n","                          0.9203192591667175,\n","                          0.9216731190681458,\n","                          0.9315398931503296,\n","                          0.9367905855178833,\n","                          0.9380722641944885,\n","                          0.934724748134613,\n","                          0.9369828104972839,\n","                          0.9416067600250244,\n","                          0.9416682124137878,\n","                          0.9435480237007141,\n","                          0.9442600011825562,\n","                          0.9420871734619141,\n","                          0.9449020028114319,\n","                          0.9446634650230408,\n","                          0.9448912143707275,\n","                          0.9449976682662964,\n","                          0.9468696117401123,\n","                          0.9456995725631714],\n","                         [0.9011245965957642,\n","                          0.9076938629150391,\n","                          0.9276767373085022,\n","                          0.9311057925224304,\n","                          0.9295063614845276,\n","                          0.9346348643302917,\n","                          0.9346567988395691,\n","                          0.9403446316719055,\n","                          0.9409598708152771,\n","                          0.9419404864311218,\n","                          0.9408616423606873,\n","                          0.9429502487182617,\n","                          0.9417439103126526,\n","                          0.9391908645629883,\n","                          0.9451178312301636,\n","                          0.9403839707374573,\n","                          0.9441912174224854,\n","                          0.9453181028366089,\n","                          0.9406080842018127,\n","                          0.9442299604415894],\n","                         [0.9094964265823364,\n","                          0.9172824025154114,\n","                          0.923332691192627,\n","                          0.9299015402793884,\n","                          0.9327312111854553,\n","                          0.9310696721076965,\n","                          0.9346845746040344,\n","                          0.9369179010391235,\n","                          0.9382564425468445,\n","                          0.9078380465507507,\n","                          0.9407808184623718,\n","                          0.9317136406898499,\n","                          0.9374861717224121,\n","                          0.9423418045043945,\n","                          0.9425053000450134,\n","                          0.9438620805740356,\n","                          0.9428294897079468,\n","                          0.9431673288345337,\n","                          0.9422979950904846,\n","                          0.9441612958908081]],\n"," 'Validation Loss': [[0.23325856029987335,\n","                      0.19388815760612488,\n","                      0.17628517746925354,\n","                      0.1707296073436737,\n","                      0.16772516071796417,\n","                      0.16269691288471222,\n","                      0.15194426476955414,\n","                      0.14602044224739075,\n","                      0.1479521244764328,\n","                      0.14105181396007538,\n","                      0.1498001366853714,\n","                      0.13955266773700714,\n","                      0.13929760456085205,\n","                      0.13505224883556366,\n","                      0.14790363609790802,\n","                      0.13106109201908112,\n","                      0.13219814002513885,\n","                      0.13147175312042236,\n","                      0.1282312572002411,\n","                      0.1351645141839981],\n","                     [0.23109480738639832,\n","                      0.20216546952724457,\n","                      0.18965111672878265,\n","                      0.17242804169654846,\n","                      0.16642598807811737,\n","                      0.15483351051807404,\n","                      0.15310858190059662,\n","                      0.16181732714176178,\n","                      0.1467382162809372,\n","                      0.15149475634098053,\n","                      0.138589546084404,\n","                      0.1361306756734848,\n","                      0.15137726068496704,\n","                      0.13264772295951843,\n","                      0.13330014050006866,\n","                      0.13744398951530457,\n","                      0.13767170906066895,\n","                      0.13325677812099457,\n","                      0.13157552480697632,\n","                      0.13003133237361908],\n","                     [0.25900396704673767,\n","                      0.2144583761692047,\n","                      0.20468419790267944,\n","                      0.19312316179275513,\n","                      0.16977331042289734,\n","                      0.15736044943332672,\n","                      0.1520805060863495,\n","                      0.16253137588500977,\n","                      0.15617956221103668,\n","                      0.14401547610759735,\n","                      0.1456366330385208,\n","                      0.13986772298812866,\n","                      0.13703477382659912,\n","                      0.14084886014461517,\n","                      0.13539773225784302,\n","                      0.13528881967067719,\n","                      0.13491252064704895,\n","                      0.13650177419185638,\n","                      0.13145288825035095,\n","                      0.13951998949050903],\n","                     [0.26433536410331726,\n","                      0.24002031981945038,\n","                      0.1845935881137848,\n","                      0.17391355335712433,\n","                      0.1786535233259201,\n","                      0.16567419469356537,\n","                      0.16360779106616974,\n","                      0.1515137255191803,\n","                      0.14694616198539734,\n","                      0.14420172572135925,\n","                      0.1484660804271698,\n","                      0.14389808475971222,\n","                      0.1442975401878357,\n","                      0.15715846419334412,\n","                      0.13921892642974854,\n","                      0.15097978711128235,\n","                      0.14140304923057556,\n","                      0.13719578087329865,\n","                      0.14670631289482117,\n","                      0.13970015943050385],\n","                     [0.24016119539737701,\n","                      0.21515215933322906,\n","                      0.19318711757659912,\n","                      0.17561325430870056,\n","                      0.1678917109966278,\n","                      0.17034459114074707,\n","                      0.15889565646648407,\n","                      0.15605366230010986,\n","                      0.1519864946603775,\n","                      0.2472761571407318,\n","                      0.14492206275463104,\n","                      0.16838687658309937,\n","                      0.15516577661037445,\n","                      0.1406424194574356,\n","                      0.14147335290908813,\n","                      0.13768340647220612,\n","                      0.14118558168411255,\n","                      0.13738201558589935,\n","                      0.14241579174995422,\n","                      0.1355184018611908]],\n"," 'Validation MCC': [[np.float64(0.8649684501104598),\n","                     np.float64(0.8865170951129345),\n","                     np.float64(0.8946488116659453),\n","                     np.float64(0.8974108243933514),\n","                     np.float64(0.8999959461479535),\n","                     np.float64(0.900703812214188),\n","                     np.float64(0.9084798131469717),\n","                     np.float64(0.9106264279497804),\n","                     np.float64(0.9094283038890679),\n","                     np.float64(0.914022809264464),\n","                     np.float64(0.909851524975915),\n","                     np.float64(0.9129125941448271),\n","                     np.float64(0.9144212223324807),\n","                     np.float64(0.9168030641430982),\n","                     np.float64(0.9111225320912394),\n","                     np.float64(0.9195845223273247),\n","                     np.float64(0.918689259241504),\n","                     np.float64(0.9192745008187232),\n","                     np.float64(0.9207294095354835),\n","                     np.float64(0.9185082362792502)],\n","                    [np.float64(0.8647827682514059),\n","                     np.float64(0.878592090997677),\n","                     np.float64(0.8837688746792168),\n","                     np.float64(0.8947937651239732),\n","                     np.float64(0.8989600454643362),\n","                     np.float64(0.9052624350699554),\n","                     np.float64(0.9057067298796792),\n","                     np.float64(0.9027319423645783),\n","                     np.float64(0.9091010746081261),\n","                     np.float64(0.9072970339737247),\n","                     np.float64(0.9141244190614817),\n","                     np.float64(0.9147843193617675),\n","                     np.float64(0.9074046467676574),\n","                     np.float64(0.917319421358828),\n","                     np.float64(0.9176606081786527),\n","                     np.float64(0.9175855516828523),\n","                     np.float64(0.9142533036662941),\n","                     np.float64(0.9176562637112752),\n","                     np.float64(0.9200491191196487),\n","                     np.float64(0.9210456857425257)],\n","                    [np.float64(0.8499625828025639),\n","                     np.float64(0.8740918286902337),\n","                     np.float64(0.8783441444378411),\n","                     np.float64(0.882766663068051),\n","                     np.float64(0.8960468643884506),\n","                     np.float64(0.9039173479511714),\n","                     np.float64(0.9058084769068223),\n","                     np.float64(0.9006048472824775),\n","                     np.float64(0.9046363014602676),\n","                     np.float64(0.9111659445154906),\n","                     np.float64(0.9113675817204873),\n","                     np.float64(0.9140470280833083),\n","                     np.float64(0.9152219754909473),\n","                     np.float64(0.912325784135616),\n","                     np.float64(0.916197443844604),\n","                     np.float64(0.9163248805374006),\n","                     np.float64(0.9163430078865749),\n","                     np.float64(0.916360094403515),\n","                     np.float64(0.919348430478765),\n","                     np.float64(0.9173120433901463)],\n","                    [np.float64(0.8501708954983348),\n","                     np.float64(0.8601281070008593),\n","                     np.float64(0.8890535748188328),\n","                     np.float64(0.8947646506107242),\n","                     np.float64(0.8918940277194455),\n","                     np.float64(0.9006919384620585),\n","                     np.float64(0.899884953033989),\n","                     np.float64(0.9087324710403831),\n","                     np.float64(0.9097359645793325),\n","                     np.float64(0.9112590533819682),\n","                     np.float64(0.9100639190661558),\n","                     np.float64(0.9128485564430455),\n","                     np.float64(0.9108392728528356),\n","                     np.float64(0.9069271547975862),\n","                     np.float64(0.9162200565252773),\n","                     np.float64(0.909237223807547),\n","                     np.float64(0.914576095179931),\n","                     np.float64(0.916907871725822),\n","                     np.float64(0.9095897640313457),\n","                     np.float64(0.9149237083816362)],\n","                    [np.float64(0.8614742275196877),\n","                     np.float64(0.8735059095736849),\n","                     np.float64(0.8837919025400551),\n","                     np.float64(0.8932764752348079),\n","                     np.float64(0.8981254759405424),\n","                     np.float64(0.896113611844571),\n","                     np.float64(0.900512107994336),\n","                     np.float64(0.9040199577024522),\n","                     np.float64(0.9058340226583884),\n","                     np.float64(0.8614192680089667),\n","                     np.float64(0.909948979202205),\n","                     np.float64(0.8963180825101429),\n","                     np.float64(0.9050762274446892),\n","                     np.float64(0.9121609964023987),\n","                     np.float64(0.9126545294515409),\n","                     np.float64(0.9146175191689293),\n","                     np.float64(0.9129145493447802),\n","                     np.float64(0.9135980675085738),\n","                     np.float64(0.9123592082981284),\n","                     np.float64(0.9152002281498773)]]}\n"]}],"source":["basePath = \"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Model Comparisons/LSTM Models/New\"\n","\n","model_results, trained_models = train_and_evaluate(simple_models_dict, X=all_data_vec, y=label_multi, epochs=n_epochs, dir_name=\"allF_multi\", basePath=basePath)\n","\n","filePath = f\"{basePath}20_Epoch_AllF_Multi_Model_Results.json\"\n","\n","with open(filePath, 'w') as f:\n","        json.dump(model_results, f, indent=4)  # indent=4 for pretty formatting"]},{"cell_type":"markdown","source":["Time estimation:\n","\n","- 15min per model\n","- 6 models per set-up (1.5 hours per set-up)\n","- 2x4 different set-ups\n","- 15min x 6 x 2 x 4\n","- 720min = 12 Hours Total !!!!"],"metadata":{"id":"g5GVjGlqxh3T"}},{"cell_type":"markdown","source":["Signal Only"],"metadata":{"id":"6Vv9BwptndUP"}},{"cell_type":"code","source":["basePath = \"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Model Comparisons/LSTM Models/New\"\n","\n","signal_multi_results, trained_models = train_and_evaluate(simple_models_dict, X=signal_data_vec, y=label_multi, epochs=n_epochs, dir_name=\"signal_multi\")\n","\n","filePath = f\"{basePath}/20_Epoch_Signal_Multi_Model_Results.json\"\n","\n","with open(filePath, 'w') as f:\n","        json.dump(signal_multi_results, f, indent=4)  # indent=4 for pretty formatting"],"metadata":{"id":"B0YXed_zpxL9","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744725308531,"user_tz":-120,"elapsed":21182775,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"60c0a693-32fb-462c-9a78-d9a93c8f69a5"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 1\n","Epoch 1/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5976 - loss: 1.1278(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.5932\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.5980 - loss: 1.1267 - val_accuracy: 0.7387 - val_loss: 0.7401 - mcc: 0.5932\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7711 - loss: 0.6469(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6929\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.7711 - loss: 0.6469 - val_accuracy: 0.8014 - val_loss: 0.5464 - mcc: 0.6929\n","Epoch 3/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7975 - loss: 0.5719(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.5360\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.7975 - loss: 0.5719 - val_accuracy: 0.7011 - val_loss: 0.8937 - mcc: 0.5360\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7769 - loss: 0.6506(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.6868\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.7769 - loss: 0.6505 - val_accuracy: 0.7998 - val_loss: 0.5716 - mcc: 0.6868\n","Epoch 5/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8031 - loss: 0.5567(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7052\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8031 - loss: 0.5566 - val_accuracy: 0.8105 - val_loss: 0.5316 - mcc: 0.7052\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7760 - loss: 0.6478(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.6984\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.7760 - loss: 0.6478 - val_accuracy: 0.8069 - val_loss: 0.5538 - mcc: 0.6984\n","Epoch 7/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8003 - loss: 0.5717(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7199\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8003 - loss: 0.5716 - val_accuracy: 0.8199 - val_loss: 0.4947 - mcc: 0.7199\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8186 - loss: 0.5029(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7069\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8185 - loss: 0.5030 - val_accuracy: 0.8116 - val_loss: 0.5250 - mcc: 0.7069\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8161 - loss: 0.5117(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.6868\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8161 - loss: 0.5117 - val_accuracy: 0.7998 - val_loss: 0.5639 - mcc: 0.6868\n","Epoch 10/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8113 - loss: 0.5349(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7031\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8113 - loss: 0.5348 - val_accuracy: 0.8100 - val_loss: 0.5465 - mcc: 0.7031\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8088 - loss: 0.5563(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7471\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8088 - loss: 0.5562 - val_accuracy: 0.8369 - val_loss: 0.4494 - mcc: 0.7471\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8262 - loss: 0.4783(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7471\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8262 - loss: 0.4783 - val_accuracy: 0.8369 - val_loss: 0.4456 - mcc: 0.7471\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8218 - loss: 0.4955(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7496\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8219 - loss: 0.4954 - val_accuracy: 0.8381 - val_loss: 0.4402 - mcc: 0.7496\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8277 - loss: 0.4774(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.4952\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8277 - loss: 0.4775 - val_accuracy: 0.6802 - val_loss: 1.0818 - mcc: 0.4952\n","Epoch 15/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7317 - loss: 0.7895(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.6750\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.7318 - loss: 0.7891 - val_accuracy: 0.7925 - val_loss: 0.5835 - mcc: 0.6750\n","Epoch 16/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7807 - loss: 0.6364(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.6961\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.7807 - loss: 0.6363 - val_accuracy: 0.8056 - val_loss: 0.5492 - mcc: 0.6961\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8033 - loss: 0.5568(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7166\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8033 - loss: 0.5568 - val_accuracy: 0.8180 - val_loss: 0.5069 - mcc: 0.7166\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8171 - loss: 0.5095(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7283\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8171 - loss: 0.5094 - val_accuracy: 0.8247 - val_loss: 0.4846 - mcc: 0.7283\n","Epoch 19/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8188 - loss: 0.5069(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7067\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8187 - loss: 0.5070 - val_accuracy: 0.8125 - val_loss: 0.5305 - mcc: 0.7067\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8080 - loss: 0.5474(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7353\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8080 - loss: 0.5473 - val_accuracy: 0.8297 - val_loss: 0.4743 - mcc: 0.7353\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 2\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5867 - loss: 1.1451(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6477\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.5869 - loss: 1.1445 - val_accuracy: 0.7772 - val_loss: 0.6524 - mcc: 0.6477\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7812 - loss: 0.6279(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6913\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.7812 - loss: 0.6279 - val_accuracy: 0.8031 - val_loss: 0.5452 - mcc: 0.6913\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8013 - loss: 0.5576(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.6813\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8013 - loss: 0.5576 - val_accuracy: 0.7970 - val_loss: 0.5888 - mcc: 0.6813\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7989 - loss: 0.5770(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.6704\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.7989 - loss: 0.5771 - val_accuracy: 0.7898 - val_loss: 0.5887 - mcc: 0.6704\n","Epoch 5/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7907 - loss: 0.5921(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.6374\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.7906 - loss: 0.5925 - val_accuracy: 0.7709 - val_loss: 0.6640 - mcc: 0.6374\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7792 - loss: 0.6336(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.6855\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.7792 - loss: 0.6335 - val_accuracy: 0.7990 - val_loss: 0.5610 - mcc: 0.6855\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7880 - loss: 0.5927(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.6857\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.7880 - loss: 0.5928 - val_accuracy: 0.7997 - val_loss: 0.5607 - mcc: 0.6857\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8004 - loss: 0.5503(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7166\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8004 - loss: 0.5503 - val_accuracy: 0.8186 - val_loss: 0.5027 - mcc: 0.7166\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8005 - loss: 0.6048(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7037\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8005 - loss: 0.6047 - val_accuracy: 0.8105 - val_loss: 0.5101 - mcc: 0.7037\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8156 - loss: 0.5025(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.6942\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8156 - loss: 0.5026 - val_accuracy: 0.8044 - val_loss: 0.5379 - mcc: 0.6942\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8016 - loss: 0.5604(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.4819\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8016 - loss: 0.5604 - val_accuracy: 0.6379 - val_loss: 0.9859 - mcc: 0.4819\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7626 - loss: 0.6663(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7122\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.7627 - loss: 0.6661 - val_accuracy: 0.8159 - val_loss: 0.5187 - mcc: 0.7122\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8133 - loss: 0.5225(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7262\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8133 - loss: 0.5225 - val_accuracy: 0.8246 - val_loss: 0.4881 - mcc: 0.7262\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8094 - loss: 0.5322(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7231\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8094 - loss: 0.5323 - val_accuracy: 0.8225 - val_loss: 0.4892 - mcc: 0.7231\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8201 - loss: 0.4945(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7011\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8201 - loss: 0.4945 - val_accuracy: 0.8088 - val_loss: 0.5348 - mcc: 0.7011\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8115 - loss: 0.5305(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.6888\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8115 - loss: 0.5305 - val_accuracy: 0.8003 - val_loss: 0.5682 - mcc: 0.6888\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8192 - loss: 0.5045(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7450\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8193 - loss: 0.5044 - val_accuracy: 0.8357 - val_loss: 0.4513 - mcc: 0.7450\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8236 - loss: 0.4993(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7278\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8236 - loss: 0.4993 - val_accuracy: 0.8253 - val_loss: 0.5003 - mcc: 0.7278\n","Epoch 19/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8232 - loss: 0.4856(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7450\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8232 - loss: 0.4856 - val_accuracy: 0.8359 - val_loss: 0.4478 - mcc: 0.7450\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8315 - loss: 0.4554(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7491\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8315 - loss: 0.4554 - val_accuracy: 0.8387 - val_loss: 0.4392 - mcc: 0.7491\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 3\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5896 - loss: 1.1485(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6553\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.5897 - loss: 1.1482 - val_accuracy: 0.7805 - val_loss: 0.6388 - mcc: 0.6553\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7825 - loss: 0.6258(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6747\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.7825 - loss: 0.6258 - val_accuracy: 0.7921 - val_loss: 0.6107 - mcc: 0.6747\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7962 - loss: 0.5919(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7070\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.7962 - loss: 0.5919 - val_accuracy: 0.8116 - val_loss: 0.5267 - mcc: 0.7070\n","Epoch 4/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8068 - loss: 0.5455(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.6680\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8068 - loss: 0.5455 - val_accuracy: 0.7864 - val_loss: 0.5981 - mcc: 0.6680\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8091 - loss: 0.5417(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7365\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8091 - loss: 0.5416 - val_accuracy: 0.8299 - val_loss: 0.4642 - mcc: 0.7365\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8119 - loss: 0.5239(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7299\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8119 - loss: 0.5239 - val_accuracy: 0.8255 - val_loss: 0.4806 - mcc: 0.7299\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8122 - loss: 0.5559(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7089\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8122 - loss: 0.5559 - val_accuracy: 0.8123 - val_loss: 0.5247 - mcc: 0.7089\n","Epoch 8/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8290 - loss: 0.4752(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.5221\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8290 - loss: 0.4752 - val_accuracy: 0.6763 - val_loss: 1.0076 - mcc: 0.5221\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7832 - loss: 0.6096(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7440\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.7833 - loss: 0.6095 - val_accuracy: 0.8347 - val_loss: 0.4520 - mcc: 0.7440\n","Epoch 10/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8301 - loss: 0.4750(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7238\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8301 - loss: 0.4750 - val_accuracy: 0.8223 - val_loss: 0.4989 - mcc: 0.7238\n","Epoch 11/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8309 - loss: 0.4704(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.6933\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8309 - loss: 0.4705 - val_accuracy: 0.8029 - val_loss: 0.5430 - mcc: 0.6933\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8273 - loss: 0.4745(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7567\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8273 - loss: 0.4744 - val_accuracy: 0.8424 - val_loss: 0.4292 - mcc: 0.7567\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8355 - loss: 0.4559(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7473\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8355 - loss: 0.4559 - val_accuracy: 0.8367 - val_loss: 0.4549 - mcc: 0.7473\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8170 - loss: 0.5164(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7572\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8170 - loss: 0.5163 - val_accuracy: 0.8430 - val_loss: 0.4235 - mcc: 0.7572\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7989 - loss: 0.5692(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.6696\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.7989 - loss: 0.5692 - val_accuracy: 0.7898 - val_loss: 0.6123 - mcc: 0.6696\n","Epoch 16/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8095 - loss: 0.5419(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7207\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8095 - loss: 0.5419 - val_accuracy: 0.8203 - val_loss: 0.5138 - mcc: 0.7207\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8057 - loss: 0.5583(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.6735\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8057 - loss: 0.5584 - val_accuracy: 0.7916 - val_loss: 0.6015 - mcc: 0.6735\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8036 - loss: 0.5641(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7151\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8036 - loss: 0.5641 - val_accuracy: 0.8168 - val_loss: 0.5195 - mcc: 0.7151\n","Epoch 19/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8230 - loss: 0.5028(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7280\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8230 - loss: 0.5027 - val_accuracy: 0.8246 - val_loss: 0.4965 - mcc: 0.7280\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8251 - loss: 0.4983(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7287\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8251 - loss: 0.4983 - val_accuracy: 0.8251 - val_loss: 0.4951 - mcc: 0.7287\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 4\n","Epoch 1/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5933 - loss: 1.1309(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6304\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.5936 - loss: 1.1300 - val_accuracy: 0.7664 - val_loss: 0.6793 - mcc: 0.6304\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7837 - loss: 0.6123(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6949\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.7837 - loss: 0.6122 - val_accuracy: 0.8058 - val_loss: 0.5418 - mcc: 0.6949\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7843 - loss: 0.6192(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.6172\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.7843 - loss: 0.6192 - val_accuracy: 0.7578 - val_loss: 0.6837 - mcc: 0.6172\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7443 - loss: 0.7271(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.6620\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.7444 - loss: 0.7268 - val_accuracy: 0.7854 - val_loss: 0.6003 - mcc: 0.6620\n","Epoch 5/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7890 - loss: 0.5906(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.6750\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.7890 - loss: 0.5906 - val_accuracy: 0.7930 - val_loss: 0.5746 - mcc: 0.6750\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7936 - loss: 0.5749(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.6818\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.7936 - loss: 0.5750 - val_accuracy: 0.7972 - val_loss: 0.5689 - mcc: 0.6818\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8027 - loss: 0.5447(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.6979\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8027 - loss: 0.5447 - val_accuracy: 0.8066 - val_loss: 0.5395 - mcc: 0.6979\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8051 - loss: 0.5362(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.6714\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8051 - loss: 0.5363 - val_accuracy: 0.7910 - val_loss: 0.5845 - mcc: 0.6714\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7958 - loss: 0.5702(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7042\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.7958 - loss: 0.5702 - val_accuracy: 0.8107 - val_loss: 0.5177 - mcc: 0.7042\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8113 - loss: 0.5149(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7061\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8113 - loss: 0.5149 - val_accuracy: 0.8109 - val_loss: 0.5165 - mcc: 0.7061\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8134 - loss: 0.5144(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7173\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8134 - loss: 0.5144 - val_accuracy: 0.8188 - val_loss: 0.4963 - mcc: 0.7173\n","Epoch 12/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8184 - loss: 0.4971(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7178\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8184 - loss: 0.4971 - val_accuracy: 0.8191 - val_loss: 0.4940 - mcc: 0.7178\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8223 - loss: 0.4856(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7290\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8223 - loss: 0.4857 - val_accuracy: 0.8265 - val_loss: 0.4728 - mcc: 0.7290\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8078 - loss: 0.5378(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7339\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8078 - loss: 0.5378 - val_accuracy: 0.8287 - val_loss: 0.4649 - mcc: 0.7339\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8293 - loss: 0.4592(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7481\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8293 - loss: 0.4592 - val_accuracy: 0.8378 - val_loss: 0.4332 - mcc: 0.7481\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8111 - loss: 0.5199(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7147\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8111 - loss: 0.5199 - val_accuracy: 0.8166 - val_loss: 0.5362 - mcc: 0.7147\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8232 - loss: 0.4823(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7396\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.8232 - loss: 0.4822 - val_accuracy: 0.8319 - val_loss: 0.4505 - mcc: 0.7396\n","Epoch 18/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8319 - loss: 0.4492(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7208\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8319 - loss: 0.4492 - val_accuracy: 0.8211 - val_loss: 0.4899 - mcc: 0.7208\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8342 - loss: 0.4489(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.6930\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8342 - loss: 0.4489 - val_accuracy: 0.8028 - val_loss: 0.5318 - mcc: 0.6930\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8294 - loss: 0.4589(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7466\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8294 - loss: 0.4588 - val_accuracy: 0.8370 - val_loss: 0.4372 - mcc: 0.7466\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 5\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5957 - loss: 1.1444(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6453\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.5960 - loss: 1.1438 - val_accuracy: 0.7745 - val_loss: 0.6456 - mcc: 0.6453\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7737 - loss: 0.6469(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6601\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.7737 - loss: 0.6469 - val_accuracy: 0.7829 - val_loss: 0.6223 - mcc: 0.6601\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7929 - loss: 0.5945(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.5642\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.7928 - loss: 0.5946 - val_accuracy: 0.7273 - val_loss: 0.8013 - mcc: 0.5642\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7560 - loss: 0.7057(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.6849\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.7560 - loss: 0.7056 - val_accuracy: 0.7986 - val_loss: 0.5671 - mcc: 0.6849\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8004 - loss: 0.5680(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7017\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8004 - loss: 0.5680 - val_accuracy: 0.8088 - val_loss: 0.5272 - mcc: 0.7017\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8088 - loss: 0.5311(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7253\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8088 - loss: 0.5310 - val_accuracy: 0.8230 - val_loss: 0.4862 - mcc: 0.7253\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8227 - loss: 0.4878(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.6992\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8226 - loss: 0.4879 - val_accuracy: 0.8072 - val_loss: 0.5358 - mcc: 0.6992\n","Epoch 8/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7715 - loss: 0.6587(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7029\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.7716 - loss: 0.6583 - val_accuracy: 0.8090 - val_loss: 0.5217 - mcc: 0.7029\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8135 - loss: 0.5182(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7214\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8135 - loss: 0.5181 - val_accuracy: 0.8203 - val_loss: 0.4875 - mcc: 0.7214\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8200 - loss: 0.4966(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.6735\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8200 - loss: 0.4967 - val_accuracy: 0.7910 - val_loss: 0.5731 - mcc: 0.6735\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8124 - loss: 0.5148(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7254\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8124 - loss: 0.5147 - val_accuracy: 0.8229 - val_loss: 0.4776 - mcc: 0.7254\n","Epoch 12/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8266 - loss: 0.4717(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7384\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8266 - loss: 0.4717 - val_accuracy: 0.8309 - val_loss: 0.4517 - mcc: 0.7384\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8297 - loss: 0.4634(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.6860\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8297 - loss: 0.4635 - val_accuracy: 0.7991 - val_loss: 0.5601 - mcc: 0.6860\n","Epoch 14/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8222 - loss: 0.4874(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7317\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8222 - loss: 0.4873 - val_accuracy: 0.8262 - val_loss: 0.4744 - mcc: 0.7317\n","Epoch 15/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8339 - loss: 0.4499(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7249\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8339 - loss: 0.4499 - val_accuracy: 0.8226 - val_loss: 0.5087 - mcc: 0.7249\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8297 - loss: 0.4707(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7586\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8297 - loss: 0.4707 - val_accuracy: 0.8438 - val_loss: 0.4109 - mcc: 0.7586\n","Epoch 17/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8303 - loss: 0.4591(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7495\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8303 - loss: 0.4590 - val_accuracy: 0.8382 - val_loss: 0.4282 - mcc: 0.7495\n","Epoch 18/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8406 - loss: 0.4328(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7649\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8407 - loss: 0.4328 - val_accuracy: 0.8474 - val_loss: 0.3998 - mcc: 0.7649\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8476 - loss: 0.4073(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7454\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8476 - loss: 0.4074 - val_accuracy: 0.8355 - val_loss: 0.4461 - mcc: 0.7454\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8441 - loss: 0.4182(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7500\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8441 - loss: 0.4181 - val_accuracy: 0.8383 - val_loss: 0.4331 - mcc: 0.7500\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.838678),\n","              'mean': np.float64(0.8337565333333334),\n","              'min': np.float64(0.825104),\n","              'std': np.float64(0.005415060634112182)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0004406259854634603),\n","                               'mean': np.float64(0.0003752203623453776),\n","                               'min': np.float64(0.0002787819703420003),\n","                               'std': np.float64(7.768308735644801e-05)},\n"," 'MCC': {'max': np.float64(0.7499800220001123),\n","         'mean': np.float64(0.7419299195653297),\n","         'min': np.float64(0.7287158960100714),\n","         'std': np.float64(0.008436596581389033)},\n"," 'Parameters': 4517,\n"," 'Train Time (s)': {'max': np.float64(374.422509431839),\n","                    'mean': np.float64(352.7578793525696),\n","                    'min': np.float64(338.9622941017151),\n","                    'std': np.float64(12.275027143606852)},\n"," 'Training Accuracy': [[0.6702138781547546,\n","                        0.7805082201957703,\n","                        0.7963038086891174,\n","                        0.7881218791007996,\n","                        0.8080359697341919,\n","                        0.7830535769462585,\n","                        0.8024376630783081,\n","                        0.8125002980232239,\n","                        0.8162382245063782,\n","                        0.8113260269165039,\n","                        0.817391574382782,\n","                        0.8255219459533691,\n","                        0.8265962600708008,\n","                        0.8206812143325806,\n","                        0.7615966200828552,\n","                        0.784430742263794,\n","                        0.8063474893569946,\n","                        0.8172237873077393,\n","                        0.8113486170768738,\n","                        0.8118932843208313],\n","                       [0.6724010705947876,\n","                        0.7872648239135742,\n","                        0.8002442121505737,\n","                        0.7912423610687256,\n","                        0.7737427353858948,\n","                        0.7843385338783264,\n","                        0.785843014717102,\n","                        0.8077341914176941,\n","                        0.8074650168418884,\n","                        0.8136894702911377,\n","                        0.80387282371521,\n","                        0.7915791869163513,\n","                        0.8144192099571228,\n","                        0.8076439499855042,\n","                        0.8173668384552002,\n","                        0.8091744184494019,\n","                        0.8243337273597717,\n","                        0.8204976916313171,\n","                        0.8221827149391174,\n","                        0.830463707447052],\n","                       [0.6811344623565674,\n","                        0.7826634049415588,\n","                        0.7951381802558899,\n","                        0.8043128848075867,\n","                        0.8145157694816589,\n","                        0.8140438795089722,\n","                        0.8167116045951843,\n","                        0.828822910785675,\n","                        0.8071982264518738,\n","                        0.8302515745162964,\n","                        0.8227904438972473,\n","                        0.835584282875061,\n","                        0.8317902684211731,\n","                        0.8307642936706543,\n","                        0.7883687615394592,\n","                        0.8136408925056458,\n","                        0.7947496771812439,\n","                        0.810380220413208,\n","                        0.8244752883911133,\n","                        0.8220263123512268],\n","                       [0.68157958984375,\n","                        0.7895113229751587,\n","                        0.7848541140556335,\n","                        0.7632649540901184,\n","                        0.7904813885688782,\n","                        0.7931957244873047,\n","                        0.8033783435821533,\n","                        0.7956159114837646,\n","                        0.7976968884468079,\n","                        0.8118554949760437,\n","                        0.8117040991783142,\n","                        0.8164786696434021,\n","                        0.8194416761398315,\n","                        0.814543604850769,\n","                        0.8324586749076843,\n","                        0.8116771578788757,\n","                        0.8266621232032776,\n","                        0.830270528793335,\n","                        0.8328599333763123,\n","                        0.8362467288970947],\n","                       [0.6813201904296875,\n","                        0.7729933857917786,\n","                        0.7765336632728577,\n","                        0.774955153465271,\n","                        0.8003684282302856,\n","                        0.8165613412857056,\n","                        0.8145813941955566,\n","                        0.7892573475837708,\n","                        0.8154875636100769,\n","                        0.8118194341659546,\n","                        0.8195009827613831,\n","                        0.8285257816314697,\n","                        0.8237046003341675,\n","                        0.8305700421333313,\n","                        0.8369044661521912,\n","                        0.8349539637565613,\n","                        0.8336339592933655,\n","                        0.8427901864051819,\n","                        0.8450288772583008,\n","                        0.8450837135314941]],\n"," 'Training Loss': [[0.9325283765792847,\n","                    0.6178639531135559,\n","                    0.5813419818878174,\n","                    0.6146284341812134,\n","                    0.5399534702301025,\n","                    0.6352598071098328,\n","                    0.5645961761474609,\n","                    0.5255698561668396,\n","                    0.51163250207901,\n","                    0.5333685874938965,\n","                    0.5182969570159912,\n","                    0.4800177812576294,\n","                    0.479286253452301,\n","                    0.5161203742027283,\n","                    0.6832985877990723,\n","                    0.6254026293754578,\n","                    0.5451379418373108,\n","                    0.5080830454826355,\n","                    0.5369516015052795,\n","                    0.5342568159103394],\n","                   [0.9312590956687927,\n","                    0.6045787930488586,\n","                    0.5679436326026917,\n","                    0.5964919924736023,\n","                    0.6628550291061401,\n","                    0.6176068782806396,\n","                    0.603781521320343,\n","                    0.5307114124298096,\n","                    0.5567967891693115,\n","                    0.509637713432312,\n","                    0.55950927734375,\n","                    0.5875193476676941,\n","                    0.5196899771690369,\n","                    0.5378381609916687,\n","                    0.5063626170158386,\n","                    0.5407602787017822,\n","                    0.4853977560997009,\n","                    0.5103688836097717,\n","                    0.4847871959209442,\n","                    0.4605400264263153],\n","                   [0.9055324792861938,\n","                    0.6303403973579407,\n","                    0.5914400219917297,\n","                    0.5566943287849426,\n","                    0.5213629603385925,\n","                    0.5157456994056702,\n","                    0.5283949375152588,\n","                    0.47421878576278687,\n","                    0.5374475121498108,\n","                    0.4771507680416107,\n","                    0.4940001666545868,\n","                    0.4522930085659027,\n","                    0.47295331954956055,\n","                    0.46879497170448303,\n","                    0.5950098037719727,\n","                    0.5287817120552063,\n","                    0.5975751280784607,\n","                    0.542169451713562,\n","                    0.49851855635643005,\n","                    0.5077481865882874],\n","                   [0.9090567827224731,\n","                    0.5953574776649475,\n","                    0.6163350939750671,\n","                    0.670358419418335,\n","                    0.5871173143386841,\n","                    0.5794227719306946,\n","                    0.5412632822990417,\n","                    0.5676881670951843,\n","                    0.5620494484901428,\n","                    0.515756368637085,\n","                    0.5193047523498535,\n","                    0.5025086998939514,\n","                    0.4990900754928589,\n","                    0.5102167725563049,\n","                    0.44859781861305237,\n","                    0.5165881514549255,\n","                    0.4678841829299927,\n","                    0.4569588303565979,\n","                    0.45020318031311035,\n","                    0.43898823857307434],\n","                   [0.9107124209403992,\n","                    0.6524456143379211,\n","                    0.6580458879470825,\n","                    0.6464635133743286,\n","                    0.5657892823219299,\n","                    0.5052158236503601,\n","                    0.5222594141960144,\n","                    0.5924513339996338,\n","                    0.5110474228858948,\n","                    0.5253267884254456,\n","                    0.4947713315486908,\n","                    0.46388736367225647,\n","                    0.48510733246803284,\n","                    0.46076345443725586,\n","                    0.4428251087665558,\n","                    0.44949230551719666,\n","                    0.44866564869880676,\n","                    0.42329663038253784,\n","                    0.41617509722709656,\n","                    0.41552412509918213]],\n"," 'Validation Accuracy': [[0.7386960983276367,\n","                          0.8014193773269653,\n","                          0.7010573744773865,\n","                          0.7997567653656006,\n","                          0.8105190396308899,\n","                          0.8068985342979431,\n","                          0.8198649287223816,\n","                          0.8115973472595215,\n","                          0.7998031377792358,\n","                          0.8099715709686279,\n","                          0.8368919491767883,\n","                          0.8369448184967041,\n","                          0.8380940556526184,\n","                          0.6802045702934265,\n","                          0.7924684882164001,\n","                          0.8055980801582336,\n","                          0.8180187940597534,\n","                          0.8247387409210205,\n","                          0.8124591708183289,\n","                          0.8296985626220703],\n","                         [0.7772340774536133,\n","                          0.8031387329101562,\n","                          0.7970446944236755,\n","                          0.789750874042511,\n","                          0.7708700299263,\n","                          0.7990215420722961,\n","                          0.7997363209724426,\n","                          0.8185685873031616,\n","                          0.8104680776596069,\n","                          0.8043758869171143,\n","                          0.6379039883613586,\n","                          0.8159032464027405,\n","                          0.8245524764060974,\n","                          0.8225339651107788,\n","                          0.8087552785873413,\n","                          0.8003181219100952,\n","                          0.8357363939285278,\n","                          0.8253145217895508,\n","                          0.8359259963035583,\n","                          0.8386778831481934],\n","                         [0.7805389165878296,\n","                          0.7920873165130615,\n","                          0.8116100430488586,\n","                          0.7864142060279846,\n","                          0.8298747539520264,\n","                          0.8255246877670288,\n","                          0.8122708201408386,\n","                          0.6763018369674683,\n","                          0.8347418904304504,\n","                          0.8222500681877136,\n","                          0.8028630614280701,\n","                          0.8424344658851624,\n","                          0.8366817831993103,\n","                          0.8429906368255615,\n","                          0.7898041009902954,\n","                          0.8203209042549133,\n","                          0.7916216850280762,\n","                          0.8168498277664185,\n","                          0.8245811462402344,\n","                          0.8251041769981384],\n","                         [0.7663641571998596,\n","                          0.8058368563652039,\n","                          0.7577561140060425,\n","                          0.7853806018829346,\n","                          0.793032169342041,\n","                          0.7971712350845337,\n","                          0.8065944910049438,\n","                          0.7910309433937073,\n","                          0.8107393980026245,\n","                          0.8109380602836609,\n","                          0.8187853097915649,\n","                          0.8191331624984741,\n","                          0.8264672756195068,\n","                          0.8286600708961487,\n","                          0.8377718925476074,\n","                          0.8166337609291077,\n","                          0.8318653702735901,\n","                          0.8211034536361694,\n","                          0.8027938604354858,\n","                          0.8370477557182312],\n","                         [0.7745172381401062,\n","                          0.7828986048698425,\n","                          0.727251410484314,\n","                          0.7985599040985107,\n","                          0.8087541460990906,\n","                          0.8229787349700928,\n","                          0.8072419166564941,\n","                          0.8090161085128784,\n","                          0.820298433303833,\n","                          0.7910155653953552,\n","                          0.8229156732559204,\n","                          0.8308666944503784,\n","                          0.7990731596946716,\n","                          0.8261803984642029,\n","                          0.8226451873779297,\n","                          0.8437530994415283,\n","                          0.8382354974746704,\n","                          0.8473987579345703,\n","                          0.8354813456535339,\n","                          0.8382542133331299]],\n"," 'Validation Loss': [[0.740055501461029,\n","                      0.5463980436325073,\n","                      0.8936861753463745,\n","                      0.5715534687042236,\n","                      0.5316016674041748,\n","                      0.5537959933280945,\n","                      0.49474242329597473,\n","                      0.5249500274658203,\n","                      0.5639393925666809,\n","                      0.5464814305305481,\n","                      0.4494284987449646,\n","                      0.4455898106098175,\n","                      0.440230131149292,\n","                      1.081813097000122,\n","                      0.5835072994232178,\n","                      0.5491536855697632,\n","                      0.5068762302398682,\n","                      0.4846005439758301,\n","                      0.5305176377296448,\n","                      0.4743371605873108],\n","                     [0.6524320244789124,\n","                      0.5452055931091309,\n","                      0.588830828666687,\n","                      0.5886778831481934,\n","                      0.6640344858169556,\n","                      0.561008632183075,\n","                      0.5607317090034485,\n","                      0.5026841759681702,\n","                      0.5101261138916016,\n","                      0.5379157066345215,\n","                      0.9859282374382019,\n","                      0.5186734199523926,\n","                      0.4880649149417877,\n","                      0.48921874165534973,\n","                      0.5347980260848999,\n","                      0.5681645274162292,\n","                      0.45129892230033875,\n","                      0.5003480911254883,\n","                      0.44783657789230347,\n","                      0.4392131268978119],\n","                     [0.6388027667999268,\n","                      0.610666811466217,\n","                      0.5267205834388733,\n","                      0.598060131072998,\n","                      0.4641721248626709,\n","                      0.4805743098258972,\n","                      0.5246953368186951,\n","                      1.007568120956421,\n","                      0.45203959941864014,\n","                      0.49889108538627625,\n","                      0.5430011749267578,\n","                      0.42922770977020264,\n","                      0.45491358637809753,\n","                      0.4234744906425476,\n","                      0.6123495697975159,\n","                      0.5138087272644043,\n","                      0.6015036702156067,\n","                      0.5194801092147827,\n","                      0.49652034044265747,\n","                      0.49512267112731934],\n","                     [0.6793035268783569,\n","                      0.5418407320976257,\n","                      0.6837214231491089,\n","                      0.6002840399742126,\n","                      0.5745506882667542,\n","                      0.568865954875946,\n","                      0.539522647857666,\n","                      0.5844722390174866,\n","                      0.5176511406898499,\n","                      0.516460120677948,\n","                      0.49625667929649353,\n","                      0.4939678907394409,\n","                      0.47276753187179565,\n","                      0.4648769199848175,\n","                      0.4332119822502136,\n","                      0.5362478494644165,\n","                      0.4504598379135132,\n","                      0.48986417055130005,\n","                      0.5318028330802917,\n","                      0.43720459938049316],\n","                     [0.6455646753311157,\n","                      0.6222852468490601,\n","                      0.8012855648994446,\n","                      0.5671250820159912,\n","                      0.5271929502487183,\n","                      0.48618632555007935,\n","                      0.5357998609542847,\n","                      0.5217127799987793,\n","                      0.48746582865715027,\n","                      0.573060154914856,\n","                      0.4775521755218506,\n","                      0.4517478048801422,\n","                      0.5601239204406738,\n","                      0.4743611514568329,\n","                      0.5087474584579468,\n","                      0.41090938448905945,\n","                      0.4281803071498871,\n","                      0.3998301327228546,\n","                      0.44608065485954285,\n","                      0.4330933094024658]],\n"," 'Validation MCC': [[np.float64(0.593183237675023),\n","                     np.float64(0.6928867721910926),\n","                     np.float64(0.5360443617165237),\n","                     np.float64(0.6867776277496006),\n","                     np.float64(0.705155440816955),\n","                     np.float64(0.6984415316702774),\n","                     np.float64(0.7199325199824871),\n","                     np.float64(0.7068956222962186),\n","                     np.float64(0.6868297568527486),\n","                     np.float64(0.7031180219479378),\n","                     np.float64(0.7471355916844495),\n","                     np.float64(0.7471262289914848),\n","                     np.float64(0.749598189880304),\n","                     np.float64(0.49515993071176867),\n","                     np.float64(0.6749515476615622),\n","                     np.float64(0.6960983741053621),\n","                     np.float64(0.7165789340963047),\n","                     np.float64(0.728271828329247),\n","                     np.float64(0.7067162480093754),\n","                     np.float64(0.7353074998280316)],\n","                    [np.float64(0.6477118737118802),\n","                     np.float64(0.6913259627535187),\n","                     np.float64(0.6813213743772946),\n","                     np.float64(0.6704143543380369),\n","                     np.float64(0.6374301137060747),\n","                     np.float64(0.6854577045928218),\n","                     np.float64(0.685709358411232),\n","                     np.float64(0.7165937941183508),\n","                     np.float64(0.7037320003490497),\n","                     np.float64(0.6942177944616708),\n","                     np.float64(0.481891848770923),\n","                     np.float64(0.7121723854377248),\n","                     np.float64(0.7261592709700546),\n","                     np.float64(0.7230790016905098),\n","                     np.float64(0.7011312823648673),\n","                     np.float64(0.6887936835023046),\n","                     np.float64(0.7449993846453747),\n","                     np.float64(0.7277766132927884),\n","                     np.float64(0.7450132202057815),\n","                     np.float64(0.7490782559337117)],\n","                    [np.float64(0.655346888370897),\n","                     np.float64(0.674712402180859),\n","                     np.float64(0.7069529461084055),\n","                     np.float64(0.6679583822882523),\n","                     np.float64(0.7364534641956333),\n","                     np.float64(0.7299453350102),\n","                     np.float64(0.7089468841230442),\n","                     np.float64(0.5220564669890652),\n","                     np.float64(0.7439604175911769),\n","                     np.float64(0.7237592552411687),\n","                     np.float64(0.6932903149018308),\n","                     np.float64(0.7567225939292669),\n","                     np.float64(0.7473161631381751),\n","                     np.float64(0.7572352434639467),\n","                     np.float64(0.6696440325453183),\n","                     np.float64(0.72068809902909),\n","                     np.float64(0.6735475237681201),\n","                     np.float64(0.715098654603811),\n","                     np.float64(0.7279574495573339),\n","                     np.float64(0.7287158960100714)],\n","                    [np.float64(0.6303549194585581),\n","                     np.float64(0.6948978720414704),\n","                     np.float64(0.6172018756814951),\n","                     np.float64(0.6619682906144242),\n","                     np.float64(0.675003738788417),\n","                     np.float64(0.6817725747709446),\n","                     np.float64(0.6978602006293498),\n","                     np.float64(0.6713616131036302),\n","                     np.float64(0.704159581168555),\n","                     np.float64(0.7061052955901197),\n","                     np.float64(0.7173168862221058),\n","                     np.float64(0.7177846682410343),\n","                     np.float64(0.7290275705910257),\n","                     np.float64(0.7339361980026335),\n","                     np.float64(0.7481142403554534),\n","                     np.float64(0.7146611008818677),\n","                     np.float64(0.7395554935794519),\n","                     np.float64(0.7207826464254815),\n","                     np.float64(0.6929884043097129),\n","                     np.float64(0.7465679240547219)],\n","                    [np.float64(0.6453237054030916),\n","                     np.float64(0.6601480116232981),\n","                     np.float64(0.5642359041130802),\n","                     np.float64(0.6849128250326163),\n","                     np.float64(0.7017325531960393),\n","                     np.float64(0.7252927729350679),\n","                     np.float64(0.6992454244039817),\n","                     np.float64(0.7028875867070873),\n","                     np.float64(0.7214064102116711),\n","                     np.float64(0.6734830695186624),\n","                     np.float64(0.7253989556265016),\n","                     np.float64(0.7383784235779788),\n","                     np.float64(0.6859588271318833),\n","                     np.float64(0.7317130782764002),\n","                     np.float64(0.7248649827928199),\n","                     np.float64(0.7585794160007794),\n","                     np.float64(0.7495401551326395),\n","                     np.float64(0.7649178838119274),\n","                     np.float64(0.745389359178909),\n","                     np.float64(0.7499800220001123)]]}\n","Training Model: LSTM_Dense, Fold: 1\n","Epoch 1/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5886 - loss: 1.1448(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6758\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.5891 - loss: 1.1435 - val_accuracy: 0.7922 - val_loss: 0.5905 - mcc: 0.6758\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7897 - loss: 0.6063(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7061\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.7897 - loss: 0.6063 - val_accuracy: 0.8112 - val_loss: 0.5351 - mcc: 0.7061\n","Epoch 3/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8067 - loss: 0.5472(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.6837\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8067 - loss: 0.5471 - val_accuracy: 0.7967 - val_loss: 0.5736 - mcc: 0.6837\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8007 - loss: 0.5628(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7240\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8007 - loss: 0.5628 - val_accuracy: 0.8231 - val_loss: 0.4917 - mcc: 0.7240\n","Epoch 5/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8132 - loss: 0.5273(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7380\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8132 - loss: 0.5273 - val_accuracy: 0.8308 - val_loss: 0.4677 - mcc: 0.7380\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8187 - loss: 0.5039(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7487\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8187 - loss: 0.5039 - val_accuracy: 0.8375 - val_loss: 0.4374 - mcc: 0.7487\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8188 - loss: 0.5081(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7559\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8188 - loss: 0.5080 - val_accuracy: 0.8419 - val_loss: 0.4241 - mcc: 0.7559\n","Epoch 8/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8314 - loss: 0.4634(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.6453\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8314 - loss: 0.4634 - val_accuracy: 0.7663 - val_loss: 0.6782 - mcc: 0.6453\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8203 - loss: 0.4895(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7638\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8204 - loss: 0.4894 - val_accuracy: 0.8467 - val_loss: 0.4073 - mcc: 0.7638\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8311 - loss: 0.4610(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7541\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8311 - loss: 0.4610 - val_accuracy: 0.8408 - val_loss: 0.4256 - mcc: 0.7541\n","Epoch 11/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8268 - loss: 0.4733(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7245\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8267 - loss: 0.4735 - val_accuracy: 0.8227 - val_loss: 0.4874 - mcc: 0.7245\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8256 - loss: 0.4737(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7613\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8256 - loss: 0.4737 - val_accuracy: 0.8459 - val_loss: 0.4099 - mcc: 0.7613\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8374 - loss: 0.4379(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7704\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8375 - loss: 0.4379 - val_accuracy: 0.8507 - val_loss: 0.3939 - mcc: 0.7704\n","Epoch 14/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8416 - loss: 0.4251(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7727\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22ms/step - accuracy: 0.8417 - loss: 0.4250 - val_accuracy: 0.8526 - val_loss: 0.3914 - mcc: 0.7727\n","Epoch 15/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8329 - loss: 0.4639(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7597\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.8329 - loss: 0.4639 - val_accuracy: 0.8448 - val_loss: 0.4165 - mcc: 0.7597\n","Epoch 16/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8369 - loss: 0.4453(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7652\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8368 - loss: 0.4453 - val_accuracy: 0.8481 - val_loss: 0.4026 - mcc: 0.7652\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8469 - loss: 0.4084(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.6416\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8469 - loss: 0.4085 - val_accuracy: 0.7709 - val_loss: 0.7354 - mcc: 0.6416\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8264 - loss: 0.4841(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7547\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8264 - loss: 0.4840 - val_accuracy: 0.8414 - val_loss: 0.4326 - mcc: 0.7547\n","Epoch 19/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8440 - loss: 0.4257(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7747\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8440 - loss: 0.4257 - val_accuracy: 0.8540 - val_loss: 0.3902 - mcc: 0.7747\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8488 - loss: 0.4042(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7162\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8488 - loss: 0.4042 - val_accuracy: 0.8173 - val_loss: 0.5074 - mcc: 0.7162\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 2\n","Epoch 1/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5961 - loss: 1.1399(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6180\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.5965 - loss: 1.1388 - val_accuracy: 0.7568 - val_loss: 0.7204 - mcc: 0.6180\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7727 - loss: 0.6527(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6843\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.7727 - loss: 0.6526 - val_accuracy: 0.7981 - val_loss: 0.5614 - mcc: 0.6843\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7948 - loss: 0.5759(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.6975\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.7948 - loss: 0.5759 - val_accuracy: 0.8073 - val_loss: 0.5576 - mcc: 0.6975\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8127 - loss: 0.5247(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7361\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8127 - loss: 0.5247 - val_accuracy: 0.8304 - val_loss: 0.4587 - mcc: 0.7361\n","Epoch 5/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7966 - loss: 0.5801(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7308\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.7966 - loss: 0.5800 - val_accuracy: 0.8269 - val_loss: 0.4778 - mcc: 0.7308\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8221 - loss: 0.4943(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7409\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8221 - loss: 0.4943 - val_accuracy: 0.8329 - val_loss: 0.4606 - mcc: 0.7409\n","Epoch 7/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8177 - loss: 0.5126(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7483\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8177 - loss: 0.5125 - val_accuracy: 0.8378 - val_loss: 0.4397 - mcc: 0.7483\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8207 - loss: 0.5055(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7028\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8207 - loss: 0.5056 - val_accuracy: 0.8094 - val_loss: 0.5335 - mcc: 0.7028\n","Epoch 9/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8165 - loss: 0.4983(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7015\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8165 - loss: 0.4984 - val_accuracy: 0.8096 - val_loss: 0.5378 - mcc: 0.7015\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8115 - loss: 0.5265(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7331\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8115 - loss: 0.5265 - val_accuracy: 0.8286 - val_loss: 0.4771 - mcc: 0.7331\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8252 - loss: 0.4802(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7312\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8252 - loss: 0.4802 - val_accuracy: 0.8278 - val_loss: 0.4813 - mcc: 0.7312\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8260 - loss: 0.4828(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.5837\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8260 - loss: 0.4828 - val_accuracy: 0.7310 - val_loss: 0.8125 - mcc: 0.5837\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7959 - loss: 0.5866(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7516\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.7959 - loss: 0.5865 - val_accuracy: 0.8402 - val_loss: 0.4296 - mcc: 0.7516\n","Epoch 14/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8376 - loss: 0.4409(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7620\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8376 - loss: 0.4409 - val_accuracy: 0.8464 - val_loss: 0.4128 - mcc: 0.7620\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8176 - loss: 0.5134(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.6705\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8176 - loss: 0.5136 - val_accuracy: 0.7909 - val_loss: 0.5893 - mcc: 0.6705\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7948 - loss: 0.5735(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.7948 - loss: 0.5734 - val_accuracy: 0.8142 - val_loss: 0.5199 - mcc: 0.7100\n","Epoch 17/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8111 - loss: 0.5259(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7132\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8111 - loss: 0.5260 - val_accuracy: 0.8163 - val_loss: 0.5138 - mcc: 0.7132\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8183 - loss: 0.4989(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7280\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8183 - loss: 0.4989 - val_accuracy: 0.8260 - val_loss: 0.4790 - mcc: 0.7280\n","Epoch 19/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8266 - loss: 0.4752(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7415\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8266 - loss: 0.4752 - val_accuracy: 0.8337 - val_loss: 0.4536 - mcc: 0.7415\n","Epoch 20/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8296 - loss: 0.4625(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7448\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8296 - loss: 0.4625 - val_accuracy: 0.8360 - val_loss: 0.4442 - mcc: 0.7448\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 3\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5753 - loss: 1.1667(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.5523\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.5753 - loss: 1.1665 - val_accuracy: 0.7151 - val_loss: 0.7983 - mcc: 0.5523\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7601 - loss: 0.6827(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6817\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.7602 - loss: 0.6824 - val_accuracy: 0.7963 - val_loss: 0.5787 - mcc: 0.6817\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8044 - loss: 0.5480(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7058\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8044 - loss: 0.5479 - val_accuracy: 0.8109 - val_loss: 0.5246 - mcc: 0.7058\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8142 - loss: 0.5205(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7227\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8142 - loss: 0.5206 - val_accuracy: 0.8212 - val_loss: 0.4879 - mcc: 0.7227\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7847 - loss: 0.6114(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7076\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.7848 - loss: 0.6114 - val_accuracy: 0.8118 - val_loss: 0.5366 - mcc: 0.7076\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8148 - loss: 0.5257(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7253\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8148 - loss: 0.5257 - val_accuracy: 0.8227 - val_loss: 0.5019 - mcc: 0.7253\n","Epoch 7/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8143 - loss: 0.5222(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7339\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8144 - loss: 0.5221 - val_accuracy: 0.8281 - val_loss: 0.4737 - mcc: 0.7339\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8233 - loss: 0.4908(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7249\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8232 - loss: 0.4908 - val_accuracy: 0.8230 - val_loss: 0.5028 - mcc: 0.7249\n","Epoch 9/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8231 - loss: 0.4996(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.6228\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8231 - loss: 0.4998 - val_accuracy: 0.7615 - val_loss: 0.6738 - mcc: 0.6228\n","Epoch 10/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7964 - loss: 0.5743(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7208\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.7965 - loss: 0.5742 - val_accuracy: 0.8197 - val_loss: 0.4977 - mcc: 0.7208\n","Epoch 11/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8245 - loss: 0.4841(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7357\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8245 - loss: 0.4841 - val_accuracy: 0.8293 - val_loss: 0.4655 - mcc: 0.7357\n","Epoch 12/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8272 - loss: 0.4747(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7407\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8272 - loss: 0.4746 - val_accuracy: 0.8322 - val_loss: 0.4494 - mcc: 0.7407\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8320 - loss: 0.4587(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7352\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8320 - loss: 0.4588 - val_accuracy: 0.8288 - val_loss: 0.4639 - mcc: 0.7352\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8375 - loss: 0.4354(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7647\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8375 - loss: 0.4353 - val_accuracy: 0.8473 - val_loss: 0.4008 - mcc: 0.7647\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8387 - loss: 0.4329(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7651\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8387 - loss: 0.4328 - val_accuracy: 0.8478 - val_loss: 0.4085 - mcc: 0.7651\n","Epoch 16/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8392 - loss: 0.4390(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7382\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8392 - loss: 0.4391 - val_accuracy: 0.8307 - val_loss: 0.4671 - mcc: 0.7382\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8245 - loss: 0.4906(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7026\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8245 - loss: 0.4907 - val_accuracy: 0.8095 - val_loss: 0.5332 - mcc: 0.7026\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8171 - loss: 0.5040(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7387\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8172 - loss: 0.5040 - val_accuracy: 0.8311 - val_loss: 0.4610 - mcc: 0.7387\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8276 - loss: 0.4749(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7355\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8276 - loss: 0.4749 - val_accuracy: 0.8293 - val_loss: 0.4697 - mcc: 0.7355\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8334 - loss: 0.4553(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7439\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8334 - loss: 0.4553 - val_accuracy: 0.8344 - val_loss: 0.4553 - mcc: 0.7439\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 4\n","Epoch 1/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5983 - loss: 1.1237(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6384\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.5987 - loss: 1.1228 - val_accuracy: 0.7708 - val_loss: 0.6678 - mcc: 0.6384\n","Epoch 2/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7654 - loss: 0.6782(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6844\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.7655 - loss: 0.6780 - val_accuracy: 0.7978 - val_loss: 0.5705 - mcc: 0.6844\n","Epoch 3/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7890 - loss: 0.6058(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.6797\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.7890 - loss: 0.6058 - val_accuracy: 0.7960 - val_loss: 0.5854 - mcc: 0.6797\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8076 - loss: 0.5325(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7090\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8076 - loss: 0.5325 - val_accuracy: 0.8120 - val_loss: 0.5168 - mcc: 0.7090\n","Epoch 5/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8117 - loss: 0.5187(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7351\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8117 - loss: 0.5186 - val_accuracy: 0.8297 - val_loss: 0.4557 - mcc: 0.7351\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8320 - loss: 0.4450(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7251\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8320 - loss: 0.4450 - val_accuracy: 0.8232 - val_loss: 0.4719 - mcc: 0.7251\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8365 - loss: 0.4310(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7462\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8365 - loss: 0.4310 - val_accuracy: 0.8367 - val_loss: 0.4310 - mcc: 0.7462\n","Epoch 8/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8387 - loss: 0.4277(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7510\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8387 - loss: 0.4276 - val_accuracy: 0.8395 - val_loss: 0.4425 - mcc: 0.7510\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8472 - loss: 0.4040(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7699\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8472 - loss: 0.4040 - val_accuracy: 0.8509 - val_loss: 0.3918 - mcc: 0.7699\n","Epoch 10/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8505 - loss: 0.3918(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7706\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8505 - loss: 0.3918 - val_accuracy: 0.8516 - val_loss: 0.3915 - mcc: 0.7706\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8538 - loss: 0.3836(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7763\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8538 - loss: 0.3836 - val_accuracy: 0.8548 - val_loss: 0.3801 - mcc: 0.7763\n","Epoch 12/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8527 - loss: 0.3865(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7722\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8527 - loss: 0.3865 - val_accuracy: 0.8517 - val_loss: 0.3895 - mcc: 0.7722\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8577 - loss: 0.3751(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7796\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8577 - loss: 0.3752 - val_accuracy: 0.8577 - val_loss: 0.3734 - mcc: 0.7796\n","Epoch 14/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8575 - loss: 0.3741(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7813\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8575 - loss: 0.3741 - val_accuracy: 0.8584 - val_loss: 0.3709 - mcc: 0.7813\n","Epoch 15/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8555 - loss: 0.3795(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7762\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8555 - loss: 0.3795 - val_accuracy: 0.8559 - val_loss: 0.3797 - mcc: 0.7762\n","Epoch 16/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8524 - loss: 0.3917(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7806\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8525 - loss: 0.3916 - val_accuracy: 0.8579 - val_loss: 0.3711 - mcc: 0.7806\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8609 - loss: 0.3637(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7811\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8609 - loss: 0.3638 - val_accuracy: 0.8584 - val_loss: 0.3727 - mcc: 0.7811\n","Epoch 18/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8590 - loss: 0.3700(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7846\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8590 - loss: 0.3700 - val_accuracy: 0.8608 - val_loss: 0.3687 - mcc: 0.7846\n","Epoch 19/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8613 - loss: 0.3627(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7795\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8612 - loss: 0.3627 - val_accuracy: 0.8578 - val_loss: 0.3769 - mcc: 0.7795\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8618 - loss: 0.3623(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7834\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8618 - loss: 0.3623 - val_accuracy: 0.8600 - val_loss: 0.3654 - mcc: 0.7834\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5915 - loss: 1.1409(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6337\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.5917 - loss: 1.1406 - val_accuracy: 0.7676 - val_loss: 0.6565 - mcc: 0.6337\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7685 - loss: 0.6640(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6042\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.7685 - loss: 0.6641 - val_accuracy: 0.7501 - val_loss: 0.7104 - mcc: 0.6042\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7703 - loss: 0.6599(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.6815\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.7703 - loss: 0.6598 - val_accuracy: 0.7962 - val_loss: 0.5833 - mcc: 0.6815\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8002 - loss: 0.5719(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.5939\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8002 - loss: 0.5719 - val_accuracy: 0.7337 - val_loss: 0.7902 - mcc: 0.5939\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8041 - loss: 0.5558(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.6840\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8041 - loss: 0.5558 - val_accuracy: 0.7972 - val_loss: 0.5631 - mcc: 0.6840\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8068 - loss: 0.5365(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.6886\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8068 - loss: 0.5365 - val_accuracy: 0.8006 - val_loss: 0.5448 - mcc: 0.6886\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7964 - loss: 0.5762(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7192\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.7964 - loss: 0.5761 - val_accuracy: 0.8181 - val_loss: 0.4893 - mcc: 0.7192\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7962 - loss: 0.5989(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.6619\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.7962 - loss: 0.5988 - val_accuracy: 0.7837 - val_loss: 0.6125 - mcc: 0.6619\n","Epoch 9/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8134 - loss: 0.5250(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7218\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8134 - loss: 0.5249 - val_accuracy: 0.8211 - val_loss: 0.5004 - mcc: 0.7218\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8273 - loss: 0.4744(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7168\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8273 - loss: 0.4744 - val_accuracy: 0.8169 - val_loss: 0.5491 - mcc: 0.7168\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8187 - loss: 0.4983(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.6676\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8186 - loss: 0.4985 - val_accuracy: 0.7873 - val_loss: 0.6016 - mcc: 0.6676\n","Epoch 12/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7973 - loss: 0.5698(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7000\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.7973 - loss: 0.5697 - val_accuracy: 0.8071 - val_loss: 0.5343 - mcc: 0.7000\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8085 - loss: 0.5419(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7086\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8085 - loss: 0.5419 - val_accuracy: 0.8117 - val_loss: 0.5163 - mcc: 0.7086\n","Epoch 14/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8192 - loss: 0.4975(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7256\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8192 - loss: 0.4975 - val_accuracy: 0.8228 - val_loss: 0.4809 - mcc: 0.7256\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8274 - loss: 0.4732(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7354\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8274 - loss: 0.4732 - val_accuracy: 0.8291 - val_loss: 0.4671 - mcc: 0.7354\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8257 - loss: 0.4832(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7324\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8257 - loss: 0.4832 - val_accuracy: 0.8276 - val_loss: 0.4727 - mcc: 0.7324\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8335 - loss: 0.4551(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7366\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8335 - loss: 0.4552 - val_accuracy: 0.8303 - val_loss: 0.4628 - mcc: 0.7366\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8353 - loss: 0.4449(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7543\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.8353 - loss: 0.4449 - val_accuracy: 0.8412 - val_loss: 0.4266 - mcc: 0.7543\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8377 - loss: 0.4381(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7588\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8377 - loss: 0.4381 - val_accuracy: 0.8443 - val_loss: 0.4235 - mcc: 0.7588\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8327 - loss: 0.4597(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7408\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8327 - loss: 0.4597 - val_accuracy: 0.8318 - val_loss: 0.4628 - mcc: 0.7408\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.8600353333333334),\n","              'mean': np.float64(0.8358956),\n","              'min': np.float64(0.8172506666666667),\n","              'std': np.float64(0.013781324204073353)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0004367364247639974),\n","                               'mean': np.float64(0.0002994873046875),\n","                               'min': np.float64(0.0002548948129018148),\n","                               'std': np.float64(6.954490174022788e-05)},\n"," 'MCC': {'max': np.float64(0.7834340291915469),\n","         'mean': np.float64(0.7458280348355327),\n","         'min': np.float64(0.7161749192250476),\n","         'std': np.float64(0.021558376985451766)},\n"," 'Parameters': 4965,\n"," 'Train Time (s)': {'max': np.float64(373.44537806510925),\n","                    'mean': np.float64(354.98502054214475),\n","                    'min': np.float64(340.05269408226013),\n","                    'std': np.float64(11.419048565074156)},\n"," 'Training Accuracy': [[0.6777069568634033,\n","                        0.7911803126335144,\n","                        0.8069486618041992,\n","                        0.8024897575378418,\n","                        0.81184983253479,\n","                        0.8191918730735779,\n","                        0.8215776085853577,\n","                        0.8300140500068665,\n","                        0.8289559483528137,\n","                        0.826683521270752,\n","                        0.8145761489868164,\n","                        0.8310584425926208,\n","                        0.8402761220932007,\n","                        0.8430954813957214,\n","                        0.834048330783844,\n","                        0.8329347968101501,\n","                        0.844819962978363,\n","                        0.8275591731071472,\n","                        0.8414620757102966,\n","                        0.8463965654373169],\n","                       [0.6681757569313049,\n","                        0.7795359492301941,\n","                        0.7968466877937317,\n","                        0.8166608214378357,\n","                        0.8053041100502014,\n","                        0.8220951557159424,\n","                        0.8238732814788818,\n","                        0.8132261633872986,\n","                        0.8102098703384399,\n","                        0.8160644769668579,\n","                        0.8251804113388062,\n","                        0.8257274627685547,\n","                        0.814817488193512,\n","                        0.8382383584976196,\n","                        0.7963556051254272,\n","                        0.8022162914276123,\n","                        0.8103575110435486,\n","                        0.8205156326293945,\n","                        0.8255923390388489,\n","                        0.8304920792579651],\n","                       [0.6275947690010071,\n","                        0.7802969813346863,\n","                        0.8102490901947021,\n","                        0.8110091686248779,\n","                        0.7954052090644836,\n","                        0.8177993893623352,\n","                        0.8189502954483032,\n","                        0.8185164332389832,\n","                        0.816275417804718,\n","                        0.8073188662528992,\n","                        0.8251433372497559,\n","                        0.8297082781791687,\n","                        0.8261240124702454,\n","                        0.8406387567520142,\n","                        0.8427963852882385,\n","                        0.8309575319290161,\n","                        0.8158804774284363,\n","                        0.822806179523468,\n","                        0.8269529342651367,\n","                        0.8345337510108948],\n","                       [0.6841810941696167,\n","                        0.7753823399543762,\n","                        0.7899577617645264,\n","                        0.8096328377723694,\n","                        0.8186583518981934,\n","                        0.8340874314308167,\n","                        0.8385496139526367,\n","                        0.8424668908119202,\n","                        0.8483482003211975,\n","                        0.8506262302398682,\n","                        0.8527963757514954,\n","                        0.8549020886421204,\n","                        0.8553225994110107,\n","                        0.85679030418396,\n","                        0.8574028611183167,\n","                        0.8565253019332886,\n","                        0.8595088124275208,\n","                        0.8600683808326721,\n","                        0.8605657815933228,\n","                        0.8614063858985901],\n","                       [0.6755867004394531,\n","                        0.7612205147743225,\n","                        0.7829808592796326,\n","                        0.7992839217185974,\n","                        0.8019202351570129,\n","                        0.8107277154922485,\n","                        0.8066695928573608,\n","                        0.7997456192970276,\n","                        0.8166510462760925,\n","                        0.8308836221694946,\n","                        0.8004937171936035,\n","                        0.8038440942764282,\n","                        0.8100036382675171,\n","                        0.8214730024337769,\n","                        0.8270586729049683,\n","                        0.8205610513687134,\n","                        0.8312528729438782,\n","                        0.8388395309448242,\n","                        0.841683030128479,\n","                        0.8284276723861694]],\n"," 'Training Loss': [[0.905098557472229,\n","                    0.6045210957527161,\n","                    0.539945125579834,\n","                    0.5568264126777649,\n","                    0.5355265736579895,\n","                    0.5004674196243286,\n","                    0.4962845742702484,\n","                    0.465232253074646,\n","                    0.46328288316726685,\n","                    0.475741982460022,\n","                    0.513992428779602,\n","                    0.4576471447944641,\n","                    0.42925727367401123,\n","                    0.41959497332572937,\n","                    0.4572547674179077,\n","                    0.45504987239837646,\n","                    0.41846761107444763,\n","                    0.4770936667919159,\n","                    0.4298098683357239,\n","                    0.41229012608528137],\n","                   [0.9294986724853516,\n","                    0.6289363503456116,\n","                    0.5715635418891907,\n","                    0.5099738836288452,\n","                    0.5536220669746399,\n","                    0.4945409297943115,\n","                    0.489855021238327,\n","                    0.5180942416191101,\n","                    0.5204952955245972,\n","                    0.5123980045318604,\n","                    0.48225995898246765,\n","                    0.484559029340744,\n","                    0.5230844020843506,\n","                    0.437967449426651,\n","                    0.5700452327728271,\n","                    0.5527257323265076,\n","                    0.5285180807113647,\n","                    0.49287304282188416,\n","                    0.4785638749599457,\n","                    0.4612715244293213],\n","                   [0.9959545135498047,\n","                    0.6257253885269165,\n","                    0.5278923511505127,\n","                    0.5278106331825256,\n","                    0.5862487554550171,\n","                    0.5143957138061523,\n","                    0.5062375664710999,\n","                    0.5109432339668274,\n","                    0.5245037078857422,\n","                    0.5392602682113647,\n","                    0.48198169469833374,\n","                    0.46449756622314453,\n","                    0.47615474462509155,\n","                    0.42468956112861633,\n","                    0.4241386353969574,\n","                    0.46836867928504944,\n","                    0.5163057446479797,\n","                    0.4873238801956177,\n","                    0.4771825075149536,\n","                    0.45045801997184753],\n","                   [0.9007112383842468,\n","                    0.6452093124389648,\n","                    0.597477912902832,\n","                    0.5235562920570374,\n","                    0.49606403708457947,\n","                    0.439850777387619,\n","                    0.42680469155311584,\n","                    0.4167378842830658,\n","                    0.39948299527168274,\n","                    0.3925369381904602,\n","                    0.3876098692417145,\n","                    0.38161933422088623,\n","                    0.3804595172405243,\n","                    0.3758767545223236,\n","                    0.37460529804229736,\n","                    0.37848004698753357,\n","                    0.3682355582714081,\n","                    0.3668456971645355,\n","                    0.36472374200820923,\n","                    0.36248713731765747],\n","                   [0.9117425084114075,\n","                    0.6837280988693237,\n","                    0.6251344680786133,\n","                    0.5745071768760681,\n","                    0.5592510104179382,\n","                    0.5259150266647339,\n","                    0.5437328815460205,\n","                    0.5809034109115601,\n","                    0.5107055306434631,\n","                    0.46370381116867065,\n","                    0.5528326630592346,\n","                    0.5492861866950989,\n","                    0.5361043810844421,\n","                    0.491342693567276,\n","                    0.47324731945991516,\n","                    0.5013685822486877,\n","                    0.4633045792579651,\n","                    0.4347572922706604,\n","                    0.4276173412799835,\n","                    0.474653035402298]],\n"," 'Validation Accuracy': [[0.7921561002731323,\n","                          0.8112191557884216,\n","                          0.7966986298561096,\n","                          0.823072075843811,\n","                          0.8307700157165527,\n","                          0.8375120162963867,\n","                          0.8418944478034973,\n","                          0.7662986516952515,\n","                          0.8467411994934082,\n","                          0.8407779932022095,\n","                          0.8226780891418457,\n","                          0.8459005355834961,\n","                          0.8506551384925842,\n","                          0.8526362180709839,\n","                          0.8447532653808594,\n","                          0.8481155037879944,\n","                          0.7708871960639954,\n","                          0.8414499759674072,\n","                          0.8540331125259399,\n","                          0.8172505497932434],\n","                         [0.7567655444145203,\n","                          0.7981258034706116,\n","                          0.8072701692581177,\n","                          0.8304020762443542,\n","                          0.8269100189208984,\n","                          0.8328510522842407,\n","                          0.8377981185913086,\n","                          0.8094331622123718,\n","                          0.8095689415931702,\n","                          0.8285881876945496,\n","                          0.8277647495269775,\n","                          0.7310196757316589,\n","                          0.8401975035667419,\n","                          0.8463894128799438,\n","                          0.7908779978752136,\n","                          0.8141718506813049,\n","                          0.8162527084350586,\n","                          0.8260331749916077,\n","                          0.8337448835372925,\n","                          0.836031973361969],\n","                         [0.7151114344596863,\n","                          0.7963024973869324,\n","                          0.810867428779602,\n","                          0.8211939930915833,\n","                          0.8117733597755432,\n","                          0.8227447271347046,\n","                          0.8280587792396545,\n","                          0.8229953050613403,\n","                          0.7615499496459961,\n","                          0.8197367191314697,\n","                          0.8292508125305176,\n","                          0.8321873545646667,\n","                          0.8288270831108093,\n","                          0.8472827076911926,\n","                          0.8478243947029114,\n","                          0.8306869864463806,\n","                          0.8094567060470581,\n","                          0.831134021282196,\n","                          0.8293378353118896,\n","                          0.834382176399231],\n","                         [0.770796000957489,\n","                          0.7978319525718689,\n","                          0.7960151433944702,\n","                          0.811959981918335,\n","                          0.8297046422958374,\n","                          0.8232107758522034,\n","                          0.8366684317588806,\n","                          0.8395131826400757,\n","                          0.8509060144424438,\n","                          0.8515720367431641,\n","                          0.8548187017440796,\n","                          0.8517458438873291,\n","                          0.8576758503913879,\n","                          0.858414888381958,\n","                          0.8558539748191833,\n","                          0.8579214811325073,\n","                          0.8583659529685974,\n","                          0.8607721328735352,\n","                          0.8577931523323059,\n","                          0.860035240650177],\n","                         [0.76761794090271,\n","                          0.7500614523887634,\n","                          0.7962182760238647,\n","                          0.7336840629577637,\n","                          0.7972431778907776,\n","                          0.800646960735321,\n","                          0.8181039094924927,\n","                          0.7836717963218689,\n","                          0.8210769295692444,\n","                          0.8169362545013428,\n","                          0.7872748970985413,\n","                          0.8070684671401978,\n","                          0.811700701713562,\n","                          0.8228440880775452,\n","                          0.8290534019470215,\n","                          0.8276464939117432,\n","                          0.8302805423736572,\n","                          0.841225266456604,\n","                          0.8442591428756714,\n","                          0.831778347492218]],\n"," 'Validation Loss': [[0.5904932618141174,\n","                      0.5350611805915833,\n","                      0.5736357569694519,\n","                      0.491658478975296,\n","                      0.4677004814147949,\n","                      0.4374358654022217,\n","                      0.4240990877151489,\n","                      0.6782300472259521,\n","                      0.40729019045829773,\n","                      0.4256218373775482,\n","                      0.48736947774887085,\n","                      0.4098896086215973,\n","                      0.3938685357570648,\n","                      0.39142680168151855,\n","                      0.41649457812309265,\n","                      0.4025909900665283,\n","                      0.7354348301887512,\n","                      0.43258175253868103,\n","                      0.3901612460613251,\n","                      0.5073814988136292],\n","                     [0.7204189300537109,\n","                      0.5614001750946045,\n","                      0.55762779712677,\n","                      0.4587286412715912,\n","                      0.4778349995613098,\n","                      0.4606296420097351,\n","                      0.43966540694236755,\n","                      0.5334847569465637,\n","                      0.5377559661865234,\n","                      0.47712841629981995,\n","                      0.481334388256073,\n","                      0.8125151991844177,\n","                      0.42964908480644226,\n","                      0.4127846360206604,\n","                      0.5892983675003052,\n","                      0.5199124217033386,\n","                      0.5138297080993652,\n","                      0.47902703285217285,\n","                      0.453621506690979,\n","                      0.44422414898872375],\n","                     [0.7983368039131165,\n","                      0.5787066221237183,\n","                      0.5246242880821228,\n","                      0.48793160915374756,\n","                      0.5365520715713501,\n","                      0.5019416213035583,\n","                      0.4736648499965668,\n","                      0.5028132200241089,\n","                      0.6737735271453857,\n","                      0.49774518609046936,\n","                      0.46553051471710205,\n","                      0.44939079880714417,\n","                      0.4638681709766388,\n","                      0.4007989168167114,\n","                      0.408529669046402,\n","                      0.46709945797920227,\n","                      0.5332192182540894,\n","                      0.46103280782699585,\n","                      0.46970587968826294,\n","                      0.45526108145713806],\n","                     [0.6678439378738403,\n","                      0.5704850554466248,\n","                      0.5853546261787415,\n","                      0.516796350479126,\n","                      0.4556509554386139,\n","                      0.4718881845474243,\n","                      0.43102091550827026,\n","                      0.44254761934280396,\n","                      0.3917548954486847,\n","                      0.39149364829063416,\n","                      0.38005566596984863,\n","                      0.38945671916007996,\n","                      0.3734479546546936,\n","                      0.3708629608154297,\n","                      0.3796517848968506,\n","                      0.37108469009399414,\n","                      0.3726577162742615,\n","                      0.36867937445640564,\n","                      0.3768630027770996,\n","                      0.3654012084007263],\n","                     [0.6564748883247375,\n","                      0.7103841304779053,\n","                      0.5833475589752197,\n","                      0.7902288436889648,\n","                      0.5630760192871094,\n","                      0.5448476076126099,\n","                      0.48928341269493103,\n","                      0.6125136017799377,\n","                      0.5003766417503357,\n","                      0.5491056442260742,\n","                      0.6015617847442627,\n","                      0.5343117117881775,\n","                      0.516348123550415,\n","                      0.48088452219963074,\n","                      0.4670715630054474,\n","                      0.4727354049682617,\n","                      0.4627998173236847,\n","                      0.4265919327735901,\n","                      0.4235345721244812,\n","                      0.4627784490585327]],\n"," 'Validation MCC': [[np.float64(0.6757561522226344),\n","                     np.float64(0.706099799494653),\n","                     np.float64(0.6836874973995792),\n","                     np.float64(0.724031971441062),\n","                     np.float64(0.7379557130677532),\n","                     np.float64(0.7486578239904396),\n","                     np.float64(0.7559498726035365),\n","                     np.float64(0.6452990440967536),\n","                     np.float64(0.7638019937837254),\n","                     np.float64(0.7541387504649502),\n","                     np.float64(0.7244899311176758),\n","                     np.float64(0.7612882128599312),\n","                     np.float64(0.7704112843328186),\n","                     np.float64(0.7726734471133487),\n","                     np.float64(0.7597481173186164),\n","                     np.float64(0.7652435433680991),\n","                     np.float64(0.6415646978556708),\n","                     np.float64(0.7547122388522823),\n","                     np.float64(0.7746741158026943),\n","                     np.float64(0.7161749192250476)],\n","                    [np.float64(0.617975549596968),\n","                     np.float64(0.6843226624062997),\n","                     np.float64(0.6975184277659877),\n","                     np.float64(0.7360595152680812),\n","                     np.float64(0.7307756862963383),\n","                     np.float64(0.7409365026258968),\n","                     np.float64(0.7482883282694978),\n","                     np.float64(0.7028498431407685),\n","                     np.float64(0.7014969146594877),\n","                     np.float64(0.7330974679391635),\n","                     np.float64(0.7312455866233079),\n","                     np.float64(0.5837207817392651),\n","                     np.float64(0.7515826329799603),\n","                     np.float64(0.7620481238418493),\n","                     np.float64(0.6704593800769334),\n","                     np.float64(0.7100112403825405),\n","                     np.float64(0.7132412884564833),\n","                     np.float64(0.7279567801559145),\n","                     np.float64(0.7415371606666402),\n","                     np.float64(0.7448338364967242)],\n","                    [np.float64(0.5522528552223648),\n","                     np.float64(0.6816660938610449),\n","                     np.float64(0.7057828560537257),\n","                     np.float64(0.7227022662319068),\n","                     np.float64(0.7076310102771738),\n","                     np.float64(0.7253194511839159),\n","                     np.float64(0.7338880346104147),\n","                     np.float64(0.7249127154471837),\n","                     np.float64(0.6228228817424972),\n","                     np.float64(0.720760121215994),\n","                     np.float64(0.7357253393157085),\n","                     np.float64(0.7406522087152821),\n","                     np.float64(0.7351618170215355),\n","                     np.float64(0.7647383903705106),\n","                     np.float64(0.7651103263750142),\n","                     np.float64(0.7382469544886141),\n","                     np.float64(0.7026370435148833),\n","                     np.float64(0.7387486718093498),\n","                     np.float64(0.7354736891221247),\n","                     np.float64(0.7439455614836329)],\n","                    [np.float64(0.6384149527006124),\n","                     np.float64(0.684377681505073),\n","                     np.float64(0.6796635027095261),\n","                     np.float64(0.7089733902713192),\n","                     np.float64(0.735080153962422),\n","                     np.float64(0.7250549831845172),\n","                     np.float64(0.7461984708975982),\n","                     np.float64(0.7509962016911333),\n","                     np.float64(0.7699382548154362),\n","                     np.float64(0.7705592279276335),\n","                     np.float64(0.7763324121348787),\n","                     np.float64(0.7722079647028052),\n","                     np.float64(0.7796327844070099),\n","                     np.float64(0.7812868708337725),\n","                     np.float64(0.7762085303847099),\n","                     np.float64(0.7805617111750704),\n","                     np.float64(0.7811177649284278),\n","                     np.float64(0.784613535739904),\n","                     np.float64(0.7794956902122019),\n","                     np.float64(0.7834340291915469)],\n","                    [np.float64(0.6337095616928498),\n","                     np.float64(0.6042486511632374),\n","                     np.float64(0.681517804121709),\n","                     np.float64(0.5938880753955826),\n","                     np.float64(0.6839580323087886),\n","                     np.float64(0.6885671914880673),\n","                     np.float64(0.7191560382779408),\n","                     np.float64(0.6619068157666058),\n","                     np.float64(0.7218067831915255),\n","                     np.float64(0.7167739677826184),\n","                     np.float64(0.6675857993347849),\n","                     np.float64(0.7000210778955316),\n","                     np.float64(0.7085940971736144),\n","                     np.float64(0.7256496086670973),\n","                     np.float64(0.7354394968617518),\n","                     np.float64(0.7323712932229343),\n","                     np.float64(0.7366350388090386),\n","                     np.float64(0.7542675125354124),\n","                     np.float64(0.7588405179126442),\n","                     np.float64(0.740751827780712)]]}\n","Training Model: LSTM_Deep, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6531 - loss: 0.9989(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6456\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.6533 - loss: 0.9983 - val_accuracy: 0.7746 - val_loss: 0.6620 - mcc: 0.6456\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7943 - loss: 0.5893(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6820\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.7943 - loss: 0.5893 - val_accuracy: 0.7972 - val_loss: 0.5745 - mcc: 0.6820\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8019 - loss: 0.5634(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.5002\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8019 - loss: 0.5633 - val_accuracy: 0.6847 - val_loss: 1.0214 - mcc: 0.5002\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7769 - loss: 0.6233(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7406\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.7770 - loss: 0.6231 - val_accuracy: 0.8330 - val_loss: 0.4510 - mcc: 0.7406\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8256 - loss: 0.4715(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.6451\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8256 - loss: 0.4715 - val_accuracy: 0.7739 - val_loss: 0.6646 - mcc: 0.6451\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8159 - loss: 0.5106(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7595\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8159 - loss: 0.5105 - val_accuracy: 0.8445 - val_loss: 0.4162 - mcc: 0.7595\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8366 - loss: 0.4382(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7510\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8366 - loss: 0.4383 - val_accuracy: 0.8392 - val_loss: 0.4368 - mcc: 0.7510\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8413 - loss: 0.4268(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7534\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8413 - loss: 0.4268 - val_accuracy: 0.8406 - val_loss: 0.4222 - mcc: 0.7534\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8413 - loss: 0.4261(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7707\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.8413 - loss: 0.4261 - val_accuracy: 0.8515 - val_loss: 0.3968 - mcc: 0.7707\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8209 - loss: 0.4890(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.6843\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8209 - loss: 0.4890 - val_accuracy: 0.7965 - val_loss: 0.5655 - mcc: 0.6843\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8082 - loss: 0.5263(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7503\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8083 - loss: 0.5262 - val_accuracy: 0.8389 - val_loss: 0.4302 - mcc: 0.7503\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7950 - loss: 0.5608(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7214\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.7949 - loss: 0.5608 - val_accuracy: 0.8209 - val_loss: 0.4825 - mcc: 0.7214\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8239 - loss: 0.4741(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7621\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8240 - loss: 0.4741 - val_accuracy: 0.8465 - val_loss: 0.4109 - mcc: 0.7621\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8380 - loss: 0.4360(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7643\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8380 - loss: 0.4360 - val_accuracy: 0.8478 - val_loss: 0.4072 - mcc: 0.7643\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8498 - loss: 0.3971(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7560\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8498 - loss: 0.3971 - val_accuracy: 0.8421 - val_loss: 0.4206 - mcc: 0.7560\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8481 - loss: 0.4030(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7787\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8481 - loss: 0.4030 - val_accuracy: 0.8566 - val_loss: 0.3770 - mcc: 0.7787\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8561 - loss: 0.3763(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7607\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8561 - loss: 0.3763 - val_accuracy: 0.8451 - val_loss: 0.4430 - mcc: 0.7607\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8535 - loss: 0.3853(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7736\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8535 - loss: 0.3854 - val_accuracy: 0.8535 - val_loss: 0.3902 - mcc: 0.7736\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8531 - loss: 0.3869(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7894\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8531 - loss: 0.3869 - val_accuracy: 0.8635 - val_loss: 0.3576 - mcc: 0.7894\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8601 - loss: 0.3636(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7890\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.8601 - loss: 0.3636 - val_accuracy: 0.8628 - val_loss: 0.3587 - mcc: 0.7890\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6366 - loss: 1.0457(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7136\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 34ms/step - accuracy: 0.6367 - loss: 1.0453 - val_accuracy: 0.8159 - val_loss: 0.5048 - mcc: 0.7136\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8068 - loss: 0.5370(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7445\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8069 - loss: 0.5370 - val_accuracy: 0.8348 - val_loss: 0.4531 - mcc: 0.7445\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8214 - loss: 0.4947(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.6210\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8214 - loss: 0.4948 - val_accuracy: 0.7594 - val_loss: 0.6641 - mcc: 0.6210\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8252 - loss: 0.4833(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7693\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8252 - loss: 0.4832 - val_accuracy: 0.8500 - val_loss: 0.3986 - mcc: 0.7693\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8528 - loss: 0.3936(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7791\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8528 - loss: 0.3936 - val_accuracy: 0.8576 - val_loss: 0.3749 - mcc: 0.7791\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8537 - loss: 0.3854(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7795\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8537 - loss: 0.3854 - val_accuracy: 0.8550 - val_loss: 0.3778 - mcc: 0.7795\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8588 - loss: 0.3693(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7859\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8588 - loss: 0.3693 - val_accuracy: 0.8618 - val_loss: 0.3604 - mcc: 0.7859\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8568 - loss: 0.3747(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7873\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.8568 - loss: 0.3747 - val_accuracy: 0.8618 - val_loss: 0.3610 - mcc: 0.7873\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8584 - loss: 0.3685(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7905\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8584 - loss: 0.3685 - val_accuracy: 0.8650 - val_loss: 0.3516 - mcc: 0.7905\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8610 - loss: 0.3622(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7927\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8610 - loss: 0.3622 - val_accuracy: 0.8657 - val_loss: 0.3486 - mcc: 0.7927\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8637 - loss: 0.3528(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7878\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8637 - loss: 0.3528 - val_accuracy: 0.8629 - val_loss: 0.3602 - mcc: 0.7878\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8655 - loss: 0.3495(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7955\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8655 - loss: 0.3495 - val_accuracy: 0.8679 - val_loss: 0.3449 - mcc: 0.7955\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8656 - loss: 0.3471(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7938\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8656 - loss: 0.3471 - val_accuracy: 0.8669 - val_loss: 0.3479 - mcc: 0.7938\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8683 - loss: 0.3414(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7979\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8683 - loss: 0.3414 - val_accuracy: 0.8693 - val_loss: 0.3385 - mcc: 0.7979\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8659 - loss: 0.3469(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8007\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 37ms/step - accuracy: 0.8659 - loss: 0.3469 - val_accuracy: 0.8707 - val_loss: 0.3369 - mcc: 0.8007\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8648 - loss: 0.3491(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7880\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8648 - loss: 0.3491 - val_accuracy: 0.8622 - val_loss: 0.3587 - mcc: 0.7880\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8680 - loss: 0.3407(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7951\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8680 - loss: 0.3407 - val_accuracy: 0.8674 - val_loss: 0.3401 - mcc: 0.7951\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8684 - loss: 0.3404(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7985\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.8684 - loss: 0.3404 - val_accuracy: 0.8698 - val_loss: 0.3370 - mcc: 0.7985\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8718 - loss: 0.3296(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7992\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.8718 - loss: 0.3296 - val_accuracy: 0.8701 - val_loss: 0.3345 - mcc: 0.7992\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8715 - loss: 0.3315(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8035\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8715 - loss: 0.3315 - val_accuracy: 0.8730 - val_loss: 0.3271 - mcc: 0.8035\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6069 - loss: 1.0454(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6892\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - accuracy: 0.6072 - loss: 1.0447 - val_accuracy: 0.7997 - val_loss: 0.5513 - mcc: 0.6892\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8229 - loss: 0.4838(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7519\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8229 - loss: 0.4838 - val_accuracy: 0.8399 - val_loss: 0.4327 - mcc: 0.7519\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8260 - loss: 0.4821(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7715\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8260 - loss: 0.4820 - val_accuracy: 0.8516 - val_loss: 0.3940 - mcc: 0.7715\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8475 - loss: 0.4063(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7670\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.8475 - loss: 0.4064 - val_accuracy: 0.8485 - val_loss: 0.4006 - mcc: 0.7670\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8512 - loss: 0.3926(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7720\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8512 - loss: 0.3926 - val_accuracy: 0.8512 - val_loss: 0.3863 - mcc: 0.7720\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8558 - loss: 0.3755(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7842\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8558 - loss: 0.3755 - val_accuracy: 0.8594 - val_loss: 0.3626 - mcc: 0.7842\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8585 - loss: 0.3685(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7836\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8585 - loss: 0.3685 - val_accuracy: 0.8597 - val_loss: 0.3636 - mcc: 0.7836\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8605 - loss: 0.3622(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7809\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.8605 - loss: 0.3622 - val_accuracy: 0.8576 - val_loss: 0.3696 - mcc: 0.7809\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8602 - loss: 0.3652(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7874\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8602 - loss: 0.3652 - val_accuracy: 0.8624 - val_loss: 0.3566 - mcc: 0.7874\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8592 - loss: 0.3671(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7878\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.8592 - loss: 0.3671 - val_accuracy: 0.8619 - val_loss: 0.3568 - mcc: 0.7878\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8630 - loss: 0.3546(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7925\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8630 - loss: 0.3546 - val_accuracy: 0.8654 - val_loss: 0.3473 - mcc: 0.7925\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8635 - loss: 0.3538(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7938\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8635 - loss: 0.3538 - val_accuracy: 0.8660 - val_loss: 0.3453 - mcc: 0.7938\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8658 - loss: 0.3472(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7907\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.8658 - loss: 0.3472 - val_accuracy: 0.8639 - val_loss: 0.3547 - mcc: 0.7907\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8660 - loss: 0.3480(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7730\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.8660 - loss: 0.3480 - val_accuracy: 0.8528 - val_loss: 0.3888 - mcc: 0.7730\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8628 - loss: 0.3565(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7933\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8628 - loss: 0.3565 - val_accuracy: 0.8659 - val_loss: 0.3440 - mcc: 0.7933\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8684 - loss: 0.3387(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7997\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8684 - loss: 0.3387 - val_accuracy: 0.8697 - val_loss: 0.3344 - mcc: 0.7997\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8673 - loss: 0.3430(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7937\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8673 - loss: 0.3430 - val_accuracy: 0.8658 - val_loss: 0.3459 - mcc: 0.7937\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8681 - loss: 0.3416(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7985\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.8681 - loss: 0.3416 - val_accuracy: 0.8688 - val_loss: 0.3427 - mcc: 0.7985\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8707 - loss: 0.3338(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7962\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8707 - loss: 0.3338 - val_accuracy: 0.8677 - val_loss: 0.3412 - mcc: 0.7962\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8707 - loss: 0.3333(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8014\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8707 - loss: 0.3333 - val_accuracy: 0.8710 - val_loss: 0.3310 - mcc: 0.8014\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 4\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6424 - loss: 1.0050(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6798\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 36ms/step - accuracy: 0.6425 - loss: 1.0047 - val_accuracy: 0.7963 - val_loss: 0.5792 - mcc: 0.6798\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7844 - loss: 0.6220(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.6746\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.7844 - loss: 0.6220 - val_accuracy: 0.7929 - val_loss: 0.5867 - mcc: 0.6746\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7847 - loss: 0.6153(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.6466\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.7847 - loss: 0.6153 - val_accuracy: 0.7767 - val_loss: 0.6406 - mcc: 0.6466\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7999 - loss: 0.5691(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7120\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.7999 - loss: 0.5691 - val_accuracy: 0.8155 - val_loss: 0.5086 - mcc: 0.7120\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8135 - loss: 0.5142(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7002\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8134 - loss: 0.5142 - val_accuracy: 0.8078 - val_loss: 0.5325 - mcc: 0.7002\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8037 - loss: 0.5528(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.6349\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8037 - loss: 0.5528 - val_accuracy: 0.7661 - val_loss: 0.6604 - mcc: 0.6349\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8084 - loss: 0.5435(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7493\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8084 - loss: 0.5434 - val_accuracy: 0.8384 - val_loss: 0.4413 - mcc: 0.7493\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7794 - loss: 0.6083(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.6390\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.7794 - loss: 0.6084 - val_accuracy: 0.7713 - val_loss: 0.6618 - mcc: 0.6390\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7823 - loss: 0.6277(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.6870\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.7823 - loss: 0.6277 - val_accuracy: 0.8002 - val_loss: 0.5561 - mcc: 0.6870\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8048 - loss: 0.5426(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7087\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8048 - loss: 0.5426 - val_accuracy: 0.8133 - val_loss: 0.5120 - mcc: 0.7087\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8162 - loss: 0.5122(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7265\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8162 - loss: 0.5122 - val_accuracy: 0.8250 - val_loss: 0.4809 - mcc: 0.7265\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8265 - loss: 0.4752(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7233\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8265 - loss: 0.4752 - val_accuracy: 0.8228 - val_loss: 0.4921 - mcc: 0.7233\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8277 - loss: 0.4766(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7485\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8277 - loss: 0.4766 - val_accuracy: 0.8384 - val_loss: 0.4407 - mcc: 0.7485\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8327 - loss: 0.4562(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7463\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8327 - loss: 0.4562 - val_accuracy: 0.8371 - val_loss: 0.4417 - mcc: 0.7463\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8001 - loss: 0.5605(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7260\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8001 - loss: 0.5604 - val_accuracy: 0.8242 - val_loss: 0.4763 - mcc: 0.7260\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8199 - loss: 0.4897(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7455\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8199 - loss: 0.4897 - val_accuracy: 0.8360 - val_loss: 0.4444 - mcc: 0.7455\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8342 - loss: 0.4495(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7539\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8342 - loss: 0.4495 - val_accuracy: 0.8411 - val_loss: 0.4279 - mcc: 0.7539\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8386 - loss: 0.4351(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7128\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8386 - loss: 0.4351 - val_accuracy: 0.8148 - val_loss: 0.5152 - mcc: 0.7128\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8267 - loss: 0.4717(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7589\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8267 - loss: 0.4717 - val_accuracy: 0.8445 - val_loss: 0.4189 - mcc: 0.7589\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8442 - loss: 0.4196(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.6225\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8442 - loss: 0.4196 - val_accuracy: 0.7625 - val_loss: 0.6385 - mcc: 0.6225\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 5\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6370 - loss: 1.0383(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7071\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.6372 - loss: 1.0375 - val_accuracy: 0.8121 - val_loss: 0.5193 - mcc: 0.7071\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7847 - loss: 0.6245(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.5942\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.7846 - loss: 0.6247 - val_accuracy: 0.7442 - val_loss: 0.7176 - mcc: 0.5942\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7815 - loss: 0.6279(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7023\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 39ms/step - accuracy: 0.7816 - loss: 0.6278 - val_accuracy: 0.8080 - val_loss: 0.5214 - mcc: 0.7023\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8163 - loss: 0.5048(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7359\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 35ms/step - accuracy: 0.8163 - loss: 0.5047 - val_accuracy: 0.8300 - val_loss: 0.4554 - mcc: 0.7359\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8320 - loss: 0.4525(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.6881\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8320 - loss: 0.4525 - val_accuracy: 0.7990 - val_loss: 0.5455 - mcc: 0.6881\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8332 - loss: 0.4449(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7616\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8332 - loss: 0.4448 - val_accuracy: 0.8461 - val_loss: 0.4034 - mcc: 0.7616\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8443 - loss: 0.4124(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7594\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.8443 - loss: 0.4125 - val_accuracy: 0.8445 - val_loss: 0.4054 - mcc: 0.7594\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8487 - loss: 0.3956(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7586\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8487 - loss: 0.3956 - val_accuracy: 0.8443 - val_loss: 0.4086 - mcc: 0.7586\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8551 - loss: 0.3791(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7767\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8551 - loss: 0.3791 - val_accuracy: 0.8555 - val_loss: 0.3773 - mcc: 0.7767\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8567 - loss: 0.3756(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7811\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8567 - loss: 0.3756 - val_accuracy: 0.8580 - val_loss: 0.3669 - mcc: 0.7811\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8368 - loss: 0.4366(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7713\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8368 - loss: 0.4366 - val_accuracy: 0.8516 - val_loss: 0.3829 - mcc: 0.7713\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8560 - loss: 0.3731(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7770\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.8560 - loss: 0.3731 - val_accuracy: 0.8554 - val_loss: 0.3766 - mcc: 0.7770\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8598 - loss: 0.3635(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7745\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8598 - loss: 0.3635 - val_accuracy: 0.8539 - val_loss: 0.3837 - mcc: 0.7745\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8623 - loss: 0.3566(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7835\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8623 - loss: 0.3566 - val_accuracy: 0.8599 - val_loss: 0.3631 - mcc: 0.7835\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8639 - loss: 0.3533(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7871\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8639 - loss: 0.3533 - val_accuracy: 0.8616 - val_loss: 0.3585 - mcc: 0.7871\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8636 - loss: 0.3523(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7878\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8636 - loss: 0.3523 - val_accuracy: 0.8624 - val_loss: 0.3569 - mcc: 0.7878\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8659 - loss: 0.3466(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7794\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8659 - loss: 0.3466 - val_accuracy: 0.8569 - val_loss: 0.3706 - mcc: 0.7794\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8695 - loss: 0.3378(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7920\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8695 - loss: 0.3378 - val_accuracy: 0.8649 - val_loss: 0.3481 - mcc: 0.7920\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8664 - loss: 0.3458(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7708\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8664 - loss: 0.3457 - val_accuracy: 0.8513 - val_loss: 0.3882 - mcc: 0.7708\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8682 - loss: 0.3394(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7932\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 37ms/step - accuracy: 0.8682 - loss: 0.3395 - val_accuracy: 0.8657 - val_loss: 0.3454 - mcc: 0.7932\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.873002),\n","              'mean': np.float64(0.8470097333333333),\n","              'min': np.float64(0.762468),\n","              'std': np.float64(0.042426018628195575)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0004783767064412435),\n","                               'mean': np.float64(0.0004494054794311524),\n","                               'min': np.float64(0.0004223152001698812),\n","                               'std': np.float64(2.125170939039732e-05)},\n"," 'MCC': {'max': np.float64(0.8034889631418218),\n","         'mean': np.float64(0.7619322052832824),\n","         'min': np.float64(0.6225004133798779),\n","         'std': np.float64(0.06991599337371447)},\n"," 'Parameters': 29925,\n"," 'Train Time (s)': {'max': np.float64(747.5260608196259),\n","                    'mean': np.float64(727.9354444503784),\n","                    'min': np.float64(697.9976599216461),\n","                    'std': np.float64(18.81460240332977)},\n"," 'Training Accuracy': [[0.73398756980896,\n","                        0.7949450016021729,\n","                        0.8132341504096985,\n","                        0.8047189712524414,\n","                        0.8249953389167786,\n","                        0.8269339799880981,\n","                        0.8320874571800232,\n","                        0.8410377502441406,\n","                        0.8412354588508606,\n","                        0.8105831742286682,\n","                        0.8181325793266296,\n","                        0.7924416661262512,\n","                        0.8296738266944885,\n","                        0.838726818561554,\n","                        0.8449822664260864,\n","                        0.8518930077552795,\n","                        0.8564348816871643,\n","                        0.8508712649345398,\n","                        0.8545689582824707,\n","                        0.8584468364715576],\n","                       [0.7272946238517761,\n","                        0.8174309134483337,\n","                        0.8165538907051086,\n","                        0.8389566540718079,\n","                        0.851738691329956,\n","                        0.8545135855674744,\n","                        0.8577437996864319,\n","                        0.8556007742881775,\n","                        0.8600172400474548,\n","                        0.8618848323822021,\n","                        0.8630844950675964,\n","                        0.8646623492240906,\n","                        0.8655303716659546,\n","                        0.8670145869255066,\n","                        0.8672986626625061,\n","                        0.8663747310638428,\n","                        0.8687717318534851,\n","                        0.8681133389472961,\n","                        0.8700666427612305,\n","                        0.8705471158027649],\n","                       [0.7186709642410278,\n","                        0.8281452655792236,\n","                        0.8336459994316101,\n","                        0.8443064093589783,\n","                        0.8526389002799988,\n","                        0.8561692833900452,\n","                        0.8581216931343079,\n","                        0.8602602481842041,\n","                        0.861345648765564,\n","                        0.8583382368087769,\n","                        0.864255964756012,\n","                        0.8644307255744934,\n","                        0.865646481513977,\n","                        0.8656511306762695,\n","                        0.8627424240112305,\n","                        0.8674817681312561,\n","                        0.8685875535011292,\n","                        0.8694990873336792,\n","                        0.8693138957023621,\n","                        0.8712741136550903],\n","                       [0.7203207015991211,\n","                        0.7793149352073669,\n","                        0.7807334065437317,\n","                        0.80450838804245,\n","                        0.8058326840400696,\n","                        0.8037692308425903,\n","                        0.821303129196167,\n","                        0.7574948668479919,\n","                        0.7882696390151978,\n","                        0.8080801367759705,\n","                        0.8169389367103577,\n","                        0.8268660306930542,\n","                        0.8267802000045776,\n","                        0.8310865163803101,\n","                        0.8027971982955933,\n","                        0.8204990029335022,\n","                        0.8322672247886658,\n","                        0.8376867771148682,\n","                        0.8287269473075867,\n","                        0.8385323882102966],\n","                       [0.7310652136802673,\n","                        0.7605092525482178,\n","                        0.7957470417022705,\n","                        0.8235534429550171,\n","                        0.8297375440597534,\n","                        0.8420018553733826,\n","                        0.8378463387489319,\n","                        0.8494628071784973,\n","                        0.8541502952575684,\n","                        0.8570109605789185,\n","                        0.8433922529220581,\n","                        0.8581203818321228,\n","                        0.8602156639099121,\n","                        0.861575186252594,\n","                        0.8632799386978149,\n","                        0.8635762333869934,\n","                        0.8646634817123413,\n","                        0.8666157126426697,\n","                        0.8674780130386353,\n","                        0.8681080937385559]],\n"," 'Training Loss': [[0.7670444250106812,\n","                    0.583110511302948,\n","                    0.5242542028427124,\n","                    0.5369383692741394,\n","                    0.4713613986968994,\n","                    0.4715266227722168,\n","                    0.4542734920978546,\n","                    0.4267478287220001,\n","                    0.42799246311187744,\n","                    0.5236954092979431,\n","                    0.4920458197593689,\n","                    0.5651929974555969,\n","                    0.45721524953842163,\n","                    0.43138498067855835,\n","                    0.4112938344478607,\n","                    0.39024773240089417,\n","                    0.37542980909347534,\n","                    0.39283275604248047,\n","                    0.38096773624420166,\n","                    0.3677848279476166],\n","                   [0.7812758684158325,\n","                    0.5061054825782776,\n","                    0.5094898343086243,\n","                    0.4382576644420624,\n","                    0.3942561149597168,\n","                    0.3830728828907013,\n","                    0.37170374393463135,\n","                    0.37732502818107605,\n","                    0.36416521668434143,\n","                    0.3590744733810425,\n","                    0.3551907539367676,\n","                    0.35110053420066833,\n","                    0.3483419120311737,\n","                    0.34408989548683167,\n","                    0.3432266116142273,\n","                    0.34668150544166565,\n","                    0.3387209177017212,\n","                    0.3405942916870117,\n","                    0.3336878716945648,\n","                    0.3334881067276001],\n","                   [0.7859073281288147,\n","                    0.4668736457824707,\n","                    0.4539704918861389,\n","                    0.41760390996932983,\n","                    0.38744211196899414,\n","                    0.37572944164276123,\n","                    0.3701692819595337,\n","                    0.3628371059894562,\n","                    0.36016225814819336,\n","                    0.36939263343811035,\n","                    0.3521682024002075,\n","                    0.35080844163894653,\n","                    0.3481280505657196,\n","                    0.3498399257659912,\n","                    0.35799646377563477,\n","                    0.3422819972038269,\n","                    0.33986160159111023,\n","                    0.33687466382980347,\n","                    0.3380528688430786,\n","                    0.331251859664917],\n","                   [0.7864110469818115,\n","                    0.6295478343963623,\n","                    0.6359508633613586,\n","                    0.5507369637489319,\n","                    0.5412953495979309,\n","                    0.5559061765670776,\n","                    0.4999428391456604,\n","                    0.6738346815109253,\n","                    0.6050050258636475,\n","                    0.5329684019088745,\n","                    0.5080593228340149,\n","                    0.4780326187610626,\n","                    0.47839054465293884,\n","                    0.4610387682914734,\n","                    0.5438034534454346,\n","                    0.48688581585884094,\n","                    0.45564356446266174,\n","                    0.4377087354660034,\n","                    0.4659401774406433,\n","                    0.4361058473587036],\n","                   [0.7710087895393372,\n","                    0.6925069093704224,\n","                    0.5788819789886475,\n","                    0.48010551929473877,\n","                    0.4615856111049652,\n","                    0.41815412044525146,\n","                    0.43311962485313416,\n","                    0.39392271637916565,\n","                    0.38054928183555603,\n","                    0.3734937012195587,\n","                    0.4138260781764984,\n","                    0.36862513422966003,\n","                    0.36233940720558167,\n","                    0.35876867175102234,\n","                    0.35402536392211914,\n","                    0.3533896505832672,\n","                    0.3501351773738861,\n","                    0.3444507122039795,\n","                    0.34201306104660034,\n","                    0.3401394784450531]],\n"," 'Validation Accuracy': [[0.7746114134788513,\n","                          0.7971610426902771,\n","                          0.6846674084663391,\n","                          0.8329848051071167,\n","                          0.7739419937133789,\n","                          0.8444973230361938,\n","                          0.8391971588134766,\n","                          0.8405993580818176,\n","                          0.8515028357505798,\n","                          0.7965033650398254,\n","                          0.8389146327972412,\n","                          0.8209187984466553,\n","                          0.8464860320091248,\n","                          0.8478475213050842,\n","                          0.8421234488487244,\n","                          0.8566206693649292,\n","                          0.8451166152954102,\n","                          0.8534976243972778,\n","                          0.8634728193283081,\n","                          0.8628373742103577],\n","                         [0.8159435987472534,\n","                          0.8347508311271667,\n","                          0.7593569159507751,\n","                          0.8499649167060852,\n","                          0.8575702905654907,\n","                          0.8550434708595276,\n","                          0.8618489503860474,\n","                          0.8618199825286865,\n","                          0.8649539947509766,\n","                          0.8657433986663818,\n","                          0.8629244565963745,\n","                          0.8679147958755493,\n","                          0.8669440746307373,\n","                          0.8693197965621948,\n","                          0.8706948161125183,\n","                          0.862241268157959,\n","                          0.8673674464225769,\n","                          0.8697648048400879,\n","                          0.8700799345970154,\n","                          0.8730020523071289],\n","                         [0.7997066378593445,\n","                          0.8399118781089783,\n","                          0.8516106009483337,\n","                          0.8484538793563843,\n","                          0.8511722087860107,\n","                          0.8594155311584473,\n","                          0.8597133159637451,\n","                          0.8575740456581116,\n","                          0.8624060153961182,\n","                          0.8619152903556824,\n","                          0.8653994202613831,\n","                          0.8660247325897217,\n","                          0.8638724684715271,\n","                          0.852782130241394,\n","                          0.8658775091171265,\n","                          0.8697344660758972,\n","                          0.8658286333084106,\n","                          0.8688099980354309,\n","                          0.8676719665527344,\n","                          0.8710012435913086],\n","                         [0.7962769269943237,\n","                          0.792910635471344,\n","                          0.7767199873924255,\n","                          0.815528154373169,\n","                          0.8077690601348877,\n","                          0.7660773396492004,\n","                          0.8383867740631104,\n","                          0.7713404893875122,\n","                          0.8002186417579651,\n","                          0.8133068084716797,\n","                          0.8249711990356445,\n","                          0.8227806091308594,\n","                          0.8384130597114563,\n","                          0.8370932340621948,\n","                          0.8241549730300903,\n","                          0.8359724879264832,\n","                          0.8410606980323792,\n","                          0.8148370981216431,\n","                          0.8445063233375549,\n","                          0.7624678611755371],\n","                         [0.8120525479316711,\n","                          0.7442049384117126,\n","                          0.8080480694770813,\n","                          0.830016016960144,\n","                          0.7990041375160217,\n","                          0.846055805683136,\n","                          0.8444695472717285,\n","                          0.8442997336387634,\n","                          0.8554526567459106,\n","                          0.8579575419425964,\n","                          0.8515853881835938,\n","                          0.8554102182388306,\n","                          0.8538791537284851,\n","                          0.8599401712417603,\n","                          0.8615678548812866,\n","                          0.8623517155647278,\n","                          0.8568610548973083,\n","                          0.8648801445960999,\n","                          0.8512600660324097,\n","                          0.8657400012016296]],\n"," 'Validation Loss': [[0.6620442867279053,\n","                      0.5744559168815613,\n","                      1.021401286125183,\n","                      0.4509691596031189,\n","                      0.6645527482032776,\n","                      0.4161982536315918,\n","                      0.4367983937263489,\n","                      0.42223456501960754,\n","                      0.39680591225624084,\n","                      0.5654564499855042,\n","                      0.43020865321159363,\n","                      0.48254504799842834,\n","                      0.41086864471435547,\n","                      0.4072381556034088,\n","                      0.4206233024597168,\n","                      0.37702226638793945,\n","                      0.4429824948310852,\n","                      0.3902028799057007,\n","                      0.35756611824035645,\n","                      0.358673632144928],\n","                     [0.5047879219055176,\n","                      0.45308566093444824,\n","                      0.6640542149543762,\n","                      0.39861437678337097,\n","                      0.3749346137046814,\n","                      0.377775639295578,\n","                      0.36044567823410034,\n","                      0.36103296279907227,\n","                      0.35155534744262695,\n","                      0.34858617186546326,\n","                      0.3601611256599426,\n","                      0.34488335251808167,\n","                      0.34791967272758484,\n","                      0.3385201692581177,\n","                      0.3368785083293915,\n","                      0.3587021827697754,\n","                      0.3401007354259491,\n","                      0.3370342552661896,\n","                      0.3344770073890686,\n","                      0.32709088921546936],\n","                     [0.5512598156929016,\n","                      0.4326861500740051,\n","                      0.3939964175224304,\n","                      0.40062326192855835,\n","                      0.38633134961128235,\n","                      0.36263272166252136,\n","                      0.3635939955711365,\n","                      0.36958834528923035,\n","                      0.35655492544174194,\n","                      0.3568075895309448,\n","                      0.3473286032676697,\n","                      0.34527039527893066,\n","                      0.35473400354385376,\n","                      0.38878005743026733,\n","                      0.34397339820861816,\n","                      0.3344249129295349,\n","                      0.3459351658821106,\n","                      0.34265556931495667,\n","                      0.3412131071090698,\n","                      0.3309899866580963],\n","                     [0.5792443752288818,\n","                      0.5867129564285278,\n","                      0.6405749917030334,\n","                      0.5085965394973755,\n","                      0.5325364470481873,\n","                      0.6603625416755676,\n","                      0.4412819445133209,\n","                      0.6618289947509766,\n","                      0.556145191192627,\n","                      0.5120397210121155,\n","                      0.4809105694293976,\n","                      0.4920601546764374,\n","                      0.4407035708427429,\n","                      0.441738486289978,\n","                      0.47628942131996155,\n","                      0.4444214999675751,\n","                      0.42794063687324524,\n","                      0.5152051448822021,\n","                      0.4188942015171051,\n","                      0.6384743452072144],\n","                     [0.5192927122116089,\n","                      0.7175785899162292,\n","                      0.5214326977729797,\n","                      0.4554474949836731,\n","                      0.5454506874084473,\n","                      0.4033788740634918,\n","                      0.40544840693473816,\n","                      0.4085544943809509,\n","                      0.3773241341114044,\n","                      0.366882860660553,\n","                      0.38291820883750916,\n","                      0.37660714983940125,\n","                      0.38373661041259766,\n","                      0.36308157444000244,\n","                      0.35849976539611816,\n","                      0.35686174035072327,\n","                      0.3705529272556305,\n","                      0.34806379675865173,\n","                      0.38820934295654297,\n","                      0.3453544080257416]],\n"," 'Validation MCC': [[np.float64(0.6455957618758008),\n","                     np.float64(0.6819903259859903),\n","                     np.float64(0.5002227217679951),\n","                     np.float64(0.7406303597031002),\n","                     np.float64(0.6451106982623668),\n","                     np.float64(0.7595262318398186),\n","                     np.float64(0.7510128790553158),\n","                     np.float64(0.7534355821133445),\n","                     np.float64(0.7707098073681772),\n","                     np.float64(0.684289064158096),\n","                     np.float64(0.7503294680547483),\n","                     np.float64(0.7213846460884338),\n","                     np.float64(0.7621435199873688),\n","                     np.float64(0.764321543333225),\n","                     np.float64(0.7560098981975555),\n","                     np.float64(0.778740419090276),\n","                     np.float64(0.7607294730821365),\n","                     np.float64(0.7736267842075776),\n","                     np.float64(0.7894158314538926),\n","                     np.float64(0.7889949848996971)],\n","                    [np.float64(0.7135877629962099),\n","                     np.float64(0.7444626375190683),\n","                     np.float64(0.620953978080368),\n","                     np.float64(0.7693442944162847),\n","                     np.float64(0.7791316083132904),\n","                     np.float64(0.7794940023765331),\n","                     np.float64(0.785936036231962),\n","                     np.float64(0.7873177192767994),\n","                     np.float64(0.7905392136852838),\n","                     np.float64(0.7926783783884057),\n","                     np.float64(0.7878348701228101),\n","                     np.float64(0.7954587449447846),\n","                     np.float64(0.7937843942361791),\n","                     np.float64(0.7979075558427906),\n","                     np.float64(0.8007061918500015),\n","                     np.float64(0.7879713149230665),\n","                     np.float64(0.795066305420385),\n","                     np.float64(0.7984638089685194),\n","                     np.float64(0.7992051361290599),\n","                     np.float64(0.8034889631418218)],\n","                    [np.float64(0.6892159935852532),\n","                     np.float64(0.7518539309239763),\n","                     np.float64(0.7715232769784607),\n","                     np.float64(0.767047988610441),\n","                     np.float64(0.7719664255431485),\n","                     np.float64(0.7842318443613937),\n","                     np.float64(0.7836148745752199),\n","                     np.float64(0.780905819067966),\n","                     np.float64(0.7874261610214588),\n","                     np.float64(0.7877910407968417),\n","                     np.float64(0.7925409707082072),\n","                     np.float64(0.7938026717853701),\n","                     np.float64(0.790720190042636),\n","                     np.float64(0.7729876526061681),\n","                     np.float64(0.7933472261907545),\n","                     np.float64(0.7996846046632485),\n","                     np.float64(0.7937244948124168),\n","                     np.float64(0.7985468385026323),\n","                     np.float64(0.7961989628039186),\n","                     np.float64(0.8014285770423023)],\n","                    [np.float64(0.6798280371247692),\n","                     np.float64(0.6746479644971354),\n","                     np.float64(0.6466297202038113),\n","                     np.float64(0.711994618556727),\n","                     np.float64(0.70017452853095),\n","                     np.float64(0.6349373000964064),\n","                     np.float64(0.7493253075504961),\n","                     np.float64(0.6390330716012657),\n","                     np.float64(0.6869914967492949),\n","                     np.float64(0.7087254643240212),\n","                     np.float64(0.7265322425502567),\n","                     np.float64(0.7232845620581754),\n","                     np.float64(0.7484891987754196),\n","                     np.float64(0.7463288392913825),\n","                     np.float64(0.7260000925860934),\n","                     np.float64(0.7454691865283863),\n","                     np.float64(0.7538923260514173),\n","                     np.float64(0.7128398346064017),\n","                     np.float64(0.7589344206702261),\n","                     np.float64(0.6225004133798779)],\n","                    [np.float64(0.7071215639160691),\n","                     np.float64(0.5942400725819516),\n","                     np.float64(0.7022756057252809),\n","                     np.float64(0.7359427034427203),\n","                     np.float64(0.6880570833218761),\n","                     np.float64(0.7616188686212936),\n","                     np.float64(0.7593809816433855),\n","                     np.float64(0.7585974254454985),\n","                     np.float64(0.7766790746025097),\n","                     np.float64(0.7811479897272203),\n","                     np.float64(0.7712925771865661),\n","                     np.float64(0.7770356356425173),\n","                     np.float64(0.774460832786045),\n","                     np.float64(0.7835394731746642),\n","                     np.float64(0.7870997837640747),\n","                     np.float64(0.7878298273599825),\n","                     np.float64(0.7794379321076483),\n","                     np.float64(0.7920405316118271),\n","                     np.float64(0.7707580380432989),\n","                     np.float64(0.7932480879527127)]]}\n","Training Model: BiLSTM, Fold: 1\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6390 - loss: 0.9997(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7756\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - accuracy: 0.6391 - loss: 0.9994 - val_accuracy: 0.8548 - val_loss: 0.4156 - mcc: 0.7756\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8530 - loss: 0.4163(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7498\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8530 - loss: 0.4163 - val_accuracy: 0.8379 - val_loss: 0.4522 - mcc: 0.7498\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8569 - loss: 0.4015(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8214\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8569 - loss: 0.4015 - val_accuracy: 0.8836 - val_loss: 0.3186 - mcc: 0.8214\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8807 - loss: 0.3283(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7877\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8807 - loss: 0.3283 - val_accuracy: 0.8612 - val_loss: 0.3781 - mcc: 0.7877\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8609 - loss: 0.3932(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7546\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8610 - loss: 0.3932 - val_accuracy: 0.8388 - val_loss: 0.4547 - mcc: 0.7546\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8598 - loss: 0.3928(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8042\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8598 - loss: 0.3928 - val_accuracy: 0.8726 - val_loss: 0.3523 - mcc: 0.8042\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8730 - loss: 0.3525(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7870\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8730 - loss: 0.3525 - val_accuracy: 0.8611 - val_loss: 0.3725 - mcc: 0.7870\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8647 - loss: 0.3723(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8013\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8647 - loss: 0.3724 - val_accuracy: 0.8710 - val_loss: 0.3562 - mcc: 0.8013\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8746 - loss: 0.3540(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8312\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8746 - loss: 0.3540 - val_accuracy: 0.8899 - val_loss: 0.3057 - mcc: 0.8312\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8825 - loss: 0.3346(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8365\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8825 - loss: 0.3346 - val_accuracy: 0.8932 - val_loss: 0.2993 - mcc: 0.8365\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8885 - loss: 0.3101(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8248\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8885 - loss: 0.3102 - val_accuracy: 0.8859 - val_loss: 0.3126 - mcc: 0.8248\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8802 - loss: 0.3369(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8249\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8802 - loss: 0.3369 - val_accuracy: 0.8860 - val_loss: 0.3120 - mcc: 0.8249\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8813 - loss: 0.3247(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8289\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.8813 - loss: 0.3247 - val_accuracy: 0.8886 - val_loss: 0.3127 - mcc: 0.8289\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8927 - loss: 0.2916(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8478\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8927 - loss: 0.2915 - val_accuracy: 0.9005 - val_loss: 0.2669 - mcc: 0.8478\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8915 - loss: 0.2990(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.7978\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8914 - loss: 0.2990 - val_accuracy: 0.8686 - val_loss: 0.3551 - mcc: 0.7978\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8779 - loss: 0.3323(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8422\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8779 - loss: 0.3323 - val_accuracy: 0.8969 - val_loss: 0.2752 - mcc: 0.8422\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8922 - loss: 0.2891(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8176\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8922 - loss: 0.2891 - val_accuracy: 0.8811 - val_loss: 0.3193 - mcc: 0.8176\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8792 - loss: 0.3307(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8286\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8792 - loss: 0.3307 - val_accuracy: 0.8883 - val_loss: 0.3105 - mcc: 0.8286\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8816 - loss: 0.3316(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8451\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8816 - loss: 0.3316 - val_accuracy: 0.8989 - val_loss: 0.2721 - mcc: 0.8451\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8781 - loss: 0.3373(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8051\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8781 - loss: 0.3373 - val_accuracy: 0.8733 - val_loss: 0.3449 - mcc: 0.8051\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 2\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6392 - loss: 1.0110(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7790\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.6395 - loss: 1.0102 - val_accuracy: 0.8574 - val_loss: 0.4058 - mcc: 0.7790\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8556 - loss: 0.4061(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8147\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.8556 - loss: 0.4061 - val_accuracy: 0.8801 - val_loss: 0.3297 - mcc: 0.8147\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8719 - loss: 0.3585(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8068\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.8718 - loss: 0.3586 - val_accuracy: 0.8748 - val_loss: 0.3485 - mcc: 0.8068\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8709 - loss: 0.3575(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8361\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8709 - loss: 0.3574 - val_accuracy: 0.8937 - val_loss: 0.2888 - mcc: 0.8361\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8818 - loss: 0.3195(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8036\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8818 - loss: 0.3195 - val_accuracy: 0.8719 - val_loss: 0.3703 - mcc: 0.8036\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8843 - loss: 0.3154(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8418\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.8843 - loss: 0.3154 - val_accuracy: 0.8971 - val_loss: 0.2738 - mcc: 0.8418\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8917 - loss: 0.2937(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8336\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8917 - loss: 0.2937 - val_accuracy: 0.8921 - val_loss: 0.2955 - mcc: 0.8336\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8901 - loss: 0.3009(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8361\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8901 - loss: 0.3009 - val_accuracy: 0.8934 - val_loss: 0.2948 - mcc: 0.8361\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8943 - loss: 0.2894(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8501\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8943 - loss: 0.2894 - val_accuracy: 0.9026 - val_loss: 0.2601 - mcc: 0.8501\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9006 - loss: 0.2667(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8476\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9006 - loss: 0.2667 - val_accuracy: 0.9007 - val_loss: 0.2726 - mcc: 0.8476\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8970 - loss: 0.2831(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7440\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8970 - loss: 0.2832 - val_accuracy: 0.8348 - val_loss: 0.4517 - mcc: 0.7440\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8653 - loss: 0.3691(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8494\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8653 - loss: 0.3690 - val_accuracy: 0.9019 - val_loss: 0.2623 - mcc: 0.8494\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9008 - loss: 0.2654(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8445\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9008 - loss: 0.2654 - val_accuracy: 0.8989 - val_loss: 0.2750 - mcc: 0.8445\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8943 - loss: 0.2851(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8289\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8943 - loss: 0.2851 - val_accuracy: 0.8892 - val_loss: 0.3094 - mcc: 0.8289\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8961 - loss: 0.2828(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8675\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8961 - loss: 0.2827 - val_accuracy: 0.9134 - val_loss: 0.2279 - mcc: 0.8675\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8997 - loss: 0.2714(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8531\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.8997 - loss: 0.2714 - val_accuracy: 0.9041 - val_loss: 0.2563 - mcc: 0.8531\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9002 - loss: 0.2679(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8655\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9002 - loss: 0.2679 - val_accuracy: 0.9121 - val_loss: 0.2298 - mcc: 0.8655\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9098 - loss: 0.2382(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8646\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9098 - loss: 0.2382 - val_accuracy: 0.9119 - val_loss: 0.2319 - mcc: 0.8646\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9056 - loss: 0.2495(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8069\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9056 - loss: 0.2496 - val_accuracy: 0.8747 - val_loss: 0.3444 - mcc: 0.8069\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8701 - loss: 0.3556(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8295\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8701 - loss: 0.3556 - val_accuracy: 0.8895 - val_loss: 0.3014 - mcc: 0.8295\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 3\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6375 - loss: 1.0057(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7624\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 34ms/step - accuracy: 0.6376 - loss: 1.0054 - val_accuracy: 0.8467 - val_loss: 0.4409 - mcc: 0.7624\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8564 - loss: 0.4093(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7917\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8564 - loss: 0.4093 - val_accuracy: 0.8642 - val_loss: 0.3820 - mcc: 0.7917\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8689 - loss: 0.3648(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8282\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.8689 - loss: 0.3647 - val_accuracy: 0.8881 - val_loss: 0.3057 - mcc: 0.8282\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8853 - loss: 0.3112(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8322\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8853 - loss: 0.3112 - val_accuracy: 0.8901 - val_loss: 0.2940 - mcc: 0.8322\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8858 - loss: 0.3090(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8204\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8858 - loss: 0.3090 - val_accuracy: 0.8830 - val_loss: 0.3223 - mcc: 0.8204\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8842 - loss: 0.3172(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8162\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8842 - loss: 0.3172 - val_accuracy: 0.8799 - val_loss: 0.3317 - mcc: 0.8162\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8757 - loss: 0.3407(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8193\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8757 - loss: 0.3407 - val_accuracy: 0.8824 - val_loss: 0.3257 - mcc: 0.8193\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8922 - loss: 0.2921(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8327\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8921 - loss: 0.2921 - val_accuracy: 0.8911 - val_loss: 0.3006 - mcc: 0.8327\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8896 - loss: 0.3037(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8486\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8896 - loss: 0.3037 - val_accuracy: 0.9012 - val_loss: 0.2658 - mcc: 0.8486\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8993 - loss: 0.2708(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8491\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.8993 - loss: 0.2708 - val_accuracy: 0.9015 - val_loss: 0.2646 - mcc: 0.8491\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9041 - loss: 0.2566(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8539\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9041 - loss: 0.2566 - val_accuracy: 0.9040 - val_loss: 0.2514 - mcc: 0.8539\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9016 - loss: 0.2640(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.7962\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9016 - loss: 0.2641 - val_accuracy: 0.8679 - val_loss: 0.3722 - mcc: 0.7962\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8777 - loss: 0.3401(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8011\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.8777 - loss: 0.3401 - val_accuracy: 0.8706 - val_loss: 0.3425 - mcc: 0.8011\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8853 - loss: 0.3073(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8369\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8853 - loss: 0.3073 - val_accuracy: 0.8933 - val_loss: 0.2886 - mcc: 0.8369\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8966 - loss: 0.2768(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8571\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8966 - loss: 0.2768 - val_accuracy: 0.9062 - val_loss: 0.2456 - mcc: 0.8571\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8969 - loss: 0.2726(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7379\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8969 - loss: 0.2726 - val_accuracy: 0.8306 - val_loss: 0.5162 - mcc: 0.7379\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8823 - loss: 0.3236(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8520\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8823 - loss: 0.3235 - val_accuracy: 0.9033 - val_loss: 0.2540 - mcc: 0.8520\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9024 - loss: 0.2588(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8452\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.9024 - loss: 0.2589 - val_accuracy: 0.8987 - val_loss: 0.2669 - mcc: 0.8452\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9036 - loss: 0.2546(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8579\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9036 - loss: 0.2546 - val_accuracy: 0.9071 - val_loss: 0.2419 - mcc: 0.8579\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8982 - loss: 0.2729(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8526\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8982 - loss: 0.2729 - val_accuracy: 0.9034 - val_loss: 0.2520 - mcc: 0.8526\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 4\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6352 - loss: 1.0038(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7217\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.6353 - loss: 1.0034 - val_accuracy: 0.8217 - val_loss: 0.5192 - mcc: 0.7217\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8442 - loss: 0.4529(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7930\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8443 - loss: 0.4529 - val_accuracy: 0.8653 - val_loss: 0.3800 - mcc: 0.7930\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8657 - loss: 0.3844(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8275\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8657 - loss: 0.3843 - val_accuracy: 0.8877 - val_loss: 0.3132 - mcc: 0.8275\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8873 - loss: 0.3070(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8004\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8873 - loss: 0.3071 - val_accuracy: 0.8712 - val_loss: 0.3640 - mcc: 0.8004\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8758 - loss: 0.3458(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8078\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8758 - loss: 0.3458 - val_accuracy: 0.8758 - val_loss: 0.3507 - mcc: 0.8078\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8847 - loss: 0.3210(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8281\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8847 - loss: 0.3210 - val_accuracy: 0.8883 - val_loss: 0.3120 - mcc: 0.8281\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8906 - loss: 0.3005(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8460\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8906 - loss: 0.3005 - val_accuracy: 0.8995 - val_loss: 0.2695 - mcc: 0.8460\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8909 - loss: 0.2972(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7469\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.8909 - loss: 0.2972 - val_accuracy: 0.8366 - val_loss: 0.4658 - mcc: 0.7469\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8793 - loss: 0.3343(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7999\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.8793 - loss: 0.3342 - val_accuracy: 0.8694 - val_loss: 0.3568 - mcc: 0.7999\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8951 - loss: 0.2828(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7722\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8951 - loss: 0.2828 - val_accuracy: 0.8509 - val_loss: 0.4047 - mcc: 0.7722\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8796 - loss: 0.3287(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8434\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8796 - loss: 0.3286 - val_accuracy: 0.8982 - val_loss: 0.2747 - mcc: 0.8434\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8933 - loss: 0.2928(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.6379\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.8933 - loss: 0.2929 - val_accuracy: 0.7695 - val_loss: 0.7948 - mcc: 0.6379\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8518 - loss: 0.4239(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8269\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8518 - loss: 0.4238 - val_accuracy: 0.8878 - val_loss: 0.3067 - mcc: 0.8269\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8888 - loss: 0.3077(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8044\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8888 - loss: 0.3077 - val_accuracy: 0.8737 - val_loss: 0.3661 - mcc: 0.8044\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8838 - loss: 0.3283(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8338\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 37ms/step - accuracy: 0.8838 - loss: 0.3283 - val_accuracy: 0.8922 - val_loss: 0.3011 - mcc: 0.8338\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8956 - loss: 0.2893(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8413\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8956 - loss: 0.2893 - val_accuracy: 0.8968 - val_loss: 0.2829 - mcc: 0.8413\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8976 - loss: 0.2766(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8384\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8976 - loss: 0.2766 - val_accuracy: 0.8947 - val_loss: 0.2812 - mcc: 0.8384\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8942 - loss: 0.2862(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8071\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8942 - loss: 0.2862 - val_accuracy: 0.8753 - val_loss: 0.3457 - mcc: 0.8071\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8719 - loss: 0.3599(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8243\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.8719 - loss: 0.3599 - val_accuracy: 0.8860 - val_loss: 0.3107 - mcc: 0.8243\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8888 - loss: 0.3023(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8393\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.8888 - loss: 0.3023 - val_accuracy: 0.8955 - val_loss: 0.2810 - mcc: 0.8393\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6444 - loss: 0.9895(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6824\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.6445 - loss: 0.9891 - val_accuracy: 0.7955 - val_loss: 0.5959 - mcc: 0.6824\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8373 - loss: 0.4624(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8029\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.8373 - loss: 0.4623 - val_accuracy: 0.8717 - val_loss: 0.3581 - mcc: 0.8029\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8702 - loss: 0.3599(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7849\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8702 - loss: 0.3599 - val_accuracy: 0.8606 - val_loss: 0.4108 - mcc: 0.7849\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8798 - loss: 0.3335(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8233\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8798 - loss: 0.3335 - val_accuracy: 0.8851 - val_loss: 0.3145 - mcc: 0.8233\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8826 - loss: 0.3238(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8278\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8826 - loss: 0.3238 - val_accuracy: 0.8880 - val_loss: 0.2999 - mcc: 0.8278\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8907 - loss: 0.2981(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8482\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8907 - loss: 0.2981 - val_accuracy: 0.9007 - val_loss: 0.2660 - mcc: 0.8482\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8916 - loss: 0.2917(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8006\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8916 - loss: 0.2917 - val_accuracy: 0.8700 - val_loss: 0.3563 - mcc: 0.8006\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8734 - loss: 0.3460(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.6917\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8734 - loss: 0.3460 - val_accuracy: 0.7939 - val_loss: 0.5989 - mcc: 0.6917\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8764 - loss: 0.3459(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8365\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8764 - loss: 0.3458 - val_accuracy: 0.8933 - val_loss: 0.2909 - mcc: 0.8365\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8928 - loss: 0.2932(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8367\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.8928 - loss: 0.2932 - val_accuracy: 0.8935 - val_loss: 0.2868 - mcc: 0.8367\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9005 - loss: 0.2663(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8470\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9005 - loss: 0.2663 - val_accuracy: 0.9000 - val_loss: 0.2647 - mcc: 0.8470\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8986 - loss: 0.2738(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8305\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.8986 - loss: 0.2738 - val_accuracy: 0.8887 - val_loss: 0.2981 - mcc: 0.8305\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8978 - loss: 0.2751(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8397\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.8978 - loss: 0.2751 - val_accuracy: 0.8953 - val_loss: 0.2805 - mcc: 0.8397\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9033 - loss: 0.2552(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8481\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9033 - loss: 0.2552 - val_accuracy: 0.9008 - val_loss: 0.2627 - mcc: 0.8481\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9007 - loss: 0.2649(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8575\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9007 - loss: 0.2649 - val_accuracy: 0.9068 - val_loss: 0.2442 - mcc: 0.8575\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8952 - loss: 0.2812(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8509\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8952 - loss: 0.2812 - val_accuracy: 0.9024 - val_loss: 0.2556 - mcc: 0.8509\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9111 - loss: 0.2318(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8506\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9111 - loss: 0.2318 - val_accuracy: 0.9023 - val_loss: 0.2567 - mcc: 0.8506\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9065 - loss: 0.2460(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8263\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9065 - loss: 0.2460 - val_accuracy: 0.8870 - val_loss: 0.3004 - mcc: 0.8263\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8875 - loss: 0.3057(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8344\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 37ms/step - accuracy: 0.8875 - loss: 0.3057 - val_accuracy: 0.8920 - val_loss: 0.2884 - mcc: 0.8344\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8984 - loss: 0.2718(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8408\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.8984 - loss: 0.2718 - val_accuracy: 0.8963 - val_loss: 0.2769 - mcc: 0.8408\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.9034433333333334),\n","              'mean': np.float64(0.8915957333333335),\n","              'min': np.float64(0.8732586666666666),\n","              'std': np.float64(0.01018177570716973)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0008648391564687094),\n","                               'mean': np.float64(0.0005206186930338542),\n","                               'min': np.float64(0.00041651240984598794),\n","                               'std': np.float64(0.00017262951842607897)},\n"," 'MCC': {'max': np.float64(0.8526414236202781),\n","         'mean': np.float64(0.8334754262791936),\n","         'min': np.float64(0.8050859604732231),\n","         'std': np.float64(0.01597758906321616)},\n"," 'Parameters': 9029,\n"," 'Train Time (s)': {'max': np.float64(744.6549818515778),\n","                    'mean': np.float64(716.9483770370483),\n","                    'min': np.float64(683.5988276004791),\n","                    'std': np.float64(20.152355037950702)},\n"," 'Training Accuracy': [[0.7462863922119141,\n","                        0.8578564524650574,\n","                        0.8669431209564209,\n","                        0.87850421667099,\n","                        0.8638527989387512,\n","                        0.8590264320373535,\n","                        0.8688340783119202,\n","                        0.8588316440582275,\n","                        0.8772470951080322,\n","                        0.8838229775428772,\n","                        0.8853203654289246,\n","                        0.8757890462875366,\n","                        0.8794907331466675,\n","                        0.8954811096191406,\n","                        0.8729081153869629,\n","                        0.8840371966362,\n","                        0.8892948627471924,\n","                        0.8838710188865662,\n","                        0.8863062262535095,\n","                        0.8647887110710144],\n","                       [0.7458440065383911,\n","                        0.8635165095329285,\n","                        0.8665816783905029,\n","                        0.8780869245529175,\n","                        0.8823031187057495,\n","                        0.8863989114761353,\n","                        0.891406238079071,\n","                        0.8868991136550903,\n","                        0.8943951725959778,\n","                        0.8995122909545898,\n","                        0.8840532898902893,\n","                        0.8810043334960938,\n","                        0.9001156687736511,\n","                        0.890575110912323,\n","                        0.901704728603363,\n","                        0.8930844664573669,\n","                        0.9037927389144897,\n","                        0.9106058478355408,\n","                        0.8911406993865967,\n","                        0.870076596736908],\n","                       [0.7408453822135925,\n","                        0.8577443957328796,\n","                        0.8759817481040955,\n","                        0.8855432271957397,\n","                        0.8825100660324097,\n","                        0.8806602954864502,\n","                        0.8773605823516846,\n","                        0.888832688331604,\n","                        0.8920443654060364,\n","                        0.8975645303726196,\n","                        0.9027704000473022,\n","                        0.8912785649299622,\n","                        0.8759332895278931,\n","                        0.8897682428359985,\n","                        0.8987709283828735,\n","                        0.8958684802055359,\n","                        0.8923684358596802,\n","                        0.8991906046867371,\n","                        0.9042705297470093,\n","                        0.8990580439567566],\n","                       [0.7308565378189087,\n","                        0.8530774712562561,\n","                        0.872982919216156,\n","                        0.8847747445106506,\n","                        0.8746451735496521,\n","                        0.8873353600502014,\n","                        0.8906617164611816,\n","                        0.8881231546401978,\n","                        0.8898816704750061,\n","                        0.8942185044288635,\n","                        0.8866013884544373,\n","                        0.881224513053894,\n","                        0.869774580001831,\n","                        0.8836367726325989,\n","                        0.8850407600402832,\n","                        0.8948990106582642,\n","                        0.8978968262672424,\n","                        0.8875451683998108,\n","                        0.8750176429748535,\n","                        0.8894538283348083],\n","                       [0.7457156777381897,\n","                        0.8507699966430664,\n","                        0.8693876266479492,\n","                        0.8797810673713684,\n","                        0.8848667740821838,\n","                        0.8912850618362427,\n","                        0.883213222026825,\n","                        0.8746005892753601,\n","                        0.8873545527458191,\n","                        0.8948965072631836,\n","                        0.9003518223762512,\n","                        0.895052969455719,\n","                        0.900378406047821,\n","                        0.9003056883811951,\n","                        0.9026128649711609,\n","                        0.8985527157783508,\n","                        0.91116863489151,\n","                        0.9067713022232056,\n","                        0.889578640460968,\n","                        0.9002926349639893]],\n"," 'Training Loss': [[0.7182856202125549,\n","                    0.40105322003364563,\n","                    0.3711403012275696,\n","                    0.3358382284641266,\n","                    0.38533779978752136,\n","                    0.39649897813796997,\n","                    0.3645645081996918,\n","                    0.3932846188545227,\n","                    0.3462308347225189,\n","                    0.3284973204135895,\n","                    0.3189006745815277,\n","                    0.35320764780044556,\n","                    0.33023908734321594,\n","                    0.2834572494029999,\n","                    0.35190990567207336,\n","                    0.31388628482818604,\n","                    0.29652926325798035,\n","                    0.31939077377319336,\n","                    0.31526368856430054,\n","                    0.3753668963909149],\n","                   [0.7226828336715698,\n","                    0.38531070947647095,\n","                    0.3729955554008484,\n","                    0.3363572359085083,\n","                    0.318166583776474,\n","                    0.3076373040676117,\n","                    0.29537954926490784,\n","                    0.3128574788570404,\n","                    0.28822457790374756,\n","                    0.2705674171447754,\n","                    0.32892894744873047,\n","                    0.3239249587059021,\n","                    0.26798316836357117,\n","                    0.2981405556201935,\n","                    0.26456114649772644,\n","                    0.2903960049152374,\n","                    0.25753068923950195,\n","                    0.23544363677501678,\n","                    0.2923976480960846,\n","                    0.3561309278011322],\n","                   [0.7310295701026917,\n","                    0.4029662311077118,\n","                    0.3439904451370239,\n","                    0.31076404452323914,\n","                    0.3195700943470001,\n","                    0.32343214750289917,\n","                    0.33671697974205017,\n","                    0.30207571387290955,\n","                    0.2944512367248535,\n","                    0.27652713656425476,\n","                    0.25929000973701477,\n","                    0.29855453968048096,\n","                    0.34258541464805603,\n","                    0.2973131835460663,\n","                    0.27084726095199585,\n","                    0.2798112630844116,\n","                    0.2921757102012634,\n","                    0.26881343126296997,\n","                    0.2526654005050659,\n","                    0.2702252268791199],\n","                   [0.7520018815994263,\n","                    0.41978150606155396,\n","                    0.3579055964946747,\n","                    0.3158818781375885,\n","                    0.35231930017471313,\n","                    0.31364962458610535,\n","                    0.2980778217315674,\n","                    0.30505311489105225,\n","                    0.300930917263031,\n","                    0.2848713994026184,\n","                    0.3089931905269623,\n","                    0.33076316118240356,\n","                    0.3642147183418274,\n","                    0.3266438841819763,\n","                    0.32255417108535767,\n","                    0.28961342573165894,\n","                    0.27541375160217285,\n","                    0.30902931094169617,\n","                    0.3498280346393585,\n","                    0.30020612478256226],\n","                   [0.7157350778579712,\n","                    0.4209164083003998,\n","                    0.3646935224533081,\n","                    0.3336670994758606,\n","                    0.31396037340164185,\n","                    0.29757121205329895,\n","                    0.3184283673763275,\n","                    0.34538811445236206,\n","                    0.3123851716518402,\n","                    0.2864893078804016,\n","                    0.26616159081459045,\n","                    0.2834811508655548,\n","                    0.26697856187820435,\n","                    0.2649708390235901,\n","                    0.25985226035118103,\n","                    0.2698213458061218,\n","                    0.23183411359786987,\n","                    0.24611228704452515,\n","                    0.30207180976867676,\n","                    0.26615768671035767]],\n"," 'Validation Accuracy': [[0.854798436164856,\n","                          0.8379220366477966,\n","                          0.883604109287262,\n","                          0.8611530661582947,\n","                          0.8388473987579346,\n","                          0.8725808262825012,\n","                          0.8611333966255188,\n","                          0.8709784150123596,\n","                          0.8899213075637817,\n","                          0.8932153582572937,\n","                          0.8858715295791626,\n","                          0.8859700560569763,\n","                          0.8885537981987,\n","                          0.9004931449890137,\n","                          0.8686478734016418,\n","                          0.8969307541847229,\n","                          0.8811017870903015,\n","                          0.8882520198822021,\n","                          0.8988804817199707,\n","                          0.8732585310935974],\n","                         [0.8573777675628662,\n","                          0.8800703883171082,\n","                          0.8747830390930176,\n","                          0.893673837184906,\n","                          0.8718714714050293,\n","                          0.8970633149147034,\n","                          0.8920994400978088,\n","                          0.8934043049812317,\n","                          0.9026435017585754,\n","                          0.9007230401039124,\n","                          0.8347641825675964,\n","                          0.9019368290901184,\n","                          0.8989424705505371,\n","                          0.8892203569412231,\n","                          0.9134352803230286,\n","                          0.904070258140564,\n","                          0.9121431708335876,\n","                          0.9119117259979248,\n","                          0.8746767640113831,\n","                          0.8894873261451721],\n","                         [0.8467458486557007,\n","                          0.8641675710678101,\n","                          0.8880985379219055,\n","                          0.8901368975639343,\n","                          0.8830089569091797,\n","                          0.8799189329147339,\n","                          0.882422685623169,\n","                          0.8910768032073975,\n","                          0.9011571407318115,\n","                          0.9014841914176941,\n","                          0.903980553150177,\n","                          0.8678825497627258,\n","                          0.870591938495636,\n","                          0.8932999968528748,\n","                          0.9062111973762512,\n","                          0.8306350708007812,\n","                          0.903272807598114,\n","                          0.8987413048744202,\n","                          0.9070520997047424,\n","                          0.903443455696106],\n","                         [0.8217228055000305,\n","                          0.8653125762939453,\n","                          0.8876940011978149,\n","                          0.8711846470832825,\n","                          0.8757571578025818,\n","                          0.8883052468299866,\n","                          0.8994868993759155,\n","                          0.8366426825523376,\n","                          0.8693768978118896,\n","                          0.850864827632904,\n","                          0.898235023021698,\n","                          0.7694868445396423,\n","                          0.8878153562545776,\n","                          0.8737354278564453,\n","                          0.8921828866004944,\n","                          0.8967809081077576,\n","                          0.8947363495826721,\n","                          0.8752526640892029,\n","                          0.885982096195221,\n","                          0.8954933881759644],\n","                         [0.7954552173614502,\n","                          0.8717207312583923,\n","                          0.8606446981430054,\n","                          0.8850799798965454,\n","                          0.8880022764205933,\n","                          0.9006966948509216,\n","                          0.8700428605079651,\n","                          0.7938675284385681,\n","                          0.8933312892913818,\n","                          0.8934566974639893,\n","                          0.8999601006507874,\n","                          0.8887244462966919,\n","                          0.8952614068984985,\n","                          0.9007938504219055,\n","                          0.9067589640617371,\n","                          0.9024391174316406,\n","                          0.9022907614707947,\n","                          0.8869590759277344,\n","                          0.89199298620224,\n","                          0.8962952494621277]],\n"," 'Validation Loss': [[0.41561922430992126,\n","                      0.45215433835983276,\n","                      0.3186357915401459,\n","                      0.3781484365463257,\n","                      0.4546607732772827,\n","                      0.3523109555244446,\n","                      0.3724539279937744,\n","                      0.35624897480010986,\n","                      0.3057170510292053,\n","                      0.29932036995887756,\n","                      0.3126038610935211,\n","                      0.3120167851448059,\n","                      0.31265175342559814,\n","                      0.2669248580932617,\n","                      0.35507044196128845,\n","                      0.2751690447330475,\n","                      0.3192972242832184,\n","                      0.3104515075683594,\n","                      0.27213847637176514,\n","                      0.34491798281669617],\n","                     [0.40583521127700806,\n","                      0.3296906650066376,\n","                      0.3484622538089752,\n","                      0.2888255715370178,\n","                      0.3703194856643677,\n","                      0.27377861738204956,\n","                      0.29547780752182007,\n","                      0.294771671295166,\n","                      0.26014527678489685,\n","                      0.2725687026977539,\n","                      0.45167839527130127,\n","                      0.2623312473297119,\n","                      0.2749696373939514,\n","                      0.3093544542789459,\n","                      0.22786307334899902,\n","                      0.25625234842300415,\n","                      0.2298089861869812,\n","                      0.23185469210147858,\n","                      0.3444156348705292,\n","                      0.3014044165611267],\n","                     [0.44093120098114014,\n","                      0.3819563686847687,\n","                      0.30566564202308655,\n","                      0.29395416378974915,\n","                      0.3223355710506439,\n","                      0.33170419931411743,\n","                      0.32567328214645386,\n","                      0.30059874057769775,\n","                      0.2657627761363983,\n","                      0.26459774374961853,\n","                      0.2513779103755951,\n","                      0.372222363948822,\n","                      0.3425309658050537,\n","                      0.2886345088481903,\n","                      0.24563485383987427,\n","                      0.5161665081977844,\n","                      0.2540087401866913,\n","                      0.2668699622154236,\n","                      0.24190810322761536,\n","                      0.25204020738601685],\n","                     [0.5192357897758484,\n","                      0.38004690408706665,\n","                      0.313188374042511,\n","                      0.36395153403282166,\n","                      0.3506770730018616,\n","                      0.31202274560928345,\n","                      0.26946163177490234,\n","                      0.4657767415046692,\n","                      0.35681116580963135,\n","                      0.40466028451919556,\n","                      0.27470073103904724,\n","                      0.7948089838027954,\n","                      0.3066752254962921,\n","                      0.3661109507083893,\n","                      0.30105265974998474,\n","                      0.282930850982666,\n","                      0.28118157386779785,\n","                      0.34566158056259155,\n","                      0.31067562103271484,\n","                      0.28102394938468933],\n","                     [0.5959243774414062,\n","                      0.3581143319606781,\n","                      0.41077035665512085,\n","                      0.31453222036361694,\n","                      0.2999191880226135,\n","                      0.26603028178215027,\n","                      0.3563181459903717,\n","                      0.5988990664482117,\n","                      0.29087918996810913,\n","                      0.2867625057697296,\n","                      0.2646964490413666,\n","                      0.2980884313583374,\n","                      0.2805160880088806,\n","                      0.26271581649780273,\n","                      0.24416261911392212,\n","                      0.25556594133377075,\n","                      0.25666606426239014,\n","                      0.3003748059272766,\n","                      0.28839829564094543,\n","                      0.27688223123550415]],\n"," 'Validation MCC': [[np.float64(0.7755758791933605),\n","                     np.float64(0.7497637135037362),\n","                     np.float64(0.8214331763885949),\n","                     np.float64(0.7876578832883366),\n","                     np.float64(0.7545682715931114),\n","                     np.float64(0.8042433844469862),\n","                     np.float64(0.7869620457221197),\n","                     np.float64(0.8013235043713185),\n","                     np.float64(0.8311636223381106),\n","                     np.float64(0.8364650479770384),\n","                     np.float64(0.8248193056672536),\n","                     np.float64(0.8248680044718526),\n","                     np.float64(0.8289124870393063),\n","                     np.float64(0.8478189410497162),\n","                     np.float64(0.7977844212778714),\n","                     np.float64(0.8422096468551803),\n","                     np.float64(0.8175709517527388),\n","                     np.float64(0.8286019528737849),\n","                     np.float64(0.8451304481858792),\n","                     np.float64(0.8050859604732231)],\n","                    [np.float64(0.7790076588517136),\n","                     np.float64(0.81466018774018),\n","                     np.float64(0.8067552368542531),\n","                     np.float64(0.8360501239635503),\n","                     np.float64(0.8035514543197081),\n","                     np.float64(0.841846568230199),\n","                     np.float64(0.8336339008317126),\n","                     np.float64(0.8361148076749935),\n","                     np.float64(0.8500521010264458),\n","                     np.float64(0.8475580523438686),\n","                     np.float64(0.743991520712164),\n","                     np.float64(0.8494209926542591),\n","                     np.float64(0.8444745704263207),\n","                     np.float64(0.8288597215324429),\n","                     np.float64(0.8675031016899788),\n","                     np.float64(0.8530991944324858),\n","                     np.float64(0.8654756058291636),\n","                     np.float64(0.8645699373485628),\n","                     np.float64(0.8068693150127519),\n","                     np.float64(0.8295493724669499)],\n","                    [np.float64(0.7624067522943965),\n","                     np.float64(0.7916998350320744),\n","                     np.float64(0.8281567554943673),\n","                     np.float64(0.8321860977996565),\n","                     np.float64(0.8204445285827958),\n","                     np.float64(0.8162281855237525),\n","                     np.float64(0.819309235424063),\n","                     np.float64(0.8326838013935706),\n","                     np.float64(0.8485919498540822),\n","                     np.float64(0.8491431472078586),\n","                     np.float64(0.8538722927240013),\n","                     np.float64(0.7961533308273344),\n","                     np.float64(0.8010763264688826),\n","                     np.float64(0.8368647090425059),\n","                     np.float64(0.8570701606557649),\n","                     np.float64(0.7379099749700085),\n","                     np.float64(0.8520393420317967),\n","                     np.float64(0.8451852004872376),\n","                     np.float64(0.85786095936702),\n","                     np.float64(0.8526414236202781)],\n","                    [np.float64(0.7216883437604097),\n","                     np.float64(0.7930374624506757),\n","                     np.float64(0.8275034057065721),\n","                     np.float64(0.8004346986860333),\n","                     np.float64(0.807776415687526),\n","                     np.float64(0.8280907734333378),\n","                     np.float64(0.8460073645466621),\n","                     np.float64(0.74694725197851),\n","                     np.float64(0.7999480846940579),\n","                     np.float64(0.7722290063176267),\n","                     np.float64(0.843358733633004),\n","                     np.float64(0.6379062914943873),\n","                     np.float64(0.8268667205137622),\n","                     np.float64(0.804448054492696),\n","                     np.float64(0.8337881560561972),\n","                     np.float64(0.8413110321114808),\n","                     np.float64(0.8383904407951172),\n","                     np.float64(0.8070861526152528),\n","                     np.float64(0.8243209927142178),\n","                     np.float64(0.8392594055441687)],\n","                    [np.float64(0.6823877634257607),\n","                     np.float64(0.8028541645698544),\n","                     np.float64(0.7848860241448072),\n","                     np.float64(0.8233431587575768),\n","                     np.float64(0.8277905096794335),\n","                     np.float64(0.8482177126818647),\n","                     np.float64(0.8005611441952247),\n","                     np.float64(0.6916913004414004),\n","                     np.float64(0.8365364508186746),\n","                     np.float64(0.8366945205399231),\n","                     np.float64(0.8470275501333537),\n","                     np.float64(0.8305298456972031),\n","                     np.float64(0.8397497898333042),\n","                     np.float64(0.8480687488947414),\n","                     np.float64(0.8574804977068197),\n","                     np.float64(0.8509341356925517),\n","                     np.float64(0.8505641435425811),\n","                     np.float64(0.8262693043189355),\n","                     np.float64(0.8344349507268712),\n","                     np.float64(0.8408409692913486)]]}\n","Training Model: BiLSTM_Dense, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6401 - loss: 0.9986(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8073\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.6404 - loss: 0.9978 - val_accuracy: 0.8749 - val_loss: 0.3616 - mcc: 0.8073\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8714 - loss: 0.3663(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8234\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8714 - loss: 0.3662 - val_accuracy: 0.8851 - val_loss: 0.3216 - mcc: 0.8234\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8850 - loss: 0.3188(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8496\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8850 - loss: 0.3188 - val_accuracy: 0.9016 - val_loss: 0.2680 - mcc: 0.8496\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8957 - loss: 0.2843(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8467\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8957 - loss: 0.2843 - val_accuracy: 0.8995 - val_loss: 0.2691 - mcc: 0.8467\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8986 - loss: 0.2748(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8549\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8986 - loss: 0.2748 - val_accuracy: 0.9051 - val_loss: 0.2589 - mcc: 0.8549\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9017 - loss: 0.2709(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8612\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9017 - loss: 0.2709 - val_accuracy: 0.9092 - val_loss: 0.2435 - mcc: 0.8612\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9091 - loss: 0.2428(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8679\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9091 - loss: 0.2428 - val_accuracy: 0.9136 - val_loss: 0.2316 - mcc: 0.8679\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9110 - loss: 0.2374(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8683\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 38ms/step - accuracy: 0.9110 - loss: 0.2374 - val_accuracy: 0.9134 - val_loss: 0.2242 - mcc: 0.8683\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9065 - loss: 0.2497(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8710\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.9065 - loss: 0.2497 - val_accuracy: 0.9153 - val_loss: 0.2231 - mcc: 0.8710\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9063 - loss: 0.2520(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8735\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9063 - loss: 0.2520 - val_accuracy: 0.9172 - val_loss: 0.2217 - mcc: 0.8735\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9119 - loss: 0.2361(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8451\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9119 - loss: 0.2361 - val_accuracy: 0.8987 - val_loss: 0.2724 - mcc: 0.8451\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9132 - loss: 0.2335(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8183\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9132 - loss: 0.2335 - val_accuracy: 0.8809 - val_loss: 0.3499 - mcc: 0.8183\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8909 - loss: 0.3024(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8597\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8909 - loss: 0.3024 - val_accuracy: 0.9076 - val_loss: 0.2513 - mcc: 0.8597\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9093 - loss: 0.2461(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8728\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 36ms/step - accuracy: 0.9093 - loss: 0.2461 - val_accuracy: 0.9168 - val_loss: 0.2242 - mcc: 0.8728\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9170 - loss: 0.2192(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8822\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9170 - loss: 0.2192 - val_accuracy: 0.9229 - val_loss: 0.2017 - mcc: 0.8822\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9126 - loss: 0.2341(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8091\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9126 - loss: 0.2341 - val_accuracy: 0.8757 - val_loss: 0.3352 - mcc: 0.8091\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8911 - loss: 0.2958(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8589\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8911 - loss: 0.2958 - val_accuracy: 0.9078 - val_loss: 0.2808 - mcc: 0.8589\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9110 - loss: 0.2396(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8572\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9110 - loss: 0.2396 - val_accuracy: 0.9068 - val_loss: 0.2489 - mcc: 0.8572\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9115 - loss: 0.2361(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8617\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9115 - loss: 0.2361 - val_accuracy: 0.9097 - val_loss: 0.2403 - mcc: 0.8617\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9122 - loss: 0.2344(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8661\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9122 - loss: 0.2344 - val_accuracy: 0.9122 - val_loss: 0.2385 - mcc: 0.8661\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 2\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6278 - loss: 1.0192(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7511\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - accuracy: 0.6280 - loss: 1.0185 - val_accuracy: 0.8402 - val_loss: 0.4676 - mcc: 0.7511\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8536 - loss: 0.4181(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8233\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.8536 - loss: 0.4180 - val_accuracy: 0.8851 - val_loss: 0.3154 - mcc: 0.8233\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8844 - loss: 0.3195(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8353\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.8844 - loss: 0.3195 - val_accuracy: 0.8923 - val_loss: 0.2926 - mcc: 0.8353\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8960 - loss: 0.2842(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8289\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8960 - loss: 0.2842 - val_accuracy: 0.8889 - val_loss: 0.3075 - mcc: 0.8289\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8916 - loss: 0.2998(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8476\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8916 - loss: 0.2998 - val_accuracy: 0.9008 - val_loss: 0.2726 - mcc: 0.8476\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9021 - loss: 0.2677(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8635\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9021 - loss: 0.2677 - val_accuracy: 0.9108 - val_loss: 0.2388 - mcc: 0.8635\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9056 - loss: 0.2544(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8627\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9056 - loss: 0.2544 - val_accuracy: 0.9098 - val_loss: 0.2397 - mcc: 0.8627\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9064 - loss: 0.2538(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8712\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9064 - loss: 0.2538 - val_accuracy: 0.9162 - val_loss: 0.2267 - mcc: 0.8712\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9140 - loss: 0.2300(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8491\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9140 - loss: 0.2300 - val_accuracy: 0.9012 - val_loss: 0.2638 - mcc: 0.8491\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9054 - loss: 0.2560(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8743\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9054 - loss: 0.2560 - val_accuracy: 0.9180 - val_loss: 0.2193 - mcc: 0.8743\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9114 - loss: 0.2370(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8647\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9114 - loss: 0.2370 - val_accuracy: 0.9117 - val_loss: 0.2517 - mcc: 0.8647\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9070 - loss: 0.2532(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8495\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9070 - loss: 0.2532 - val_accuracy: 0.9022 - val_loss: 0.2653 - mcc: 0.8495\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8973 - loss: 0.2816(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8532\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.8973 - loss: 0.2817 - val_accuracy: 0.9045 - val_loss: 0.2610 - mcc: 0.8532\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8906 - loss: 0.2995(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8089\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8905 - loss: 0.2995 - val_accuracy: 0.8763 - val_loss: 0.3472 - mcc: 0.8089\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8864 - loss: 0.3181(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8472\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8864 - loss: 0.3181 - val_accuracy: 0.9002 - val_loss: 0.2726 - mcc: 0.8472\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9035 - loss: 0.2646(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8604\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9035 - loss: 0.2646 - val_accuracy: 0.9093 - val_loss: 0.2457 - mcc: 0.8604\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9040 - loss: 0.2641(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8583\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9040 - loss: 0.2641 - val_accuracy: 0.9076 - val_loss: 0.2534 - mcc: 0.8583\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9079 - loss: 0.2518(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8532\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9079 - loss: 0.2518 - val_accuracy: 0.9047 - val_loss: 0.2578 - mcc: 0.8532\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8982 - loss: 0.2774(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8623\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 36ms/step - accuracy: 0.8982 - loss: 0.2773 - val_accuracy: 0.9103 - val_loss: 0.2399 - mcc: 0.8623\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9035 - loss: 0.2620(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8662\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9035 - loss: 0.2620 - val_accuracy: 0.9125 - val_loss: 0.2350 - mcc: 0.8662\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 3\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6517 - loss: 0.9954(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7621\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - accuracy: 0.6518 - loss: 0.9950 - val_accuracy: 0.8458 - val_loss: 0.4404 - mcc: 0.7621\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8631 - loss: 0.3871(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7923\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 34ms/step - accuracy: 0.8631 - loss: 0.3870 - val_accuracy: 0.8649 - val_loss: 0.3770 - mcc: 0.7923\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8821 - loss: 0.3252(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8352\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8821 - loss: 0.3252 - val_accuracy: 0.8926 - val_loss: 0.2977 - mcc: 0.8352\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8914 - loss: 0.2956(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8401\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8914 - loss: 0.2956 - val_accuracy: 0.8956 - val_loss: 0.2863 - mcc: 0.8401\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8955 - loss: 0.2846(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8475\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8955 - loss: 0.2846 - val_accuracy: 0.9003 - val_loss: 0.2672 - mcc: 0.8475\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8972 - loss: 0.2818(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7605\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.8972 - loss: 0.2819 - val_accuracy: 0.8444 - val_loss: 0.4452 - mcc: 0.7605\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8822 - loss: 0.3313(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8309\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8822 - loss: 0.3312 - val_accuracy: 0.8899 - val_loss: 0.3165 - mcc: 0.8309\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8968 - loss: 0.2898(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8401\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8968 - loss: 0.2898 - val_accuracy: 0.8956 - val_loss: 0.2902 - mcc: 0.8401\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9005 - loss: 0.2714(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8424\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9004 - loss: 0.2714 - val_accuracy: 0.8966 - val_loss: 0.2819 - mcc: 0.8424\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9028 - loss: 0.2644(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8602\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9028 - loss: 0.2644 - val_accuracy: 0.9080 - val_loss: 0.2445 - mcc: 0.8602\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9059 - loss: 0.2544(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8630\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.9059 - loss: 0.2544 - val_accuracy: 0.9104 - val_loss: 0.2393 - mcc: 0.8630\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9100 - loss: 0.2393(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8658\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9100 - loss: 0.2393 - val_accuracy: 0.9122 - val_loss: 0.2324 - mcc: 0.8658\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9114 - loss: 0.2355(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8540\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9114 - loss: 0.2354 - val_accuracy: 0.9045 - val_loss: 0.2544 - mcc: 0.8540\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9086 - loss: 0.2450(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8721\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9086 - loss: 0.2450 - val_accuracy: 0.9161 - val_loss: 0.2200 - mcc: 0.8721\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9157 - loss: 0.2240(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8491\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9157 - loss: 0.2240 - val_accuracy: 0.9006 - val_loss: 0.2653 - mcc: 0.8491\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9077 - loss: 0.2477(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8597\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.9077 - loss: 0.2477 - val_accuracy: 0.9082 - val_loss: 0.2453 - mcc: 0.8597\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9154 - loss: 0.2247(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8227\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9154 - loss: 0.2247 - val_accuracy: 0.8842 - val_loss: 0.3177 - mcc: 0.8227\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8941 - loss: 0.2910(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8599\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.8941 - loss: 0.2910 - val_accuracy: 0.9083 - val_loss: 0.2466 - mcc: 0.8599\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9049 - loss: 0.2540(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8611\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9049 - loss: 0.2540 - val_accuracy: 0.9092 - val_loss: 0.2424 - mcc: 0.8611\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9028 - loss: 0.2670(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8523\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9028 - loss: 0.2671 - val_accuracy: 0.9035 - val_loss: 0.2657 - mcc: 0.8523\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 4\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6681 - loss: 0.9560(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7960\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - accuracy: 0.6683 - loss: 0.9552 - val_accuracy: 0.8683 - val_loss: 0.3779 - mcc: 0.7960\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8722 - loss: 0.3647(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8388\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8722 - loss: 0.3647 - val_accuracy: 0.8953 - val_loss: 0.2973 - mcc: 0.8388\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8934 - loss: 0.2942(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8249\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8934 - loss: 0.2942 - val_accuracy: 0.8866 - val_loss: 0.3180 - mcc: 0.8249\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8972 - loss: 0.2828(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8368\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8972 - loss: 0.2828 - val_accuracy: 0.8939 - val_loss: 0.2948 - mcc: 0.8368\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8840 - loss: 0.3234(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8518\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8840 - loss: 0.3234 - val_accuracy: 0.9034 - val_loss: 0.2627 - mcc: 0.8518\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9011 - loss: 0.2714(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8479\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9011 - loss: 0.2714 - val_accuracy: 0.9011 - val_loss: 0.2656 - mcc: 0.8479\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8988 - loss: 0.2775(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8371\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.8988 - loss: 0.2775 - val_accuracy: 0.8937 - val_loss: 0.2876 - mcc: 0.8371\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9020 - loss: 0.2656(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8610\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9020 - loss: 0.2656 - val_accuracy: 0.9090 - val_loss: 0.2446 - mcc: 0.8610\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9115 - loss: 0.2351(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8628\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9115 - loss: 0.2351 - val_accuracy: 0.9109 - val_loss: 0.2427 - mcc: 0.8628\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9128 - loss: 0.2304(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8652\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9128 - loss: 0.2304 - val_accuracy: 0.9123 - val_loss: 0.2384 - mcc: 0.8652\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9149 - loss: 0.2283(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8697\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9149 - loss: 0.2282 - val_accuracy: 0.9147 - val_loss: 0.2289 - mcc: 0.8697\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9161 - loss: 0.2223(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8688\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9161 - loss: 0.2223 - val_accuracy: 0.9145 - val_loss: 0.2261 - mcc: 0.8688\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9167 - loss: 0.2212(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8706\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9167 - loss: 0.2212 - val_accuracy: 0.9154 - val_loss: 0.2333 - mcc: 0.8706\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9180 - loss: 0.2178(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8798\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9180 - loss: 0.2178 - val_accuracy: 0.9212 - val_loss: 0.2056 - mcc: 0.8798\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9132 - loss: 0.2298(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8655\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9132 - loss: 0.2298 - val_accuracy: 0.9122 - val_loss: 0.2347 - mcc: 0.8655\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9124 - loss: 0.2337(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8415\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9124 - loss: 0.2337 - val_accuracy: 0.8971 - val_loss: 0.2826 - mcc: 0.8415\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9044 - loss: 0.2571(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8688\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9045 - loss: 0.2571 - val_accuracy: 0.9142 - val_loss: 0.2264 - mcc: 0.8688\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9066 - loss: 0.2472(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8682\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.9066 - loss: 0.2472 - val_accuracy: 0.9142 - val_loss: 0.2276 - mcc: 0.8682\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9154 - loss: 0.2231(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8770\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9154 - loss: 0.2231 - val_accuracy: 0.9197 - val_loss: 0.2117 - mcc: 0.8770\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9192 - loss: 0.2106(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8829\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9192 - loss: 0.2106 - val_accuracy: 0.9234 - val_loss: 0.2030 - mcc: 0.8829\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 5\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6598 - loss: 0.9737(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7718\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.6601 - loss: 0.9730 - val_accuracy: 0.8524 - val_loss: 0.4240 - mcc: 0.7718\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8691 - loss: 0.3683(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8032\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - accuracy: 0.8691 - loss: 0.3682 - val_accuracy: 0.8718 - val_loss: 0.3498 - mcc: 0.8032\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8825 - loss: 0.3288(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8344\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8825 - loss: 0.3288 - val_accuracy: 0.8910 - val_loss: 0.2947 - mcc: 0.8344\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8963 - loss: 0.2822(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8379\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.8963 - loss: 0.2822 - val_accuracy: 0.8938 - val_loss: 0.2817 - mcc: 0.8379\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9013 - loss: 0.2678(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8289\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9013 - loss: 0.2679 - val_accuracy: 0.8887 - val_loss: 0.2978 - mcc: 0.8289\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8722 - loss: 0.3577(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8284\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8722 - loss: 0.3577 - val_accuracy: 0.8879 - val_loss: 0.3025 - mcc: 0.8284\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8946 - loss: 0.2907(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8500\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8946 - loss: 0.2907 - val_accuracy: 0.9020 - val_loss: 0.2686 - mcc: 0.8500\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8836 - loss: 0.3237(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8489\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8837 - loss: 0.3236 - val_accuracy: 0.9014 - val_loss: 0.2691 - mcc: 0.8489\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8918 - loss: 0.2990(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8439\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8918 - loss: 0.2990 - val_accuracy: 0.8982 - val_loss: 0.2800 - mcc: 0.8439\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9082 - loss: 0.2491(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8653\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9082 - loss: 0.2491 - val_accuracy: 0.9119 - val_loss: 0.2360 - mcc: 0.8653\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9121 - loss: 0.2374(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8622\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9121 - loss: 0.2374 - val_accuracy: 0.9098 - val_loss: 0.2416 - mcc: 0.8622\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9032 - loss: 0.2649(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8672\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9032 - loss: 0.2648 - val_accuracy: 0.9131 - val_loss: 0.2276 - mcc: 0.8672\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9140 - loss: 0.2294(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8551\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9140 - loss: 0.2295 - val_accuracy: 0.9053 - val_loss: 0.2516 - mcc: 0.8551\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9109 - loss: 0.2353(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8607\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9109 - loss: 0.2353 - val_accuracy: 0.9090 - val_loss: 0.2412 - mcc: 0.8607\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9137 - loss: 0.2302(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8616\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9136 - loss: 0.2303 - val_accuracy: 0.9094 - val_loss: 0.2430 - mcc: 0.8616\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9032 - loss: 0.2652(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8633\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9032 - loss: 0.2652 - val_accuracy: 0.9107 - val_loss: 0.2428 - mcc: 0.8633\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9084 - loss: 0.2480(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8670\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9084 - loss: 0.2480 - val_accuracy: 0.9129 - val_loss: 0.2344 - mcc: 0.8670\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9110 - loss: 0.2384(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8598\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.9110 - loss: 0.2384 - val_accuracy: 0.9080 - val_loss: 0.2449 - mcc: 0.8598\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9155 - loss: 0.2261(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8705\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9155 - loss: 0.2261 - val_accuracy: 0.9153 - val_loss: 0.2227 - mcc: 0.8705\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9200 - loss: 0.2113(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8768\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9200 - loss: 0.2113 - val_accuracy: 0.9190 - val_loss: 0.2110 - mcc: 0.8768\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.9234126666666667),\n","              'mean': np.float64(0.9141157333333334),\n","              'min': np.float64(0.9034933333333334),\n","              'std': np.float64(0.006770350926076287)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0008646751244862875),\n","                               'mean': np.float64(0.0005373300711313884),\n","                               'min': np.float64(0.0004338306585947673),\n","                               'std': np.float64(0.00016466449399546855)},\n"," 'MCC': {'max': np.float64(0.8828706251665726),\n","         'mean': np.float64(0.8688394818748716),\n","         'min': np.float64(0.8523041032301794),\n","         'std': np.float64(0.010470519167337168)},\n"," 'Parameters': 9829,\n"," 'Train Time (s)': {'max': np.float64(702.5649733543396),\n","                    'mean': np.float64(691.3008722305298),\n","                    'min': np.float64(674.5143580436707),\n","                    'std': np.float64(9.321541881166729)},\n"," 'Training Accuracy': [[0.7469625473022461,\n","                        0.8774141669273376,\n","                        0.8895835876464844,\n","                        0.8987714648246765,\n","                        0.901048481464386,\n","                        0.9027150869369507,\n","                        0.9078897833824158,\n","                        0.912240743637085,\n","                        0.9095242619514465,\n","                        0.9060717225074768,\n","                        0.9119040369987488,\n","                        0.9171157479286194,\n","                        0.8956800103187561,\n","                        0.9119539260864258,\n","                        0.9171532392501831,\n","                        0.9073924422264099,\n","                        0.9044756889343262,\n","                        0.9046748876571655,\n","                        0.9104172587394714,\n","                        0.909518301486969],\n","                       [0.7327112555503845,\n","                        0.8645273447036743,\n","                        0.8845421671867371,\n","                        0.8960651159286499,\n","                        0.8963999152183533,\n","                        0.9026622176170349,\n","                        0.9054043889045715,\n","                        0.9073963165283203,\n","                        0.9119002223014832,\n","                        0.9078515768051147,\n","                        0.911618709564209,\n","                        0.9046214818954468,\n","                        0.8924239873886108,\n","                        0.8846996426582336,\n","                        0.8914098143577576,\n","                        0.9022784233093262,\n","                        0.9041810631752014,\n","                        0.9088319540023804,\n","                        0.90054851770401,\n","                        0.9024397730827332],\n","                       [0.7505883574485779,\n","                        0.8696712255477905,\n","                        0.8831566572189331,\n","                        0.8925598859786987,\n","                        0.8950952887535095,\n","                        0.8878597617149353,\n","                        0.8878777623176575,\n","                        0.9007062911987305,\n","                        0.8949460387229919,\n","                        0.9040961861610413,\n","                        0.907799243927002,\n","                        0.9074979424476624,\n","                        0.913953959941864,\n","                        0.9118066430091858,\n","                        0.9155538082122803,\n","                        0.9115041494369507,\n","                        0.9104798436164856,\n","                        0.8996250033378601,\n","                        0.9056674242019653,\n","                        0.8989342451095581],\n","                       [0.7696633338928223,\n","                        0.880634605884552,\n","                        0.8940520286560059,\n","                        0.8972167372703552,\n","                        0.8831710815429688,\n","                        0.9015021324157715,\n","                        0.8939017057418823,\n","                        0.9045111536979675,\n","                        0.9123156666755676,\n","                        0.9127970933914185,\n","                        0.9147158861160278,\n","                        0.91620272397995,\n","                        0.9172459244728088,\n","                        0.9191718101501465,\n","                        0.9113453030586243,\n","                        0.905474066734314,\n","                        0.9086357355117798,\n","                        0.9079108834266663,\n","                        0.9159173965454102,\n","                        0.9197002649307251],\n","                       [0.759939432144165,\n","                        0.8742027878761292,\n","                        0.8875520825386047,\n","                        0.8975293040275574,\n","                        0.898004412651062,\n","                        0.8701496720314026,\n","                        0.8977157473564148,\n","                        0.8918014168739319,\n","                        0.8948624730110168,\n","                        0.9073795676231384,\n","                        0.9111130237579346,\n","                        0.9108617901802063,\n","                        0.9100747108459473,\n","                        0.9117544889450073,\n","                        0.9100662469863892,\n","                        0.9032806754112244,\n","                        0.9098250269889832,\n","                        0.9136826395988464,\n","                        0.9159372448921204,\n","                        0.9203903079032898]],\n"," 'Training Loss': [[0.7064982652664185,\n","                    0.3452962338924408,\n","                    0.3044499456882477,\n","                    0.27568820118904114,\n","                    0.26995643973350525,\n","                    0.26671329140663147,\n","                    0.24634063243865967,\n","                    0.2333178073167801,\n","                    0.24005337059497833,\n","                    0.2539556920528412,\n","                    0.23597835004329681,\n","                    0.22086083889007568,\n","                    0.2875808775424957,\n","                    0.2364526391029358,\n","                    0.21869508922100067,\n","                    0.2497626096010208,\n","                    0.2572169303894043,\n","                    0.258343368768692,\n","                    0.23973076045513153,\n","                    0.2452283501625061],\n","                   [0.7486029267311096,\n","                    0.3836272060871124,\n","                    0.31928133964538574,\n","                    0.28372326493263245,\n","                    0.2840867042541504,\n","                    0.2651938498020172,\n","                    0.2562844455242157,\n","                    0.2503938674926758,\n","                    0.2356892079114914,\n","                    0.24814198911190033,\n","                    0.2367202341556549,\n","                    0.25815510749816895,\n","                    0.29792073369026184,\n","                    0.31565046310424805,\n","                    0.302083283662796,\n","                    0.2672139108181,\n","                    0.26228514313697815,\n","                    0.24643126130104065,\n","                    0.27094584703445435,\n","                    0.26529210805892944],\n","                   [0.7086760401725769,\n","                    0.3656618595123291,\n","                    0.3242052495479584,\n","                    0.2924782633781433,\n","                    0.2867640554904938,\n","                    0.31237512826919556,\n","                    0.31584811210632324,\n","                    0.2747305929660797,\n","                    0.2895766794681549,\n","                    0.26056718826293945,\n","                    0.24817268550395966,\n","                    0.24818526208400726,\n","                    0.22876745462417603,\n","                    0.23564264178276062,\n","                    0.22371912002563477,\n","                    0.23539020121097565,\n","                    0.24114251136779785,\n","                    0.2740132808685303,\n","                    0.25357553362846375,\n","                    0.28691381216049194],\n","                   [0.6598882079124451,\n","                    0.3372763395309448,\n","                    0.291092187166214,\n","                    0.2817399203777313,\n","                    0.3261848986148834,\n","                    0.27088284492492676,\n","                    0.2914734184741974,\n","                    0.2572627067565918,\n","                    0.23239323496818542,\n","                    0.2320212423801422,\n","                    0.2273949384689331,\n","                    0.22191168367862701,\n","                    0.22022593021392822,\n","                    0.21328887343406677,\n","                    0.237918883562088,\n","                    0.25556132197380066,\n","                    0.24367095530033112,\n","                    0.24498532712459564,\n","                    0.22133652865886688,\n","                    0.20975758135318756],\n","                   [0.683151364326477,\n","                    0.35432112216949463,\n","                    0.3121896982192993,\n","                    0.27822285890579224,\n","                    0.27734968066215515,\n","                    0.36204761266708374,\n","                    0.28110912442207336,\n","                    0.30046623945236206,\n","                    0.29180824756622314,\n","                    0.24977776408195496,\n","                    0.23967447876930237,\n","                    0.23951344192028046,\n","                    0.24124586582183838,\n","                    0.23426993191242218,\n","                    0.24205295741558075,\n","                    0.2667625844478607,\n","                    0.24470332264900208,\n","                    0.23137196898460388,\n","                    0.2241620123386383,\n","                    0.2103864997625351]],\n"," 'Validation Accuracy': [[0.8749414086341858,\n","                          0.885149359703064,\n","                          0.9016200304031372,\n","                          0.8994762301445007,\n","                          0.9051202535629272,\n","                          0.90921950340271,\n","                          0.913576602935791,\n","                          0.9133955240249634,\n","                          0.9153105616569519,\n","                          0.9172426462173462,\n","                          0.8986627459526062,\n","                          0.8809433579444885,\n","                          0.9076461791992188,\n","                          0.9167810082435608,\n","                          0.9228653311729431,\n","                          0.8756913542747498,\n","                          0.9077778458595276,\n","                          0.9067628383636475,\n","                          0.9096717834472656,\n","                          0.9122040867805481],\n","                         [0.8402413129806519,\n","                          0.8850752711296082,\n","                          0.8922948241233826,\n","                          0.8888638615608215,\n","                          0.900813639163971,\n","                          0.910804271697998,\n","                          0.9097658395767212,\n","                          0.916154146194458,\n","                          0.9011940360069275,\n","                          0.9180453419685364,\n","                          0.9116615653038025,\n","                          0.902219295501709,\n","                          0.904500424861908,\n","                          0.8763066530227661,\n","                          0.9002458453178406,\n","                          0.9092824459075928,\n","                          0.907623291015625,\n","                          0.9046840071678162,\n","                          0.9102547764778137,\n","                          0.9124920964241028],\n","                         [0.8458107709884644,\n","                          0.8649393916130066,\n","                          0.8925999402999878,\n","                          0.8955877423286438,\n","                          0.9003387689590454,\n","                          0.8443661332130432,\n","                          0.8898726105690002,\n","                          0.8955893516540527,\n","                          0.8966425061225891,\n","                          0.9079994559288025,\n","                          0.910405695438385,\n","                          0.9122311472892761,\n","                          0.9044786691665649,\n","                          0.9160982370376587,\n","                          0.9005541205406189,\n","                          0.9082219004631042,\n","                          0.8841984868049622,\n","                          0.9083167314529419,\n","                          0.9091618061065674,\n","                          0.9034931659698486],\n","                         [0.8682912588119507,\n","                          0.8952674269676208,\n","                          0.8866181373596191,\n","                          0.8938643336296082,\n","                          0.9034372568130493,\n","                          0.9011009931564331,\n","                          0.8937477469444275,\n","                          0.9089803695678711,\n","                          0.9108579158782959,\n","                          0.9122799634933472,\n","                          0.9147272109985352,\n","                          0.9145094156265259,\n","                          0.9153593182563782,\n","                          0.9212292432785034,\n","                          0.9122254848480225,\n","                          0.8971214294433594,\n","                          0.9142184853553772,\n","                          0.9142200350761414,\n","                          0.9196808338165283,\n","                          0.9234126210212708],\n","                         [0.8524040579795837,\n","                          0.871829092502594,\n","                          0.8910278081893921,\n","                          0.8938034772872925,\n","                          0.8886699080467224,\n","                          0.8878770470619202,\n","                          0.9019679427146912,\n","                          0.9013580679893494,\n","                          0.8982033133506775,\n","                          0.9118521213531494,\n","                          0.9098187685012817,\n","                          0.9131046533584595,\n","                          0.9053025841712952,\n","                          0.9090367555618286,\n","                          0.909436821937561,\n","                          0.9106752276420593,\n","                          0.9129139184951782,\n","                          0.9080347418785095,\n","                          0.9152626991271973,\n","                          0.9189766645431519]],\n"," 'Validation Loss': [[0.361600786447525,\n","                      0.32160061597824097,\n","                      0.2679970860481262,\n","                      0.26914340257644653,\n","                      0.2588890790939331,\n","                      0.24347826838493347,\n","                      0.2316320538520813,\n","                      0.22424805164337158,\n","                      0.22307133674621582,\n","                      0.22166621685028076,\n","                      0.27237701416015625,\n","                      0.3499177098274231,\n","                      0.2512737214565277,\n","                      0.2242124378681183,\n","                      0.2016867846250534,\n","                      0.33523133397102356,\n","                      0.2808245122432709,\n","                      0.2489279806613922,\n","                      0.24030077457427979,\n","                      0.23853035271167755],\n","                     [0.4675814211368561,\n","                      0.3153763711452484,\n","                      0.2926420271396637,\n","                      0.307535320520401,\n","                      0.27264225482940674,\n","                      0.23884643614292145,\n","                      0.2396947294473648,\n","                      0.22667154669761658,\n","                      0.2637995779514313,\n","                      0.2192976027727127,\n","                      0.2517071068286896,\n","                      0.26532450318336487,\n","                      0.2609657645225525,\n","                      0.34717342257499695,\n","                      0.27258625626564026,\n","                      0.24569423496723175,\n","                      0.25341373682022095,\n","                      0.25777867436408997,\n","                      0.23986664414405823,\n","                      0.2349952608346939],\n","                     [0.4404451847076416,\n","                      0.3769914507865906,\n","                      0.2977309823036194,\n","                      0.28631627559661865,\n","                      0.2671807110309601,\n","                      0.44522303342819214,\n","                      0.31651198863983154,\n","                      0.2901946008205414,\n","                      0.281876802444458,\n","                      0.2445247620344162,\n","                      0.2393445074558258,\n","                      0.23242893815040588,\n","                      0.2544131278991699,\n","                      0.22001247107982635,\n","                      0.2653167247772217,\n","                      0.24528095126152039,\n","                      0.31771257519721985,\n","                      0.2466340810060501,\n","                      0.24244266748428345,\n","                      0.2656939625740051],\n","                     [0.3778859078884125,\n","                      0.29729002714157104,\n","                      0.31795546412467957,\n","                      0.294767826795578,\n","                      0.2626848816871643,\n","                      0.26563605666160583,\n","                      0.2876425087451935,\n","                      0.2445976287126541,\n","                      0.24266672134399414,\n","                      0.2384454756975174,\n","                      0.22885455191135406,\n","                      0.2261434942483902,\n","                      0.2333003282546997,\n","                      0.20559698343276978,\n","                      0.23465600609779358,\n","                      0.2826319634914398,\n","                      0.22642989456653595,\n","                      0.22764043509960175,\n","                      0.21165785193443298,\n","                      0.20296184718608856],\n","                     [0.42400112748146057,\n","                      0.3498065173625946,\n","                      0.2947376072406769,\n","                      0.281668096780777,\n","                      0.2977602183818817,\n","                      0.30248910188674927,\n","                      0.268642395734787,\n","                      0.26910197734832764,\n","                      0.27996116876602173,\n","                      0.2360328584909439,\n","                      0.24158410727977753,\n","                      0.22763925790786743,\n","                      0.25158098340034485,\n","                      0.24118861556053162,\n","                      0.24296240508556366,\n","                      0.24279384315013885,\n","                      0.2344280183315277,\n","                      0.24491670727729797,\n","                      0.22274012863636017,\n","                      0.21097664535045624]],\n"," 'Validation MCC': [[np.float64(0.8072690955130345),\n","                     np.float64(0.8234031872395886),\n","                     np.float64(0.8495685367303843),\n","                     np.float64(0.8466598652974251),\n","                     np.float64(0.8548826727467672),\n","                     np.float64(0.8612181573809182),\n","                     np.float64(0.8679019805655458),\n","                     np.float64(0.8682833273147116),\n","                     np.float64(0.8710293280742191),\n","                     np.float64(0.8735478045629497),\n","                     np.float64(0.8450944267044236),\n","                     np.float64(0.818278722371662),\n","                     np.float64(0.8596823095257616),\n","                     np.float64(0.872767592352616),\n","                     np.float64(0.8821561185120242),\n","                     np.float64(0.8091016192708004),\n","                     np.float64(0.8589125911607884),\n","                     np.float64(0.8572315964320626),\n","                     np.float64(0.8617014272522997),\n","                     np.float64(0.8660710716254637)],\n","                    [np.float64(0.7510929351575085),\n","                     np.float64(0.8233311337427038),\n","                     np.float64(0.8352700832843442),\n","                     np.float64(0.8289181539202285),\n","                     np.float64(0.8476248183203388),\n","                     np.float64(0.8635016262966428),\n","                     np.float64(0.8626736586573824),\n","                     np.float64(0.8712267110739887),\n","                     np.float64(0.8490926754374376),\n","                     np.float64(0.8743228681883747),\n","                     np.float64(0.8646705972729282),\n","                     np.float64(0.849490765432378),\n","                     np.float64(0.8532399753249026),\n","                     np.float64(0.808903385195073),\n","                     np.float64(0.8472356224166988),\n","                     np.float64(0.8604375173289869),\n","                     np.float64(0.8582686792544053),\n","                     np.float64(0.8531858248515246),\n","                     np.float64(0.862283156312853),\n","                     np.float64(0.8661700230164558)],\n","                    [np.float64(0.762088888649517),\n","                     np.float64(0.7922889069134821),\n","                     np.float64(0.8352353946341321),\n","                     np.float64(0.8401170057296682),\n","                     np.float64(0.8475154914750966),\n","                     np.float64(0.7604962396763252),\n","                     np.float64(0.8308944884645622),\n","                     np.float64(0.8401477441493614),\n","                     np.float64(0.8423649534975375),\n","                     np.float64(0.8602241146916316),\n","                     np.float64(0.8630011941180845),\n","                     np.float64(0.8658356720860539),\n","                     np.float64(0.8539581436782924),\n","                     np.float64(0.8720746784055204),\n","                     np.float64(0.8490667356304785),\n","                     np.float64(0.8597096188163467),\n","                     np.float64(0.8226543485793723),\n","                     np.float64(0.8598501962840848),\n","                     np.float64(0.861127566545149),\n","                     np.float64(0.8523041032301794)],\n","                    [np.float64(0.7959505731070639),\n","                     np.float64(0.838830359455932),\n","                     np.float64(0.824880147797332),\n","                     np.float64(0.8368105588405502),\n","                     np.float64(0.8517630935605852),\n","                     np.float64(0.8479314932870069),\n","                     np.float64(0.8370951119289279),\n","                     np.float64(0.861027045554941),\n","                     np.float64(0.8628419814530734),\n","                     np.float64(0.8652258872197367),\n","                     np.float64(0.8696748430370026),\n","                     np.float64(0.8688144706295221),\n","                     np.float64(0.870568955819539),\n","                     np.float64(0.8797932430811638),\n","                     np.float64(0.8655103262442787),\n","                     np.float64(0.841489065897588),\n","                     np.float64(0.8688130512040222),\n","                     np.float64(0.868200222405096),\n","                     np.float64(0.8769797339414881),\n","                     np.float64(0.8828706251665726)],\n","                    [np.float64(0.771814470761393),\n","                     np.float64(0.8031532964499031),\n","                     np.float64(0.8343514859830592),\n","                     np.float64(0.8379018090683548),\n","                     np.float64(0.8288736819998341),\n","                     np.float64(0.82840617388442),\n","                     np.float64(0.8500239386425467),\n","                     np.float64(0.8488661248189707),\n","                     np.float64(0.843912236561481),\n","                     np.float64(0.8652887734801576),\n","                     np.float64(0.8621664595554746),\n","                     np.float64(0.8672162431981917),\n","                     np.float64(0.8551053952556315),\n","                     np.float64(0.8607330745970299),\n","                     np.float64(0.8615986311810447),\n","                     np.float64(0.8633317093898035),\n","                     np.float64(0.8669984511970865),\n","                     np.float64(0.859788052590594),\n","                     np.float64(0.8704949657293112),\n","                     np.float64(0.8767815863356858)]]}\n","Training Model: BiLSTM_Deep, Fold: 1\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7238 - loss: 0.7915(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8323\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 68ms/step - accuracy: 0.7240 - loss: 0.7912 - val_accuracy: 0.8906 - val_loss: 0.3090 - mcc: 0.8323\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8944 - loss: 0.3002(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8643\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 65ms/step - accuracy: 0.8944 - loss: 0.3002 - val_accuracy: 0.9113 - val_loss: 0.2450 - mcc: 0.8643\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9133 - loss: 0.2364(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8805\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9133 - loss: 0.2364 - val_accuracy: 0.9214 - val_loss: 0.2106 - mcc: 0.8805\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9188 - loss: 0.2145(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8913\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9188 - loss: 0.2145 - val_accuracy: 0.9287 - val_loss: 0.1862 - mcc: 0.8913\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9225 - loss: 0.2031(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8957\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9225 - loss: 0.2031 - val_accuracy: 0.9317 - val_loss: 0.1768 - mcc: 0.8957\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9287 - loss: 0.1848(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8980\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 64ms/step - accuracy: 0.9287 - loss: 0.1848 - val_accuracy: 0.9328 - val_loss: 0.1719 - mcc: 0.8980\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9251 - loss: 0.1950(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8953\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 66ms/step - accuracy: 0.9251 - loss: 0.1950 - val_accuracy: 0.9312 - val_loss: 0.1786 - mcc: 0.8953\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9295 - loss: 0.1827(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9009\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9295 - loss: 0.1827 - val_accuracy: 0.9349 - val_loss: 0.1673 - mcc: 0.9009\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9338 - loss: 0.1700(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9017\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9338 - loss: 0.1700 - val_accuracy: 0.9355 - val_loss: 0.1631 - mcc: 0.9017\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9354 - loss: 0.1636(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9034\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 65ms/step - accuracy: 0.9354 - loss: 0.1636 - val_accuracy: 0.9367 - val_loss: 0.1663 - mcc: 0.9034\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9361 - loss: 0.1615(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8925\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9361 - loss: 0.1615 - val_accuracy: 0.9293 - val_loss: 0.1797 - mcc: 0.8925\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9344 - loss: 0.1674(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9043\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9344 - loss: 0.1674 - val_accuracy: 0.9373 - val_loss: 0.1605 - mcc: 0.9043\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9382 - loss: 0.1557(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9088\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9382 - loss: 0.1557 - val_accuracy: 0.9402 - val_loss: 0.1515 - mcc: 0.9088\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9406 - loss: 0.1471(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9068\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9406 - loss: 0.1471 - val_accuracy: 0.9389 - val_loss: 0.1534 - mcc: 0.9068\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9398 - loss: 0.1529(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9023\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 66ms/step - accuracy: 0.9398 - loss: 0.1529 - val_accuracy: 0.9358 - val_loss: 0.1640 - mcc: 0.9023\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9373 - loss: 0.1575(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9120\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 68ms/step - accuracy: 0.9373 - loss: 0.1575 - val_accuracy: 0.9419 - val_loss: 0.1453 - mcc: 0.9120\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9410 - loss: 0.1479(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9025\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 65ms/step - accuracy: 0.9410 - loss: 0.1479 - val_accuracy: 0.9360 - val_loss: 0.1631 - mcc: 0.9025\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9410 - loss: 0.1471(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9140\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9410 - loss: 0.1470 - val_accuracy: 0.9435 - val_loss: 0.1413 - mcc: 0.9140\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9425 - loss: 0.1422(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9146\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9425 - loss: 0.1421 - val_accuracy: 0.9439 - val_loss: 0.1394 - mcc: 0.9146\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9450 - loss: 0.1355(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9127\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 66ms/step - accuracy: 0.9450 - loss: 0.1355 - val_accuracy: 0.9425 - val_loss: 0.1448 - mcc: 0.9127\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7260 - loss: 0.7711(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8565\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 66ms/step - accuracy: 0.7261 - loss: 0.7707 - val_accuracy: 0.9060 - val_loss: 0.2660 - mcc: 0.8565\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8986 - loss: 0.2869(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8663\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.8986 - loss: 0.2869 - val_accuracy: 0.9127 - val_loss: 0.2347 - mcc: 0.8663\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9125 - loss: 0.2379(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8784\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9125 - loss: 0.2378 - val_accuracy: 0.9207 - val_loss: 0.2077 - mcc: 0.8784\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9185 - loss: 0.2182(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8905\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9185 - loss: 0.2182 - val_accuracy: 0.9286 - val_loss: 0.1850 - mcc: 0.8905\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9215 - loss: 0.2075(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8932\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 62ms/step - accuracy: 0.9215 - loss: 0.2075 - val_accuracy: 0.9302 - val_loss: 0.1810 - mcc: 0.8932\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9288 - loss: 0.1849(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8997\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 64ms/step - accuracy: 0.9288 - loss: 0.1850 - val_accuracy: 0.9345 - val_loss: 0.1706 - mcc: 0.8997\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9273 - loss: 0.1894(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8944\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 68ms/step - accuracy: 0.9273 - loss: 0.1894 - val_accuracy: 0.9311 - val_loss: 0.1745 - mcc: 0.8944\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9314 - loss: 0.1737(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9031\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 66ms/step - accuracy: 0.9314 - loss: 0.1737 - val_accuracy: 0.9363 - val_loss: 0.1595 - mcc: 0.9031\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9344 - loss: 0.1674(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8989\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 65ms/step - accuracy: 0.9344 - loss: 0.1674 - val_accuracy: 0.9339 - val_loss: 0.1699 - mcc: 0.8989\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9351 - loss: 0.1659(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9023\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9351 - loss: 0.1659 - val_accuracy: 0.9361 - val_loss: 0.1608 - mcc: 0.9023\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9359 - loss: 0.1615(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9007\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 63ms/step - accuracy: 0.9359 - loss: 0.1615 - val_accuracy: 0.9351 - val_loss: 0.1628 - mcc: 0.9007\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9396 - loss: 0.1522(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8822\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9396 - loss: 0.1522 - val_accuracy: 0.9227 - val_loss: 0.2103 - mcc: 0.8822\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9351 - loss: 0.1665(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8978\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9351 - loss: 0.1666 - val_accuracy: 0.9330 - val_loss: 0.1713 - mcc: 0.8978\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9236 - loss: 0.2017(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9038\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9236 - loss: 0.2017 - val_accuracy: 0.9370 - val_loss: 0.1596 - mcc: 0.9038\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9389 - loss: 0.1545(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9063\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9389 - loss: 0.1545 - val_accuracy: 0.9386 - val_loss: 0.1539 - mcc: 0.9063\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9396 - loss: 0.1522(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9107\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 64ms/step - accuracy: 0.9396 - loss: 0.1522 - val_accuracy: 0.9414 - val_loss: 0.1459 - mcc: 0.9107\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9423 - loss: 0.1443(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9081\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 63ms/step - accuracy: 0.9423 - loss: 0.1443 - val_accuracy: 0.9400 - val_loss: 0.1509 - mcc: 0.9081\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9410 - loss: 0.1478(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9040\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - accuracy: 0.9410 - loss: 0.1478 - val_accuracy: 0.9371 - val_loss: 0.1591 - mcc: 0.9040\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9424 - loss: 0.1445(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9114\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9424 - loss: 0.1445 - val_accuracy: 0.9421 - val_loss: 0.1426 - mcc: 0.9114\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9408 - loss: 0.1472(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9121\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9408 - loss: 0.1472 - val_accuracy: 0.9422 - val_loss: 0.1421 - mcc: 0.9121\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7416 - loss: 0.7381(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8508\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 65ms/step - accuracy: 0.7419 - loss: 0.7374 - val_accuracy: 0.9019 - val_loss: 0.2705 - mcc: 0.8508\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8994 - loss: 0.2817(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8623\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.8994 - loss: 0.2817 - val_accuracy: 0.9098 - val_loss: 0.2455 - mcc: 0.8623\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9071 - loss: 0.2569(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8702\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 63ms/step - accuracy: 0.9071 - loss: 0.2569 - val_accuracy: 0.9150 - val_loss: 0.2325 - mcc: 0.8702\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9167 - loss: 0.2251(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8725\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 64ms/step - accuracy: 0.9167 - loss: 0.2251 - val_accuracy: 0.9163 - val_loss: 0.2249 - mcc: 0.8725\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9113 - loss: 0.2425(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8516\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 64ms/step - accuracy: 0.9113 - loss: 0.2425 - val_accuracy: 0.9024 - val_loss: 0.2641 - mcc: 0.8516\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9140 - loss: 0.2333(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8668\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9140 - loss: 0.2333 - val_accuracy: 0.9127 - val_loss: 0.2395 - mcc: 0.8668\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9165 - loss: 0.2249(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8875\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9165 - loss: 0.2249 - val_accuracy: 0.9259 - val_loss: 0.1933 - mcc: 0.8875\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9239 - loss: 0.2008(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.6957\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 64ms/step - accuracy: 0.9238 - loss: 0.2009 - val_accuracy: 0.8037 - val_loss: 0.5670 - mcc: 0.6957\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8894 - loss: 0.3034(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8680\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.8894 - loss: 0.3032 - val_accuracy: 0.9135 - val_loss: 0.2277 - mcc: 0.8680\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9216 - loss: 0.2075(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8879\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9216 - loss: 0.2075 - val_accuracy: 0.9265 - val_loss: 0.1907 - mcc: 0.8879\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9269 - loss: 0.1888(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8966\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - accuracy: 0.9269 - loss: 0.1888 - val_accuracy: 0.9322 - val_loss: 0.1751 - mcc: 0.8966\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9331 - loss: 0.1708(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8941\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 63ms/step - accuracy: 0.9331 - loss: 0.1708 - val_accuracy: 0.9304 - val_loss: 0.1794 - mcc: 0.8941\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9336 - loss: 0.1693(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9026\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9336 - loss: 0.1693 - val_accuracy: 0.9360 - val_loss: 0.1623 - mcc: 0.9026\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9352 - loss: 0.1636(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8968\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 68ms/step - accuracy: 0.9352 - loss: 0.1636 - val_accuracy: 0.9321 - val_loss: 0.1744 - mcc: 0.8968\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9329 - loss: 0.1729(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9020\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9329 - loss: 0.1729 - val_accuracy: 0.9353 - val_loss: 0.1631 - mcc: 0.9020\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9363 - loss: 0.1618(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9051\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9363 - loss: 0.1618 - val_accuracy: 0.9376 - val_loss: 0.1573 - mcc: 0.9051\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9385 - loss: 0.1552(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9091\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9385 - loss: 0.1552 - val_accuracy: 0.9404 - val_loss: 0.1496 - mcc: 0.9091\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9372 - loss: 0.1591(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9059\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 67ms/step - accuracy: 0.9372 - loss: 0.1591 - val_accuracy: 0.9379 - val_loss: 0.1565 - mcc: 0.9059\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9412 - loss: 0.1479(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9061\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 65ms/step - accuracy: 0.9412 - loss: 0.1479 - val_accuracy: 0.9379 - val_loss: 0.1584 - mcc: 0.9061\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9410 - loss: 0.1466(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8939\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9410 - loss: 0.1466 - val_accuracy: 0.9301 - val_loss: 0.1814 - mcc: 0.8939\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 4\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7329 - loss: 0.7495(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8416\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 66ms/step - accuracy: 0.7331 - loss: 0.7492 - val_accuracy: 0.8971 - val_loss: 0.2992 - mcc: 0.8416\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9038 - loss: 0.2699(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8604\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9038 - loss: 0.2699 - val_accuracy: 0.9086 - val_loss: 0.2405 - mcc: 0.8604\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9160 - loss: 0.2241(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8793\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 65ms/step - accuracy: 0.9160 - loss: 0.2241 - val_accuracy: 0.9209 - val_loss: 0.2082 - mcc: 0.8793\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9209 - loss: 0.2099(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8750\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 65ms/step - accuracy: 0.9209 - loss: 0.2099 - val_accuracy: 0.9184 - val_loss: 0.2139 - mcc: 0.8750\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9277 - loss: 0.1883(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8901\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9277 - loss: 0.1883 - val_accuracy: 0.9276 - val_loss: 0.1837 - mcc: 0.8901\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9305 - loss: 0.1805(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8867\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9305 - loss: 0.1805 - val_accuracy: 0.9257 - val_loss: 0.1895 - mcc: 0.8867\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9326 - loss: 0.1740(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8946\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9326 - loss: 0.1740 - val_accuracy: 0.9307 - val_loss: 0.1785 - mcc: 0.8946\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9311 - loss: 0.1760(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9008\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9311 - loss: 0.1760 - val_accuracy: 0.9352 - val_loss: 0.1647 - mcc: 0.9008\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9251 - loss: 0.1977(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8990\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9251 - loss: 0.1976 - val_accuracy: 0.9340 - val_loss: 0.1688 - mcc: 0.8990\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9376 - loss: 0.1579(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8993\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9376 - loss: 0.1579 - val_accuracy: 0.9342 - val_loss: 0.1671 - mcc: 0.8993\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9377 - loss: 0.1588(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9011\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9377 - loss: 0.1588 - val_accuracy: 0.9350 - val_loss: 0.1656 - mcc: 0.9011\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9384 - loss: 0.1557(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9064\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 65ms/step - accuracy: 0.9384 - loss: 0.1557 - val_accuracy: 0.9388 - val_loss: 0.1542 - mcc: 0.9064\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9394 - loss: 0.1526(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9059\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9394 - loss: 0.1526 - val_accuracy: 0.9384 - val_loss: 0.1542 - mcc: 0.9059\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9404 - loss: 0.1490(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9089\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 64ms/step - accuracy: 0.9404 - loss: 0.1490 - val_accuracy: 0.9404 - val_loss: 0.1497 - mcc: 0.9089\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9436 - loss: 0.1404(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9067\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9436 - loss: 0.1405 - val_accuracy: 0.9388 - val_loss: 0.1530 - mcc: 0.9067\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9421 - loss: 0.1457(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9102\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9421 - loss: 0.1457 - val_accuracy: 0.9411 - val_loss: 0.1499 - mcc: 0.9102\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9444 - loss: 0.1378(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9053\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 64ms/step - accuracy: 0.9444 - loss: 0.1378 - val_accuracy: 0.9379 - val_loss: 0.1544 - mcc: 0.9053\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9434 - loss: 0.1414(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9107\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.9434 - loss: 0.1414 - val_accuracy: 0.9415 - val_loss: 0.1458 - mcc: 0.9107\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9450 - loss: 0.1365(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9115\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 67ms/step - accuracy: 0.9450 - loss: 0.1365 - val_accuracy: 0.9420 - val_loss: 0.1461 - mcc: 0.9115\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9451 - loss: 0.1355(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9041\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9451 - loss: 0.1355 - val_accuracy: 0.9370 - val_loss: 0.1600 - mcc: 0.9041\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7281 - loss: 0.7837(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8440\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 67ms/step - accuracy: 0.7282 - loss: 0.7833 - val_accuracy: 0.8982 - val_loss: 0.2948 - mcc: 0.8440\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8993 - loss: 0.2857(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8548\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.8993 - loss: 0.2856 - val_accuracy: 0.9050 - val_loss: 0.2583 - mcc: 0.8548\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9042 - loss: 0.2662(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8193\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - accuracy: 0.9042 - loss: 0.2662 - val_accuracy: 0.8825 - val_loss: 0.3163 - mcc: 0.8193\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9143 - loss: 0.2296(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8794\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9143 - loss: 0.2296 - val_accuracy: 0.9210 - val_loss: 0.2087 - mcc: 0.8794\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8968 - loss: 0.2937(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8754\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.8968 - loss: 0.2937 - val_accuracy: 0.9180 - val_loss: 0.2195 - mcc: 0.8754\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9185 - loss: 0.2184(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8825\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9185 - loss: 0.2184 - val_accuracy: 0.9228 - val_loss: 0.2043 - mcc: 0.8825\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9279 - loss: 0.1898(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8809\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.9279 - loss: 0.1898 - val_accuracy: 0.9221 - val_loss: 0.2006 - mcc: 0.8809\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9257 - loss: 0.1962(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8887\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 65ms/step - accuracy: 0.9257 - loss: 0.1962 - val_accuracy: 0.9269 - val_loss: 0.1905 - mcc: 0.8887\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9311 - loss: 0.1783(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8941\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 67ms/step - accuracy: 0.9311 - loss: 0.1783 - val_accuracy: 0.9305 - val_loss: 0.1757 - mcc: 0.8941\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9331 - loss: 0.1725(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8927\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 66ms/step - accuracy: 0.9331 - loss: 0.1725 - val_accuracy: 0.9296 - val_loss: 0.1809 - mcc: 0.8927\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9332 - loss: 0.1700(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8732\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 67ms/step - accuracy: 0.9332 - loss: 0.1700 - val_accuracy: 0.9169 - val_loss: 0.2292 - mcc: 0.8732\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9337 - loss: 0.1702(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8956\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9337 - loss: 0.1702 - val_accuracy: 0.9313 - val_loss: 0.1746 - mcc: 0.8956\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9324 - loss: 0.1726(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8668\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - accuracy: 0.9324 - loss: 0.1727 - val_accuracy: 0.9129 - val_loss: 0.2317 - mcc: 0.8668\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9203 - loss: 0.2069(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8891\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.9203 - loss: 0.2069 - val_accuracy: 0.9273 - val_loss: 0.1880 - mcc: 0.8891\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9356 - loss: 0.1644(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8972\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 68ms/step - accuracy: 0.9356 - loss: 0.1644 - val_accuracy: 0.9326 - val_loss: 0.1701 - mcc: 0.8972\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9346 - loss: 0.1674(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8973\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9346 - loss: 0.1674 - val_accuracy: 0.9324 - val_loss: 0.1698 - mcc: 0.8973\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9392 - loss: 0.1541(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9007\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 64ms/step - accuracy: 0.9392 - loss: 0.1541 - val_accuracy: 0.9348 - val_loss: 0.1625 - mcc: 0.9007\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9379 - loss: 0.1583(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9024\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 64ms/step - accuracy: 0.9379 - loss: 0.1583 - val_accuracy: 0.9358 - val_loss: 0.1605 - mcc: 0.9024\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9309 - loss: 0.1789(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8945\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 65ms/step - accuracy: 0.9309 - loss: 0.1789 - val_accuracy: 0.9308 - val_loss: 0.1764 - mcc: 0.8945\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9393 - loss: 0.1518(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9016\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 66ms/step - accuracy: 0.9393 - loss: 0.1518 - val_accuracy: 0.9355 - val_loss: 0.1605 - mcc: 0.9016\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.9425313333333334),\n","              'mean': np.float64(0.9374629333333333),\n","              'min': np.float64(0.9300713333333334),\n","              'std': np.float64(0.004630260095406216)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.001075321594874064),\n","                               'mean': np.float64(0.0009070480187733968),\n","                               'min': np.float64(0.0008643518288930257),\n","                               'std': np.float64(8.413855745146769e-05)},\n"," 'MCC': {'max': np.float64(0.9127377771092331),\n","         'mean': np.float64(0.9048789022330572),\n","         'min': np.float64(0.8938761371874021),\n","         'std': np.float64(0.00701588429050789)},\n"," 'Parameters': 76133,\n"," 'Train Time (s)': {'max': np.float64(1440.656200170517),\n","                    'mean': np.float64(1369.2872632026672),\n","                    'min': np.float64(1276.2334032058716),\n","                    'std': np.float64(56.59567585246842)},\n"," 'Training Accuracy': [[0.8209025859832764,\n","                        0.8982231616973877,\n","                        0.9141436219215393,\n","                        0.919768214225769,\n","                        0.923740804195404,\n","                        0.9289419651031494,\n","                        0.9239829182624817,\n","                        0.9317176342010498,\n","                        0.932675302028656,\n","                        0.9356575608253479,\n","                        0.9365246295928955,\n","                        0.9350269436836243,\n","                        0.939679741859436,\n","                        0.9402595162391663,\n","                        0.9373200535774231,\n","                        0.9397729635238647,\n","                        0.9417442083358765,\n","                        0.9419188499450684,\n","                        0.9433234333992004,\n","                        0.9438781142234802],\n","                       [0.8314790725708008,\n","                        0.8998957276344299,\n","                        0.9138153195381165,\n","                        0.9201422333717346,\n","                        0.9199507832527161,\n","                        0.9274023175239563,\n","                        0.927860677242279,\n","                        0.9338723421096802,\n","                        0.9314059019088745,\n","                        0.9335097670555115,\n","                        0.9342295527458191,\n","                        0.9380027055740356,\n","                        0.9304250478744507,\n","                        0.9296823740005493,\n","                        0.9372433423995972,\n","                        0.9391021132469177,\n","                        0.9408905506134033,\n","                        0.941252589225769,\n","                        0.9424278140068054,\n","                        0.9417548775672913],\n","                       [0.8363850116729736,\n","                        0.9003980159759521,\n","                        0.906337559223175,\n","                        0.9155473709106445,\n","                        0.9155313968658447,\n","                        0.9098717570304871,\n","                        0.9174920916557312,\n","                        0.9112221598625183,\n","                        0.9063210487365723,\n","                        0.9243512153625488,\n","                        0.9281381964683533,\n","                        0.9329574108123779,\n","                        0.9324092864990234,\n","                        0.9359316229820251,\n","                        0.934557318687439,\n","                        0.937360942363739,\n","                        0.9374645352363586,\n","                        0.9392686486244202,\n","                        0.9402565360069275,\n","                        0.9410639405250549],\n","                       [0.8342664241790771,\n","                        0.9068764448165894,\n","                        0.9177762866020203,\n","                        0.9216516017913818,\n","                        0.9277521967887878,\n","                        0.9265304207801819,\n","                        0.9316604733467102,\n","                        0.9326287508010864,\n","                        0.9307013154029846,\n","                        0.9365510940551758,\n","                        0.9392225742340088,\n","                        0.9368000626564026,\n","                        0.9395735859870911,\n","                        0.9405361413955688,\n","                        0.9418058395385742,\n","                        0.9426412582397461,\n","                        0.9430257678031921,\n","                        0.9423733949661255,\n","                        0.9441991448402405,\n","                        0.9448826313018799],\n","                       [0.8268624544143677,\n","                        0.9046377539634705,\n","                        0.9045403003692627,\n","                        0.9194449782371521,\n","                        0.9020252823829651,\n","                        0.9219380021095276,\n","                        0.926351010799408,\n","                        0.9271329045295715,\n","                        0.9311652183532715,\n","                        0.9317079782485962,\n","                        0.9342809915542603,\n","                        0.9327881932258606,\n","                        0.9248918294906616,\n","                        0.9276179075241089,\n","                        0.9363366961479187,\n","                        0.9340951442718506,\n","                        0.9383442401885986,\n","                        0.9376786947250366,\n","                        0.9352053999900818,\n","                        0.9396800994873047]],\n"," 'Training Loss': [[0.522572934627533,\n","                    0.28718629479408264,\n","                    0.23201045393943787,\n","                    0.21165382862091064,\n","                    0.19973337650299072,\n","                    0.18338190019130707,\n","                    0.1983836442232132,\n","                    0.17605255544185638,\n","                    0.17220552265644073,\n","                    0.16297553479671478,\n","                    0.16054542362689972,\n","                    0.1647988110780716,\n","                    0.15110036730766296,\n","                    0.14840348064899445,\n","                    0.15841345489025116,\n","                    0.15072323381900787,\n","                    0.14512468874454498,\n","                    0.1443084180355072,\n","                    0.1400139033794403,\n","                    0.13847039639949799],\n","                   [0.4855189621448517,\n","                    0.28149083256721497,\n","                    0.23352764546871185,\n","                    0.2131384015083313,\n","                    0.2130325734615326,\n","                    0.19087691605091095,\n","                    0.18802376091480255,\n","                    0.16862726211547852,\n","                    0.1768290400505066,\n","                    0.17059758305549622,\n","                    0.1682075709104538,\n","                    0.15693072974681854,\n","                    0.18021069467067719,\n","                    0.18284541368484497,\n","                    0.1596224308013916,\n","                    0.15309172868728638,\n","                    0.14834046363830566,\n","                    0.14731529355049133,\n","                    0.1437450796365738,\n","                    0.14536309242248535],\n","                   [0.4705562889575958,\n","                    0.27988290786743164,\n","                    0.2601065933704376,\n","                    0.22930318117141724,\n","                    0.2281891405582428,\n","                    0.24681279063224792,\n","                    0.21988403797149658,\n","                    0.23973719775676727,\n","                    0.25211307406425476,\n","                    0.19835638999938965,\n","                    0.18678121268749237,\n","                    0.17098718881607056,\n","                    0.17384488880634308,\n","                    0.1624985635280609,\n","                    0.16709621250629425,\n","                    0.1580287516117096,\n","                    0.15761680901050568,\n","                    0.15237018465995789,\n","                    0.14936621487140656,\n","                    0.14742036163806915],\n","                   [0.4742702543735504,\n","                    0.25778982043266296,\n","                    0.21973735094070435,\n","                    0.2071247100830078,\n","                    0.18816831707954407,\n","                    0.19283661246299744,\n","                    0.1757274717092514,\n","                    0.17162646353244781,\n","                    0.17872479557991028,\n","                    0.16013987362384796,\n","                    0.15329888463020325,\n","                    0.1598227173089981,\n","                    0.1521587073802948,\n","                    0.14876052737236023,\n","                    0.14547307789325714,\n","                    0.14330105483531952,\n","                    0.14166617393493652,\n","                    0.14337384700775146,\n","                    0.13853436708450317,\n","                    0.1363331824541092],\n","                   [0.502577543258667,\n","                    0.2658371031284332,\n","                    0.26559528708457947,\n","                    0.2147146463394165,\n","                    0.2738197147846222,\n","                    0.20844122767448425,\n","                    0.19232095777988434,\n","                    0.1911747306585312,\n","                    0.17857766151428223,\n","                    0.17695580422878265,\n","                    0.1673169583082199,\n","                    0.1734270453453064,\n","                    0.19738152623176575,\n","                    0.18654745817184448,\n","                    0.1623246967792511,\n","                    0.16747421026229858,\n","                    0.1555691510438919,\n","                    0.1588493287563324,\n","                    0.16540087759494781,\n","                    0.15127070248126984]],\n"," 'Validation Accuracy': [[0.8906274437904358,\n","                          0.911310613155365,\n","                          0.9213777780532837,\n","                          0.9286783337593079,\n","                          0.9316745400428772,\n","                          0.9328135848045349,\n","                          0.9312490224838257,\n","                          0.9348540306091309,\n","                          0.9354776740074158,\n","                          0.9366726279258728,\n","                          0.9292913675308228,\n","                          0.9372844099998474,\n","                          0.9402127265930176,\n","                          0.9388961791992188,\n","                          0.9357900619506836,\n","                          0.9419155120849609,\n","                          0.9359502196311951,\n","                          0.9435322284698486,\n","                          0.94394451379776,\n","                          0.9425309300422668],\n","                         [0.9059906601905823,\n","                          0.9127428531646729,\n","                          0.9207014441490173,\n","                          0.9286426305770874,\n","                          0.9302185773849487,\n","                          0.9344784617424011,\n","                          0.9311482906341553,\n","                          0.9363188743591309,\n","                          0.9338613152503967,\n","                          0.9361216425895691,\n","                          0.9350839853286743,\n","                          0.9227432608604431,\n","                          0.932985782623291,\n","                          0.9370401501655579,\n","                          0.9385908246040344,\n","                          0.9413601756095886,\n","                          0.9399522542953491,\n","                          0.9370758533477783,\n","                          0.9420825242996216,\n","                          0.9422335028648376],\n","                         [0.9019249081611633,\n","                          0.9097645878791809,\n","                          0.9150083065032959,\n","                          0.9163459539413452,\n","                          0.9023520350456238,\n","                          0.9127299189567566,\n","                          0.9259213209152222,\n","                          0.8036579489707947,\n","                          0.9134936332702637,\n","                          0.9265037178993225,\n","                          0.9322007894515991,\n","                          0.930449366569519,\n","                          0.9359952211380005,\n","                          0.9320898652076721,\n","                          0.9353479146957397,\n","                          0.9376159906387329,\n","                          0.9403507709503174,\n","                          0.9379321336746216,\n","                          0.9378971457481384,\n","                          0.9300714731216431],\n","                         [0.8970931768417358,\n","                          0.9085512757301331,\n","                          0.920947253704071,\n","                          0.918415367603302,\n","                          0.9275639653205872,\n","                          0.9257391691207886,\n","                          0.9306852221488953,\n","                          0.9351763725280762,\n","                          0.9339802265167236,\n","                          0.9342311024665833,\n","                          0.9350345134735107,\n","                          0.9387911558151245,\n","                          0.9384304285049438,\n","                          0.9403842091560364,\n","                          0.9388059377670288,\n","                          0.9411478638648987,\n","                          0.9379320740699768,\n","                          0.9415227174758911,\n","                          0.9419713616371155,\n","                          0.936988353729248],\n","                         [0.8982263207435608,\n","                          0.9049842357635498,\n","                          0.8825392127037048,\n","                          0.9209685325622559,\n","                          0.9180179834365845,\n","                          0.9227614998817444,\n","                          0.9220735430717468,\n","                          0.9269440770149231,\n","                          0.930540919303894,\n","                          0.9296071529388428,\n","                          0.9169286489486694,\n","                          0.9313321709632874,\n","                          0.9129119515419006,\n","                          0.9273269176483154,\n","                          0.9325972199440002,\n","                          0.9323525428771973,\n","                          0.9348199963569641,\n","                          0.935828685760498,\n","                          0.9307560324668884,\n","                          0.9354907870292664]],\n"," 'Validation Loss': [[0.3089698553085327,\n","                      0.24499833583831787,\n","                      0.2105613648891449,\n","                      0.18617792427539825,\n","                      0.17679579555988312,\n","                      0.17194336652755737,\n","                      0.1786460131406784,\n","                      0.16730782389640808,\n","                      0.16314472258090973,\n","                      0.16628047823905945,\n","                      0.17968565225601196,\n","                      0.16047035157680511,\n","                      0.15145385265350342,\n","                      0.15340273082256317,\n","                      0.16399268805980682,\n","                      0.14533348381519318,\n","                      0.1630583554506302,\n","                      0.14126504957675934,\n","                      0.13938899338245392,\n","                      0.14475953578948975],\n","                     [0.26603737473487854,\n","                      0.2347063422203064,\n","                      0.20768843591213226,\n","                      0.18499545753002167,\n","                      0.18100738525390625,\n","                      0.1705572009086609,\n","                      0.1745002716779709,\n","                      0.15954440832138062,\n","                      0.16990450024604797,\n","                      0.1607522815465927,\n","                      0.162776380777359,\n","                      0.2102777510881424,\n","                      0.17126330733299255,\n","                      0.15955404937267303,\n","                      0.15390877425670624,\n","                      0.14592309296131134,\n","                      0.15086738765239716,\n","                      0.15911276638507843,\n","                      0.142570361495018,\n","                      0.14206378161907196],\n","                     [0.2704922556877136,\n","                      0.24553704261779785,\n","                      0.23245856165885925,\n","                      0.22485534846782684,\n","                      0.26414018869400024,\n","                      0.23946915566921234,\n","                      0.19331154227256775,\n","                      0.5670175552368164,\n","                      0.227697491645813,\n","                      0.19065973162651062,\n","                      0.17510215938091278,\n","                      0.17942459881305695,\n","                      0.16234539449214935,\n","                      0.174355611205101,\n","                      0.16312555968761444,\n","                      0.15732525289058685,\n","                      0.14958584308624268,\n","                      0.15648788213729858,\n","                      0.15835967659950256,\n","                      0.18136537075042725],\n","                     [0.299224317073822,\n","                      0.24050013720989227,\n","                      0.20816108584403992,\n","                      0.21385742723941803,\n","                      0.1836639791727066,\n","                      0.18954695761203766,\n","                      0.17850829660892487,\n","                      0.1647288203239441,\n","                      0.16877973079681396,\n","                      0.1671200394630432,\n","                      0.16559983789920807,\n","                      0.15422415733337402,\n","                      0.1542268991470337,\n","                      0.1496637910604477,\n","                      0.15300080180168152,\n","                      0.14990577101707458,\n","                      0.15443706512451172,\n","                      0.1458306759595871,\n","                      0.14605975151062012,\n","                      0.1599552035331726],\n","                     [0.2947971522808075,\n","                      0.2583097815513611,\n","                      0.3162534832954407,\n","                      0.20874719321727753,\n","                      0.2194540947675705,\n","                      0.2042766511440277,\n","                      0.2006010264158249,\n","                      0.19045406579971313,\n","                      0.1756569892168045,\n","                      0.1809203028678894,\n","                      0.22916410863399506,\n","                      0.17455479502677917,\n","                      0.23173072934150696,\n","                      0.18796087801456451,\n","                      0.17014384269714355,\n","                      0.16979371011257172,\n","                      0.16246159374713898,\n","                      0.16050565242767334,\n","                      0.17638826370239258,\n","                      0.1604665219783783]],\n"," 'Validation MCC': [[np.float64(0.8322903256905471),\n","                     np.float64(0.8642755953060121),\n","                     np.float64(0.8805454193770398),\n","                     np.float64(0.8912595781842557),\n","                     np.float64(0.8956812939936515),\n","                     np.float64(0.8979500370373834),\n","                     np.float64(0.8953027303476373),\n","                     np.float64(0.900853622673985),\n","                     np.float64(0.9017074288742869),\n","                     np.float64(0.9034085528269943),\n","                     np.float64(0.892548259513861),\n","                     np.float64(0.9043021361380527),\n","                     np.float64(0.9088136193540972),\n","                     np.float64(0.9067681774053968),\n","                     np.float64(0.9022530829602522),\n","                     np.float64(0.912020167368151),\n","                     np.float64(0.9024998678991192),\n","                     np.float64(0.9140439159492163),\n","                     np.float64(0.9146433051523889),\n","                     np.float64(0.9127377771092331)],\n","                    [np.float64(0.8564810810453802),\n","                     np.float64(0.8663237067196571),\n","                     np.float64(0.8783939162122044),\n","                     np.float64(0.8905083257726616),\n","                     np.float64(0.8932339773620391),\n","                     np.float64(0.8996506057494399),\n","                     np.float64(0.8944364791200489),\n","                     np.float64(0.9031400415116184),\n","                     np.float64(0.8988748527057798),\n","                     np.float64(0.902273714463781),\n","                     np.float64(0.9006516816728142),\n","                     np.float64(0.8821500593913281),\n","                     np.float64(0.8977682945979516),\n","                     np.float64(0.9038491912824126),\n","                     np.float64(0.9062898397563172),\n","                     np.float64(0.9106584727842307),\n","                     np.float64(0.9081485302269162),\n","                     np.float64(0.9040441800546505),\n","                     np.float64(0.9113617914320363),\n","                     np.float64(0.9120887440202972)],\n","                    [np.float64(0.8508434086610654),\n","                     np.float64(0.8622982745539449),\n","                     np.float64(0.8702044584738355),\n","                     np.float64(0.8725177219468058),\n","                     np.float64(0.8515776650561354),\n","                     np.float64(0.8667879893751781),\n","                     np.float64(0.8875077279952354),\n","                     np.float64(0.6956838451891328),\n","                     np.float64(0.8679821056796293),\n","                     np.float64(0.8879341507956092),\n","                     np.float64(0.896561354918545),\n","                     np.float64(0.8940635587113568),\n","                     np.float64(0.9025935167165806),\n","                     np.float64(0.8967507027457279),\n","                     np.float64(0.9019748979275357),\n","                     np.float64(0.9050531517088714),\n","                     np.float64(0.9091463498274468),\n","                     np.float64(0.905928290089207),\n","                     np.float64(0.9060727942509902),\n","                     np.float64(0.8938761371874021)],\n","                    [np.float64(0.8415797901811475),\n","                     np.float64(0.860434501042927),\n","                     np.float64(0.8792528456537508),\n","                     np.float64(0.8749779494424197),\n","                     np.float64(0.8901433190128138),\n","                     np.float64(0.8867232016367124),\n","                     np.float64(0.8945790253366809),\n","                     np.float64(0.9007724869480649),\n","                     np.float64(0.8989855747696859),\n","                     np.float64(0.8993346540479016),\n","                     np.float64(0.901085874083101),\n","                     np.float64(0.9064168170445033),\n","                     np.float64(0.9058850017944046),\n","                     np.float64(0.9089171564633521),\n","                     np.float64(0.9066556418237552),\n","                     np.float64(0.9101998435499559),\n","                     np.float64(0.9053469643846455),\n","                     np.float64(0.9107185343517646),\n","                     np.float64(0.9115196267113127),\n","                     np.float64(0.9040793381647731)],\n","                    [np.float64(0.844044917206871),\n","                     np.float64(0.8548363713843148),\n","                     np.float64(0.8193045716437938),\n","                     np.float64(0.8793763155079954),\n","                     np.float64(0.8753750522283681),\n","                     np.float64(0.8824973723278805),\n","                     np.float64(0.8809133909014917),\n","                     np.float64(0.888717314629684),\n","                     np.float64(0.8941437927837721),\n","                     np.float64(0.8926525050311797),\n","                     np.float64(0.8732280650954861),\n","                     np.float64(0.895633898306357),\n","                     np.float64(0.8668452996278659),\n","                     np.float64(0.8890697448217388),\n","                     np.float64(0.8972480789257524),\n","                     np.float64(0.8972614120924314),\n","                     np.float64(0.9007130727541318),\n","                     np.float64(0.9024174604945394),\n","                     np.float64(0.8945437085539553),\n","                     np.float64(0.9016125146835805)]]}\n"]}]},{"cell_type":"markdown","source":["Hilbert Amps"],"metadata":{"id":"ghjNC4Lgnqej"}},{"cell_type":"code","source":["\n","basePath = \"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Model Comparisons/LSTM Models/New/\"\n","\n","\n","hilbert_multi_results, trained_models = train_and_evaluate(simple_models_dict, X=hilbert_data_vec, y=label_multi, epochs=n_epochs, dir_name=\"hilbert_multi\", basePath=basePath)\n","\n","\n","filePath = f\"{basePath}/20_Epoch_Hilbert_Multi_Model_Results.json\"\n","\n","with open(filePath, 'w') as f:\n","        json.dump(hilbert_multi_results, f, indent=4)  # indent=4 for pretty formatting"],"metadata":{"id":"H9MP7Og0qBIX","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744754420840,"user_tz":-120,"elapsed":21595220,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"afd29c2a-5514-4fc8-f6c1-5802eff66f4a"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6891 - loss: 0.8477(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7294\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - accuracy: 0.6893 - loss: 0.8472 - val_accuracy: 0.8226 - val_loss: 0.4704 - mcc: 0.7294\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8246 - loss: 0.4726(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7736\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.8246 - loss: 0.4725 - val_accuracy: 0.8528 - val_loss: 0.3845 - mcc: 0.7736\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8478 - loss: 0.4054(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7375\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.8478 - loss: 0.4054 - val_accuracy: 0.8301 - val_loss: 0.5058 - mcc: 0.7375\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8575 - loss: 0.3823(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7965\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8575 - loss: 0.3822 - val_accuracy: 0.8674 - val_loss: 0.3453 - mcc: 0.7965\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8605 - loss: 0.3706(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8011\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8605 - loss: 0.3706 - val_accuracy: 0.8711 - val_loss: 0.3410 - mcc: 0.8011\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8691 - loss: 0.3460(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8054\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8690 - loss: 0.3460 - val_accuracy: 0.8737 - val_loss: 0.3339 - mcc: 0.8054\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8698 - loss: 0.3389(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7954\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8698 - loss: 0.3389 - val_accuracy: 0.8672 - val_loss: 0.3568 - mcc: 0.7954\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8650 - loss: 0.3588(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8167\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8651 - loss: 0.3587 - val_accuracy: 0.8805 - val_loss: 0.3059 - mcc: 0.8167\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8792 - loss: 0.3124(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8211\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8792 - loss: 0.3124 - val_accuracy: 0.8832 - val_loss: 0.3001 - mcc: 0.8211\n","Epoch 10/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8802 - loss: 0.3060(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8202\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8802 - loss: 0.3060 - val_accuracy: 0.8830 - val_loss: 0.2974 - mcc: 0.8202\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8816 - loss: 0.3026(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8203\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8816 - loss: 0.3026 - val_accuracy: 0.8825 - val_loss: 0.3000 - mcc: 0.8203\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8828 - loss: 0.2989(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8122\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8828 - loss: 0.2989 - val_accuracy: 0.8766 - val_loss: 0.3220 - mcc: 0.8122\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8668 - loss: 0.3593(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8177\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8668 - loss: 0.3593 - val_accuracy: 0.8813 - val_loss: 0.3028 - mcc: 0.8177\n","Epoch 14/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8766 - loss: 0.3150(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7670\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8766 - loss: 0.3152 - val_accuracy: 0.8492 - val_loss: 0.4081 - mcc: 0.7670\n","Epoch 15/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8600 - loss: 0.3757(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8073\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8600 - loss: 0.3757 - val_accuracy: 0.8748 - val_loss: 0.3269 - mcc: 0.8073\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8743 - loss: 0.3257(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8088\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8743 - loss: 0.3257 - val_accuracy: 0.8758 - val_loss: 0.3302 - mcc: 0.8088\n","Epoch 17/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8769 - loss: 0.3192(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8195\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8770 - loss: 0.3192 - val_accuracy: 0.8823 - val_loss: 0.3028 - mcc: 0.8195\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8755 - loss: 0.3215(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8193\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8755 - loss: 0.3215 - val_accuracy: 0.8824 - val_loss: 0.3044 - mcc: 0.8193\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8822 - loss: 0.3023(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.7941\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8822 - loss: 0.3023 - val_accuracy: 0.8667 - val_loss: 0.3514 - mcc: 0.7941\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8805 - loss: 0.3078(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8263\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8805 - loss: 0.3077 - val_accuracy: 0.8868 - val_loss: 0.2875 - mcc: 0.8263\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 2\n","Epoch 1/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6604 - loss: 0.9228(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7020\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.6608 - loss: 0.9216 - val_accuracy: 0.8091 - val_loss: 0.5070 - mcc: 0.7020\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8209 - loss: 0.4768(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7583\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8209 - loss: 0.4767 - val_accuracy: 0.8436 - val_loss: 0.4085 - mcc: 0.7583\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8381 - loss: 0.4338(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7627\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - accuracy: 0.8381 - loss: 0.4337 - val_accuracy: 0.8470 - val_loss: 0.4171 - mcc: 0.7627\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8513 - loss: 0.3974(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7805\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8513 - loss: 0.3974 - val_accuracy: 0.8580 - val_loss: 0.3744 - mcc: 0.7805\n","Epoch 5/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8596 - loss: 0.3689(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7779\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8596 - loss: 0.3689 - val_accuracy: 0.8572 - val_loss: 0.3811 - mcc: 0.7779\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8631 - loss: 0.3605(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8030\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8631 - loss: 0.3605 - val_accuracy: 0.8723 - val_loss: 0.3328 - mcc: 0.8030\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8617 - loss: 0.3697(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.6604\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8617 - loss: 0.3699 - val_accuracy: 0.7848 - val_loss: 0.6237 - mcc: 0.6604\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8116 - loss: 0.5455(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7565\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.8116 - loss: 0.5453 - val_accuracy: 0.8433 - val_loss: 0.4275 - mcc: 0.7565\n","Epoch 9/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8416 - loss: 0.4307(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7700\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - accuracy: 0.8416 - loss: 0.4307 - val_accuracy: 0.8520 - val_loss: 0.4141 - mcc: 0.7700\n","Epoch 10/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8509 - loss: 0.4079(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7862\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.8509 - loss: 0.4078 - val_accuracy: 0.8617 - val_loss: 0.3689 - mcc: 0.7862\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8608 - loss: 0.3669(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8002\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8608 - loss: 0.3669 - val_accuracy: 0.8706 - val_loss: 0.3372 - mcc: 0.8002\n","Epoch 12/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8688 - loss: 0.3418(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8038\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8688 - loss: 0.3418 - val_accuracy: 0.8730 - val_loss: 0.3296 - mcc: 0.8038\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8737 - loss: 0.3260(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8083\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8737 - loss: 0.3260 - val_accuracy: 0.8756 - val_loss: 0.3217 - mcc: 0.8083\n","Epoch 14/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8717 - loss: 0.3375(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8104\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8717 - loss: 0.3375 - val_accuracy: 0.8771 - val_loss: 0.3183 - mcc: 0.8104\n","Epoch 15/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8716 - loss: 0.3331(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8118\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8716 - loss: 0.3331 - val_accuracy: 0.8781 - val_loss: 0.3154 - mcc: 0.8118\n","Epoch 16/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8774 - loss: 0.3149(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8116\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8774 - loss: 0.3149 - val_accuracy: 0.8778 - val_loss: 0.3144 - mcc: 0.8116\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8794 - loss: 0.3105(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8191\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8794 - loss: 0.3105 - val_accuracy: 0.8826 - val_loss: 0.3114 - mcc: 0.8191\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8754 - loss: 0.3244(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8201\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8754 - loss: 0.3244 - val_accuracy: 0.8830 - val_loss: 0.3011 - mcc: 0.8201\n","Epoch 19/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8806 - loss: 0.3056(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8241\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8806 - loss: 0.3056 - val_accuracy: 0.8859 - val_loss: 0.2926 - mcc: 0.8241\n","Epoch 20/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8802 - loss: 0.3048(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8260\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8802 - loss: 0.3047 - val_accuracy: 0.8871 - val_loss: 0.2903 - mcc: 0.8260\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 3\n","Epoch 1/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6965 - loss: 0.8270(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7257\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.6968 - loss: 0.8261 - val_accuracy: 0.8231 - val_loss: 0.4703 - mcc: 0.7257\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8343 - loss: 0.4387(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7703\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8343 - loss: 0.4386 - val_accuracy: 0.8512 - val_loss: 0.3978 - mcc: 0.7703\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8490 - loss: 0.4028(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7597\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8490 - loss: 0.4028 - val_accuracy: 0.8448 - val_loss: 0.4404 - mcc: 0.7597\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8546 - loss: 0.3955(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7865\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8546 - loss: 0.3955 - val_accuracy: 0.8612 - val_loss: 0.3659 - mcc: 0.7865\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8606 - loss: 0.3804(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7942\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 17ms/step - accuracy: 0.8606 - loss: 0.3804 - val_accuracy: 0.8666 - val_loss: 0.3540 - mcc: 0.7942\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8704 - loss: 0.3429(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7791\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.8704 - loss: 0.3429 - val_accuracy: 0.8563 - val_loss: 0.4082 - mcc: 0.7791\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8679 - loss: 0.3515(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8085\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.8679 - loss: 0.3515 - val_accuracy: 0.8751 - val_loss: 0.3255 - mcc: 0.8085\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8733 - loss: 0.3350(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8050\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.8733 - loss: 0.3350 - val_accuracy: 0.8726 - val_loss: 0.3320 - mcc: 0.8050\n","Epoch 9/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8769 - loss: 0.3196(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7957\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8769 - loss: 0.3196 - val_accuracy: 0.8675 - val_loss: 0.3689 - mcc: 0.7957\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8740 - loss: 0.3331(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7983\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8740 - loss: 0.3331 - val_accuracy: 0.8690 - val_loss: 0.3523 - mcc: 0.7983\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8718 - loss: 0.3395(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.7994\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8718 - loss: 0.3396 - val_accuracy: 0.8699 - val_loss: 0.3486 - mcc: 0.7994\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8744 - loss: 0.3319(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8151\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8744 - loss: 0.3319 - val_accuracy: 0.8792 - val_loss: 0.3093 - mcc: 0.8151\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8688 - loss: 0.3449(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7958\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8688 - loss: 0.3451 - val_accuracy: 0.8669 - val_loss: 0.3528 - mcc: 0.7958\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8702 - loss: 0.3446(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7884\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8702 - loss: 0.3446 - val_accuracy: 0.8625 - val_loss: 0.3823 - mcc: 0.7884\n","Epoch 15/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8629 - loss: 0.3690(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8054\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8629 - loss: 0.3689 - val_accuracy: 0.8732 - val_loss: 0.3305 - mcc: 0.8054\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8754 - loss: 0.3258(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8157\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.8754 - loss: 0.3257 - val_accuracy: 0.8798 - val_loss: 0.3098 - mcc: 0.8157\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8797 - loss: 0.3113(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8176\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - accuracy: 0.8797 - loss: 0.3113 - val_accuracy: 0.8812 - val_loss: 0.3057 - mcc: 0.8176\n","Epoch 18/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8841 - loss: 0.2975(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.7951\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8841 - loss: 0.2976 - val_accuracy: 0.8669 - val_loss: 0.3471 - mcc: 0.7951\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8750 - loss: 0.3231(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8192\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8750 - loss: 0.3231 - val_accuracy: 0.8820 - val_loss: 0.2979 - mcc: 0.8192\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8848 - loss: 0.2915(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8073\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8848 - loss: 0.2915 - val_accuracy: 0.8749 - val_loss: 0.3225 - mcc: 0.8073\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 4\n","Epoch 1/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6931 - loss: 0.8516(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7025\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.6934 - loss: 0.8509 - val_accuracy: 0.8085 - val_loss: 0.5248 - mcc: 0.7025\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8178 - loss: 0.4813(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7597\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8178 - loss: 0.4813 - val_accuracy: 0.8448 - val_loss: 0.4099 - mcc: 0.7597\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8467 - loss: 0.4083(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7736\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8467 - loss: 0.4082 - val_accuracy: 0.8535 - val_loss: 0.3880 - mcc: 0.7736\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8586 - loss: 0.3752(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7784\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8586 - loss: 0.3753 - val_accuracy: 0.8569 - val_loss: 0.3854 - mcc: 0.7784\n","Epoch 5/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8595 - loss: 0.3731(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7938\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8595 - loss: 0.3730 - val_accuracy: 0.8667 - val_loss: 0.3526 - mcc: 0.7938\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8656 - loss: 0.3524(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7934\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8656 - loss: 0.3524 - val_accuracy: 0.8662 - val_loss: 0.3562 - mcc: 0.7934\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8693 - loss: 0.3447(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8032\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8693 - loss: 0.3447 - val_accuracy: 0.8728 - val_loss: 0.3328 - mcc: 0.8032\n","Epoch 8/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8743 - loss: 0.3296(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8056\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8743 - loss: 0.3296 - val_accuracy: 0.8740 - val_loss: 0.3287 - mcc: 0.8056\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8714 - loss: 0.3383(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8054\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.8714 - loss: 0.3383 - val_accuracy: 0.8741 - val_loss: 0.3295 - mcc: 0.8054\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8741 - loss: 0.3327(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8067\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.8741 - loss: 0.3327 - val_accuracy: 0.8750 - val_loss: 0.3277 - mcc: 0.8067\n","Epoch 11/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8747 - loss: 0.3264(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8108\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8747 - loss: 0.3264 - val_accuracy: 0.8770 - val_loss: 0.3166 - mcc: 0.8108\n","Epoch 12/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8797 - loss: 0.3133(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8146\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8797 - loss: 0.3133 - val_accuracy: 0.8799 - val_loss: 0.3116 - mcc: 0.8146\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8793 - loss: 0.3103(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8191\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8793 - loss: 0.3102 - val_accuracy: 0.8826 - val_loss: 0.3020 - mcc: 0.8191\n","Epoch 14/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8797 - loss: 0.3110(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7829\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8797 - loss: 0.3111 - val_accuracy: 0.8602 - val_loss: 0.3702 - mcc: 0.7829\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8754 - loss: 0.3240(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8166\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8754 - loss: 0.3239 - val_accuracy: 0.8807 - val_loss: 0.3047 - mcc: 0.8166\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8831 - loss: 0.2992(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8200\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8831 - loss: 0.2992 - val_accuracy: 0.8832 - val_loss: 0.2987 - mcc: 0.8200\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8845 - loss: 0.2940(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8146\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8845 - loss: 0.2940 - val_accuracy: 0.8795 - val_loss: 0.3090 - mcc: 0.8146\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8859 - loss: 0.2893(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8236\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8859 - loss: 0.2893 - val_accuracy: 0.8855 - val_loss: 0.2930 - mcc: 0.8236\n","Epoch 19/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8868 - loss: 0.2883(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8059\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8868 - loss: 0.2883 - val_accuracy: 0.8743 - val_loss: 0.3289 - mcc: 0.8059\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8820 - loss: 0.3020(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8242\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8821 - loss: 0.3020 - val_accuracy: 0.8852 - val_loss: 0.2918 - mcc: 0.8242\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6615 - loss: 0.9079(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.6922\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.6616 - loss: 0.9077 - val_accuracy: 0.8009 - val_loss: 0.5449 - mcc: 0.6922\n","Epoch 2/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8156 - loss: 0.4993(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7520\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8156 - loss: 0.4991 - val_accuracy: 0.8394 - val_loss: 0.4312 - mcc: 0.7520\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8481 - loss: 0.4087(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7811\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8481 - loss: 0.4087 - val_accuracy: 0.8578 - val_loss: 0.3712 - mcc: 0.7811\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8582 - loss: 0.3855(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7936\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8583 - loss: 0.3854 - val_accuracy: 0.8659 - val_loss: 0.3526 - mcc: 0.7936\n","Epoch 5/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8695 - loss: 0.3423(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8011\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8695 - loss: 0.3423 - val_accuracy: 0.8701 - val_loss: 0.3366 - mcc: 0.8011\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8711 - loss: 0.3425(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8015\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 18ms/step - accuracy: 0.8711 - loss: 0.3425 - val_accuracy: 0.8703 - val_loss: 0.3393 - mcc: 0.8015\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8736 - loss: 0.3326(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8024\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - accuracy: 0.8736 - loss: 0.3326 - val_accuracy: 0.8715 - val_loss: 0.3355 - mcc: 0.8024\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8772 - loss: 0.3179(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8772 - loss: 0.3179 - val_accuracy: 0.8761 - val_loss: 0.3183 - mcc: 0.8100\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8744 - loss: 0.3326(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8130\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8744 - loss: 0.3326 - val_accuracy: 0.8780 - val_loss: 0.3131 - mcc: 0.8130\n","Epoch 10/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8809 - loss: 0.3072(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8148\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8809 - loss: 0.3072 - val_accuracy: 0.8797 - val_loss: 0.3071 - mcc: 0.8148\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8804 - loss: 0.3063(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8183\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8804 - loss: 0.3063 - val_accuracy: 0.8814 - val_loss: 0.3007 - mcc: 0.8183\n","Epoch 12/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8825 - loss: 0.3002(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8165\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.8825 - loss: 0.3002 - val_accuracy: 0.8801 - val_loss: 0.3021 - mcc: 0.8165\n","Epoch 13/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8846 - loss: 0.2944(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8206\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8846 - loss: 0.2944 - val_accuracy: 0.8827 - val_loss: 0.2952 - mcc: 0.8206\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8862 - loss: 0.2890(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8214\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8862 - loss: 0.2890 - val_accuracy: 0.8833 - val_loss: 0.2947 - mcc: 0.8214\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8867 - loss: 0.2867(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8240\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8867 - loss: 0.2867 - val_accuracy: 0.8851 - val_loss: 0.2887 - mcc: 0.8240\n","Epoch 16/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8882 - loss: 0.2838(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8255\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8882 - loss: 0.2838 - val_accuracy: 0.8864 - val_loss: 0.2857 - mcc: 0.8255\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8885 - loss: 0.2816(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8258\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8885 - loss: 0.2816 - val_accuracy: 0.8862 - val_loss: 0.2850 - mcc: 0.8258\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8901 - loss: 0.2764(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8207\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8901 - loss: 0.2764 - val_accuracy: 0.8833 - val_loss: 0.2939 - mcc: 0.8207\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8916 - loss: 0.2732(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8266\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8916 - loss: 0.2732 - val_accuracy: 0.8870 - val_loss: 0.2831 - mcc: 0.8266\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8904 - loss: 0.2753(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8196\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8904 - loss: 0.2753 - val_accuracy: 0.8822 - val_loss: 0.2961 - mcc: 0.8196\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.887134),\n","              'mean': np.float64(0.8832538666666666),\n","              'min': np.float64(0.874882),\n","              'std': np.float64(0.004541042002790862)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.00044528802235921226),\n","                               'mean': np.float64(0.0003808409531911214),\n","                               'min': np.float64(0.0002843348185221354),\n","                               'std': np.float64(7.715432592893189e-05)},\n"," 'MCC': {'max': np.float64(0.8263289199979857),\n","         'mean': np.float64(0.8206838304161035),\n","         'min': np.float64(0.8073259187826336),\n","         'std': np.float64(0.007099002304622331)},\n"," 'Parameters': 5029,\n"," 'Train Time (s)': {'max': np.float64(359.75003457069397),\n","                    'mean': np.float64(350.96020884513854),\n","                    'min': np.float64(332.6571674346924),\n","                    'std': np.float64(9.44641647008758)},\n"," 'Training Accuracy': [[0.7609913945198059,\n","                        0.835236668586731,\n","                        0.8505269885063171,\n","                        0.8590725064277649,\n","                        0.8623451590538025,\n","                        0.8650301098823547,\n","                        0.8665668368339539,\n","                        0.8712642192840576,\n","                        0.8774763941764832,\n","                        0.8792511224746704,\n","                        0.8815045356750488,\n","                        0.8828247785568237,\n","                        0.8680371642112732,\n","                        0.8688252568244934,\n","                        0.8645342588424683,\n","                        0.8750035762786865,\n","                        0.8782879114151001,\n","                        0.8773857951164246,\n","                        0.8797513842582703,\n","                        0.8824633955955505],\n","                       [0.7462306022644043,\n","                        0.8263674378395081,\n","                        0.839979887008667,\n","                        0.8530970811843872,\n","                        0.8605733513832092,\n","                        0.863637387752533,\n","                        0.8459296822547913,\n","                        0.8245227336883545,\n","                        0.8438657522201538,\n","                        0.8544031381607056,\n","                        0.8635662198066711,\n","                        0.8675784468650818,\n","                        0.872809648513794,\n","                        0.8727258443832397,\n","                        0.8715824484825134,\n","                        0.8776697516441345,\n","                        0.8783004283905029,\n","                        0.8770573735237122,\n","                        0.8809106349945068,\n","                        0.8830581903457642],\n","                       [0.7697611451148987,\n","                        0.8395989537239075,\n","                        0.8514094352722168,\n","                        0.8578540086746216,\n","                        0.8613015413284302,\n","                        0.8682317137718201,\n","                        0.8711302876472473,\n","                        0.8713962435722351,\n","                        0.8764289021492004,\n","                        0.870736837387085,\n","                        0.8700141310691833,\n","                        0.8763086795806885,\n","                        0.8585271835327148,\n","                        0.8731861710548401,\n","                        0.8665552735328674,\n","                        0.8781883120536804,\n","                        0.8777439594268799,\n","                        0.8774693012237549,\n","                        0.8782544136047363,\n","                        0.8819063305854797],\n","                       [0.7561964392662048,\n","                        0.8285589218139648,\n","                        0.8507461547851562,\n","                        0.8543375134468079,\n","                        0.8624670505523682,\n","                        0.8675389289855957,\n","                        0.8700218200683594,\n","                        0.8723214268684387,\n","                        0.8717157244682312,\n","                        0.8741395473480225,\n","                        0.8755139708518982,\n","                        0.8779526352882385,\n","                        0.8798488974571228,\n","                        0.8737635016441345,\n","                        0.8782960772514343,\n","                        0.8832880258560181,\n","                        0.8847750425338745,\n","                        0.8846805095672607,\n","                        0.8825747966766357,\n","                        0.8851140737533569],\n","                       [0.732488751411438,\n","                        0.8267686367034912,\n","                        0.8531318306922913,\n","                        0.8607873320579529,\n","                        0.8697569370269775,\n","                        0.8697846531867981,\n","                        0.8743943572044373,\n","                        0.8774217963218689,\n","                        0.8766396641731262,\n","                        0.8817595839500427,\n","                        0.8826252222061157,\n","                        0.8841100335121155,\n","                        0.8856103420257568,\n","                        0.8862606883049011,\n","                        0.887000560760498,\n","                        0.8881089091300964,\n","                        0.888731837272644,\n","                        0.8893860578536987,\n","                        0.8907163739204407,\n","                        0.8904944062232971]],\n"," 'Training Loss': [[0.6358586549758911,\n","                    0.43890494108200073,\n","                    0.4010988473892212,\n","                    0.3761523365974426,\n","                    0.3639576733112335,\n","                    0.3589484393596649,\n","                    0.3512897193431854,\n","                    0.336934894323349,\n","                    0.3157199025154114,\n","                    0.30906638503074646,\n","                    0.3012600839138031,\n","                    0.2982649803161621,\n","                    0.35039782524108887,\n","                    0.3443630337715149,\n","                    0.35845133662223816,\n","                    0.32361721992492676,\n","                    0.3139977753162384,\n","                    0.31675592064857483,\n","                    0.3099197447299957,\n","                    0.29955625534057617],\n","                   [0.6932677626609802,\n","                    0.4589499831199646,\n","                    0.43152186274528503,\n","                    0.3915976285934448,\n","                    0.3665291666984558,\n","                    0.35907238721847534,\n","                    0.44381478428840637,\n","                    0.4948670566082001,\n","                    0.4269513487815857,\n","                    0.3944104313850403,\n","                    0.3580726385116577,\n","                    0.34489408135414124,\n","                    0.3271789848804474,\n","                    0.3309295177459717,\n","                    0.33527645468711853,\n","                    0.3143143355846405,\n","                    0.31415867805480957,\n","                    0.3184475600719452,\n","                    0.3050781786441803,\n","                    0.2974497377872467],\n","                   [0.6164191365242004,\n","                    0.4272559881210327,\n","                    0.3985321819782257,\n","                    0.382001668214798,\n","                    0.37577328085899353,\n","                    0.3508891463279724,\n","                    0.34141555428504944,\n","                    0.34070754051208496,\n","                    0.32224041223526,\n","                    0.3462138772010803,\n","                    0.34881970286369324,\n","                    0.32360365986824036,\n","                    0.3778884708881378,\n","                    0.3366493582725525,\n","                    0.3571688234806061,\n","                    0.3170470893383026,\n","                    0.3167133927345276,\n","                    0.3173011243343353,\n","                    0.3119235336780548,\n","                    0.30112624168395996],\n","                   [0.6568270325660706,\n","                    0.4533988833427429,\n","                    0.396504282951355,\n","                    0.38671255111694336,\n","                    0.36504483222961426,\n","                    0.3494704067707062,\n","                    0.3413826823234558,\n","                    0.3343629837036133,\n","                    0.3390105068683624,\n","                    0.3323846161365509,\n","                    0.3249516785144806,\n","                    0.3179904520511627,\n","                    0.3098573088645935,\n","                    0.33263200521469116,\n","                    0.3141638934612274,\n","                    0.29803115129470825,\n","                    0.2932541072368622,\n","                    0.29408133029937744,\n","                    0.3007466495037079,\n","                    0.2918751537799835],\n","                   [0.7197213768959045,\n","                    0.46844741702079773,\n","                    0.39281365275382996,\n","                    0.37384501099586487,\n","                    0.34260791540145874,\n","                    0.3485681414604187,\n","                    0.3288775384426117,\n","                    0.3174515664577484,\n","                    0.32410189509391785,\n","                    0.3048664629459381,\n","                    0.30055510997772217,\n","                    0.296071320772171,\n","                    0.29130229353904724,\n","                    0.288593590259552,\n","                    0.2863900363445282,\n","                    0.28332337737083435,\n","                    0.28088295459747314,\n","                    0.27902886271476746,\n","                    0.2743799090385437,\n","                    0.27614107728004456]],\n"," 'Validation Accuracy': [[0.8225932717323303,\n","                          0.8527632355690002,\n","                          0.830149233341217,\n","                          0.867418110370636,\n","                          0.8710817694664001,\n","                          0.8736587762832642,\n","                          0.8672294020652771,\n","                          0.8805381655693054,\n","                          0.8832113146781921,\n","                          0.8830180764198303,\n","                          0.8825486898422241,\n","                          0.8765682578086853,\n","                          0.8813340067863464,\n","                          0.849196195602417,\n","                          0.8747703433036804,\n","                          0.8758018612861633,\n","                          0.8822799324989319,\n","                          0.8823634386062622,\n","                          0.8666673302650452,\n","                          0.8868480920791626],\n","                         [0.8090980648994446,\n","                          0.8435760140419006,\n","                          0.8469899296760559,\n","                          0.858008861541748,\n","                          0.8571714162826538,\n","                          0.8722538352012634,\n","                          0.7847927808761597,\n","                          0.8432642221450806,\n","                          0.8520053029060364,\n","                          0.861709475517273,\n","                          0.8705951571464539,\n","                          0.8730105757713318,\n","                          0.8756203055381775,\n","                          0.877122700214386,\n","                          0.8780909776687622,\n","                          0.8778355121612549,\n","                          0.882599949836731,\n","                          0.8829751014709473,\n","                          0.8858736753463745,\n","                          0.8871341347694397],\n","                         [0.8231432437896729,\n","                          0.8512498140335083,\n","                          0.8448246121406555,\n","                          0.8611670732498169,\n","                          0.8665659427642822,\n","                          0.8563294410705566,\n","                          0.8750884532928467,\n","                          0.8725554347038269,\n","                          0.86746746301651,\n","                          0.8690418004989624,\n","                          0.8699122667312622,\n","                          0.8791772723197937,\n","                          0.8669173121452332,\n","                          0.8625009655952454,\n","                          0.8732489943504333,\n","                          0.8797794580459595,\n","                          0.8811960220336914,\n","                          0.8668845891952515,\n","                          0.8819766640663147,\n","                          0.8748819828033447],\n","                         [0.8084571957588196,\n","                          0.8448132872581482,\n","                          0.8534775972366333,\n","                          0.856946587562561,\n","                          0.8667176365852356,\n","                          0.8661912679672241,\n","                          0.8727630376815796,\n","                          0.8739528656005859,\n","                          0.8740628957748413,\n","                          0.8749743103981018,\n","                          0.8770405054092407,\n","                          0.8799360394477844,\n","                          0.8826004266738892,\n","                          0.8601542115211487,\n","                          0.8807190656661987,\n","                          0.8832223415374756,\n","                          0.8794628381729126,\n","                          0.8855360746383667,\n","                          0.8742535710334778,\n","                          0.8852260112762451],\n","                         [0.800875186920166,\n","                          0.8393617868423462,\n","                          0.8577872514724731,\n","                          0.8658924698829651,\n","                          0.8700655102729797,\n","                          0.8702800273895264,\n","                          0.8715330958366394,\n","                          0.876115083694458,\n","                          0.8780021071434021,\n","                          0.8796709179878235,\n","                          0.8814226984977722,\n","                          0.8801385164260864,\n","                          0.8827425241470337,\n","                          0.8833415508270264,\n","                          0.8850623369216919,\n","                          0.8864275813102722,\n","                          0.8861585855484009,\n","                          0.8832861185073853,\n","                          0.8869737386703491,\n","                          0.8821792602539062]],\n"," 'Validation Loss': [[0.47039881348609924,\n","                      0.38449081778526306,\n","                      0.5058274865150452,\n","                      0.34528279304504395,\n","                      0.34102705121040344,\n","                      0.3339044153690338,\n","                      0.35676297545433044,\n","                      0.3058636486530304,\n","                      0.3001164495944977,\n","                      0.29736506938934326,\n","                      0.30004820227622986,\n","                      0.32201188802719116,\n","                      0.3027629554271698,\n","                      0.408143013715744,\n","                      0.3269089162349701,\n","                      0.33024847507476807,\n","                      0.30277779698371887,\n","                      0.3044471740722656,\n","                      0.3514309525489807,\n","                      0.28750935196876526],\n","                     [0.5069718956947327,\n","                      0.40847867727279663,\n","                      0.41711580753326416,\n","                      0.3743644058704376,\n","                      0.38106483221054077,\n","                      0.33278223872184753,\n","                      0.6237062811851501,\n","                      0.42749032378196716,\n","                      0.4141083061695099,\n","                      0.36889907717704773,\n","                      0.3371523320674896,\n","                      0.3296186327934265,\n","                      0.32170313596725464,\n","                      0.3182801902294159,\n","                      0.31541773676872253,\n","                      0.3144403398036957,\n","                      0.3113783001899719,\n","                      0.30111461877822876,\n","                      0.29264503717422485,\n","                      0.2902505695819855],\n","                     [0.4703116714954376,\n","                      0.39777612686157227,\n","                      0.440350741147995,\n","                      0.36586397886276245,\n","                      0.35403868556022644,\n","                      0.4081774055957794,\n","                      0.325461208820343,\n","                      0.3319697976112366,\n","                      0.3689236342906952,\n","                      0.35230791568756104,\n","                      0.34859585762023926,\n","                      0.30932512879371643,\n","                      0.3527812957763672,\n","                      0.38231533765792847,\n","                      0.33045539259910583,\n","                      0.30977508425712585,\n","                      0.3057202398777008,\n","                      0.34714075922966003,\n","                      0.2978828251361847,\n","                      0.3224932849407196],\n","                     [0.5248003602027893,\n","                      0.40985026955604553,\n","                      0.3880411684513092,\n","                      0.38535037636756897,\n","                      0.3525615930557251,\n","                      0.3561643362045288,\n","                      0.3327796161174774,\n","                      0.3287152349948883,\n","                      0.32945379614830017,\n","                      0.3277427852153778,\n","                      0.31659018993377686,\n","                      0.31158584356307983,\n","                      0.302045077085495,\n","                      0.3702068626880646,\n","                      0.30471107363700867,\n","                      0.2986874282360077,\n","                      0.30903270840644836,\n","                      0.2930229604244232,\n","                      0.32885488867759705,\n","                      0.29181450605392456],\n","                     [0.5449103713035583,\n","                      0.4311712682247162,\n","                      0.3711859881877899,\n","                      0.352649450302124,\n","                      0.3366241455078125,\n","                      0.339275985956192,\n","                      0.3355151116847992,\n","                      0.31828299164772034,\n","                      0.31309300661087036,\n","                      0.3071165382862091,\n","                      0.30067047476768494,\n","                      0.3021094501018524,\n","                      0.29518887400627136,\n","                      0.29473310708999634,\n","                      0.28872013092041016,\n","                      0.285683274269104,\n","                      0.28504738211631775,\n","                      0.29387128353118896,\n","                      0.28313374519348145,\n","                      0.2961471676826477]],\n"," 'Validation MCC': [[np.float64(0.7294474850693904),\n","                     np.float64(0.7736023206576013),\n","                     np.float64(0.7374961660378442),\n","                     np.float64(0.7965148950117124),\n","                     np.float64(0.8011145279601475),\n","                     np.float64(0.805428105946563),\n","                     np.float64(0.7954264574809361),\n","                     np.float64(0.816736099376451),\n","                     np.float64(0.8211019388143812),\n","                     np.float64(0.8201752905620014),\n","                     np.float64(0.8203225376682145),\n","                     np.float64(0.8121777908269338),\n","                     np.float64(0.8177340452003017),\n","                     np.float64(0.766953135084204),\n","                     np.float64(0.807329753172898),\n","                     np.float64(0.8088302984137447),\n","                     np.float64(0.8194628879621212),\n","                     np.float64(0.8193081348911115),\n","                     np.float64(0.7941043693232445),\n","                     np.float64(0.8263289199979857)],\n","                    [np.float64(0.7019786253388738),\n","                     np.float64(0.7583005062480233),\n","                     np.float64(0.7627016947749485),\n","                     np.float64(0.7804800236063056),\n","                     np.float64(0.7779484953783544),\n","                     np.float64(0.803005731412001),\n","                     np.float64(0.6603742924718145),\n","                     np.float64(0.7565325997926126),\n","                     np.float64(0.7699550257029198),\n","                     np.float64(0.7861914956253471),\n","                     np.float64(0.8001932953484311),\n","                     np.float64(0.8037695081170625),\n","                     np.float64(0.8083008245250953),\n","                     np.float64(0.8103815807477257),\n","                     np.float64(0.8117598049042544),\n","                     np.float64(0.8115860175047596),\n","                     np.float64(0.8190538665247149),\n","                     np.float64(0.820135802639885),\n","                     np.float64(0.8241374224907568),\n","                     np.float64(0.8259831406789736)],\n","                    [np.float64(0.7257232540997455),\n","                     np.float64(0.7702527945298465),\n","                     np.float64(0.7596879401360851),\n","                     np.float64(0.7865004208340466),\n","                     np.float64(0.7941923917443948),\n","                     np.float64(0.7790847690098968),\n","                     np.float64(0.8084794653901807),\n","                     np.float64(0.8049649873406544),\n","                     np.float64(0.7956657747306877),\n","                     np.float64(0.7983385425544448),\n","                     np.float64(0.7994467803764125),\n","                     np.float64(0.815079520933611),\n","                     np.float64(0.7958059781897344),\n","                     np.float64(0.7884384781065997),\n","                     np.float64(0.8054381718645771),\n","                     np.float64(0.815736955724228),\n","                     np.float64(0.8175606556225411),\n","                     np.float64(0.7950683797857381),\n","                     np.float64(0.8191604286923282),\n","                     np.float64(0.8073259187826336)],\n","                    [np.float64(0.7024519988503334),\n","                     np.float64(0.7596960882751408),\n","                     np.float64(0.7736247053491166),\n","                     np.float64(0.7784495300640356),\n","                     np.float64(0.7938351687249199),\n","                     np.float64(0.7933772828784127),\n","                     np.float64(0.8031668523628381),\n","                     np.float64(0.80558561034776),\n","                     np.float64(0.8054033036581593),\n","                     np.float64(0.8066665984393746),\n","                     np.float64(0.8107781527917591),\n","                     np.float64(0.8145507487967341),\n","                     np.float64(0.8190821228128444),\n","                     np.float64(0.7829066873841973),\n","                     np.float64(0.8166343340185339),\n","                     np.float64(0.819997503252621),\n","                     np.float64(0.8146076599290905),\n","                     np.float64(0.8235850477927302),\n","                     np.float64(0.8058654919961938),\n","                     np.float64(0.8242017678449594)],\n","                    [np.float64(0.6922228068892635),\n","                     np.float64(0.7519709397402613),\n","                     np.float64(0.781120866405491),\n","                     np.float64(0.7935692043876807),\n","                     np.float64(0.8011134140520211),\n","                     np.float64(0.8015210311943274),\n","                     np.float64(0.8023993437805784),\n","                     np.float64(0.8099853613070648),\n","                     np.float64(0.8129921491422506),\n","                     np.float64(0.8148453909650851),\n","                     np.float64(0.8182595011677527),\n","                     np.float64(0.8164727753307415),\n","                     np.float64(0.8206404657137597),\n","                     np.float64(0.8213634987144667),\n","                     np.float64(0.8239650910673061),\n","                     np.float64(0.8255239657907281),\n","                     np.float64(0.8257735323402352),\n","                     np.float64(0.820683354555087),\n","                     np.float64(0.8266241091556801),\n","                     np.float64(0.8195794047759654)]]}\n","Training Model: LSTM_Dense, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6710 - loss: 0.9011(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7503\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.6712 - loss: 0.9004 - val_accuracy: 0.8377 - val_loss: 0.4306 - mcc: 0.7503\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8265 - loss: 0.4729(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7734\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8265 - loss: 0.4728 - val_accuracy: 0.8535 - val_loss: 0.3875 - mcc: 0.7734\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8540 - loss: 0.3850(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7903\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8540 - loss: 0.3850 - val_accuracy: 0.8628 - val_loss: 0.3615 - mcc: 0.7903\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8609 - loss: 0.3655(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8034\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8609 - loss: 0.3654 - val_accuracy: 0.8715 - val_loss: 0.3346 - mcc: 0.8034\n","Epoch 5/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8662 - loss: 0.3521(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7988\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8662 - loss: 0.3521 - val_accuracy: 0.8693 - val_loss: 0.3509 - mcc: 0.7988\n","Epoch 6/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8659 - loss: 0.3565(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8046\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8660 - loss: 0.3564 - val_accuracy: 0.8731 - val_loss: 0.3345 - mcc: 0.8046\n","Epoch 7/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8719 - loss: 0.3353(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8108\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 21ms/step - accuracy: 0.8719 - loss: 0.3354 - val_accuracy: 0.8768 - val_loss: 0.3222 - mcc: 0.8108\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8726 - loss: 0.3330(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8148\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8726 - loss: 0.3330 - val_accuracy: 0.8793 - val_loss: 0.3136 - mcc: 0.8148\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8728 - loss: 0.3322(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8090\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8728 - loss: 0.3322 - val_accuracy: 0.8756 - val_loss: 0.3272 - mcc: 0.8090\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8769 - loss: 0.3245(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8122\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8769 - loss: 0.3245 - val_accuracy: 0.8778 - val_loss: 0.3201 - mcc: 0.8122\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8773 - loss: 0.3204(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8170\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8773 - loss: 0.3204 - val_accuracy: 0.8802 - val_loss: 0.3113 - mcc: 0.8170\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8779 - loss: 0.3178(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8115\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8779 - loss: 0.3178 - val_accuracy: 0.8776 - val_loss: 0.3265 - mcc: 0.8115\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8769 - loss: 0.3230(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8239\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8770 - loss: 0.3230 - val_accuracy: 0.8853 - val_loss: 0.2956 - mcc: 0.8239\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8805 - loss: 0.3066(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8212\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8805 - loss: 0.3065 - val_accuracy: 0.8836 - val_loss: 0.2990 - mcc: 0.8212\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8807 - loss: 0.3071(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8245\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8807 - loss: 0.3070 - val_accuracy: 0.8856 - val_loss: 0.2946 - mcc: 0.8245\n","Epoch 16/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8830 - loss: 0.2977(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8274\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8830 - loss: 0.2977 - val_accuracy: 0.8877 - val_loss: 0.2873 - mcc: 0.8274\n","Epoch 17/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8832 - loss: 0.2971(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8278\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8832 - loss: 0.2971 - val_accuracy: 0.8874 - val_loss: 0.2868 - mcc: 0.8278\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8845 - loss: 0.2935(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8290\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8845 - loss: 0.2935 - val_accuracy: 0.8886 - val_loss: 0.2901 - mcc: 0.8290\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8854 - loss: 0.2879(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8350\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8854 - loss: 0.2879 - val_accuracy: 0.8923 - val_loss: 0.2712 - mcc: 0.8350\n","Epoch 20/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8867 - loss: 0.2814(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8376\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8867 - loss: 0.2814 - val_accuracy: 0.8939 - val_loss: 0.2656 - mcc: 0.8376\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 2\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6798 - loss: 0.8905(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7477\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.6801 - loss: 0.8898 - val_accuracy: 0.8381 - val_loss: 0.4336 - mcc: 0.7477\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8391 - loss: 0.4319(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7833\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8391 - loss: 0.4318 - val_accuracy: 0.8601 - val_loss: 0.3681 - mcc: 0.7833\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8455 - loss: 0.4181(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7804\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8455 - loss: 0.4181 - val_accuracy: 0.8571 - val_loss: 0.3781 - mcc: 0.7804\n","Epoch 4/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8562 - loss: 0.3827(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7798\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8562 - loss: 0.3827 - val_accuracy: 0.8576 - val_loss: 0.3768 - mcc: 0.7798\n","Epoch 5/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8629 - loss: 0.3594(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8010\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8629 - loss: 0.3594 - val_accuracy: 0.8714 - val_loss: 0.3367 - mcc: 0.8010\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8637 - loss: 0.3591(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7847\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8637 - loss: 0.3591 - val_accuracy: 0.8589 - val_loss: 0.3656 - mcc: 0.7847\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8655 - loss: 0.3541(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7411\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8655 - loss: 0.3542 - val_accuracy: 0.8340 - val_loss: 0.4552 - mcc: 0.7411\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8430 - loss: 0.4556(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7973\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.8430 - loss: 0.4554 - val_accuracy: 0.8688 - val_loss: 0.3535 - mcc: 0.7973\n","Epoch 9/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8642 - loss: 0.3614(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8058\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8642 - loss: 0.3614 - val_accuracy: 0.8741 - val_loss: 0.3305 - mcc: 0.8058\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8707 - loss: 0.3391(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8076\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8707 - loss: 0.3391 - val_accuracy: 0.8747 - val_loss: 0.3251 - mcc: 0.8076\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8742 - loss: 0.3265(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8148\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8742 - loss: 0.3265 - val_accuracy: 0.8802 - val_loss: 0.3099 - mcc: 0.8148\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8746 - loss: 0.3235(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8052\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8746 - loss: 0.3236 - val_accuracy: 0.8736 - val_loss: 0.3296 - mcc: 0.8052\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8730 - loss: 0.3294(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8093\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8730 - loss: 0.3294 - val_accuracy: 0.8765 - val_loss: 0.3342 - mcc: 0.8093\n","Epoch 14/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8746 - loss: 0.3252(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8127\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8746 - loss: 0.3252 - val_accuracy: 0.8784 - val_loss: 0.3143 - mcc: 0.8127\n","Epoch 15/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8777 - loss: 0.3148(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8185\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8777 - loss: 0.3148 - val_accuracy: 0.8823 - val_loss: 0.3030 - mcc: 0.8185\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8796 - loss: 0.3098(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.7875\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8796 - loss: 0.3098 - val_accuracy: 0.8624 - val_loss: 0.3672 - mcc: 0.7875\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8686 - loss: 0.3476(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8081\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8686 - loss: 0.3475 - val_accuracy: 0.8748 - val_loss: 0.3306 - mcc: 0.8081\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8765 - loss: 0.3242(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8119\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8765 - loss: 0.3242 - val_accuracy: 0.8774 - val_loss: 0.3163 - mcc: 0.8119\n","Epoch 19/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8818 - loss: 0.3033(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8199\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8818 - loss: 0.3033 - val_accuracy: 0.8829 - val_loss: 0.3013 - mcc: 0.8199\n","Epoch 20/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8839 - loss: 0.2969(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.7312\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8839 - loss: 0.2970 - val_accuracy: 0.8274 - val_loss: 0.4582 - mcc: 0.7312\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 3\n","Epoch 1/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6946 - loss: 0.8371(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7615\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.6949 - loss: 0.8362 - val_accuracy: 0.8447 - val_loss: 0.4147 - mcc: 0.7615\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8516 - loss: 0.3956(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7862\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21ms/step - accuracy: 0.8516 - loss: 0.3956 - val_accuracy: 0.8610 - val_loss: 0.3683 - mcc: 0.7862\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8632 - loss: 0.3625(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7863\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8632 - loss: 0.3625 - val_accuracy: 0.8616 - val_loss: 0.3676 - mcc: 0.7863\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8673 - loss: 0.3513(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7993\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8672 - loss: 0.3513 - val_accuracy: 0.8693 - val_loss: 0.3424 - mcc: 0.7993\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8705 - loss: 0.3387(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7025\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.8705 - loss: 0.3387 - val_accuracy: 0.8084 - val_loss: 0.7538 - mcc: 0.7025\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8601 - loss: 0.3823(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8049\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8601 - loss: 0.3822 - val_accuracy: 0.8732 - val_loss: 0.3310 - mcc: 0.8049\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8734 - loss: 0.3299(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8113\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8734 - loss: 0.3299 - val_accuracy: 0.8767 - val_loss: 0.3139 - mcc: 0.8113\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8772 - loss: 0.3175(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8129\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8772 - loss: 0.3175 - val_accuracy: 0.8768 - val_loss: 0.3120 - mcc: 0.8129\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8790 - loss: 0.3118(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8146\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8790 - loss: 0.3118 - val_accuracy: 0.8791 - val_loss: 0.3122 - mcc: 0.8146\n","Epoch 10/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8790 - loss: 0.3110(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8173\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8790 - loss: 0.3110 - val_accuracy: 0.8813 - val_loss: 0.3055 - mcc: 0.8173\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8815 - loss: 0.3059(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8196\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8815 - loss: 0.3060 - val_accuracy: 0.8825 - val_loss: 0.3010 - mcc: 0.8196\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8838 - loss: 0.2953(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8225\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8838 - loss: 0.2953 - val_accuracy: 0.8844 - val_loss: 0.2959 - mcc: 0.8225\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8848 - loss: 0.2935(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8221\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8848 - loss: 0.2935 - val_accuracy: 0.8842 - val_loss: 0.2948 - mcc: 0.8221\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8845 - loss: 0.2963(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8239\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8845 - loss: 0.2963 - val_accuracy: 0.8837 - val_loss: 0.2913 - mcc: 0.8239\n","Epoch 15/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8873 - loss: 0.2873(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8254\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8873 - loss: 0.2873 - val_accuracy: 0.8863 - val_loss: 0.2893 - mcc: 0.8254\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8877 - loss: 0.2849(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8268\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.8877 - loss: 0.2849 - val_accuracy: 0.8873 - val_loss: 0.2866 - mcc: 0.8268\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8891 - loss: 0.2794(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.7726\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8891 - loss: 0.2794 - val_accuracy: 0.8524 - val_loss: 0.3954 - mcc: 0.7726\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8831 - loss: 0.3000(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8296\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8831 - loss: 0.2999 - val_accuracy: 0.8887 - val_loss: 0.2812 - mcc: 0.8296\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8876 - loss: 0.2864(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8330\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8876 - loss: 0.2864 - val_accuracy: 0.8912 - val_loss: 0.2749 - mcc: 0.8330\n","Epoch 20/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8876 - loss: 0.2840(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8325\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8876 - loss: 0.2840 - val_accuracy: 0.8909 - val_loss: 0.2743 - mcc: 0.8325\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 4\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6878 - loss: 0.8205(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7508\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step - accuracy: 0.6880 - loss: 0.8199 - val_accuracy: 0.8398 - val_loss: 0.4288 - mcc: 0.7508\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8458 - loss: 0.4107(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7636\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8458 - loss: 0.4107 - val_accuracy: 0.8482 - val_loss: 0.4219 - mcc: 0.7636\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8553 - loss: 0.3877(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7783\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8553 - loss: 0.3878 - val_accuracy: 0.8566 - val_loss: 0.3842 - mcc: 0.7783\n","Epoch 4/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8581 - loss: 0.3830(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7838\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 20ms/step - accuracy: 0.8581 - loss: 0.3830 - val_accuracy: 0.8600 - val_loss: 0.3772 - mcc: 0.7838\n","Epoch 5/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8663 - loss: 0.3532(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7967\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8663 - loss: 0.3532 - val_accuracy: 0.8686 - val_loss: 0.3509 - mcc: 0.7967\n","Epoch 6/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8533 - loss: 0.3918(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7300\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8533 - loss: 0.3919 - val_accuracy: 0.8267 - val_loss: 0.4730 - mcc: 0.7300\n","Epoch 7/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8469 - loss: 0.4172(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7777\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.8470 - loss: 0.4171 - val_accuracy: 0.8566 - val_loss: 0.3870 - mcc: 0.7777\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8586 - loss: 0.3805(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7875\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.8586 - loss: 0.3804 - val_accuracy: 0.8632 - val_loss: 0.3675 - mcc: 0.7875\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8666 - loss: 0.3530(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7604\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8666 - loss: 0.3530 - val_accuracy: 0.8457 - val_loss: 0.4175 - mcc: 0.7604\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8624 - loss: 0.3687(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.7974\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8624 - loss: 0.3687 - val_accuracy: 0.8681 - val_loss: 0.3462 - mcc: 0.7974\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8713 - loss: 0.3400(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8036\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 19ms/step - accuracy: 0.8713 - loss: 0.3400 - val_accuracy: 0.8728 - val_loss: 0.3359 - mcc: 0.8036\n","Epoch 12/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8711 - loss: 0.3379(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8054\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8711 - loss: 0.3379 - val_accuracy: 0.8725 - val_loss: 0.3313 - mcc: 0.8054\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8746 - loss: 0.3286(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8131\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8746 - loss: 0.3286 - val_accuracy: 0.8786 - val_loss: 0.3114 - mcc: 0.8131\n","Epoch 14/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8789 - loss: 0.3129(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8017\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8789 - loss: 0.3129 - val_accuracy: 0.8712 - val_loss: 0.3409 - mcc: 0.8017\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8776 - loss: 0.3178(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8060\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8776 - loss: 0.3178 - val_accuracy: 0.8742 - val_loss: 0.3336 - mcc: 0.8060\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8792 - loss: 0.3129(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8179\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.8792 - loss: 0.3129 - val_accuracy: 0.8818 - val_loss: 0.3043 - mcc: 0.8179\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8748 - loss: 0.3312(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8169\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step - accuracy: 0.8748 - loss: 0.3311 - val_accuracy: 0.8811 - val_loss: 0.3069 - mcc: 0.8169\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8826 - loss: 0.3032(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8170\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8826 - loss: 0.3032 - val_accuracy: 0.8808 - val_loss: 0.3074 - mcc: 0.8170\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8839 - loss: 0.2972(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8095\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8839 - loss: 0.2972 - val_accuracy: 0.8766 - val_loss: 0.3251 - mcc: 0.8095\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8803 - loss: 0.3098(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8161\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 21ms/step - accuracy: 0.8803 - loss: 0.3098 - val_accuracy: 0.8807 - val_loss: 0.3071 - mcc: 0.8161\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 5\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6711 - loss: 0.8846(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7469\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.6713 - loss: 0.8839 - val_accuracy: 0.8336 - val_loss: 0.4458 - mcc: 0.7469\n","Epoch 2/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8441 - loss: 0.4180(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7753\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8441 - loss: 0.4179 - val_accuracy: 0.8547 - val_loss: 0.3924 - mcc: 0.7753\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8596 - loss: 0.3742(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7828\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8596 - loss: 0.3742 - val_accuracy: 0.8591 - val_loss: 0.3702 - mcc: 0.7828\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8630 - loss: 0.3637(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.7920\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8630 - loss: 0.3637 - val_accuracy: 0.8637 - val_loss: 0.3541 - mcc: 0.7920\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8680 - loss: 0.3488(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.7936\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8680 - loss: 0.3488 - val_accuracy: 0.8656 - val_loss: 0.3520 - mcc: 0.7936\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8653 - loss: 0.3577(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.7819\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8653 - loss: 0.3578 - val_accuracy: 0.8586 - val_loss: 0.3811 - mcc: 0.7819\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8661 - loss: 0.3575(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.7964\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8661 - loss: 0.3575 - val_accuracy: 0.8676 - val_loss: 0.3463 - mcc: 0.7964\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8678 - loss: 0.3570(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7965\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.8678 - loss: 0.3569 - val_accuracy: 0.8679 - val_loss: 0.3496 - mcc: 0.7965\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8723 - loss: 0.3370(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8023\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8723 - loss: 0.3370 - val_accuracy: 0.8713 - val_loss: 0.3352 - mcc: 0.8023\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8718 - loss: 0.3329(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8044\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8719 - loss: 0.3329 - val_accuracy: 0.8711 - val_loss: 0.3275 - mcc: 0.8044\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8692 - loss: 0.3437(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8042\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8692 - loss: 0.3437 - val_accuracy: 0.8726 - val_loss: 0.3310 - mcc: 0.8042\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8758 - loss: 0.3251(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8102\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.8758 - loss: 0.3251 - val_accuracy: 0.8766 - val_loss: 0.3191 - mcc: 0.8102\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8728 - loss: 0.3385(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.7941\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8727 - loss: 0.3386 - val_accuracy: 0.8665 - val_loss: 0.3604 - mcc: 0.7941\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8719 - loss: 0.3443(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8087\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8719 - loss: 0.3443 - val_accuracy: 0.8755 - val_loss: 0.3240 - mcc: 0.8087\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8777 - loss: 0.3187(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8062\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8777 - loss: 0.3187 - val_accuracy: 0.8739 - val_loss: 0.3284 - mcc: 0.8062\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8790 - loss: 0.3163(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8106\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 19ms/step - accuracy: 0.8790 - loss: 0.3163 - val_accuracy: 0.8765 - val_loss: 0.3196 - mcc: 0.8106\n","Epoch 17/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8821 - loss: 0.3052(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8124\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 21ms/step - accuracy: 0.8821 - loss: 0.3052 - val_accuracy: 0.8768 - val_loss: 0.3114 - mcc: 0.8124\n","Epoch 18/20\n","\u001b[1m747/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8806 - loss: 0.3156(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8156\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.8806 - loss: 0.3156 - val_accuracy: 0.8801 - val_loss: 0.3089 - mcc: 0.8156\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8783 - loss: 0.3196(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8096\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 20ms/step - accuracy: 0.8783 - loss: 0.3196 - val_accuracy: 0.8755 - val_loss: 0.3227 - mcc: 0.8096\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8827 - loss: 0.3033(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8155\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.8827 - loss: 0.3033 - val_accuracy: 0.8797 - val_loss: 0.3077 - mcc: 0.8155\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.8939086666666667),\n","              'mean': np.float64(0.8745277333333334),\n","              'min': np.float64(0.8274466666666667),\n","              'std': np.float64(0.024182878103135522)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0004461874961853027),\n","                               'mean': np.float64(0.0003139662901560466),\n","                               'min': np.float64(0.00024980727831522623),\n","                               'std': np.float64(6.824283126420511e-05)},\n"," 'MCC': {'max': np.float64(0.837560485879717),\n","         'mean': np.float64(0.8065768877067873),\n","         'min': np.float64(0.7312456596759038),\n","         'std': np.float64(0.03867025000644613)},\n"," 'Parameters': 5477,\n"," 'Train Time (s)': {'max': np.float64(380.01873564720154),\n","                    'mean': np.float64(356.23503909111025),\n","                    'min': np.float64(329.6357669830322),\n","                    'std': np.float64(16.15186955219383)},\n"," 'Training Accuracy': [[0.766575276851654,\n","                        0.8338190913200378,\n","                        0.8563952445983887,\n","                        0.864658534526825,\n","                        0.8642399311065674,\n","                        0.8685539364814758,\n","                        0.8711449503898621,\n","                        0.8727372884750366,\n","                        0.8719936609268188,\n","                        0.8751685619354248,\n","                        0.877000629901886,\n","                        0.8751076459884644,\n","                        0.8776291608810425,\n","                        0.8816448450088501,\n","                        0.8823596239089966,\n","                        0.8840276002883911,\n","                        0.8837980031967163,\n","                        0.8855105042457581,\n","                        0.886903703212738,\n","                        0.8889293074607849],\n","                       [0.7627147436141968,\n","                        0.8445027470588684,\n","                        0.8465355634689331,\n","                        0.8576087355613708,\n","                        0.8629767298698425,\n","                        0.8633496761322021,\n","                        0.8592478632926941,\n","                        0.8538045883178711,\n","                        0.8674821257591248,\n","                        0.8704620003700256,\n","                        0.8743306994438171,\n","                        0.8718706965446472,\n","                        0.8743600249290466,\n","                        0.8756487369537354,\n","                        0.8779009580612183,\n","                        0.8762384653091431,\n","                        0.8719810247421265,\n","                        0.8757436275482178,\n","                        0.8796658515930176,\n","                        0.8818274140357971],\n","                       [0.7771992087364197,\n","                        0.8550825119018555,\n","                        0.8635238409042358,\n","                        0.8653804659843445,\n","                        0.8693352341651917,\n","                        0.8699542880058289,\n","                        0.8762774467468262,\n","                        0.8751753568649292,\n","                        0.8805714249610901,\n","                        0.8799588084220886,\n","                        0.8812291622161865,\n","                        0.884161114692688,\n","                        0.8847121596336365,\n","                        0.8860805034637451,\n","                        0.8861228823661804,\n","                        0.8880836367607117,\n","                        0.8875596523284912,\n","                        0.8870579600334167,\n","                        0.8879086375236511,\n","                        0.8898364901542664],\n","                       [0.7720561623573303,\n","                        0.8484446406364441,\n","                        0.8533790111541748,\n","                        0.8583240509033203,\n","                        0.8659873008728027,\n","                        0.845084011554718,\n","                        0.8518359661102295,\n","                        0.8615729212760925,\n","                        0.8656593561172485,\n","                        0.8666417598724365,\n","                        0.8714737296104431,\n","                        0.8716368675231934,\n","                        0.8773583769798279,\n","                        0.8776467442512512,\n","                        0.8791689276695251,\n","                        0.880556583404541,\n","                        0.8780981302261353,\n","                        0.8837507963180542,\n","                        0.8829005360603333,\n","                        0.8799481391906738],\n","                       [0.7655290365219116,\n","                        0.8503219485282898,\n","                        0.8628426194190979,\n","                        0.8638601303100586,\n","                        0.868440568447113,\n","                        0.8620781302452087,\n","                        0.8682535886764526,\n","                        0.8694965243339539,\n","                        0.8736752271652222,\n","                        0.8745377659797668,\n","                        0.8710333704948425,\n","                        0.8765695691108704,\n","                        0.8662241101264954,\n","                        0.8743664026260376,\n","                        0.8788928985595703,\n","                        0.8803783655166626,\n","                        0.8822413086891174,\n","                        0.8794598579406738,\n","                        0.8765158653259277,\n","                        0.8822520971298218]],\n"," 'Training Loss': [[0.6364873647689819,\n","                    0.4507027566432953,\n","                    0.37871065735816956,\n","                    0.3552021384239197,\n","                    0.3613157570362091,\n","                    0.3478219509124756,\n","                    0.3377537429332733,\n","                    0.3323339521884918,\n","                    0.3367408514022827,\n","                    0.3277100622653961,\n","                    0.3207828104496002,\n","                    0.32665199041366577,\n","                    0.31809887290000916,\n","                    0.30318573117256165,\n","                    0.3010956943035126,\n","                    0.29543912410736084,\n","                    0.2954084575176239,\n","                    0.29044753313064575,\n","                    0.2841695547103882,\n","                    0.27692458033561707],\n","                   [0.6518272757530212,\n","                    0.4151782691478729,\n","                    0.4178254306316376,\n","                    0.37747105956077576,\n","                    0.35816484689712524,\n","                    0.36096101999282837,\n","                    0.3800961375236511,\n","                    0.41062334179878235,\n","                    0.34940221905708313,\n","                    0.3390893340110779,\n","                    0.32536959648132324,\n","                    0.3335034251213074,\n","                    0.3255515396595001,\n","                    0.3226948380470276,\n","                    0.3149970471858978,\n","                    0.3214452862739563,\n","                    0.3360670506954193,\n","                    0.32384249567985535,\n","                    0.3084036409854889,\n","                    0.3045302629470825],\n","                   [0.6006599068641663,\n","                    0.3857268989086151,\n","                    0.3611953556537628,\n","                    0.3576370179653168,\n","                    0.3426310122013092,\n","                    0.34319889545440674,\n","                    0.320022851228714,\n","                    0.3242524266242981,\n","                    0.3066716194152832,\n","                    0.309334933757782,\n","                    0.30760657787323,\n","                    0.29606908559799194,\n","                    0.2949744164943695,\n","                    0.2910091280937195,\n","                    0.28937017917633057,\n","                    0.2834705114364624,\n","                    0.2863351106643677,\n","                    0.2869228422641754,\n","                    0.2857085168361664,\n","                    0.2779408097267151],\n","                   [0.5976254343986511,\n","                    0.4044387638568878,\n","                    0.3954941928386688,\n","                    0.37949666380882263,\n","                    0.3516886830329895,\n","                    0.42069581151008606,\n","                    0.40270963311195374,\n","                    0.3703048825263977,\n","                    0.35344642400741577,\n","                    0.3555223345756531,\n","                    0.33988529443740845,\n","                    0.33908799290657043,\n","                    0.3187927007675171,\n","                    0.31817084550857544,\n","                    0.31388092041015625,\n","                    0.30784475803375244,\n","                    0.31828784942626953,\n","                    0.2980617880821228,\n","                    0.30157244205474854,\n","                    0.31032827496528625],\n","                   [0.6302466988563538,\n","                    0.40109989047050476,\n","                    0.3647240102291107,\n","                    0.3617374300956726,\n","                    0.3479262590408325,\n","                    0.3676375448703766,\n","                    0.3497692942619324,\n","                    0.35108682513237,\n","                    0.33363550901412964,\n","                    0.3279353380203247,\n","                    0.342074990272522,\n","                    0.32377225160598755,\n","                    0.3657766580581665,\n","                    0.3339454233646393,\n","                    0.3166161775588989,\n","                    0.31146493554115295,\n","                    0.3042297661304474,\n","                    0.3162188231945038,\n","                    0.3240697979927063,\n","                    0.30284571647644043]],\n"," 'Validation Accuracy': [[0.8376774191856384,\n","                          0.8534906506538391,\n","                          0.8628172278404236,\n","                          0.8715165853500366,\n","                          0.8693419694900513,\n","                          0.8731306195259094,\n","                          0.8767907023429871,\n","                          0.8792672753334045,\n","                          0.875582754611969,\n","                          0.877831220626831,\n","                          0.8801708221435547,\n","                          0.8776276707649231,\n","                          0.8852870464324951,\n","                          0.8836416602134705,\n","                          0.8855730891227722,\n","                          0.887671947479248,\n","                          0.8874219655990601,\n","                          0.8886326551437378,\n","                          0.8923078775405884,\n","                          0.8939085006713867],\n","                         [0.8380681276321411,\n","                          0.8601232767105103,\n","                          0.8570873141288757,\n","                          0.8576495051383972,\n","                          0.871390163898468,\n","                          0.8589082956314087,\n","                          0.834046483039856,\n","                          0.8688458800315857,\n","                          0.8740786910057068,\n","                          0.8746950626373291,\n","                          0.8801767826080322,\n","                          0.8736087083816528,\n","                          0.8764917254447937,\n","                          0.8783602118492126,\n","                          0.8823279738426208,\n","                          0.8623545169830322,\n","                          0.8748093843460083,\n","                          0.8774366974830627,\n","                          0.8828919529914856,\n","                          0.8274467587471008],\n","                         [0.8447188138961792,\n","                          0.8610367178916931,\n","                          0.8615793585777283,\n","                          0.8693339824676514,\n","                          0.8083743453025818,\n","                          0.8731827139854431,\n","                          0.8767216205596924,\n","                          0.8767844438552856,\n","                          0.8790600299835205,\n","                          0.8812519311904907,\n","                          0.8824994564056396,\n","                          0.8843512535095215,\n","                          0.8842374682426453,\n","                          0.8836733102798462,\n","                          0.8862736821174622,\n","                          0.8872732520103455,\n","                          0.8524428009986877,\n","                          0.8887375593185425,\n","                          0.8911750316619873,\n","                          0.8908582925796509],\n","                         [0.8398473262786865,\n","                          0.8481554388999939,\n","                          0.8566411733627319,\n","                          0.8599786758422852,\n","                          0.8686175346374512,\n","                          0.8266552090644836,\n","                          0.8565760254859924,\n","                          0.8631674647331238,\n","                          0.8457270264625549,\n","                          0.8680580854415894,\n","                          0.8728460073471069,\n","                          0.8724942207336426,\n","                          0.8785873055458069,\n","                          0.8712067008018494,\n","                          0.8741793036460876,\n","                          0.881771981716156,\n","                          0.8810805678367615,\n","                          0.8808465003967285,\n","                          0.8765707015991211,\n","                          0.880710780620575],\n","                         [0.8336488604545593,\n","                          0.8546850681304932,\n","                          0.8590571880340576,\n","                          0.8636775612831116,\n","                          0.8655807971954346,\n","                          0.8585711121559143,\n","                          0.8675979971885681,\n","                          0.8678684234619141,\n","                          0.8713086843490601,\n","                          0.8711146116256714,\n","                          0.872628927230835,\n","                          0.8765532374382019,\n","                          0.8665029406547546,\n","                          0.8754900097846985,\n","                          0.8738846778869629,\n","                          0.8765018582344055,\n","                          0.8768475651741028,\n","                          0.8800792098045349,\n","                          0.8755108118057251,\n","                          0.879714846611023]],\n"," 'Validation Loss': [[0.4306054711341858,\n","                      0.3875136375427246,\n","                      0.3615126609802246,\n","                      0.3346293270587921,\n","                      0.35089054703712463,\n","                      0.33445900678634644,\n","                      0.3222024738788605,\n","                      0.31361091136932373,\n","                      0.32715386152267456,\n","                      0.320095032453537,\n","                      0.3113485276699066,\n","                      0.3265016973018646,\n","                      0.2955729067325592,\n","                      0.29897746443748474,\n","                      0.294614315032959,\n","                      0.2872861921787262,\n","                      0.2868441641330719,\n","                      0.29009175300598145,\n","                      0.27115321159362793,\n","                      0.265571266412735],\n","                     [0.4335724413394928,\n","                      0.3680919110774994,\n","                      0.3781000077724457,\n","                      0.37676888704299927,\n","                      0.33672428131103516,\n","                      0.36562252044677734,\n","                      0.45515739917755127,\n","                      0.3535427749156952,\n","                      0.33045676350593567,\n","                      0.32511764764785767,\n","                      0.30989938974380493,\n","                      0.32956743240356445,\n","                      0.3341984450817108,\n","                      0.31425368785858154,\n","                      0.3029933273792267,\n","                      0.3672199249267578,\n","                      0.33062729239463806,\n","                      0.316275417804718,\n","                      0.3012896776199341,\n","                      0.4581677317619324],\n","                     [0.4147278666496277,\n","                      0.36828526854515076,\n","                      0.3676219582557678,\n","                      0.34238648414611816,\n","                      0.7537642121315002,\n","                      0.3309878408908844,\n","                      0.3138846158981323,\n","                      0.3119965195655823,\n","                      0.3121621608734131,\n","                      0.30551594495773315,\n","                      0.30098316073417664,\n","                      0.2958527207374573,\n","                      0.2947752773761749,\n","                      0.29130440950393677,\n","                      0.2892778813838959,\n","                      0.28662267327308655,\n","                      0.39536866545677185,\n","                      0.28120797872543335,\n","                      0.2749163508415222,\n","                      0.2743498980998993],\n","                     [0.4287576973438263,\n","                      0.4218907356262207,\n","                      0.38423892855644226,\n","                      0.37718337774276733,\n","                      0.35091739892959595,\n","                      0.4730413854122162,\n","                      0.38700351119041443,\n","                      0.36753180623054504,\n","                      0.41748473048210144,\n","                      0.34618881344795227,\n","                      0.3358900547027588,\n","                      0.33134475350379944,\n","                      0.3113826811313629,\n","                      0.34094107151031494,\n","                      0.3335835635662079,\n","                      0.30430516600608826,\n","                      0.3068830072879791,\n","                      0.3074324131011963,\n","                      0.32511237263679504,\n","                      0.3071001470088959],\n","                     [0.445809543132782,\n","                      0.3924027979373932,\n","                      0.37023666501045227,\n","                      0.3540690541267395,\n","                      0.3519577085971832,\n","                      0.381134569644928,\n","                      0.3462633490562439,\n","                      0.34957221150398254,\n","                      0.33516091108322144,\n","                      0.32745030522346497,\n","                      0.3309912085533142,\n","                      0.31911787390708923,\n","                      0.36041969060897827,\n","                      0.32397839426994324,\n","                      0.32836323976516724,\n","                      0.31963464617729187,\n","                      0.31140461564064026,\n","                      0.30894801020622253,\n","                      0.3227493166923523,\n","                      0.3077479302883148]],\n"," 'Validation MCC': [[np.float64(0.7503488105569864),\n","                     np.float64(0.7734313910292736),\n","                     np.float64(0.7902924086425652),\n","                     np.float64(0.8033776003339913),\n","                     np.float64(0.7988381503631551),\n","                     np.float64(0.8045557280873948),\n","                     np.float64(0.8107913909389999),\n","                     np.float64(0.8148473763393598),\n","                     np.float64(0.8090077194390507),\n","                     np.float64(0.8122076980202071),\n","                     np.float64(0.8169564880751958),\n","                     np.float64(0.8114599937978666),\n","                     np.float64(0.8239069896779543),\n","                     np.float64(0.821185284723149),\n","                     np.float64(0.8245329868361854),\n","                     np.float64(0.8274139477170883),\n","                     np.float64(0.8277935846189376),\n","                     np.float64(0.8289769731270579),\n","                     np.float64(0.8350109841870191),\n","                     np.float64(0.837560485879717)],\n","                    [np.float64(0.7477232814424674),\n","                     np.float64(0.7833286797981976),\n","                     np.float64(0.7803580322822741),\n","                     np.float64(0.7797543433545229),\n","                     np.float64(0.8010434589662169),\n","                     np.float64(0.784741852124028),\n","                     np.float64(0.7411191408289833),\n","                     np.float64(0.7972992423951131),\n","                     np.float64(0.8058403142537637),\n","                     np.float64(0.8075535760542015),\n","                     np.float64(0.8148217721799663),\n","                     np.float64(0.8052302462425754),\n","                     np.float64(0.8092723970106075),\n","                     np.float64(0.812710117694992),\n","                     np.float64(0.8185358829319787),\n","                     np.float64(0.7875176215600285),\n","                     np.float64(0.808063236500992),\n","                     np.float64(0.8119176004670853),\n","                     np.float64(0.8199386887485244),\n","                     np.float64(0.7312456596759038)],\n","                    [np.float64(0.761477544628013),\n","                     np.float64(0.7861623369107267),\n","                     np.float64(0.7862587148612592),\n","                     np.float64(0.7992645631451105),\n","                     np.float64(0.7024800871949182),\n","                     np.float64(0.8049388192335044),\n","                     np.float64(0.811271383370426),\n","                     np.float64(0.8128630428634507),\n","                     np.float64(0.8146234194856998),\n","                     np.float64(0.8173426244682526),\n","                     np.float64(0.8195605363734331),\n","                     np.float64(0.8224913334998433),\n","                     np.float64(0.8220789492832435),\n","                     np.float64(0.8239063695282823),\n","                     np.float64(0.8253520371976517),\n","                     np.float64(0.826848916892789),\n","                     np.float64(0.7725869027592768),\n","                     np.float64(0.8296396674029901),\n","                     np.float64(0.8330059413409598),\n","                     np.float64(0.8325077942664609)],\n","                    [np.float64(0.7508364103601817),\n","                     np.float64(0.7635976975389223),\n","                     np.float64(0.7783345357459464),\n","                     np.float64(0.7838049499813527),\n","                     np.float64(0.7967322283545717),\n","                     np.float64(0.7300151402386966),\n","                     np.float64(0.7776904126641129),\n","                     np.float64(0.7875463974636958),\n","                     np.float64(0.7604159018577722),\n","                     np.float64(0.7974070939740877),\n","                     np.float64(0.8036158362312139),\n","                     np.float64(0.8053682691257608),\n","                     np.float64(0.8131146362142606),\n","                     np.float64(0.8016747422731921),\n","                     np.float64(0.8060438049012035),\n","                     np.float64(0.8179203268946305),\n","                     np.float64(0.8168555554631949),\n","                     np.float64(0.8170243544846588),\n","                     np.float64(0.8094995544865815),\n","                     np.float64(0.816071729628776)],\n","                    [np.float64(0.7469110295613601),\n","                     np.float64(0.7752723475185621),\n","                     np.float64(0.7827833702376518),\n","                     np.float64(0.7920141105436492),\n","                     np.float64(0.7936300623800465),\n","                     np.float64(0.7818667107438728),\n","                     np.float64(0.7964343432641657),\n","                     np.float64(0.7964591322340163),\n","                     np.float64(0.8023150109115513),\n","                     np.float64(0.8044390769834708),\n","                     np.float64(0.8042075073932692),\n","                     np.float64(0.8101985378889902),\n","                     np.float64(0.7941075390867962),\n","                     np.float64(0.8086740203471884),\n","                     np.float64(0.80616755292263),\n","                     np.float64(0.8106067359542766),\n","                     np.float64(0.8123947080846903),\n","                     np.float64(0.8155969744988529),\n","                     np.float64(0.8095528396039607),\n","                     np.float64(0.815498769083079)]]}\n","Training Model: LSTM_Deep, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7073 - loss: 0.7488(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7680\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.7075 - loss: 0.7482 - val_accuracy: 0.8495 - val_loss: 0.4052 - mcc: 0.7680\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8553 - loss: 0.3809(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7852\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8553 - loss: 0.3809 - val_accuracy: 0.8609 - val_loss: 0.3676 - mcc: 0.7852\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8644 - loss: 0.3568(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8044\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8644 - loss: 0.3568 - val_accuracy: 0.8722 - val_loss: 0.3382 - mcc: 0.8044\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8719 - loss: 0.3344(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8164\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8719 - loss: 0.3344 - val_accuracy: 0.8803 - val_loss: 0.3113 - mcc: 0.8164\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8767 - loss: 0.3200(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8229\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8767 - loss: 0.3200 - val_accuracy: 0.8844 - val_loss: 0.3002 - mcc: 0.8229\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8806 - loss: 0.3063(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8201\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8806 - loss: 0.3063 - val_accuracy: 0.8829 - val_loss: 0.2999 - mcc: 0.8201\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8839 - loss: 0.2963(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8295\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.8839 - loss: 0.2963 - val_accuracy: 0.8891 - val_loss: 0.2855 - mcc: 0.8295\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8846 - loss: 0.2925(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8280\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.8846 - loss: 0.2925 - val_accuracy: 0.8880 - val_loss: 0.2860 - mcc: 0.8280\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8871 - loss: 0.2811(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8385\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8871 - loss: 0.2811 - val_accuracy: 0.8948 - val_loss: 0.2641 - mcc: 0.8385\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8940 - loss: 0.2642(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8401\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.8940 - loss: 0.2642 - val_accuracy: 0.8959 - val_loss: 0.2612 - mcc: 0.8401\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8922 - loss: 0.2684(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8425\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8922 - loss: 0.2683 - val_accuracy: 0.8973 - val_loss: 0.2556 - mcc: 0.8425\n","Epoch 12/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8938 - loss: 0.2629(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8471\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.8938 - loss: 0.2629 - val_accuracy: 0.9000 - val_loss: 0.2504 - mcc: 0.8471\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8969 - loss: 0.2550(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8455\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.8969 - loss: 0.2550 - val_accuracy: 0.8993 - val_loss: 0.2505 - mcc: 0.8455\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8980 - loss: 0.2511(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8468\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8980 - loss: 0.2511 - val_accuracy: 0.9000 - val_loss: 0.2486 - mcc: 0.8468\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8994 - loss: 0.2481(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8460\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8994 - loss: 0.2481 - val_accuracy: 0.8996 - val_loss: 0.2497 - mcc: 0.8460\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8984 - loss: 0.2500(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8476\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 32ms/step - accuracy: 0.8984 - loss: 0.2500 - val_accuracy: 0.9006 - val_loss: 0.2473 - mcc: 0.8476\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8989 - loss: 0.2477(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8495\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8989 - loss: 0.2477 - val_accuracy: 0.9016 - val_loss: 0.2420 - mcc: 0.8495\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9007 - loss: 0.2432(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8510\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9007 - loss: 0.2432 - val_accuracy: 0.9025 - val_loss: 0.2412 - mcc: 0.8510\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9004 - loss: 0.2438(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8468\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9004 - loss: 0.2438 - val_accuracy: 0.8999 - val_loss: 0.2490 - mcc: 0.8468\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9023 - loss: 0.2396(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8514\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9023 - loss: 0.2396 - val_accuracy: 0.9030 - val_loss: 0.2385 - mcc: 0.8514\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7147 - loss: 0.7652(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7699\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.7148 - loss: 0.7649 - val_accuracy: 0.8513 - val_loss: 0.3900 - mcc: 0.7699\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8531 - loss: 0.3859(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7931\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.8531 - loss: 0.3859 - val_accuracy: 0.8662 - val_loss: 0.3556 - mcc: 0.7931\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8663 - loss: 0.3543(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8038\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8663 - loss: 0.3543 - val_accuracy: 0.8728 - val_loss: 0.3337 - mcc: 0.8038\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8740 - loss: 0.3310(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8156\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.8740 - loss: 0.3310 - val_accuracy: 0.8804 - val_loss: 0.3095 - mcc: 0.8156\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8749 - loss: 0.3253(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8189\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8749 - loss: 0.3253 - val_accuracy: 0.8822 - val_loss: 0.3026 - mcc: 0.8189\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8814 - loss: 0.3037(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8287\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.8814 - loss: 0.3037 - val_accuracy: 0.8887 - val_loss: 0.2816 - mcc: 0.8287\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8837 - loss: 0.2964(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8268\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.8837 - loss: 0.2964 - val_accuracy: 0.8875 - val_loss: 0.2840 - mcc: 0.8268\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8841 - loss: 0.2961(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8295\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8841 - loss: 0.2961 - val_accuracy: 0.8892 - val_loss: 0.2835 - mcc: 0.8295\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8878 - loss: 0.2840(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8373\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8879 - loss: 0.2840 - val_accuracy: 0.8939 - val_loss: 0.2663 - mcc: 0.8373\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8892 - loss: 0.2780(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8384\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8892 - loss: 0.2780 - val_accuracy: 0.8946 - val_loss: 0.2637 - mcc: 0.8384\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8894 - loss: 0.2779(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8347\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8894 - loss: 0.2779 - val_accuracy: 0.8925 - val_loss: 0.2670 - mcc: 0.8347\n","Epoch 12/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8913 - loss: 0.2707(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8400\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8913 - loss: 0.2707 - val_accuracy: 0.8958 - val_loss: 0.2572 - mcc: 0.8400\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8955 - loss: 0.2590(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8414\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8955 - loss: 0.2590 - val_accuracy: 0.8969 - val_loss: 0.2535 - mcc: 0.8414\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8949 - loss: 0.2598(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8431\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.8949 - loss: 0.2598 - val_accuracy: 0.8980 - val_loss: 0.2517 - mcc: 0.8431\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8980 - loss: 0.2512(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8463\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.8980 - loss: 0.2512 - val_accuracy: 0.8999 - val_loss: 0.2451 - mcc: 0.8463\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8983 - loss: 0.2502(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8469\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8983 - loss: 0.2502 - val_accuracy: 0.9004 - val_loss: 0.2444 - mcc: 0.8469\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8983 - loss: 0.2507(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8470\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.8983 - loss: 0.2507 - val_accuracy: 0.9003 - val_loss: 0.2460 - mcc: 0.8470\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9012 - loss: 0.2439(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8475\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.9012 - loss: 0.2439 - val_accuracy: 0.9006 - val_loss: 0.2440 - mcc: 0.8475\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9010 - loss: 0.2433(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8505\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9010 - loss: 0.2433 - val_accuracy: 0.9028 - val_loss: 0.2398 - mcc: 0.8505\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9012 - loss: 0.2434(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8471\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9012 - loss: 0.2434 - val_accuracy: 0.9005 - val_loss: 0.2446 - mcc: 0.8471\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 3\n","Epoch 1/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7078 - loss: 0.7685(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7732\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.7081 - loss: 0.7676 - val_accuracy: 0.8531 - val_loss: 0.3924 - mcc: 0.7732\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8563 - loss: 0.3819(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7890\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 32ms/step - accuracy: 0.8563 - loss: 0.3818 - val_accuracy: 0.8633 - val_loss: 0.3646 - mcc: 0.7890\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8689 - loss: 0.3456(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.7939\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.8689 - loss: 0.3456 - val_accuracy: 0.8664 - val_loss: 0.3578 - mcc: 0.7939\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8717 - loss: 0.3416(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8081\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8717 - loss: 0.3416 - val_accuracy: 0.8751 - val_loss: 0.3270 - mcc: 0.8081\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8724 - loss: 0.3338(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8000\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.8724 - loss: 0.3338 - val_accuracy: 0.8698 - val_loss: 0.3439 - mcc: 0.8000\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8736 - loss: 0.3349(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8180\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 34ms/step - accuracy: 0.8737 - loss: 0.3349 - val_accuracy: 0.8817 - val_loss: 0.3072 - mcc: 0.8180\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8769 - loss: 0.3234(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8207\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8769 - loss: 0.3234 - val_accuracy: 0.8828 - val_loss: 0.3067 - mcc: 0.8207\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8794 - loss: 0.3171(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8189\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8794 - loss: 0.3171 - val_accuracy: 0.8821 - val_loss: 0.2994 - mcc: 0.8189\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8860 - loss: 0.2901(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8312\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8860 - loss: 0.2901 - val_accuracy: 0.8897 - val_loss: 0.2772 - mcc: 0.8312\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8799 - loss: 0.3107(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8220\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8799 - loss: 0.3107 - val_accuracy: 0.8840 - val_loss: 0.2969 - mcc: 0.8220\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8846 - loss: 0.2936(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8276\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.8846 - loss: 0.2936 - val_accuracy: 0.8877 - val_loss: 0.2814 - mcc: 0.8276\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8886 - loss: 0.2814(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8279\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8886 - loss: 0.2814 - val_accuracy: 0.8879 - val_loss: 0.2817 - mcc: 0.8279\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8901 - loss: 0.2746(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8331\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8901 - loss: 0.2746 - val_accuracy: 0.8911 - val_loss: 0.2729 - mcc: 0.8331\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8927 - loss: 0.2676(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.7574\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.8927 - loss: 0.2676 - val_accuracy: 0.8431 - val_loss: 0.4010 - mcc: 0.7574\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8737 - loss: 0.3223(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8356\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8738 - loss: 0.3223 - val_accuracy: 0.8928 - val_loss: 0.2661 - mcc: 0.8356\n","Epoch 16/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8884 - loss: 0.2797(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8370\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8884 - loss: 0.2796 - val_accuracy: 0.8933 - val_loss: 0.2638 - mcc: 0.8370\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8961 - loss: 0.2588(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8409\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8961 - loss: 0.2588 - val_accuracy: 0.8962 - val_loss: 0.2558 - mcc: 0.8409\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8963 - loss: 0.2568(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8397\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8963 - loss: 0.2568 - val_accuracy: 0.8950 - val_loss: 0.2590 - mcc: 0.8397\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8974 - loss: 0.2545(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8403\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.8974 - loss: 0.2545 - val_accuracy: 0.8957 - val_loss: 0.2612 - mcc: 0.8403\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8977 - loss: 0.2519(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8419\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 37ms/step - accuracy: 0.8977 - loss: 0.2519 - val_accuracy: 0.8966 - val_loss: 0.2525 - mcc: 0.8419\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 4\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7145 - loss: 0.7497(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7803\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.7147 - loss: 0.7491 - val_accuracy: 0.8580 - val_loss: 0.3728 - mcc: 0.7803\n","Epoch 2/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8586 - loss: 0.3673(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7990\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8587 - loss: 0.3673 - val_accuracy: 0.8690 - val_loss: 0.3403 - mcc: 0.7990\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8740 - loss: 0.3230(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8086\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 36ms/step - accuracy: 0.8740 - loss: 0.3229 - val_accuracy: 0.8762 - val_loss: 0.3180 - mcc: 0.8086\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8809 - loss: 0.3037(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8155\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 33ms/step - accuracy: 0.8809 - loss: 0.3037 - val_accuracy: 0.8798 - val_loss: 0.3035 - mcc: 0.8155\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8827 - loss: 0.2977(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8165\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.8827 - loss: 0.2977 - val_accuracy: 0.8810 - val_loss: 0.3089 - mcc: 0.8165\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8876 - loss: 0.2845(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8177\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.8876 - loss: 0.2845 - val_accuracy: 0.8819 - val_loss: 0.2980 - mcc: 0.8177\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8897 - loss: 0.2770(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8302\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 36ms/step - accuracy: 0.8897 - loss: 0.2770 - val_accuracy: 0.8900 - val_loss: 0.2756 - mcc: 0.8302\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8924 - loss: 0.2680(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8335\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.8924 - loss: 0.2680 - val_accuracy: 0.8917 - val_loss: 0.2705 - mcc: 0.8335\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8914 - loss: 0.2700(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8345\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.8914 - loss: 0.2700 - val_accuracy: 0.8928 - val_loss: 0.2684 - mcc: 0.8345\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8941 - loss: 0.2633(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8336\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 32ms/step - accuracy: 0.8941 - loss: 0.2633 - val_accuracy: 0.8922 - val_loss: 0.2717 - mcc: 0.8336\n","Epoch 11/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8956 - loss: 0.2581(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8289\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8956 - loss: 0.2581 - val_accuracy: 0.8886 - val_loss: 0.2756 - mcc: 0.8289\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8937 - loss: 0.2623(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8369\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8937 - loss: 0.2623 - val_accuracy: 0.8938 - val_loss: 0.2663 - mcc: 0.8369\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8980 - loss: 0.2521(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8405\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8980 - loss: 0.2521 - val_accuracy: 0.8959 - val_loss: 0.2576 - mcc: 0.8405\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8978 - loss: 0.2532(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8393\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8978 - loss: 0.2532 - val_accuracy: 0.8957 - val_loss: 0.2620 - mcc: 0.8393\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9009 - loss: 0.2457(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8412\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9009 - loss: 0.2457 - val_accuracy: 0.8965 - val_loss: 0.2558 - mcc: 0.8412\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8998 - loss: 0.2465(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8406\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8998 - loss: 0.2465 - val_accuracy: 0.8965 - val_loss: 0.2579 - mcc: 0.8406\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9002 - loss: 0.2463(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8419\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9002 - loss: 0.2463 - val_accuracy: 0.8962 - val_loss: 0.2575 - mcc: 0.8419\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8995 - loss: 0.2464(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8424\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8995 - loss: 0.2464 - val_accuracy: 0.8977 - val_loss: 0.2547 - mcc: 0.8424\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9014 - loss: 0.2438(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8446\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9014 - loss: 0.2438 - val_accuracy: 0.8989 - val_loss: 0.2498 - mcc: 0.8446\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9013 - loss: 0.2414(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8457\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9013 - loss: 0.2414 - val_accuracy: 0.8998 - val_loss: 0.2479 - mcc: 0.8457\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 5\n","Epoch 1/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7099 - loss: 0.7675(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7434\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 34ms/step - accuracy: 0.7102 - loss: 0.7667 - val_accuracy: 0.8338 - val_loss: 0.4307 - mcc: 0.7434\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8545 - loss: 0.3799(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7943\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.8545 - loss: 0.3799 - val_accuracy: 0.8668 - val_loss: 0.3413 - mcc: 0.7943\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8703 - loss: 0.3370(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8100\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 36ms/step - accuracy: 0.8703 - loss: 0.3370 - val_accuracy: 0.8756 - val_loss: 0.3156 - mcc: 0.8100\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8753 - loss: 0.3199(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8171\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 32ms/step - accuracy: 0.8753 - loss: 0.3198 - val_accuracy: 0.8808 - val_loss: 0.3008 - mcc: 0.8171\n","Epoch 5/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8849 - loss: 0.2918(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8177\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8849 - loss: 0.2918 - val_accuracy: 0.8813 - val_loss: 0.3018 - mcc: 0.8177\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8772 - loss: 0.3164(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8257\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8772 - loss: 0.3163 - val_accuracy: 0.8861 - val_loss: 0.2840 - mcc: 0.8257\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8883 - loss: 0.2783(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8310\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.8883 - loss: 0.2783 - val_accuracy: 0.8898 - val_loss: 0.2756 - mcc: 0.8310\n","Epoch 8/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8922 - loss: 0.2686(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8303\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.8921 - loss: 0.2686 - val_accuracy: 0.8895 - val_loss: 0.2738 - mcc: 0.8303\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8922 - loss: 0.2684(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8284\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8922 - loss: 0.2684 - val_accuracy: 0.8883 - val_loss: 0.2790 - mcc: 0.8284\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8948 - loss: 0.2621(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8372\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.8948 - loss: 0.2621 - val_accuracy: 0.8937 - val_loss: 0.2626 - mcc: 0.8372\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8962 - loss: 0.2569(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8282\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8962 - loss: 0.2569 - val_accuracy: 0.8881 - val_loss: 0.2816 - mcc: 0.8282\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8975 - loss: 0.2531(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8345\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8975 - loss: 0.2531 - val_accuracy: 0.8919 - val_loss: 0.2668 - mcc: 0.8345\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8989 - loss: 0.2502(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8378\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8989 - loss: 0.2502 - val_accuracy: 0.8943 - val_loss: 0.2585 - mcc: 0.8378\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9003 - loss: 0.2456(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8383\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.9003 - loss: 0.2456 - val_accuracy: 0.8942 - val_loss: 0.2579 - mcc: 0.8383\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8968 - loss: 0.2541(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8404\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.8969 - loss: 0.2541 - val_accuracy: 0.8960 - val_loss: 0.2558 - mcc: 0.8404\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9013 - loss: 0.2429(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8414\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.9013 - loss: 0.2429 - val_accuracy: 0.8965 - val_loss: 0.2540 - mcc: 0.8414\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9007 - loss: 0.2427(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8410\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9007 - loss: 0.2427 - val_accuracy: 0.8964 - val_loss: 0.2556 - mcc: 0.8410\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9016 - loss: 0.2429(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8440\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9016 - loss: 0.2429 - val_accuracy: 0.8982 - val_loss: 0.2520 - mcc: 0.8440\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9012 - loss: 0.2429(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8458\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9012 - loss: 0.2429 - val_accuracy: 0.8993 - val_loss: 0.2484 - mcc: 0.8458\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9034 - loss: 0.2367(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8421\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9034 - loss: 0.2367 - val_accuracy: 0.8966 - val_loss: 0.2527 - mcc: 0.8421\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.903032),\n","              'mean': np.float64(0.8993133333333333),\n","              'min': np.float64(0.896602),\n","              'std': np.float64(0.0024445525107425438)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0005166980425516764),\n","                               'mean': np.float64(0.0004631172339121501),\n","                               'min': np.float64(0.0004432125886281331),\n","                               'std': np.float64(2.7394833278397816e-05)},\n"," 'MCC': {'max': np.float64(0.8514005340549287),\n","         'mean': np.float64(0.8456326700908917),\n","         'min': np.float64(0.8419199381047279),\n","         'std': np.float64(0.00351488562794376)},\n"," 'Parameters': 30949,\n"," 'Train Time (s)': {'max': np.float64(756.3495981693268),\n","                    'mean': np.float64(719.3355206966401),\n","                    'min': np.float64(683.413804769516),\n","                    'std': np.float64(23.934254409146973)},\n"," 'Training Accuracy': [[0.7935581803321838,\n","                        0.852222740650177,\n","                        0.866355299949646,\n","                        0.873859703540802,\n","                        0.8781045079231262,\n","                        0.8811893463134766,\n","                        0.8841748237609863,\n","                        0.8874944448471069,\n","                        0.8897119760513306,\n","                        0.8923045992851257,\n","                        0.8940964341163635,\n","                        0.8943195939064026,\n","                        0.8960332870483398,\n","                        0.8966940641403198,\n","                        0.8976768255233765,\n","                        0.8978031277656555,\n","                        0.8992124199867249,\n","                        0.9001134634017944,\n","                        0.9011133909225464,\n","                        0.9015331864356995],\n","                       [0.785601794719696,\n","                        0.857252836227417,\n","                        0.8654369115829468,\n","                        0.8747150897979736,\n","                        0.8768145442008972,\n","                        0.8824777603149414,\n","                        0.8837856650352478,\n","                        0.8844060301780701,\n","                        0.8880646228790283,\n","                        0.8897202014923096,\n","                        0.8898273706436157,\n","                        0.8926641345024109,\n","                        0.8945512771606445,\n","                        0.8956942558288574,\n","                        0.8971685171127319,\n","                        0.8977358341217041,\n","                        0.8982148170471191,\n","                        0.8996588587760925,\n","                        0.9005327224731445,\n","                        0.9008324146270752],\n","                       [0.7897675633430481,\n","                        0.8613345623016357,\n","                        0.8659839034080505,\n","                        0.8728058338165283,\n","                        0.8742693066596985,\n","                        0.8759754300117493,\n","                        0.8784891963005066,\n","                        0.8779955506324768,\n","                        0.8879424929618835,\n","                        0.879845142364502,\n","                        0.8880654573440552,\n","                        0.888143002986908,\n","                        0.8914185166358948,\n","                        0.8915039300918579,\n","                        0.88323575258255,\n","                        0.8920702934265137,\n","                        0.8950368762016296,\n","                        0.8956055641174316,\n","                        0.8970559239387512,\n","                        0.8975273966789246],\n","                       [0.7934730648994446,\n","                        0.8618865609169006,\n","                        0.8761376738548279,\n","                        0.880846381187439,\n","                        0.8849349617958069,\n","                        0.8869615793228149,\n","                        0.8894245028495789,\n","                        0.891657292842865,\n","                        0.891887366771698,\n","                        0.8934903144836426,\n","                        0.8954803347587585,\n","                        0.8960760831832886,\n","                        0.8974406123161316,\n","                        0.8979946374893188,\n","                        0.8993088603019714,\n","                        0.8995976448059082,\n","                        0.900038480758667,\n","                        0.9007454514503479,\n","                        0.9014815092086792,\n","                        0.9017011523246765],\n","                       [0.7851971983909607,\n","                        0.8594295382499695,\n","                        0.8721278309822083,\n","                        0.8798006176948547,\n","                        0.8850973844528198,\n","                        0.8841267228126526,\n","                        0.8895241022109985,\n","                        0.8917713165283203,\n","                        0.8929492831230164,\n","                        0.8949388265609741,\n","                        0.8959081768989563,\n","                        0.8966859579086304,\n","                        0.8981906175613403,\n","                        0.8990203738212585,\n","                        0.8984413146972656,\n","                        0.9007261395454407,\n","                        0.9010763168334961,\n","                        0.9017540812492371,\n","                        0.9016004204750061,\n","                        0.9034396409988403]],\n"," 'Training Loss': [[0.5405887961387634,\n","                    0.39239946007728577,\n","                    0.3513753116130829,\n","                    0.32847803831100464,\n","                    0.31509116291999817,\n","                    0.30565980076789856,\n","                    0.2960509657859802,\n","                    0.28491175174713135,\n","                    0.27608925104141235,\n","                    0.26741471886634827,\n","                    0.2633991539478302,\n","                    0.26107195019721985,\n","                    0.2572210431098938,\n","                    0.2537217140197754,\n","                    0.2517821192741394,\n","                    0.2513684332370758,\n","                    0.24734652042388916,\n","                    0.24490870535373688,\n","                    0.24253962934017181,\n","                    0.24157586693763733],\n","                   [0.568466305732727,\n","                    0.37595638632774353,\n","                    0.35738465189933777,\n","                    0.3280731737613678,\n","                    0.31843140721321106,\n","                    0.3007902204990387,\n","                    0.29620587825775146,\n","                    0.2947215735912323,\n","                    0.2832450866699219,\n","                    0.27737095952033997,\n","                    0.27586260437965393,\n","                    0.26760122179985046,\n","                    0.261380672454834,\n","                    0.2578094005584717,\n","                    0.2537890076637268,\n","                    0.2520297169685364,\n","                    0.2505626976490021,\n","                    0.24624766409397125,\n","                    0.2441355437040329,\n","                    0.24341332912445068],\n","                   [0.5521079301834106,\n","                    0.3693428635597229,\n","                    0.35634636878967285,\n","                    0.3380656838417053,\n","                    0.331229031085968,\n","                    0.3265896141529083,\n","                    0.31915047764778137,\n","                    0.3203340470790863,\n","                    0.2852901518344879,\n","                    0.31068167090415955,\n","                    0.284242182970047,\n","                    0.2833079993724823,\n","                    0.2717008590698242,\n","                    0.27141234278678894,\n","                    0.29549944400787354,\n","                    0.269535094499588,\n","                    0.26084011793136597,\n","                    0.2577040493488312,\n","                    0.2545100152492523,\n","                    0.25245431065559387],\n","                   [0.5406826138496399,\n","                    0.35799047350883484,\n","                    0.31632304191589355,\n","                    0.3030277192592621,\n","                    0.29109618067741394,\n","                    0.28538551926612854,\n","                    0.2774544060230255,\n","                    0.2706763446331024,\n","                    0.26909682154655457,\n","                    0.26456961035728455,\n","                    0.2588786482810974,\n","                    0.25706201791763306,\n","                    0.25365278124809265,\n","                    0.25244465470314026,\n","                    0.24890215694904327,\n","                    0.24761350452899933,\n","                    0.2464115470647812,\n","                    0.2443607598543167,\n","                    0.24270139634609222,\n","                    0.24127772450447083],\n","                   [0.5640451908111572,\n","                    0.3663053512573242,\n","                    0.3308504521846771,\n","                    0.3064616918563843,\n","                    0.29001080989837646,\n","                    0.29363906383514404,\n","                    0.2757044732570648,\n","                    0.2695872187614441,\n","                    0.2667458653450012,\n","                    0.2608519196510315,\n","                    0.2569555938243866,\n","                    0.25580692291259766,\n","                    0.2518053650856018,\n","                    0.24874456226825714,\n","                    0.2500498592853546,\n","                    0.24349330365657806,\n","                    0.2423418164253235,\n","                    0.24139060080051422,\n","                    0.24156072735786438,\n","                    0.23675446212291718]],\n"," 'Validation Accuracy': [[0.849503755569458,\n","                          0.8609142899513245,\n","                          0.8722354173660278,\n","                          0.8803130984306335,\n","                          0.8844226002693176,\n","                          0.8829256296157837,\n","                          0.8891218304634094,\n","                          0.8879551887512207,\n","                          0.8948453068733215,\n","                          0.8958584666252136,\n","                          0.8973195552825928,\n","                          0.8999537229537964,\n","                          0.8992912769317627,\n","                          0.8999892473220825,\n","                          0.8995698094367981,\n","                          0.9005858302116394,\n","                          0.9016269445419312,\n","                          0.9024970531463623,\n","                          0.8999346494674683,\n","                          0.9030320644378662],\n","                         [0.8512731194496155,\n","                          0.866187334060669,\n","                          0.8728253841400146,\n","                          0.8804452419281006,\n","                          0.8821502327919006,\n","                          0.8886672258377075,\n","                          0.8874781131744385,\n","                          0.8892006278038025,\n","                          0.8939001560211182,\n","                          0.8946136832237244,\n","                          0.8925266861915588,\n","                          0.8957819938659668,\n","                          0.896872341632843,\n","                          0.897953450679779,\n","                          0.8998515009880066,\n","                          0.9004042744636536,\n","                          0.9003424644470215,\n","                          0.9005706310272217,\n","                          0.9027809500694275,\n","                          0.9004746675491333],\n","                         [0.8530629277229309,\n","                          0.8632636666297913,\n","                          0.8664286136627197,\n","                          0.8750548362731934,\n","                          0.8697545528411865,\n","                          0.881708562374115,\n","                          0.8827703595161438,\n","                          0.882120668888092,\n","                          0.8897479772567749,\n","                          0.8840205669403076,\n","                          0.8876991271972656,\n","                          0.8878604173660278,\n","                          0.8911498785018921,\n","                          0.8430505990982056,\n","                          0.8927509784698486,\n","                          0.8933392763137817,\n","                          0.8961991667747498,\n","                          0.8950240612030029,\n","                          0.8957277536392212,\n","                          0.8966479301452637],\n","                         [0.8580053448677063,\n","                          0.8689695596694946,\n","                          0.8761512041091919,\n","                          0.8797607421875,\n","                          0.8810478448867798,\n","                          0.881876528263092,\n","                          0.8899618983268738,\n","                          0.8917080760002136,\n","                          0.8928211331367493,\n","                          0.8921706676483154,\n","                          0.8886074423789978,\n","                          0.8938326239585876,\n","                          0.8958698511123657,\n","                          0.8956958055496216,\n","                          0.896540105342865,\n","                          0.8964666128158569,\n","                          0.8962257504463196,\n","                          0.8977157473564148,\n","                          0.8988872170448303,\n","                          0.8998100757598877],\n","                         [0.833781898021698,\n","                          0.8667680025100708,\n","                          0.8756394982337952,\n","                          0.8808013796806335,\n","                          0.8813187479972839,\n","                          0.886087954044342,\n","                          0.889832615852356,\n","                          0.889546811580658,\n","                          0.8883150815963745,\n","                          0.8936634659767151,\n","                          0.8880899548530579,\n","                          0.8919491171836853,\n","                          0.8942933678627014,\n","                          0.8941650390625,\n","                          0.8960443735122681,\n","                          0.8965006470680237,\n","                          0.8963950872421265,\n","                          0.8981751203536987,\n","                          0.8993152379989624,\n","                          0.896602213382721]],\n"," 'Validation Loss': [[0.40524014830589294,\n","                      0.36764222383499146,\n","                      0.33815544843673706,\n","                      0.3112659454345703,\n","                      0.3001503050327301,\n","                      0.29990866780281067,\n","                      0.28549739718437195,\n","                      0.28596505522727966,\n","                      0.26407721638679504,\n","                      0.2611526548862457,\n","                      0.255562961101532,\n","                      0.2503890097141266,\n","                      0.2504946291446686,\n","                      0.24858014285564423,\n","                      0.2497337907552719,\n","                      0.24730998277664185,\n","                      0.24203403294086456,\n","                      0.24115636944770813,\n","                      0.2490057796239853,\n","                      0.238486647605896],\n","                     [0.3899741470813751,\n","                      0.3556326925754547,\n","                      0.3336627781391144,\n","                      0.3094968795776367,\n","                      0.3026377856731415,\n","                      0.28155991435050964,\n","                      0.28400030732154846,\n","                      0.2834987938404083,\n","                      0.2662859261035919,\n","                      0.2637016475200653,\n","                      0.26696062088012695,\n","                      0.25716495513916016,\n","                      0.2535392940044403,\n","                      0.25174498558044434,\n","                      0.24513669312000275,\n","                      0.24438756704330444,\n","                      0.24597719311714172,\n","                      0.24396124482154846,\n","                      0.23976415395736694,\n","                      0.24461831152439117],\n","                     [0.3923649489879608,\n","                      0.36463361978530884,\n","                      0.3578121066093445,\n","                      0.32701805233955383,\n","                      0.3438916802406311,\n","                      0.30718472599983215,\n","                      0.3066509962081909,\n","                      0.2994389235973358,\n","                      0.27723613381385803,\n","                      0.2968582510948181,\n","                      0.28138017654418945,\n","                      0.2817439138889313,\n","                      0.27287527918815613,\n","                      0.4010487496852875,\n","                      0.26608040928840637,\n","                      0.26377037167549133,\n","                      0.2558322250843048,\n","                      0.2589810788631439,\n","                      0.26116466522216797,\n","                      0.2525104582309723],\n","                     [0.372775137424469,\n","                      0.3402799367904663,\n","                      0.3180041015148163,\n","                      0.3035254180431366,\n","                      0.3089083433151245,\n","                      0.29799655079841614,\n","                      0.2755899727344513,\n","                      0.27046534419059753,\n","                      0.2684122622013092,\n","                      0.27172598242759705,\n","                      0.27559053897857666,\n","                      0.2663119435310364,\n","                      0.25760725140571594,\n","                      0.2619991898536682,\n","                      0.2557825446128845,\n","                      0.257941335439682,\n","                      0.25752589106559753,\n","                      0.2546936571598053,\n","                      0.24979302287101746,\n","                      0.24793848395347595],\n","                     [0.4306749403476715,\n","                      0.3412722945213318,\n","                      0.31562814116477966,\n","                      0.30078238248825073,\n","                      0.3017650842666626,\n","                      0.2839692234992981,\n","                      0.27559638023376465,\n","                      0.27375930547714233,\n","                      0.27901917695999146,\n","                      0.2626242935657501,\n","                      0.2815856635570526,\n","                      0.2667850852012634,\n","                      0.2584768831729889,\n","                      0.2579236626625061,\n","                      0.2558234930038452,\n","                      0.2539941966533661,\n","                      0.2555844187736511,\n","                      0.2520400881767273,\n","                      0.2484487146139145,\n","                      0.2527221739292145]],\n"," 'Validation MCC': [[np.float64(0.7679517066924112),\n","                     np.float64(0.7852445756250388),\n","                     np.float64(0.8043716424016335),\n","                     np.float64(0.8163877607632728),\n","                     np.float64(0.822878845733174),\n","                     np.float64(0.8201238972412234),\n","                     np.float64(0.8294818337994417),\n","                     np.float64(0.8280333189597017),\n","                     np.float64(0.8384612809563716),\n","                     np.float64(0.8400666022714995),\n","                     np.float64(0.8425169422417795),\n","                     np.float64(0.8470627369483831),\n","                     np.float64(0.8455268608938604),\n","                     np.float64(0.8467667640284209),\n","                     np.float64(0.8459987586932852),\n","                     np.float64(0.8476152385868836),\n","                     np.float64(0.8495253634933427),\n","                     np.float64(0.8509504313642443),\n","                     np.float64(0.8468252330514112),\n","                     np.float64(0.8514005340549287)],\n","                    [np.float64(0.7698958549091185),\n","                     np.float64(0.7931007461939463),\n","                     np.float64(0.8037917596781983),\n","                     np.float64(0.815572342937074),\n","                     np.float64(0.8189157900893529),\n","                     np.float64(0.8287293789792534),\n","                     np.float64(0.8268494482067934),\n","                     np.float64(0.8295143051549881),\n","                     np.float64(0.8372507099504412),\n","                     np.float64(0.8383695997017205),\n","                     np.float64(0.8347012820508141),\n","                     np.float64(0.8399683874834396),\n","                     np.float64(0.8413574586402499),\n","                     np.float64(0.8430635699478528),\n","                     np.float64(0.8462566475364716),\n","                     np.float64(0.8468987480445189),\n","                     np.float64(0.8470199988510536),\n","                     np.float64(0.847493165726131),\n","                     np.float64(0.8505289424565433),\n","                     np.float64(0.8470921549450915)],\n","                    [np.float64(0.7731614805818497),\n","                     np.float64(0.7890201305926026),\n","                     np.float64(0.7939134203950242),\n","                     np.float64(0.8081375338077543),\n","                     np.float64(0.8000358232918738),\n","                     np.float64(0.8180068471011064),\n","                     np.float64(0.8207010678801586),\n","                     np.float64(0.8188907615057085),\n","                     np.float64(0.8311708727898067),\n","                     np.float64(0.821952782716772),\n","                     np.float64(0.8275727159719837),\n","                     np.float64(0.8278906384793058),\n","                     np.float64(0.8331240571498576),\n","                     np.float64(0.7573784742672237),\n","                     np.float64(0.835557692672431),\n","                     np.float64(0.8370069999086863),\n","                     np.float64(0.8408763049170362),\n","                     np.float64(0.8397208764098993),\n","                     np.float64(0.8402678429903088),\n","                     np.float64(0.8419199381047279)],\n","                    [np.float64(0.7802535331281191),\n","                     np.float64(0.7990410631027909),\n","                     np.float64(0.8086025831702852),\n","                     np.float64(0.8154889708330181),\n","                     np.float64(0.8164642751131037),\n","                     np.float64(0.8176848896100148),\n","                     np.float64(0.830208868858523),\n","                     np.float64(0.8335257333154167),\n","                     np.float64(0.8345247013279985),\n","                     np.float64(0.8335703590938791),\n","                     np.float64(0.828917555715679),\n","                     np.float64(0.8368704301483094),\n","                     np.float64(0.84053741355369),\n","                     np.float64(0.8393291329858013),\n","                     np.float64(0.841165942115301),\n","                     np.float64(0.8405541672011473),\n","                     np.float64(0.8418889515344549),\n","                     np.float64(0.8424045025068879),\n","                     np.float64(0.8446158072184032),\n","                     np.float64(0.8456660472420513)],\n","                    [np.float64(0.7433811728043749),\n","                     np.float64(0.7943457557283052),\n","                     np.float64(0.8099505612045964),\n","                     np.float64(0.8171301864612047),\n","                     np.float64(0.8176955562974095),\n","                     np.float64(0.8257257110965691),\n","                     np.float64(0.8309706382889249),\n","                     np.float64(0.830316463673036),\n","                     np.float64(0.8284157629868498),\n","                     np.float64(0.8371925891627906),\n","                     np.float64(0.828217394962158),\n","                     np.float64(0.8344954296678325),\n","                     np.float64(0.8377628407446029),\n","                     np.float64(0.8383289130663797),\n","                     np.float64(0.8403502408938888),\n","                     np.float64(0.8413834041724267),\n","                     np.float64(0.8409573953922989),\n","                     np.float64(0.8439666052614182),\n","                     np.float64(0.8457972846741101),\n","                     np.float64(0.8420846761076598)]]}\n","Training Model: BiLSTM, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7100 - loss: 0.7584(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7996\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.7102 - loss: 0.7578 - val_accuracy: 0.8702 - val_loss: 0.3426 - mcc: 0.7996\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8734 - loss: 0.3399(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8374\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8734 - loss: 0.3399 - val_accuracy: 0.8940 - val_loss: 0.2793 - mcc: 0.8374\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8874 - loss: 0.2979(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8490\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8874 - loss: 0.2979 - val_accuracy: 0.9009 - val_loss: 0.2583 - mcc: 0.8490\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8981 - loss: 0.2689(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8532\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8981 - loss: 0.2689 - val_accuracy: 0.9039 - val_loss: 0.2526 - mcc: 0.8532\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9029 - loss: 0.2548(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8630\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.9029 - loss: 0.2548 - val_accuracy: 0.9100 - val_loss: 0.2319 - mcc: 0.8630\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9078 - loss: 0.2407(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8690\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9078 - loss: 0.2407 - val_accuracy: 0.9143 - val_loss: 0.2221 - mcc: 0.8690\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9093 - loss: 0.2360(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8616\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9093 - loss: 0.2360 - val_accuracy: 0.9095 - val_loss: 0.2426 - mcc: 0.8616\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9074 - loss: 0.2463(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8661\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9073 - loss: 0.2463 - val_accuracy: 0.9124 - val_loss: 0.2278 - mcc: 0.8661\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9098 - loss: 0.2359(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.7644\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9098 - loss: 0.2359 - val_accuracy: 0.8467 - val_loss: 0.5485 - mcc: 0.7644\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8975 - loss: 0.2761(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8753\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8975 - loss: 0.2760 - val_accuracy: 0.9183 - val_loss: 0.2117 - mcc: 0.8753\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9153 - loss: 0.2178(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8749\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 37ms/step - accuracy: 0.9153 - loss: 0.2178 - val_accuracy: 0.9177 - val_loss: 0.2113 - mcc: 0.8749\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9152 - loss: 0.2184(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8711\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9152 - loss: 0.2184 - val_accuracy: 0.9156 - val_loss: 0.2194 - mcc: 0.8711\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9173 - loss: 0.2108(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8780\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9173 - loss: 0.2108 - val_accuracy: 0.9201 - val_loss: 0.2038 - mcc: 0.8780\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9187 - loss: 0.2059(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8805\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9187 - loss: 0.2059 - val_accuracy: 0.9219 - val_loss: 0.2016 - mcc: 0.8805\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9213 - loss: 0.2039(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8849\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9213 - loss: 0.2039 - val_accuracy: 0.9245 - val_loss: 0.1916 - mcc: 0.8849\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9204 - loss: 0.2030(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8824\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9204 - loss: 0.2030 - val_accuracy: 0.9230 - val_loss: 0.1974 - mcc: 0.8824\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9213 - loss: 0.2012(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8825\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9213 - loss: 0.2012 - val_accuracy: 0.9227 - val_loss: 0.1996 - mcc: 0.8825\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9207 - loss: 0.2046(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8895\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9207 - loss: 0.2046 - val_accuracy: 0.9274 - val_loss: 0.1849 - mcc: 0.8895\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9248 - loss: 0.1908(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8919\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9248 - loss: 0.1908 - val_accuracy: 0.9290 - val_loss: 0.1797 - mcc: 0.8919\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9250 - loss: 0.1908(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8809\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9250 - loss: 0.1908 - val_accuracy: 0.9217 - val_loss: 0.1988 - mcc: 0.8809\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7024 - loss: 0.7679(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8008\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 34ms/step - accuracy: 0.7025 - loss: 0.7676 - val_accuracy: 0.8712 - val_loss: 0.3501 - mcc: 0.8008\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8774 - loss: 0.3331(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8330\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8774 - loss: 0.3331 - val_accuracy: 0.8916 - val_loss: 0.2970 - mcc: 0.8330\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8864 - loss: 0.3108(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8373\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.8864 - loss: 0.3108 - val_accuracy: 0.8942 - val_loss: 0.2835 - mcc: 0.8373\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8925 - loss: 0.2866(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8451\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8925 - loss: 0.2866 - val_accuracy: 0.8994 - val_loss: 0.2706 - mcc: 0.8451\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9000 - loss: 0.2632(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8349\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9000 - loss: 0.2632 - val_accuracy: 0.8925 - val_loss: 0.2974 - mcc: 0.8349\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8946 - loss: 0.2835(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8577\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8946 - loss: 0.2835 - val_accuracy: 0.9074 - val_loss: 0.2459 - mcc: 0.8577\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9052 - loss: 0.2520(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8600\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.9052 - loss: 0.2520 - val_accuracy: 0.9088 - val_loss: 0.2421 - mcc: 0.8600\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9017 - loss: 0.2639(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.7829\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9017 - loss: 0.2638 - val_accuracy: 0.8550 - val_loss: 0.4408 - mcc: 0.7829\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8985 - loss: 0.2741(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8533\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8985 - loss: 0.2741 - val_accuracy: 0.9047 - val_loss: 0.2550 - mcc: 0.8533\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9083 - loss: 0.2413(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8619\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9083 - loss: 0.2413 - val_accuracy: 0.9098 - val_loss: 0.2388 - mcc: 0.8619\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9060 - loss: 0.2467(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8591\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9060 - loss: 0.2467 - val_accuracy: 0.9081 - val_loss: 0.2390 - mcc: 0.8591\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9051 - loss: 0.2496(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8643\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9052 - loss: 0.2496 - val_accuracy: 0.9118 - val_loss: 0.2310 - mcc: 0.8643\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9110 - loss: 0.2319(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8550\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9110 - loss: 0.2319 - val_accuracy: 0.9057 - val_loss: 0.2537 - mcc: 0.8550\n","Epoch 14/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9112 - loss: 0.2305(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8736\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.9113 - loss: 0.2305 - val_accuracy: 0.9174 - val_loss: 0.2130 - mcc: 0.8736\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9179 - loss: 0.2103(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8760\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.9179 - loss: 0.2103 - val_accuracy: 0.9185 - val_loss: 0.2098 - mcc: 0.8760\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9163 - loss: 0.2149(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8774\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.9163 - loss: 0.2149 - val_accuracy: 0.9202 - val_loss: 0.2051 - mcc: 0.8774\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9168 - loss: 0.2148(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8769\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9168 - loss: 0.2148 - val_accuracy: 0.9196 - val_loss: 0.2078 - mcc: 0.8769\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9203 - loss: 0.2048(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8848\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9203 - loss: 0.2048 - val_accuracy: 0.9247 - val_loss: 0.1916 - mcc: 0.8848\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9195 - loss: 0.2068(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8766\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9195 - loss: 0.2069 - val_accuracy: 0.9196 - val_loss: 0.2060 - mcc: 0.8766\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9167 - loss: 0.2136(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8824\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9167 - loss: 0.2136 - val_accuracy: 0.9232 - val_loss: 0.1954 - mcc: 0.8824\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6991 - loss: 0.7799(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7759\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.6993 - loss: 0.7793 - val_accuracy: 0.8546 - val_loss: 0.3932 - mcc: 0.7759\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8646 - loss: 0.3648(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8031\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8646 - loss: 0.3648 - val_accuracy: 0.8722 - val_loss: 0.3504 - mcc: 0.8031\n","Epoch 3/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8770 - loss: 0.3338(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8118\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.8770 - loss: 0.3338 - val_accuracy: 0.8778 - val_loss: 0.3343 - mcc: 0.8118\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8851 - loss: 0.3110(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8413\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.8851 - loss: 0.3109 - val_accuracy: 0.8965 - val_loss: 0.2769 - mcc: 0.8413\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8966 - loss: 0.2751(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8450\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8966 - loss: 0.2751 - val_accuracy: 0.8988 - val_loss: 0.2736 - mcc: 0.8450\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8921 - loss: 0.2926(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8349\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.8921 - loss: 0.2926 - val_accuracy: 0.8925 - val_loss: 0.2956 - mcc: 0.8349\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8993 - loss: 0.2733(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8518\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 32ms/step - accuracy: 0.8993 - loss: 0.2733 - val_accuracy: 0.9031 - val_loss: 0.2589 - mcc: 0.8518\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9037 - loss: 0.2574(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8579\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9037 - loss: 0.2573 - val_accuracy: 0.9069 - val_loss: 0.2458 - mcc: 0.8579\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9028 - loss: 0.2590(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8550\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.9028 - loss: 0.2590 - val_accuracy: 0.9052 - val_loss: 0.2505 - mcc: 0.8550\n","Epoch 10/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9078 - loss: 0.2446(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8637\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.9078 - loss: 0.2446 - val_accuracy: 0.9108 - val_loss: 0.2382 - mcc: 0.8637\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9051 - loss: 0.2568(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8597\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9051 - loss: 0.2568 - val_accuracy: 0.9076 - val_loss: 0.2419 - mcc: 0.8597\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9102 - loss: 0.2351(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8648\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9102 - loss: 0.2351 - val_accuracy: 0.9115 - val_loss: 0.2305 - mcc: 0.8648\n","Epoch 13/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9175 - loss: 0.2129(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8770\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9175 - loss: 0.2129 - val_accuracy: 0.9192 - val_loss: 0.2042 - mcc: 0.8770\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9193 - loss: 0.2056(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8678\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9193 - loss: 0.2056 - val_accuracy: 0.9134 - val_loss: 0.2212 - mcc: 0.8678\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9190 - loss: 0.2069(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8745\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9190 - loss: 0.2069 - val_accuracy: 0.9179 - val_loss: 0.2091 - mcc: 0.8745\n","Epoch 16/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9216 - loss: 0.2005(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8804\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.9216 - loss: 0.2005 - val_accuracy: 0.9216 - val_loss: 0.1999 - mcc: 0.8804\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9224 - loss: 0.1985(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8838\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 32ms/step - accuracy: 0.9224 - loss: 0.1985 - val_accuracy: 0.9235 - val_loss: 0.1934 - mcc: 0.8838\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9236 - loss: 0.1940(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8814\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9236 - loss: 0.1940 - val_accuracy: 0.9224 - val_loss: 0.1951 - mcc: 0.8814\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9248 - loss: 0.1922(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8873\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9248 - loss: 0.1922 - val_accuracy: 0.9258 - val_loss: 0.1872 - mcc: 0.8873\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9276 - loss: 0.1841(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8863\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9276 - loss: 0.1841 - val_accuracy: 0.9256 - val_loss: 0.1870 - mcc: 0.8863\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 4\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7112 - loss: 0.7685(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7807\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 33ms/step - accuracy: 0.7113 - loss: 0.7683 - val_accuracy: 0.8585 - val_loss: 0.3709 - mcc: 0.7807\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8577 - loss: 0.3780(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8140\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8577 - loss: 0.3780 - val_accuracy: 0.8787 - val_loss: 0.3253 - mcc: 0.8140\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8799 - loss: 0.3249(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8202\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8799 - loss: 0.3249 - val_accuracy: 0.8836 - val_loss: 0.3228 - mcc: 0.8202\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8892 - loss: 0.3006(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8371\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.8892 - loss: 0.3006 - val_accuracy: 0.8945 - val_loss: 0.2877 - mcc: 0.8371\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8962 - loss: 0.2801(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8529\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8962 - loss: 0.2801 - val_accuracy: 0.9045 - val_loss: 0.2576 - mcc: 0.8529\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8993 - loss: 0.2709(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8558\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8993 - loss: 0.2708 - val_accuracy: 0.9062 - val_loss: 0.2534 - mcc: 0.8558\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9062 - loss: 0.2491(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8301\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9061 - loss: 0.2491 - val_accuracy: 0.8899 - val_loss: 0.2942 - mcc: 0.8301\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8962 - loss: 0.2746(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8547\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8962 - loss: 0.2746 - val_accuracy: 0.9047 - val_loss: 0.2458 - mcc: 0.8547\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8888 - loss: 0.2941(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8158\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8888 - loss: 0.2941 - val_accuracy: 0.8787 - val_loss: 0.3364 - mcc: 0.8158\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8971 - loss: 0.2691(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8266\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 32ms/step - accuracy: 0.8971 - loss: 0.2690 - val_accuracy: 0.8877 - val_loss: 0.3021 - mcc: 0.8266\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9005 - loss: 0.2641(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8126\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9005 - loss: 0.2641 - val_accuracy: 0.8764 - val_loss: 0.3520 - mcc: 0.8126\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9035 - loss: 0.2558(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8636\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9035 - loss: 0.2558 - val_accuracy: 0.9111 - val_loss: 0.2314 - mcc: 0.8636\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9110 - loss: 0.2305(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8665\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9110 - loss: 0.2305 - val_accuracy: 0.9125 - val_loss: 0.2252 - mcc: 0.8665\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9140 - loss: 0.2238(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8737\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9140 - loss: 0.2238 - val_accuracy: 0.9177 - val_loss: 0.2122 - mcc: 0.8737\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9179 - loss: 0.2098(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8778\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9179 - loss: 0.2098 - val_accuracy: 0.9200 - val_loss: 0.2073 - mcc: 0.8778\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9207 - loss: 0.2018(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8792\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9207 - loss: 0.2018 - val_accuracy: 0.9209 - val_loss: 0.2026 - mcc: 0.8792\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9228 - loss: 0.1952(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8786\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9228 - loss: 0.1952 - val_accuracy: 0.9208 - val_loss: 0.2037 - mcc: 0.8786\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9185 - loss: 0.2093(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8811\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9185 - loss: 0.2093 - val_accuracy: 0.9224 - val_loss: 0.1979 - mcc: 0.8811\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9250 - loss: 0.1904(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8849\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.9250 - loss: 0.1904 - val_accuracy: 0.9247 - val_loss: 0.1916 - mcc: 0.8849\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9270 - loss: 0.1840(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8821\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 32ms/step - accuracy: 0.9270 - loss: 0.1840 - val_accuracy: 0.9229 - val_loss: 0.1983 - mcc: 0.8821\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7022 - loss: 0.7736(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7909\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.7023 - loss: 0.7733 - val_accuracy: 0.8644 - val_loss: 0.3587 - mcc: 0.7909\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8667 - loss: 0.3596(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8132\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8667 - loss: 0.3596 - val_accuracy: 0.8782 - val_loss: 0.3275 - mcc: 0.8132\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8862 - loss: 0.3056(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8364\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 35ms/step - accuracy: 0.8862 - loss: 0.3055 - val_accuracy: 0.8935 - val_loss: 0.2867 - mcc: 0.8364\n","Epoch 4/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8937 - loss: 0.2861(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8304\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.8936 - loss: 0.2861 - val_accuracy: 0.8891 - val_loss: 0.2943 - mcc: 0.8304\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8962 - loss: 0.2809(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8503\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 35ms/step - accuracy: 0.8962 - loss: 0.2809 - val_accuracy: 0.9019 - val_loss: 0.2591 - mcc: 0.8503\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9009 - loss: 0.2659(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8446\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9009 - loss: 0.2659 - val_accuracy: 0.8983 - val_loss: 0.2620 - mcc: 0.8446\n","Epoch 7/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8929 - loss: 0.2881(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8424\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.8929 - loss: 0.2881 - val_accuracy: 0.8973 - val_loss: 0.2737 - mcc: 0.8424\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9033 - loss: 0.2549(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8562\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9033 - loss: 0.2549 - val_accuracy: 0.9059 - val_loss: 0.2400 - mcc: 0.8562\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9007 - loss: 0.2628(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8478\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.9007 - loss: 0.2628 - val_accuracy: 0.9006 - val_loss: 0.2607 - mcc: 0.8478\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9052 - loss: 0.2511(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8573\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9052 - loss: 0.2511 - val_accuracy: 0.9065 - val_loss: 0.2417 - mcc: 0.8573\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9108 - loss: 0.2319(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8369\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9108 - loss: 0.2319 - val_accuracy: 0.8935 - val_loss: 0.2811 - mcc: 0.8369\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9031 - loss: 0.2560(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8392\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9031 - loss: 0.2560 - val_accuracy: 0.8949 - val_loss: 0.2751 - mcc: 0.8392\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9046 - loss: 0.2528(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8598\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9046 - loss: 0.2528 - val_accuracy: 0.9081 - val_loss: 0.2400 - mcc: 0.8598\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9134 - loss: 0.2256(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8616\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9134 - loss: 0.2256 - val_accuracy: 0.9094 - val_loss: 0.2385 - mcc: 0.8616\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9154 - loss: 0.2199(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8716\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.9154 - loss: 0.2199 - val_accuracy: 0.9158 - val_loss: 0.2159 - mcc: 0.8716\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9162 - loss: 0.2152(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8713\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9162 - loss: 0.2152 - val_accuracy: 0.9155 - val_loss: 0.2145 - mcc: 0.8713\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9160 - loss: 0.2174(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8655\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9160 - loss: 0.2174 - val_accuracy: 0.9117 - val_loss: 0.2288 - mcc: 0.8655\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9184 - loss: 0.2134(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8749\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.9184 - loss: 0.2134 - val_accuracy: 0.9182 - val_loss: 0.2086 - mcc: 0.8749\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9200 - loss: 0.2066(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8782\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 36ms/step - accuracy: 0.9200 - loss: 0.2066 - val_accuracy: 0.9200 - val_loss: 0.2041 - mcc: 0.8782\n","Epoch 20/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9155 - loss: 0.2200(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8703\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.9155 - loss: 0.2201 - val_accuracy: 0.9150 - val_loss: 0.2211 - mcc: 0.8703\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.9255753333333333),\n","              'mean': np.float64(0.9216861333333334),\n","              'min': np.float64(0.9150413333333334),\n","              'std': np.float64(0.0035552759017800106)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.0008710265954335531),\n","                               'mean': np.float64(0.0005267770449320475),\n","                               'min': np.float64(0.00040927728017171226),\n","                               'std': np.float64(0.0001731099550675135)},\n"," 'MCC': {'max': np.float64(0.8863417167015922),\n","         'mean': np.float64(0.8804023963262096),\n","         'min': np.float64(0.8702647860202374),\n","         'std': np.float64(0.005389082705963419)},\n"," 'Parameters': 10053,\n"," 'Train Time (s)': {'max': np.float64(742.2007250785828),\n","                    'mean': np.float64(691.5868473052979),\n","                    'min': np.float64(654.7482562065125),\n","                    'std': np.float64(33.0587227527555)},\n"," 'Training Accuracy': [[0.7972084879875183,\n","                        0.8777924180030823,\n","                        0.8893775939941406,\n","                        0.8970657587051392,\n","                        0.9046888947486877,\n","                        0.9084046483039856,\n","                        0.905348002910614,\n","                        0.9040570259094238,\n","                        0.9099524021148682,\n","                        0.9056864380836487,\n","                        0.9142870903015137,\n","                        0.9163181781768799,\n","                        0.9172200560569763,\n","                        0.9188952445983887,\n","                        0.9197834730148315,\n","                        0.9202845692634583,\n","                        0.9214147925376892,\n","                        0.9215996861457825,\n","                        0.9254217147827148,\n","                        0.9239373207092285],\n","                       [0.7999517321586609,\n","                        0.8766624331474304,\n","                        0.885286271572113,\n","                        0.8940514326095581,\n","                        0.8992565870285034,\n","                        0.8970751166343689,\n","                        0.9039576053619385,\n","                        0.9048769474029541,\n","                        0.901406466960907,\n","                        0.9045610427856445,\n","                        0.9031960368156433,\n","                        0.9077860713005066,\n","                        0.9108263254165649,\n","                        0.9139540195465088,\n","                        0.9173895120620728,\n","                        0.9182019829750061,\n","                        0.9162540435791016,\n","                        0.9204972386360168,\n","                        0.9166299700737,\n","                        0.9195241928100586],\n","                       [0.7926104068756104,\n","                        0.865709125995636,\n","                        0.8767828941345215,\n","                        0.88944011926651,\n","                        0.8971303105354309,\n","                        0.8892828226089478,\n","                        0.8994762301445007,\n","                        0.9046622514724731,\n","                        0.9059880971908569,\n","                        0.9075288772583008,\n","                        0.9055132865905762,\n","                        0.9124271869659424,\n","                        0.9172319173812866,\n","                        0.9203008413314819,\n","                        0.9205995798110962,\n","                        0.921073317527771,\n","                        0.9228780269622803,\n","                        0.9236570000648499,\n","                        0.9254119992256165,\n","                        0.9254388809204102],\n","                       [0.7898343205451965,\n","                        0.8657519817352295,\n","                        0.8811709880828857,\n","                        0.889378011226654,\n","                        0.8998523950576782,\n","                        0.9018089771270752,\n","                        0.9036402702331543,\n","                        0.8980455994606018,\n","                        0.8854891657829285,\n","                        0.9003928899765015,\n","                        0.9040358066558838,\n","                        0.9067238569259644,\n","                        0.9119415879249573,\n","                        0.9140402674674988,\n","                        0.9185332655906677,\n","                        0.9207008481025696,\n","                        0.9216873645782471,\n","                        0.9200853705406189,\n","                        0.9247694611549377,\n","                        0.9251521825790405],\n","                       [0.7920013070106506,\n","                        0.8681480288505554,\n","                        0.8891991972923279,\n","                        0.8934096693992615,\n","                        0.898648738861084,\n","                        0.8968648910522461,\n","                        0.8904542922973633,\n","                        0.9052296280860901,\n","                        0.8949830532073975,\n","                        0.9073954820632935,\n","                        0.9095788598060608,\n","                        0.9025428295135498,\n","                        0.9077864289283752,\n","                        0.9132634997367859,\n","                        0.9166166186332703,\n","                        0.9161770939826965,\n","                        0.9161563515663147,\n","                        0.9169124960899353,\n","                        0.9209601879119873,\n","                        0.9141455888748169]],\n"," 'Training Loss': [[0.5322188138961792,\n","                    0.32612499594688416,\n","                    0.2938965857028961,\n","                    0.27452000975608826,\n","                    0.24990014731884003,\n","                    0.23842036724090576,\n","                    0.25002261996269226,\n","                    0.25403696298599243,\n","                    0.23690718412399292,\n","                    0.24840345978736877,\n","                    0.22124609351158142,\n","                    0.21456526219844818,\n","                    0.21176207065582275,\n","                    0.20691658556461334,\n","                    0.20620840787887573,\n","                    0.20440344512462616,\n","                    0.2012743055820465,\n","                    0.20192332565784454,\n","                    0.18931573629379272,\n","                    0.19316837191581726],\n","                   [0.52723628282547,\n","                    0.3374580442905426,\n","                    0.312294602394104,\n","                    0.2821146845817566,\n","                    0.2672278881072998,\n","                    0.2765127420425415,\n","                    0.2561790943145752,\n","                    0.25383153557777405,\n","                    0.26430976390838623,\n","                    0.2535058856010437,\n","                    0.255241721868515,\n","                    0.24300462007522583,\n","                    0.2327994406223297,\n","                    0.2228824496269226,\n","                    0.21251538395881653,\n","                    0.21033082902431488,\n","                    0.216888427734375,\n","                    0.20391879975795746,\n","                    0.21495895087718964,\n","                    0.20646807551383972],\n","                   [0.550632119178772,\n","                    0.36451831459999084,\n","                    0.33707794547080994,\n","                    0.29917433857917786,\n","                    0.27461159229278564,\n","                    0.30102914571762085,\n","                    0.27145636081695557,\n","                    0.25402897596359253,\n","                    0.2486996054649353,\n","                    0.2448306530714035,\n","                    0.25357306003570557,\n","                    0.23002608120441437,\n","                    0.213627889752388,\n","                    0.20362307131290436,\n","                    0.20277349650859833,\n","                    0.20134419202804565,\n","                    0.19685129821300507,\n","                    0.1942574828863144,\n","                    0.18945880234241486,\n","                    0.18947505950927734],\n","                   [0.5531792640686035,\n","                    0.3572506606578827,\n","                    0.3227425813674927,\n","                    0.30110204219818115,\n","                    0.2689431309700012,\n","                    0.2642192244529724,\n","                    0.2556884288787842,\n","                    0.2696710526943207,\n","                    0.300553560256958,\n","                    0.2595013976097107,\n","                    0.2512650787830353,\n","                    0.24368025362491608,\n","                    0.22730951011180878,\n","                    0.22311335802078247,\n","                    0.2079525738954544,\n","                    0.20218870043754578,\n","                    0.19903749227523804,\n","                    0.205430805683136,\n","                    0.18951617181301117,\n","                    0.18887831270694733],\n","                   [0.5459427833557129,\n","                    0.35642147064208984,\n","                    0.29842469096183777,\n","                    0.28840798139572144,\n","                    0.2731369137763977,\n","                    0.2765764892101288,\n","                    0.2961636781692505,\n","                    0.24829280376434326,\n","                    0.2799639105796814,\n","                    0.24348948895931244,\n","                    0.23559430241584778,\n","                    0.2568504512310028,\n","                    0.2439752221107483,\n","                    0.2268068641424179,\n","                    0.21615765988826752,\n","                    0.2161775380373001,\n","                    0.21710866689682007,\n","                    0.21719534695148468,\n","                    0.20392397046089172,\n","                    0.22529123723506927]],\n"," 'Validation Accuracy': [[0.8701872825622559,\n","                          0.8939591646194458,\n","                          0.9009272456169128,\n","                          0.9038693904876709,\n","                          0.9099825024604797,\n","                          0.9143469333648682,\n","                          0.909511148929596,\n","                          0.9124499559402466,\n","                          0.8466812372207642,\n","                          0.9182969331741333,\n","                          0.9177356958389282,\n","                          0.9155988693237305,\n","                          0.9200891852378845,\n","                          0.9218613505363464,\n","                          0.92453533411026,\n","                          0.9229559898376465,\n","                          0.9227461814880371,\n","                          0.9273741841316223,\n","                          0.9289759397506714,\n","                          0.9216601848602295],\n","                         [0.8711685538291931,\n","                          0.8915607333183289,\n","                          0.8942468166351318,\n","                          0.8993940353393555,\n","                          0.8925254940986633,\n","                          0.9074093699455261,\n","                          0.9088329076766968,\n","                          0.855048418045044,\n","                          0.9047207832336426,\n","                          0.9097906351089478,\n","                          0.9081239700317383,\n","                          0.9117759466171265,\n","                          0.9057120084762573,\n","                          0.9174188375473022,\n","                          0.9184710383415222,\n","                          0.9202154874801636,\n","                          0.9195911288261414,\n","                          0.9246826767921448,\n","                          0.9195518493652344,\n","                          0.9232365489006042],\n","                         [0.854591965675354,\n","                          0.8721902966499329,\n","                          0.8777967691421509,\n","                          0.8965446949005127,\n","                          0.8988285064697266,\n","                          0.8924741148948669,\n","                          0.9031465649604797,\n","                          0.9069347381591797,\n","                          0.9051600098609924,\n","                          0.910785973072052,\n","                          0.9075733423233032,\n","                          0.911474347114563,\n","                          0.9192265868186951,\n","                          0.9134129881858826,\n","                          0.9179048538208008,\n","                          0.9215771555900574,\n","                          0.9235273599624634,\n","                          0.9223617315292358,\n","                          0.9257981181144714,\n","                          0.9255754947662354],\n","                         [0.8584699034690857,\n","                          0.8786826729774475,\n","                          0.8836406469345093,\n","                          0.8945096135139465,\n","                          0.9044957756996155,\n","                          0.9061633944511414,\n","                          0.8899163603782654,\n","                          0.9047227501869202,\n","                          0.8786947131156921,\n","                          0.8876959681510925,\n","                          0.8764147162437439,\n","                          0.9110665917396545,\n","                          0.9124721884727478,\n","                          0.9177032113075256,\n","                          0.9199642539024353,\n","                          0.9209470748901367,\n","                          0.9208048582077026,\n","                          0.9223889112472534,\n","                          0.9246808290481567,\n","                          0.9229177236557007],\n","                         [0.864437997341156,\n","                          0.87815260887146,\n","                          0.8935081958770752,\n","                          0.8891026377677917,\n","                          0.9019408226013184,\n","                          0.8982905149459839,\n","                          0.8972945809364319,\n","                          0.9058887362480164,\n","                          0.9006198048591614,\n","                          0.9065032601356506,\n","                          0.8935388922691345,\n","                          0.8948535919189453,\n","                          0.9080613851547241,\n","                          0.909421443939209,\n","                          0.9158031940460205,\n","                          0.9155264496803284,\n","                          0.9117128252983093,\n","                          0.9181514382362366,\n","                          0.9199808239936829,\n","                          0.9150410890579224]],\n"," 'Validation Loss': [[0.34262073040008545,\n","                      0.27929338812828064,\n","                      0.2582726776599884,\n","                      0.25262078642845154,\n","                      0.23188863694667816,\n","                      0.22210828959941864,\n","                      0.24263067543506622,\n","                      0.22778792679309845,\n","                      0.5485403537750244,\n","                      0.21173095703125,\n","                      0.21129420399665833,\n","                      0.21935394406318665,\n","                      0.20381887257099152,\n","                      0.20158760249614716,\n","                      0.19155293703079224,\n","                      0.19735591113567352,\n","                      0.19963626563549042,\n","                      0.18485203385353088,\n","                      0.17970043420791626,\n","                      0.1988324373960495],\n","                     [0.350077360868454,\n","                      0.2969740629196167,\n","                      0.28350165486335754,\n","                      0.2705969512462616,\n","                      0.2973632216453552,\n","                      0.2459205538034439,\n","                      0.2421490103006363,\n","                      0.44076624512672424,\n","                      0.2549838125705719,\n","                      0.2388099730014801,\n","                      0.23904524743556976,\n","                      0.2309841364622116,\n","                      0.25366300344467163,\n","                      0.21301263570785522,\n","                      0.20982740819454193,\n","                      0.20507343113422394,\n","                      0.2077532559633255,\n","                      0.191620871424675,\n","                      0.20604220032691956,\n","                      0.19544731080532074],\n","                     [0.393241286277771,\n","                      0.3503876030445099,\n","                      0.3343430757522583,\n","                      0.2769123613834381,\n","                      0.2736075818538666,\n","                      0.2955896258354187,\n","                      0.2588607370853424,\n","                      0.2458207756280899,\n","                      0.250518262386322,\n","                      0.23824350535869598,\n","                      0.24191473424434662,\n","                      0.23052549362182617,\n","                      0.20420698821544647,\n","                      0.22124898433685303,\n","                      0.2091476023197174,\n","                      0.19986771047115326,\n","                      0.19338330626487732,\n","                      0.1951303780078888,\n","                      0.1871904879808426,\n","                      0.18701551854610443],\n","                     [0.3708662986755371,\n","                      0.32531628012657166,\n","                      0.32278159260749817,\n","                      0.2877435088157654,\n","                      0.25763779878616333,\n","                      0.25340521335601807,\n","                      0.29417884349823,\n","                      0.24582256376743317,\n","                      0.33642512559890747,\n","                      0.30212700366973877,\n","                      0.3519648015499115,\n","                      0.23143257200717926,\n","                      0.22518408298492432,\n","                      0.2121804654598236,\n","                      0.20726852118968964,\n","                      0.2025887668132782,\n","                      0.20371897518634796,\n","                      0.1978520154953003,\n","                      0.19159747660160065,\n","                      0.19828063249588013],\n","                     [0.35870808362960815,\n","                      0.3275189995765686,\n","                      0.2867063283920288,\n","                      0.29426032304763794,\n","                      0.25914645195007324,\n","                      0.26204031705856323,\n","                      0.2736528813838959,\n","                      0.24003906548023224,\n","                      0.26070427894592285,\n","                      0.24172037839889526,\n","                      0.28105616569519043,\n","                      0.2750997245311737,\n","                      0.23999135196208954,\n","                      0.23852677643299103,\n","                      0.2158685326576233,\n","                      0.2144911140203476,\n","                      0.22877205908298492,\n","                      0.20858387649059296,\n","                      0.2040654867887497,\n","                      0.22111321985721588]],\n"," 'Validation MCC': [[np.float64(0.7996016881096802),\n","                     np.float64(0.8373634410142968),\n","                     np.float64(0.8489765703232711),\n","                     np.float64(0.8531684070146812),\n","                     np.float64(0.8630315769693943),\n","                     np.float64(0.8689915828819251),\n","                     np.float64(0.8615933289351869),\n","                     np.float64(0.8660836840872304),\n","                     np.float64(0.764436457773559),\n","                     np.float64(0.8752678009834023),\n","                     np.float64(0.874948927433535),\n","                     np.float64(0.8711071753365831),\n","                     np.float64(0.8779712840778784),\n","                     np.float64(0.8805375687552695),\n","                     np.float64(0.8849205442520115),\n","                     np.float64(0.8823835198688068),\n","                     np.float64(0.8824765934669048),\n","                     np.float64(0.8894820971906735),\n","                     np.float64(0.8918713512190367),\n","                     np.float64(0.8808937179432268)],\n","                    [np.float64(0.8007571181781654),\n","                     np.float64(0.8329932873656687),\n","                     np.float64(0.8372691570896071),\n","                     np.float64(0.845102615977813),\n","                     np.float64(0.8349197345756815),\n","                     np.float64(0.8576994972327521),\n","                     np.float64(0.8599542192565366),\n","                     np.float64(0.7829305452421674),\n","                     np.float64(0.8533298547071017),\n","                     np.float64(0.8619484712218961),\n","                     np.float64(0.8590681768879627),\n","                     np.float64(0.8643038000315793),\n","                     np.float64(0.8550021325181182),\n","                     np.float64(0.8735827340083898),\n","                     np.float64(0.875988211924908),\n","                     np.float64(0.8774113520704829),\n","                     np.float64(0.876944967031339),\n","                     np.float64(0.884804539052354),\n","                     np.float64(0.8766339924217907),\n","                     np.float64(0.8824174181738256)],\n","                    [np.float64(0.7759418314205713),\n","                     np.float64(0.8030898086932097),\n","                     np.float64(0.8117593658162702),\n","                     np.float64(0.8412588885190485),\n","                     np.float64(0.8449648737157851),\n","                     np.float64(0.834913199255096),\n","                     np.float64(0.8517780155711152),\n","                     np.float64(0.857850637801649),\n","                     np.float64(0.8549925570426425),\n","                     np.float64(0.8636640641463067),\n","                     np.float64(0.859738515574157),\n","                     np.float64(0.8647737138082401),\n","                     np.float64(0.8770439246131021),\n","                     np.float64(0.8677807235265874),\n","                     np.float64(0.8745179397619574),\n","                     np.float64(0.8803524497612787),\n","                     np.float64(0.883750916091606),\n","                     np.float64(0.8813848101421907),\n","                     np.float64(0.8873145481637749),\n","                     np.float64(0.8863417167015922)],\n","                    [np.float64(0.7806955234998827),\n","                     np.float64(0.8140405179905967),\n","                     np.float64(0.8201581667555051),\n","                     np.float64(0.8371054899760296),\n","                     np.float64(0.8528717912311357),\n","                     np.float64(0.8558017146390836),\n","                     np.float64(0.8301298129364963),\n","                     np.float64(0.8547212668159837),\n","                     np.float64(0.8157780041415041),\n","                     np.float64(0.8266392739097799),\n","                     np.float64(0.8126232526931119),\n","                     np.float64(0.8635674871170911),\n","                     np.float64(0.8664644234079774),\n","                     np.float64(0.8736919453141334),\n","                     np.float64(0.8778224221047982),\n","                     np.float64(0.8791964989805535),\n","                     np.float64(0.8785569058660339),\n","                     np.float64(0.881087272712369),\n","                     np.float64(0.8849073115229019),\n","                     np.float64(0.882094342792166)],\n","                    [np.float64(0.7909103246716143),\n","                     np.float64(0.8131694934319545),\n","                     np.float64(0.8363675253854581),\n","                     np.float64(0.8303536422342707),\n","                     np.float64(0.8503374825221656),\n","                     np.float64(0.8445834196416614),\n","                     np.float64(0.8423627054479732),\n","                     np.float64(0.8562187346163181),\n","                     np.float64(0.847825629868517),\n","                     np.float64(0.8573076775178475),\n","                     np.float64(0.836893105689375),\n","                     np.float64(0.8391675934805641),\n","                     np.float64(0.8597797918262622),\n","                     np.float64(0.8615747182528155),\n","                     np.float64(0.8716034117460396),\n","                     np.float64(0.8713197872650077),\n","                     np.float64(0.8655399522419245),\n","                     np.float64(0.8748637728230692),\n","                     np.float64(0.878167326933909),\n","                     np.float64(0.8702647860202374)]]}\n","Training Model: BiLSTM_Dense, Fold: 1\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6940 - loss: 0.7987(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7943\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.6943 - loss: 0.7981 - val_accuracy: 0.8658 - val_loss: 0.3617 - mcc: 0.7943\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8747 - loss: 0.3346(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8385\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8747 - loss: 0.3346 - val_accuracy: 0.8948 - val_loss: 0.2816 - mcc: 0.8385\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8926 - loss: 0.2856(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8607\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8926 - loss: 0.2856 - val_accuracy: 0.9088 - val_loss: 0.2371 - mcc: 0.8607\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8985 - loss: 0.2691(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8649\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.8985 - loss: 0.2691 - val_accuracy: 0.9112 - val_loss: 0.2309 - mcc: 0.8649\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9068 - loss: 0.2421(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8711\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9068 - loss: 0.2421 - val_accuracy: 0.9156 - val_loss: 0.2181 - mcc: 0.8711\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9131 - loss: 0.2253(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8763\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9131 - loss: 0.2253 - val_accuracy: 0.9191 - val_loss: 0.2073 - mcc: 0.8763\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9148 - loss: 0.2185(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8658\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9148 - loss: 0.2185 - val_accuracy: 0.9122 - val_loss: 0.2319 - mcc: 0.8658\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9146 - loss: 0.2193(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8769\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9146 - loss: 0.2193 - val_accuracy: 0.9194 - val_loss: 0.2080 - mcc: 0.8769\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9153 - loss: 0.2160(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8838\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9154 - loss: 0.2159 - val_accuracy: 0.9235 - val_loss: 0.1934 - mcc: 0.8838\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9196 - loss: 0.2039(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8665\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9196 - loss: 0.2040 - val_accuracy: 0.9127 - val_loss: 0.2240 - mcc: 0.8665\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9112 - loss: 0.2293(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8851\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9112 - loss: 0.2293 - val_accuracy: 0.9246 - val_loss: 0.1904 - mcc: 0.8851\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9214 - loss: 0.1995(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8836\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9214 - loss: 0.1995 - val_accuracy: 0.9237 - val_loss: 0.1990 - mcc: 0.8836\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9241 - loss: 0.1926(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8732\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9241 - loss: 0.1926 - val_accuracy: 0.9163 - val_loss: 0.2141 - mcc: 0.8732\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9215 - loss: 0.2006(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8898\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9215 - loss: 0.2006 - val_accuracy: 0.9278 - val_loss: 0.1827 - mcc: 0.8898\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9272 - loss: 0.1846(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8935\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.9272 - loss: 0.1846 - val_accuracy: 0.9301 - val_loss: 0.1775 - mcc: 0.8935\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9273 - loss: 0.1837(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8907\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9273 - loss: 0.1837 - val_accuracy: 0.9283 - val_loss: 0.1794 - mcc: 0.8907\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9304 - loss: 0.1759(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8986\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9304 - loss: 0.1759 - val_accuracy: 0.9335 - val_loss: 0.1673 - mcc: 0.8986\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9310 - loss: 0.1743(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8975\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.9310 - loss: 0.1743 - val_accuracy: 0.9327 - val_loss: 0.1682 - mcc: 0.8975\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9297 - loss: 0.1779(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8974\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9297 - loss: 0.1779 - val_accuracy: 0.9328 - val_loss: 0.1686 - mcc: 0.8974\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9327 - loss: 0.1698(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8990\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9327 - loss: 0.1698 - val_accuracy: 0.9337 - val_loss: 0.1648 - mcc: 0.8990\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 2\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7176 - loss: 0.7586(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8139\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 34ms/step - accuracy: 0.7178 - loss: 0.7580 - val_accuracy: 0.8789 - val_loss: 0.3276 - mcc: 0.8139\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8800 - loss: 0.3215(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8427\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8800 - loss: 0.3215 - val_accuracy: 0.8980 - val_loss: 0.2726 - mcc: 0.8427\n","Epoch 3/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8965 - loss: 0.2745(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8321\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8965 - loss: 0.2745 - val_accuracy: 0.8903 - val_loss: 0.3014 - mcc: 0.8321\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8969 - loss: 0.2770(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8511\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8969 - loss: 0.2770 - val_accuracy: 0.9030 - val_loss: 0.2576 - mcc: 0.8511\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9031 - loss: 0.2553(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8574\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9031 - loss: 0.2553 - val_accuracy: 0.9071 - val_loss: 0.2462 - mcc: 0.8574\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9037 - loss: 0.2546(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8318\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9037 - loss: 0.2546 - val_accuracy: 0.8902 - val_loss: 0.3069 - mcc: 0.8318\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8905 - loss: 0.3106(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8574\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8905 - loss: 0.3106 - val_accuracy: 0.9071 - val_loss: 0.2516 - mcc: 0.8574\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9073 - loss: 0.2494(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8695\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9073 - loss: 0.2494 - val_accuracy: 0.9148 - val_loss: 0.2258 - mcc: 0.8695\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9100 - loss: 0.2382(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8559\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9100 - loss: 0.2382 - val_accuracy: 0.9062 - val_loss: 0.2533 - mcc: 0.8559\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9120 - loss: 0.2330(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8758\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9121 - loss: 0.2329 - val_accuracy: 0.9191 - val_loss: 0.2124 - mcc: 0.8758\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9019 - loss: 0.2631(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8711\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.9019 - loss: 0.2631 - val_accuracy: 0.9157 - val_loss: 0.2223 - mcc: 0.8711\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9144 - loss: 0.2230(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8803\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9144 - loss: 0.2230 - val_accuracy: 0.9220 - val_loss: 0.2026 - mcc: 0.8803\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9195 - loss: 0.2075(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8862\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.9195 - loss: 0.2075 - val_accuracy: 0.9256 - val_loss: 0.1920 - mcc: 0.8862\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9201 - loss: 0.2069(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8864\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9201 - loss: 0.2069 - val_accuracy: 0.9257 - val_loss: 0.1903 - mcc: 0.8864\n","Epoch 15/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9221 - loss: 0.2012(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8854\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.9221 - loss: 0.2012 - val_accuracy: 0.9248 - val_loss: 0.1929 - mcc: 0.8854\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9182 - loss: 0.2119(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8916\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9182 - loss: 0.2119 - val_accuracy: 0.9291 - val_loss: 0.1801 - mcc: 0.8916\n","Epoch 17/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9233 - loss: 0.1965(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8844\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9233 - loss: 0.1965 - val_accuracy: 0.9246 - val_loss: 0.1967 - mcc: 0.8844\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9224 - loss: 0.1993(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8802\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9224 - loss: 0.1994 - val_accuracy: 0.9219 - val_loss: 0.2016 - mcc: 0.8802\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9212 - loss: 0.2038(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8909\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9212 - loss: 0.2038 - val_accuracy: 0.9287 - val_loss: 0.1834 - mcc: 0.8909\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9266 - loss: 0.1873(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8923\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.9266 - loss: 0.1873 - val_accuracy: 0.9296 - val_loss: 0.1785 - mcc: 0.8923\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 3\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7253 - loss: 0.7486(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7936\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 38ms/step - accuracy: 0.7255 - loss: 0.7480 - val_accuracy: 0.8657 - val_loss: 0.3608 - mcc: 0.7936\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8792 - loss: 0.3250(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8348\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 34ms/step - accuracy: 0.8792 - loss: 0.3250 - val_accuracy: 0.8903 - val_loss: 0.2867 - mcc: 0.8348\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8961 - loss: 0.2760(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8389\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.8961 - loss: 0.2760 - val_accuracy: 0.8951 - val_loss: 0.2817 - mcc: 0.8389\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8948 - loss: 0.2841(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8407\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.8948 - loss: 0.2841 - val_accuracy: 0.8960 - val_loss: 0.2806 - mcc: 0.8407\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9032 - loss: 0.2588(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8595\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.9032 - loss: 0.2588 - val_accuracy: 0.9079 - val_loss: 0.2382 - mcc: 0.8595\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9053 - loss: 0.2491(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8588\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9053 - loss: 0.2490 - val_accuracy: 0.9076 - val_loss: 0.2432 - mcc: 0.8588\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9071 - loss: 0.2448(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8589\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9071 - loss: 0.2448 - val_accuracy: 0.9077 - val_loss: 0.2464 - mcc: 0.8589\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9095 - loss: 0.2375(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8405\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9095 - loss: 0.2375 - val_accuracy: 0.8953 - val_loss: 0.3069 - mcc: 0.8405\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9110 - loss: 0.2351(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8752\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9110 - loss: 0.2351 - val_accuracy: 0.9179 - val_loss: 0.2069 - mcc: 0.8752\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9195 - loss: 0.2045(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8689\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.9194 - loss: 0.2046 - val_accuracy: 0.9142 - val_loss: 0.2212 - mcc: 0.8689\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9173 - loss: 0.2126(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8759\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - accuracy: 0.9173 - loss: 0.2125 - val_accuracy: 0.9186 - val_loss: 0.2059 - mcc: 0.8759\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9224 - loss: 0.1964(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8844\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9224 - loss: 0.1964 - val_accuracy: 0.9242 - val_loss: 0.1883 - mcc: 0.8844\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9241 - loss: 0.1922(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8857\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.9241 - loss: 0.1922 - val_accuracy: 0.9249 - val_loss: 0.1893 - mcc: 0.8857\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9219 - loss: 0.1987(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8777\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9219 - loss: 0.1987 - val_accuracy: 0.9196 - val_loss: 0.2064 - mcc: 0.8777\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9271 - loss: 0.1840(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8919\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.9271 - loss: 0.1840 - val_accuracy: 0.9290 - val_loss: 0.1759 - mcc: 0.8919\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9219 - loss: 0.2009(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8854\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9219 - loss: 0.2009 - val_accuracy: 0.9249 - val_loss: 0.1897 - mcc: 0.8854\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9260 - loss: 0.1867(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8869\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9260 - loss: 0.1867 - val_accuracy: 0.9253 - val_loss: 0.1870 - mcc: 0.8869\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9270 - loss: 0.1823(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8939\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9270 - loss: 0.1823 - val_accuracy: 0.9304 - val_loss: 0.1734 - mcc: 0.8939\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9297 - loss: 0.1759(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8946\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9297 - loss: 0.1759 - val_accuracy: 0.9305 - val_loss: 0.1719 - mcc: 0.8946\n","Epoch 20/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9322 - loss: 0.1696(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8958\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9322 - loss: 0.1696 - val_accuracy: 0.9315 - val_loss: 0.1698 - mcc: 0.8958\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 4\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6966 - loss: 0.8193(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.7807\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.6968 - loss: 0.8189 - val_accuracy: 0.8569 - val_loss: 0.3729 - mcc: 0.7807\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8597 - loss: 0.3750(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.7935\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.8597 - loss: 0.3750 - val_accuracy: 0.8639 - val_loss: 0.3634 - mcc: 0.7935\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8827 - loss: 0.3125(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8421\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 34ms/step - accuracy: 0.8827 - loss: 0.3125 - val_accuracy: 0.8966 - val_loss: 0.2727 - mcc: 0.8421\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8960 - loss: 0.2770(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8549\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8960 - loss: 0.2769 - val_accuracy: 0.9046 - val_loss: 0.2497 - mcc: 0.8549\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9063 - loss: 0.2482(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8618\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9063 - loss: 0.2482 - val_accuracy: 0.9099 - val_loss: 0.2416 - mcc: 0.8618\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9122 - loss: 0.2314(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8684\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9122 - loss: 0.2314 - val_accuracy: 0.9138 - val_loss: 0.2267 - mcc: 0.8684\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9154 - loss: 0.2192(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8763\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.9154 - loss: 0.2192 - val_accuracy: 0.9190 - val_loss: 0.2108 - mcc: 0.8763\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9156 - loss: 0.2227(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8717\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 35ms/step - accuracy: 0.9156 - loss: 0.2227 - val_accuracy: 0.9165 - val_loss: 0.2174 - mcc: 0.8717\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9197 - loss: 0.2073(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8751\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9197 - loss: 0.2073 - val_accuracy: 0.9188 - val_loss: 0.2123 - mcc: 0.8751\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9060 - loss: 0.2454(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8754\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 37ms/step - accuracy: 0.9060 - loss: 0.2454 - val_accuracy: 0.9181 - val_loss: 0.2108 - mcc: 0.8754\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9197 - loss: 0.2053(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8776\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 35ms/step - accuracy: 0.9197 - loss: 0.2053 - val_accuracy: 0.9199 - val_loss: 0.2126 - mcc: 0.8776\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9235 - loss: 0.1948(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8793\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9235 - loss: 0.1948 - val_accuracy: 0.9211 - val_loss: 0.2044 - mcc: 0.8793\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9235 - loss: 0.1953(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8860\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9235 - loss: 0.1953 - val_accuracy: 0.9255 - val_loss: 0.1955 - mcc: 0.8860\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9252 - loss: 0.1915(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8862\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9252 - loss: 0.1915 - val_accuracy: 0.9256 - val_loss: 0.1919 - mcc: 0.8862\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9279 - loss: 0.1831(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8861\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9279 - loss: 0.1831 - val_accuracy: 0.9258 - val_loss: 0.1911 - mcc: 0.8861\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9284 - loss: 0.1808(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8850\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.9284 - loss: 0.1808 - val_accuracy: 0.9250 - val_loss: 0.1939 - mcc: 0.8850\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9282 - loss: 0.1817(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8926\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9282 - loss: 0.1817 - val_accuracy: 0.9297 - val_loss: 0.1799 - mcc: 0.8926\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9316 - loss: 0.1726(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8933\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9316 - loss: 0.1726 - val_accuracy: 0.9299 - val_loss: 0.1817 - mcc: 0.8933\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9334 - loss: 0.1664(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8884\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9334 - loss: 0.1664 - val_accuracy: 0.9270 - val_loss: 0.1903 - mcc: 0.8884\n","Epoch 20/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9323 - loss: 0.1706(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8976\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9323 - loss: 0.1706 - val_accuracy: 0.9330 - val_loss: 0.1723 - mcc: 0.8976\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 5\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7239 - loss: 0.7435(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8039\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.7240 - loss: 0.7432 - val_accuracy: 0.8725 - val_loss: 0.3533 - mcc: 0.8039\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8840 - loss: 0.3119(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8384\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 33ms/step - accuracy: 0.8840 - loss: 0.3119 - val_accuracy: 0.8943 - val_loss: 0.2828 - mcc: 0.8384\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8999 - loss: 0.2674(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8494\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.8999 - loss: 0.2674 - val_accuracy: 0.9014 - val_loss: 0.2613 - mcc: 0.8494\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9069 - loss: 0.2475(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8581\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9069 - loss: 0.2475 - val_accuracy: 0.9071 - val_loss: 0.2419 - mcc: 0.8581\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8994 - loss: 0.2705(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8339\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.8994 - loss: 0.2705 - val_accuracy: 0.8916 - val_loss: 0.2940 - mcc: 0.8339\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9022 - loss: 0.2631(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8630\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 33ms/step - accuracy: 0.9023 - loss: 0.2630 - val_accuracy: 0.9100 - val_loss: 0.2340 - mcc: 0.8630\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9117 - loss: 0.2341(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8628\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9117 - loss: 0.2341 - val_accuracy: 0.9104 - val_loss: 0.2344 - mcc: 0.8628\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9127 - loss: 0.2316(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8596\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9127 - loss: 0.2316 - val_accuracy: 0.9077 - val_loss: 0.2444 - mcc: 0.8596\n","Epoch 9/20\n","\u001b[1m748/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9121 - loss: 0.2337(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8736\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9121 - loss: 0.2337 - val_accuracy: 0.9170 - val_loss: 0.2142 - mcc: 0.8736\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9138 - loss: 0.2298(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8750\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9138 - loss: 0.2298 - val_accuracy: 0.9181 - val_loss: 0.2133 - mcc: 0.8750\n","Epoch 11/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9188 - loss: 0.2111(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8749\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 34ms/step - accuracy: 0.9188 - loss: 0.2111 - val_accuracy: 0.9180 - val_loss: 0.2076 - mcc: 0.8749\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9216 - loss: 0.2036(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.8800\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9216 - loss: 0.2036 - val_accuracy: 0.9212 - val_loss: 0.2034 - mcc: 0.8800\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9197 - loss: 0.2081(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.8710\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9197 - loss: 0.2081 - val_accuracy: 0.9156 - val_loss: 0.2206 - mcc: 0.8710\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9266 - loss: 0.1879(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.8740\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.9266 - loss: 0.1880 - val_accuracy: 0.9176 - val_loss: 0.2151 - mcc: 0.8740\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9221 - loss: 0.2020(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.8844\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9221 - loss: 0.2020 - val_accuracy: 0.9241 - val_loss: 0.1922 - mcc: 0.8844\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9267 - loss: 0.1866(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.8884\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9267 - loss: 0.1866 - val_accuracy: 0.9267 - val_loss: 0.1846 - mcc: 0.8884\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9289 - loss: 0.1818(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.8877\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 33ms/step - accuracy: 0.9289 - loss: 0.1818 - val_accuracy: 0.9264 - val_loss: 0.1885 - mcc: 0.8877\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9296 - loss: 0.1784(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.8915\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 33ms/step - accuracy: 0.9296 - loss: 0.1784 - val_accuracy: 0.9289 - val_loss: 0.1789 - mcc: 0.8915\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9302 - loss: 0.1771(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.8477\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 36ms/step - accuracy: 0.9302 - loss: 0.1771 - val_accuracy: 0.9005 - val_loss: 0.2684 - mcc: 0.8477\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9243 - loss: 0.1953(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.8865\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 33ms/step - accuracy: 0.9243 - loss: 0.1953 - val_accuracy: 0.9252 - val_loss: 0.1878 - mcc: 0.8865\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.9337246666666666),\n","              'mean': np.float64(0.9305834666666668),\n","              'min': np.float64(0.925154),\n","              'std': np.float64(0.003061374111226664)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.000634894847869873),\n","                               'mean': np.float64(0.0005089506467183431),\n","                               'min': np.float64(0.00042585094769795735),\n","                               'std': np.float64(7.533780753668332e-05)},\n"," 'MCC': {'max': np.float64(0.8989714759469004),\n","         'mean': np.float64(0.8942366552622728),\n","         'min': np.float64(0.8864925989001998),\n","         'std': np.float64(0.004466300571537034)},\n"," 'Parameters': 10853,\n"," 'Train Time (s)': {'max': np.float64(757.7117671966553),\n","                    'mean': np.float64(711.3879706382752),\n","                    'min': np.float64(688.0424335002899),\n","                    'std': np.float64(25.190688439734245)},\n"," 'Training Accuracy': [[0.7883387207984924,\n","                        0.8808450698852539,\n","                        0.8975639939308167,\n","                        0.9016460180282593,\n","                        0.9068253636360168,\n","                        0.9130169153213501,\n","                        0.9146261215209961,\n","                        0.9185588955879211,\n","                        0.9183630347251892,\n","                        0.9131070375442505,\n","                        0.9172003865242004,\n","                        0.9222484827041626,\n","                        0.9253244400024414,\n","                        0.9241060614585876,\n","                        0.926535427570343,\n","                        0.927155613899231,\n","                        0.9303226470947266,\n","                        0.9306707382202148,\n","                        0.928773045539856,\n","                        0.9319749474525452],\n","                       [0.8039931654930115,\n","                        0.8836621046066284,\n","                        0.893579363822937,\n","                        0.8989738821983337,\n","                        0.9036965370178223,\n","                        0.900696873664856,\n","                        0.8981490135192871,\n","                        0.9070645570755005,\n","                        0.911055326461792,\n","                        0.9134712815284729,\n","                        0.9066554307937622,\n","                        0.9158076047897339,\n","                        0.9203035235404968,\n","                        0.9194151163101196,\n","                        0.92257159948349,\n","                        0.9206365346908569,\n","                        0.9239804148674011,\n","                        0.920694887638092,\n","                        0.9211724400520325,\n","                        0.9273157715797424],\n","                       [0.8022645711898804,\n","                        0.8850396275520325,\n","                        0.8956599831581116,\n","                        0.8953124284744263,\n","                        0.9043183326721191,\n","                        0.9060096144676208,\n","                        0.9056335687637329,\n","                        0.9122536778450012,\n","                        0.9133739471435547,\n","                        0.9161368012428284,\n","                        0.9191408753395081,\n","                        0.922324538230896,\n","                        0.9235048890113831,\n","                        0.9238719344139099,\n","                        0.927608072757721,\n","                        0.9196810722351074,\n","                        0.9267008304595947,\n","                        0.9288588166236877,\n","                        0.9297634959220886,\n","                        0.9319135546684265],\n","                       [0.7868887782096863,\n","                        0.8657070398330688,\n","                        0.8878105282783508,\n","                        0.898717999458313,\n","                        0.904564619064331,\n","                        0.9134173393249512,\n","                        0.9169734120368958,\n","                        0.9133111834526062,\n","                        0.9197872877120972,\n","                        0.9115338325500488,\n","                        0.9217595458030701,\n","                        0.9240580201148987,\n","                        0.9255691170692444,\n","                        0.9243581295013428,\n","                        0.9279163479804993,\n","                        0.9278638362884521,\n","                        0.9289015531539917,\n","                        0.9317150115966797,\n","                        0.9327546954154968,\n","                        0.9325951337814331],\n","                       [0.8078394532203674,\n","                        0.8899072408676147,\n","                        0.8998235464096069,\n","                        0.9063591361045837,\n","                        0.8990409970283508,\n","                        0.9071298241615295,\n","                        0.91172194480896,\n","                        0.9110004901885986,\n","                        0.915591299533844,\n","                        0.9143523573875427,\n","                        0.9206536412239075,\n","                        0.9199415445327759,\n","                        0.922616183757782,\n","                        0.9236828684806824,\n","                        0.9235461950302124,\n","                        0.927256166934967,\n","                        0.9263362884521484,\n","                        0.9291686415672302,\n","                        0.9307551980018616,\n","                        0.9273982048034668]],\n"," 'Training Loss': [[0.5560892820358276,\n","                    0.3182509243488312,\n","                    0.2718128561973572,\n","                    0.26079273223876953,\n","                    0.24344277381896973,\n","                    0.22450557351112366,\n","                    0.21973247826099396,\n","                    0.20794354379177094,\n","                    0.2085840255022049,\n","                    0.2219635397195816,\n","                    0.21244283020496368,\n","                    0.19743432104587555,\n","                    0.1898256093263626,\n","                    0.19322627782821655,\n","                    0.1869446337223053,\n","                    0.18435238301753998,\n","                    0.17511944472789764,\n","                    0.1747376024723053,\n","                    0.1815239042043686,\n","                    0.171207457780838],\n","                   [0.5248240232467651,\n","                    0.31361687183380127,\n","                    0.2870323657989502,\n","                    0.27047935128211975,\n","                    0.2549941837787628,\n","                    0.2679048180580139,\n","                    0.28208670020103455,\n","                    0.2496246099472046,\n","                    0.2352766990661621,\n","                    0.2273995578289032,\n","                    0.2489214539527893,\n","                    0.21970847249031067,\n","                    0.2052888423204422,\n","                    0.20794455707073212,\n","                    0.1989244818687439,\n","                    0.20448455214500427,\n","                    0.19511045515537262,\n","                    0.20427443087100983,\n","                    0.20424708724021912,\n","                    0.18539728224277496],\n","                   [0.5305556058883667,\n","                    0.3094002902507782,\n","                    0.27887576818466187,\n","                    0.2829954922199249,\n","                    0.2544254660606384,\n","                    0.2475879043340683,\n","                    0.24846899509429932,\n","                    0.2285318821668625,\n","                    0.22420252859592438,\n","                    0.21549658477306366,\n","                    0.20677585899829865,\n","                    0.19663389027118683,\n","                    0.19426937401294708,\n","                    0.1937113255262375,\n","                    0.1818966418504715,\n","                    0.20585337281227112,\n","                    0.18483121693134308,\n","                    0.1788656860589981,\n","                    0.17572806775569916,\n","                    0.16968314349651337],\n","                   [0.570035457611084,\n","                    0.35930106043815613,\n","                    0.2988955080509186,\n","                    0.26933494210243225,\n","                    0.2541954815387726,\n","                    0.22703897953033447,\n","                    0.21518096327781677,\n","                    0.22810234129428864,\n","                    0.20691922307014465,\n","                    0.2290039211511612,\n","                    0.20047995448112488,\n","                    0.19473405182361603,\n","                    0.18942132592201233,\n","                    0.1940540373325348,\n","                    0.18226586282253265,\n","                    0.182901993393898,\n","                    0.18004420399665833,\n","                    0.1723179966211319,\n","                    0.1678009331226349,\n","                    0.16876983642578125],\n","                   [0.5124576091766357,\n","                    0.2946538031101227,\n","                    0.2684279680252075,\n","                    0.24899646639823914,\n","                    0.27066755294799805,\n","                    0.2480013370513916,\n","                    0.23273205757141113,\n","                    0.23648427426815033,\n","                    0.2226397842168808,\n","                    0.2276376187801361,\n","                    0.2058422714471817,\n","                    0.20857900381088257,\n","                    0.19938376545906067,\n","                    0.1968996524810791,\n","                    0.1977396458387375,\n","                    0.18582311272621155,\n","                    0.18934234976768494,\n","                    0.18067309260368347,\n","                    0.17556457221508026,\n","                    0.18581020832061768]],\n"," 'Validation Accuracy': [[0.8657901287078857,\n","                          0.8947548270225525,\n","                          0.9088391065597534,\n","                          0.9111745357513428,\n","                          0.9156202077865601,\n","                          0.9190781116485596,\n","                          0.9122297167778015,\n","                          0.9193918704986572,\n","                          0.9234541058540344,\n","                          0.9127146005630493,\n","                          0.9246228933334351,\n","                          0.9237149953842163,\n","                          0.9163145422935486,\n","                          0.9278239011764526,\n","                          0.930101215839386,\n","                          0.9283344149589539,\n","                          0.9334625601768494,\n","                          0.9326878786087036,\n","                          0.9327648282051086,\n","                          0.933724582195282],\n","                         [0.8788642287254333,\n","                          0.8979594707489014,\n","                          0.890302836894989,\n","                          0.9030351638793945,\n","                          0.9071365594863892,\n","                          0.8902223110198975,\n","                          0.9071191549301147,\n","                          0.9148474335670471,\n","                          0.9061826467514038,\n","                          0.9190611839294434,\n","                          0.9156686067581177,\n","                          0.9219905734062195,\n","                          0.9255567789077759,\n","                          0.9257411956787109,\n","                          0.924843430519104,\n","                          0.9290722012519836,\n","                          0.9245578646659851,\n","                          0.9219247102737427,\n","                          0.9286631941795349,\n","                          0.9295682907104492],\n","                         [0.8657373785972595,\n","                          0.8903122544288635,\n","                          0.8951228857040405,\n","                          0.8960328698158264,\n","                          0.9078595638275146,\n","                          0.9076025485992432,\n","                          0.9076958894729614,\n","                          0.8952826261520386,\n","                          0.9179182648658752,\n","                          0.9142411351203918,\n","                          0.9185847043991089,\n","                          0.9242016673088074,\n","                          0.9248831272125244,\n","                          0.9196253418922424,\n","                          0.9289863109588623,\n","                          0.924868643283844,\n","                          0.9252898693084717,\n","                          0.9303680658340454,\n","                          0.930546224117279,\n","                          0.9315187931060791],\n","                         [0.8569474816322327,\n","                          0.8638612627983093,\n","                          0.8966242074966431,\n","                          0.9045699238777161,\n","                          0.9099456667900085,\n","                          0.9138189554214478,\n","                          0.9190462827682495,\n","                          0.9165468215942383,\n","                          0.9187590479850769,\n","                          0.9181020259857178,\n","                          0.9198518991470337,\n","                          0.921110212802887,\n","                          0.9254825711250305,\n","                          0.9256061315536499,\n","                          0.9257603287696838,\n","                          0.9250044226646423,\n","                          0.9297478795051575,\n","                          0.9298747777938843,\n","                          0.9270302653312683,\n","                          0.9329512715339661],\n","                         [0.8725000619888306,\n","                          0.8942751288414001,\n","                          0.9014133214950562,\n","                          0.9070519208908081,\n","                          0.8915941715240479,\n","                          0.9099780321121216,\n","                          0.9103758931159973,\n","                          0.9077048897743225,\n","                          0.91701340675354,\n","                          0.9180651307106018,\n","                          0.9180132746696472,\n","                          0.9211587905883789,\n","                          0.9155625700950623,\n","                          0.9175691604614258,\n","                          0.9241402745246887,\n","                          0.9267337322235107,\n","                          0.9264129996299744,\n","                          0.9288694858551025,\n","                          0.9005193114280701,\n","                          0.9251542091369629]],\n"," 'Validation Loss': [[0.361703097820282,\n","                      0.2815958559513092,\n","                      0.23705919086933136,\n","                      0.23089152574539185,\n","                      0.21814395487308502,\n","                      0.20728754997253418,\n","                      0.23193855583667755,\n","                      0.20801325142383575,\n","                      0.19344641268253326,\n","                      0.224029079079628,\n","                      0.19044403731822968,\n","                      0.19903354346752167,\n","                      0.21411462128162384,\n","                      0.18270233273506165,\n","                      0.177526593208313,\n","                      0.17936721444129944,\n","                      0.16726866364479065,\n","                      0.1681792289018631,\n","                      0.1685708612203598,\n","                      0.16479577124118805],\n","                     [0.32755157351493835,\n","                      0.2725622057914734,\n","                      0.30137112736701965,\n","                      0.25761258602142334,\n","                      0.246224507689476,\n","                      0.3069021701812744,\n","                      0.25158676505088806,\n","                      0.2258065938949585,\n","                      0.25327375531196594,\n","                      0.21244150400161743,\n","                      0.22232936322689056,\n","                      0.20261307060718536,\n","                      0.19195544719696045,\n","                      0.19030173122882843,\n","                      0.19291198253631592,\n","                      0.1801491230726242,\n","                      0.1967017650604248,\n","                      0.20156775414943695,\n","                      0.18337500095367432,\n","                      0.1785394549369812],\n","                     [0.36082446575164795,\n","                      0.2867439091205597,\n","                      0.28172674775123596,\n","                      0.2805914282798767,\n","                      0.23823103308677673,\n","                      0.24323835968971252,\n","                      0.24644020199775696,\n","                      0.306944340467453,\n","                      0.20693479478359222,\n","                      0.22120286524295807,\n","                      0.2059420794248581,\n","                      0.1883401870727539,\n","                      0.18933679163455963,\n","                      0.2063749134540558,\n","                      0.17591646313667297,\n","                      0.18971653282642365,\n","                      0.18701356649398804,\n","                      0.17344190180301666,\n","                      0.17187167704105377,\n","                      0.16980357468128204],\n","                     [0.3729037344455719,\n","                      0.36337822675704956,\n","                      0.27268433570861816,\n","                      0.24971269071102142,\n","                      0.2415723204612732,\n","                      0.2267187535762787,\n","                      0.2108238935470581,\n","                      0.21735547482967377,\n","                      0.21228797733783722,\n","                      0.21084220707416534,\n","                      0.2125900238752365,\n","                      0.20443595945835114,\n","                      0.19547311961650848,\n","                      0.19188207387924194,\n","                      0.19112007319927216,\n","                      0.19385485351085663,\n","                      0.1799488663673401,\n","                      0.18168853223323822,\n","                      0.19026029109954834,\n","                      0.172296404838562],\n","                     [0.3532669246196747,\n","                      0.2827780246734619,\n","                      0.26126787066459656,\n","                      0.24189673364162445,\n","                      0.2939590513706207,\n","                      0.2340480089187622,\n","                      0.23440305888652802,\n","                      0.24439333379268646,\n","                      0.2141934335231781,\n","                      0.21331603825092316,\n","                      0.2075718343257904,\n","                      0.20340101420879364,\n","                      0.2205793708562851,\n","                      0.21509495377540588,\n","                      0.192170649766922,\n","                      0.18459481000900269,\n","                      0.18854644894599915,\n","                      0.17889532446861267,\n","                      0.2684159278869629,\n","                      0.18784716725349426]],\n"," 'Validation MCC': [[np.float64(0.7942730492409528),\n","                     np.float64(0.8384898485931601),\n","                     np.float64(0.8606716069509887),\n","                     np.float64(0.8648805854297988),\n","                     np.float64(0.8710519058269591),\n","                     np.float64(0.8763200007675405),\n","                     np.float64(0.8657665468705007),\n","                     np.float64(0.8768722660383761),\n","                     np.float64(0.883776479330779),\n","                     np.float64(0.8664971086204556),\n","                     np.float64(0.885073044152515),\n","                     np.float64(0.8836045610115739),\n","                     np.float64(0.8732480308415034),\n","                     np.float64(0.8898366784701849),\n","                     np.float64(0.893492564510942),\n","                     np.float64(0.8906891715130065),\n","                     np.float64(0.8986266344115252),\n","                     np.float64(0.8975288114554785),\n","                     np.float64(0.8973565230715658),\n","                     np.float64(0.8989714759469004)],\n","                    [np.float64(0.8138850163259191),\n","                     np.float64(0.8427305657441961),\n","                     np.float64(0.8320538743998074),\n","                     np.float64(0.8511065014779876),\n","                     np.float64(0.8574400357872951),\n","                     np.float64(0.8318080813365903),\n","                     np.float64(0.8573952097201342),\n","                     np.float64(0.8694740439349317),\n","                     np.float64(0.8559497119257539),\n","                     np.float64(0.8757945478677299),\n","                     np.float64(0.8711033570765173),\n","                     np.float64(0.8802540006351806),\n","                     np.float64(0.886169542559774),\n","                     np.float64(0.8863861622933645),\n","                     np.float64(0.8853742494639034),\n","                     np.float64(0.8916014938289415),\n","                     np.float64(0.8843907037290383),\n","                     np.float64(0.8802128563825143),\n","                     np.float64(0.8908867151129937),\n","                     np.float64(0.8923147169486937)],\n","                    [np.float64(0.7935939950762274),\n","                     np.float64(0.8347505099194497),\n","                     np.float64(0.8389383843580518),\n","                     np.float64(0.8406853861919771),\n","                     np.float64(0.8594572706555593),\n","                     np.float64(0.8587925545553016),\n","                     np.float64(0.8589134043343706),\n","                     np.float64(0.8404744677377963),\n","                     np.float64(0.8752045812658271),\n","                     np.float64(0.8688590179009116),\n","                     np.float64(0.875924096740364),\n","                     np.float64(0.8844364837653803),\n","                     np.float64(0.8857483269186762),\n","                     np.float64(0.8776991445772644),\n","                     np.float64(0.8919275701456063),\n","                     np.float64(0.885395229380527),\n","                     np.float64(0.8868677820648802),\n","                     np.float64(0.8939041704146854),\n","                     np.float64(0.8945544799243808),\n","                     np.float64(0.8958367416003788)],\n","                    [np.float64(0.7806557885834401),\n","                     np.float64(0.7935249116767809),\n","                     np.float64(0.8420841295476551),\n","                     np.float64(0.854853869683032),\n","                     np.float64(0.8618445629031043),\n","                     np.float64(0.8684372238570742),\n","                     np.float64(0.8762589655184428),\n","                     np.float64(0.8716661707168191),\n","                     np.float64(0.8750860076424968),\n","                     np.float64(0.8753743060792022),\n","                     np.float64(0.8775823918287277),\n","                     np.float64(0.8792760381332279),\n","                     np.float64(0.8860083627476373),\n","                     np.float64(0.8861897314813075),\n","                     np.float64(0.8861364377618794),\n","                     np.float64(0.8850440201374979),\n","                     np.float64(0.8926240322428469),\n","                     np.float64(0.8932559612185932),\n","                     np.float64(0.8884177502296938),\n","                     np.float64(0.8975677429151908)],\n","                    [np.float64(0.8038675065596936),\n","                     np.float64(0.8383690947396568),\n","                     np.float64(0.849400480570114),\n","                     np.float64(0.8581222190667283),\n","                     np.float64(0.8339096439150084),\n","                     np.float64(0.8629696763860555),\n","                     np.float64(0.8627844166107157),\n","                     np.float64(0.8596345480817242),\n","                     np.float64(0.8735712218721547),\n","                     np.float64(0.8749597157424999),\n","                     np.float64(0.8748775843062596),\n","                     np.float64(0.8799968748968485),\n","                     np.float64(0.8709956625296388),\n","                     np.float64(0.8740034445317626),\n","                     np.float64(0.8844246116397912),\n","                     np.float64(0.8883564763200028),\n","                     np.float64(0.8877471920677766),\n","                     np.float64(0.8915477342203876),\n","                     np.float64(0.8476827397166077),\n","                     np.float64(0.8864925989001998)]]}\n","Training Model: BiLSTM_Deep, Fold: 1\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7597 - loss: 0.6441(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8562\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 86ms/step - accuracy: 0.7599 - loss: 0.6438 - val_accuracy: 0.9061 - val_loss: 0.2478 - mcc: 0.8562\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9063 - loss: 0.2454(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8617\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 66ms/step - accuracy: 0.9063 - loss: 0.2454 - val_accuracy: 0.9098 - val_loss: 0.2407 - mcc: 0.8617\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9174 - loss: 0.2125(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8804\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 60ms/step - accuracy: 0.9174 - loss: 0.2125 - val_accuracy: 0.9217 - val_loss: 0.2006 - mcc: 0.8804\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9245 - loss: 0.1900(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8909\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9245 - loss: 0.1900 - val_accuracy: 0.9283 - val_loss: 0.1822 - mcc: 0.8909\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9286 - loss: 0.1801(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8959\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9286 - loss: 0.1801 - val_accuracy: 0.9315 - val_loss: 0.1733 - mcc: 0.8959\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9308 - loss: 0.1762(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8990\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9308 - loss: 0.1762 - val_accuracy: 0.9333 - val_loss: 0.1648 - mcc: 0.8990\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9286 - loss: 0.1807(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.9022\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9286 - loss: 0.1807 - val_accuracy: 0.9358 - val_loss: 0.1602 - mcc: 0.9022\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9342 - loss: 0.1635(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9022\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9342 - loss: 0.1635 - val_accuracy: 0.9358 - val_loss: 0.1604 - mcc: 0.9022\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9373 - loss: 0.1560(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9059\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9373 - loss: 0.1561 - val_accuracy: 0.9381 - val_loss: 0.1540 - mcc: 0.9059\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9365 - loss: 0.1571(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9024\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 60ms/step - accuracy: 0.9365 - loss: 0.1571 - val_accuracy: 0.9356 - val_loss: 0.1638 - mcc: 0.9024\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9371 - loss: 0.1560(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9069\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 61ms/step - accuracy: 0.9371 - loss: 0.1560 - val_accuracy: 0.9387 - val_loss: 0.1503 - mcc: 0.9069\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9394 - loss: 0.1486(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9086\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 61ms/step - accuracy: 0.9394 - loss: 0.1486 - val_accuracy: 0.9401 - val_loss: 0.1482 - mcc: 0.9086\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9385 - loss: 0.1534(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9108\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9385 - loss: 0.1534 - val_accuracy: 0.9413 - val_loss: 0.1447 - mcc: 0.9108\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9420 - loss: 0.1433(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9093\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9420 - loss: 0.1433 - val_accuracy: 0.9402 - val_loss: 0.1478 - mcc: 0.9093\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9436 - loss: 0.1391(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9115\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 62ms/step - accuracy: 0.9436 - loss: 0.1391 - val_accuracy: 0.9415 - val_loss: 0.1460 - mcc: 0.9115\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9428 - loss: 0.1409(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9036\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 59ms/step - accuracy: 0.9428 - loss: 0.1409 - val_accuracy: 0.9366 - val_loss: 0.1548 - mcc: 0.9036\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9416 - loss: 0.1443(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9092\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 60ms/step - accuracy: 0.9416 - loss: 0.1443 - val_accuracy: 0.9403 - val_loss: 0.1472 - mcc: 0.9092\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9436 - loss: 0.1377(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9135\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9436 - loss: 0.1377 - val_accuracy: 0.9432 - val_loss: 0.1416 - mcc: 0.9135\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9436 - loss: 0.1366(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9111\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 62ms/step - accuracy: 0.9436 - loss: 0.1366 - val_accuracy: 0.9412 - val_loss: 0.1444 - mcc: 0.9111\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9446 - loss: 0.1346(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9161\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step - accuracy: 0.9446 - loss: 0.1346 - val_accuracy: 0.9449 - val_loss: 0.1354 - mcc: 0.9161\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 2\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7812 - loss: 0.5860(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8414\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 62ms/step - accuracy: 0.7813 - loss: 0.5857 - val_accuracy: 0.8970 - val_loss: 0.2858 - mcc: 0.8414\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9002 - loss: 0.2729(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8601\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step - accuracy: 0.9002 - loss: 0.2729 - val_accuracy: 0.9091 - val_loss: 0.2546 - mcc: 0.8601\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9144 - loss: 0.2285(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8752\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 60ms/step - accuracy: 0.9144 - loss: 0.2285 - val_accuracy: 0.9187 - val_loss: 0.2124 - mcc: 0.8752\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9238 - loss: 0.1963(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8918\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9238 - loss: 0.1963 - val_accuracy: 0.9294 - val_loss: 0.1781 - mcc: 0.8918\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9219 - loss: 0.2013(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8863\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9219 - loss: 0.2013 - val_accuracy: 0.9258 - val_loss: 0.1913 - mcc: 0.8863\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9262 - loss: 0.1873(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8935\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 61ms/step - accuracy: 0.9262 - loss: 0.1873 - val_accuracy: 0.9305 - val_loss: 0.1754 - mcc: 0.8935\n","Epoch 7/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9290 - loss: 0.1775(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8966\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9290 - loss: 0.1775 - val_accuracy: 0.9324 - val_loss: 0.1681 - mcc: 0.8966\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9334 - loss: 0.1664(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8951\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9334 - loss: 0.1664 - val_accuracy: 0.9314 - val_loss: 0.1711 - mcc: 0.8951\n","Epoch 9/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9344 - loss: 0.1631(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9007\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 59ms/step - accuracy: 0.9344 - loss: 0.1631 - val_accuracy: 0.9352 - val_loss: 0.1629 - mcc: 0.9007\n","Epoch 10/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9358 - loss: 0.1599(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9063\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 60ms/step - accuracy: 0.9358 - loss: 0.1599 - val_accuracy: 0.9387 - val_loss: 0.1507 - mcc: 0.9063\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9378 - loss: 0.1536(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9045\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9378 - loss: 0.1536 - val_accuracy: 0.9376 - val_loss: 0.1538 - mcc: 0.9045\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9384 - loss: 0.1528(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9043\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9384 - loss: 0.1528 - val_accuracy: 0.9375 - val_loss: 0.1591 - mcc: 0.9043\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9389 - loss: 0.1511(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9104\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 60ms/step - accuracy: 0.9389 - loss: 0.1511 - val_accuracy: 0.9413 - val_loss: 0.1444 - mcc: 0.9104\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9403 - loss: 0.1497(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9103\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9403 - loss: 0.1497 - val_accuracy: 0.9410 - val_loss: 0.1463 - mcc: 0.9103\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9411 - loss: 0.1481(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9090\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 61ms/step - accuracy: 0.9411 - loss: 0.1481 - val_accuracy: 0.9405 - val_loss: 0.1504 - mcc: 0.9090\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9419 - loss: 0.1444(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9133\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step - accuracy: 0.9419 - loss: 0.1444 - val_accuracy: 0.9433 - val_loss: 0.1396 - mcc: 0.9133\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9435 - loss: 0.1409(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9072\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9435 - loss: 0.1409 - val_accuracy: 0.9394 - val_loss: 0.1491 - mcc: 0.9072\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9423 - loss: 0.1411(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9130\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step - accuracy: 0.9423 - loss: 0.1411 - val_accuracy: 0.9432 - val_loss: 0.1412 - mcc: 0.9130\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9438 - loss: 0.1383(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9116\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 59ms/step - accuracy: 0.9438 - loss: 0.1383 - val_accuracy: 0.9420 - val_loss: 0.1438 - mcc: 0.9116\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9445 - loss: 0.1353(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9108\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9445 - loss: 0.1353 - val_accuracy: 0.9417 - val_loss: 0.1420 - mcc: 0.9108\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 3\n","Epoch 1/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7761 - loss: 0.5908(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8582\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 64ms/step - accuracy: 0.7762 - loss: 0.5905 - val_accuracy: 0.9073 - val_loss: 0.2458 - mcc: 0.8582\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9062 - loss: 0.2505(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8742\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step - accuracy: 0.9062 - loss: 0.2505 - val_accuracy: 0.9177 - val_loss: 0.2155 - mcc: 0.8742\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9198 - loss: 0.2079(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8818\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9198 - loss: 0.2079 - val_accuracy: 0.9225 - val_loss: 0.1977 - mcc: 0.8818\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9259 - loss: 0.1883(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8818\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 59ms/step - accuracy: 0.9259 - loss: 0.1883 - val_accuracy: 0.9218 - val_loss: 0.1994 - mcc: 0.8818\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9303 - loss: 0.1775(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8891\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9303 - loss: 0.1776 - val_accuracy: 0.9274 - val_loss: 0.1835 - mcc: 0.8891\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9314 - loss: 0.1738(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.9004\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 60ms/step - accuracy: 0.9314 - loss: 0.1738 - val_accuracy: 0.9346 - val_loss: 0.1627 - mcc: 0.9004\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9357 - loss: 0.1611(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.9025\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9357 - loss: 0.1612 - val_accuracy: 0.9361 - val_loss: 0.1595 - mcc: 0.9025\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9362 - loss: 0.1590(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9023\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 60ms/step - accuracy: 0.9362 - loss: 0.1590 - val_accuracy: 0.9360 - val_loss: 0.1606 - mcc: 0.9023\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9375 - loss: 0.1548(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9054\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9375 - loss: 0.1548 - val_accuracy: 0.9379 - val_loss: 0.1549 - mcc: 0.9054\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9383 - loss: 0.1518(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8936\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step - accuracy: 0.9383 - loss: 0.1518 - val_accuracy: 0.9290 - val_loss: 0.1749 - mcc: 0.8936\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9395 - loss: 0.1486(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9026\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9395 - loss: 0.1486 - val_accuracy: 0.9361 - val_loss: 0.1624 - mcc: 0.9026\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9413 - loss: 0.1441(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9058\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 63ms/step - accuracy: 0.9413 - loss: 0.1441 - val_accuracy: 0.9381 - val_loss: 0.1522 - mcc: 0.9058\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9416 - loss: 0.1440(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9124\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 60ms/step - accuracy: 0.9416 - loss: 0.1440 - val_accuracy: 0.9423 - val_loss: 0.1422 - mcc: 0.9124\n","Epoch 14/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9425 - loss: 0.1413(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9085\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 61ms/step - accuracy: 0.9425 - loss: 0.1413 - val_accuracy: 0.9398 - val_loss: 0.1516 - mcc: 0.9085\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9455 - loss: 0.1341(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9127\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 59ms/step - accuracy: 0.9455 - loss: 0.1341 - val_accuracy: 0.9425 - val_loss: 0.1415 - mcc: 0.9127\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9430 - loss: 0.1397(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9126\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 60ms/step - accuracy: 0.9430 - loss: 0.1397 - val_accuracy: 0.9425 - val_loss: 0.1416 - mcc: 0.9126\n","Epoch 17/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9449 - loss: 0.1356(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9153\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9449 - loss: 0.1356 - val_accuracy: 0.9443 - val_loss: 0.1383 - mcc: 0.9153\n","Epoch 18/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9459 - loss: 0.1313(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9098\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 61ms/step - accuracy: 0.9459 - loss: 0.1313 - val_accuracy: 0.9405 - val_loss: 0.1473 - mcc: 0.9098\n","Epoch 19/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9467 - loss: 0.1307(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9154\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9467 - loss: 0.1307 - val_accuracy: 0.9444 - val_loss: 0.1369 - mcc: 0.9154\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9475 - loss: 0.1285(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9072\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 59ms/step - accuracy: 0.9475 - loss: 0.1285 - val_accuracy: 0.9391 - val_loss: 0.1505 - mcc: 0.9072\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 4\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7594 - loss: 0.6512(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8300\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 65ms/step - accuracy: 0.7596 - loss: 0.6506 - val_accuracy: 0.8852 - val_loss: 0.3008 - mcc: 0.8300\n","Epoch 2/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8973 - loss: 0.2773(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8780\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 59ms/step - accuracy: 0.8973 - loss: 0.2773 - val_accuracy: 0.9203 - val_loss: 0.2084 - mcc: 0.8780\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9178 - loss: 0.2133(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8785\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 59ms/step - accuracy: 0.9178 - loss: 0.2133 - val_accuracy: 0.9208 - val_loss: 0.2005 - mcc: 0.8785\n","Epoch 4/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9252 - loss: 0.1913(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8862\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9252 - loss: 0.1913 - val_accuracy: 0.9258 - val_loss: 0.1878 - mcc: 0.8862\n","Epoch 5/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9300 - loss: 0.1782(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8923\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9300 - loss: 0.1782 - val_accuracy: 0.9294 - val_loss: 0.1814 - mcc: 0.8923\n","Epoch 6/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9327 - loss: 0.1692(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8993\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9327 - loss: 0.1692 - val_accuracy: 0.9342 - val_loss: 0.1660 - mcc: 0.8993\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9301 - loss: 0.1765(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.9028\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 59ms/step - accuracy: 0.9301 - loss: 0.1765 - val_accuracy: 0.9363 - val_loss: 0.1598 - mcc: 0.9028\n","Epoch 8/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9352 - loss: 0.1620(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.9038\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9352 - loss: 0.1620 - val_accuracy: 0.9370 - val_loss: 0.1586 - mcc: 0.9038\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9364 - loss: 0.1583(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.9032\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9364 - loss: 0.1583 - val_accuracy: 0.9366 - val_loss: 0.1573 - mcc: 0.9032\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9391 - loss: 0.1503(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.9070\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 65ms/step - accuracy: 0.9391 - loss: 0.1503 - val_accuracy: 0.9392 - val_loss: 0.1530 - mcc: 0.9070\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9399 - loss: 0.1482(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.9071\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 59ms/step - accuracy: 0.9399 - loss: 0.1482 - val_accuracy: 0.9388 - val_loss: 0.1529 - mcc: 0.9071\n","Epoch 12/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9398 - loss: 0.1486(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9026\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 65ms/step - accuracy: 0.9398 - loss: 0.1486 - val_accuracy: 0.9359 - val_loss: 0.1607 - mcc: 0.9026\n","Epoch 13/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9411 - loss: 0.1450(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9025\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 59ms/step - accuracy: 0.9411 - loss: 0.1450 - val_accuracy: 0.9361 - val_loss: 0.1609 - mcc: 0.9025\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9451 - loss: 0.1351(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9090\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 62ms/step - accuracy: 0.9451 - loss: 0.1351 - val_accuracy: 0.9406 - val_loss: 0.1505 - mcc: 0.9090\n","Epoch 15/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9452 - loss: 0.1332(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9046\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 60ms/step - accuracy: 0.9452 - loss: 0.1332 - val_accuracy: 0.9372 - val_loss: 0.1580 - mcc: 0.9046\n","Epoch 16/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9450 - loss: 0.1359(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9108\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9450 - loss: 0.1359 - val_accuracy: 0.9415 - val_loss: 0.1485 - mcc: 0.9108\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9460 - loss: 0.1328(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9110\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9460 - loss: 0.1328 - val_accuracy: 0.9418 - val_loss: 0.1445 - mcc: 0.9110\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9429 - loss: 0.1402(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9124\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9429 - loss: 0.1402 - val_accuracy: 0.9425 - val_loss: 0.1449 - mcc: 0.9124\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9486 - loss: 0.1253(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9109\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.9486 - loss: 0.1253 - val_accuracy: 0.9416 - val_loss: 0.1447 - mcc: 0.9109\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9472 - loss: 0.1283(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9123\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9472 - loss: 0.1283 - val_accuracy: 0.9424 - val_loss: 0.1454 - mcc: 0.9123\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 5\n","Epoch 1/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7577 - loss: 0.6572(3000, 500, 5)\n","\n","Epoch 1 - MCC: 0.8298\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 61ms/step - accuracy: 0.7579 - loss: 0.6567 - val_accuracy: 0.8884 - val_loss: 0.3111 - mcc: 0.8298\n","Epoch 2/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9018 - loss: 0.2654(3000, 500, 5)\n","\n","Epoch 2 - MCC: 0.8573\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9018 - loss: 0.2654 - val_accuracy: 0.9058 - val_loss: 0.2406 - mcc: 0.8573\n","Epoch 3/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8965 - loss: 0.2855(3000, 500, 5)\n","\n","Epoch 3 - MCC: 0.8746\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 58ms/step - accuracy: 0.8965 - loss: 0.2854 - val_accuracy: 0.9177 - val_loss: 0.2139 - mcc: 0.8746\n","Epoch 4/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9150 - loss: 0.2248(3000, 500, 5)\n","\n","Epoch 4 - MCC: 0.8688\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 61ms/step - accuracy: 0.9150 - loss: 0.2248 - val_accuracy: 0.9142 - val_loss: 0.2270 - mcc: 0.8688\n","Epoch 5/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9229 - loss: 0.1969(3000, 500, 5)\n","\n","Epoch 5 - MCC: 0.8768\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 60ms/step - accuracy: 0.9229 - loss: 0.1969 - val_accuracy: 0.9189 - val_loss: 0.2087 - mcc: 0.8768\n","Epoch 6/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9284 - loss: 0.1824(3000, 500, 5)\n","\n","Epoch 6 - MCC: 0.8880\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9284 - loss: 0.1824 - val_accuracy: 0.9266 - val_loss: 0.1852 - mcc: 0.8880\n","Epoch 7/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9329 - loss: 0.1685(3000, 500, 5)\n","\n","Epoch 7 - MCC: 0.8952\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 59ms/step - accuracy: 0.9329 - loss: 0.1685 - val_accuracy: 0.9308 - val_loss: 0.1739 - mcc: 0.8952\n","Epoch 8/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9312 - loss: 0.1735(3000, 500, 5)\n","\n","Epoch 8 - MCC: 0.8993\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 61ms/step - accuracy: 0.9312 - loss: 0.1735 - val_accuracy: 0.9340 - val_loss: 0.1645 - mcc: 0.8993\n","Epoch 9/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9364 - loss: 0.1574(3000, 500, 5)\n","\n","Epoch 9 - MCC: 0.8935\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9364 - loss: 0.1574 - val_accuracy: 0.9299 - val_loss: 0.1749 - mcc: 0.8935\n","Epoch 10/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9378 - loss: 0.1552(3000, 500, 5)\n","\n","Epoch 10 - MCC: 0.8979\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 61ms/step - accuracy: 0.9378 - loss: 0.1552 - val_accuracy: 0.9329 - val_loss: 0.1686 - mcc: 0.8979\n","Epoch 11/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9408 - loss: 0.1481(3000, 500, 5)\n","\n","Epoch 11 - MCC: 0.8984\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9408 - loss: 0.1481 - val_accuracy: 0.9332 - val_loss: 0.1756 - mcc: 0.8984\n","Epoch 12/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9397 - loss: 0.1499(3000, 500, 5)\n","\n","Epoch 12 - MCC: 0.9040\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 61ms/step - accuracy: 0.9397 - loss: 0.1499 - val_accuracy: 0.9370 - val_loss: 0.1540 - mcc: 0.9040\n","Epoch 13/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9420 - loss: 0.1441(3000, 500, 5)\n","\n","Epoch 13 - MCC: 0.9052\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 61ms/step - accuracy: 0.9420 - loss: 0.1441 - val_accuracy: 0.9378 - val_loss: 0.1543 - mcc: 0.9052\n","Epoch 14/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9411 - loss: 0.1467(3000, 500, 5)\n","\n","Epoch 14 - MCC: 0.9000\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9411 - loss: 0.1467 - val_accuracy: 0.9340 - val_loss: 0.1640 - mcc: 0.9000\n","Epoch 15/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9434 - loss: 0.1403(3000, 500, 5)\n","\n","Epoch 15 - MCC: 0.9037\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 59ms/step - accuracy: 0.9434 - loss: 0.1403 - val_accuracy: 0.9366 - val_loss: 0.1589 - mcc: 0.9037\n","Epoch 16/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9433 - loss: 0.1413(3000, 500, 5)\n","\n","Epoch 16 - MCC: 0.9074\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 59ms/step - accuracy: 0.9433 - loss: 0.1413 - val_accuracy: 0.9391 - val_loss: 0.1488 - mcc: 0.9074\n","Epoch 17/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9448 - loss: 0.1358(3000, 500, 5)\n","\n","Epoch 17 - MCC: 0.9071\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 60ms/step - accuracy: 0.9448 - loss: 0.1358 - val_accuracy: 0.9388 - val_loss: 0.1499 - mcc: 0.9071\n","Epoch 18/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9443 - loss: 0.1386(3000, 500, 5)\n","\n","Epoch 18 - MCC: 0.9116\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 59ms/step - accuracy: 0.9443 - loss: 0.1386 - val_accuracy: 0.9419 - val_loss: 0.1427 - mcc: 0.9116\n","Epoch 19/20\n","\u001b[1m749/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9472 - loss: 0.1299(3000, 500, 5)\n","\n","Epoch 19 - MCC: 0.9124\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 60ms/step - accuracy: 0.9472 - loss: 0.1299 - val_accuracy: 0.9424 - val_loss: 0.1405 - mcc: 0.9124\n","Epoch 20/20\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9464 - loss: 0.1318(3000, 500, 5)\n","\n","Epoch 20 - MCC: 0.9097\n","\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 59ms/step - accuracy: 0.9464 - loss: 0.1318 - val_accuracy: 0.9407 - val_loss: 0.1460 - mcc: 0.9097\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n","\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': np.float64(0.9448586666666666),\n","              'mean': np.float64(0.9417373333333334),\n","              'min': np.float64(0.9390853333333333),\n","              'std': np.float64(0.001911754145048754)},\n"," 'Inference Time (s/sample)': {'max': np.float64(0.001723623514175415),\n","                               'mean': np.float64(0.0010133616924285888),\n","                               'min': np.float64(0.000732048749923706),\n","                               'std': np.float64(0.00035915038545372355)},\n"," 'MCC': {'max': np.float64(0.9160973739729593),\n","         'mean': np.float64(0.9112201282070316),\n","         'min': np.float64(0.90718377490913),\n","         'std': np.float64(0.0029584867615140504)},\n"," 'Parameters': 78181,\n"," 'Train Time (s)': {'max': np.float64(1575.5616037845612),\n","                    'mean': np.float64(1465.2052896499633),\n","                    'min': np.float64(1388.9571986198425),\n","                    'std': np.float64(63.04917060808317)},\n"," 'Training Accuracy': [[0.8433642387390137,\n","                        0.9097338318824768,\n","                        0.9195203185081482,\n","                        0.925039529800415,\n","                        0.9280030727386475,\n","                        0.9292344450950623,\n","                        0.9304335713386536,\n","                        0.9343985319137573,\n","                        0.9364683628082275,\n","                        0.937566339969635,\n","                        0.9381137490272522,\n","                        0.9395635724067688,\n","                        0.9378942251205444,\n","                        0.941232442855835,\n","                        0.9423505067825317,\n","                        0.9422523379325867,\n","                        0.9418950080871582,\n","                        0.9439767003059387,\n","                        0.9435566663742065,\n","                        0.9452420473098755],\n","                       [0.849926769733429,\n","                        0.9017362594604492,\n","                        0.915480375289917,\n","                        0.9231758713722229,\n","                        0.9184826016426086,\n","                        0.9251352548599243,\n","                        0.929310142993927,\n","                        0.9330345988273621,\n","                        0.9335176348686218,\n","                        0.9358001947402954,\n","                        0.9364962577819824,\n","                        0.9385067820549011,\n","                        0.9393936991691589,\n","                        0.9397860765457153,\n","                        0.9392248392105103,\n","                        0.9422433972358704,\n","                        0.9429557919502258,\n","                        0.9425748586654663,\n","                        0.9434288144111633,\n","                        0.9443746209144592],\n","                       [0.8516817092895508,\n","                        0.9082958102226257,\n","                        0.9203393459320068,\n","                        0.9261558651924133,\n","                        0.9278374314308167,\n","                        0.9335794448852539,\n","                        0.9343054294586182,\n","                        0.9362500905990601,\n","                        0.9375662207603455,\n","                        0.9383700489997864,\n","                        0.939977765083313,\n","                        0.9409219622612,\n","                        0.940823495388031,\n","                        0.9422340393066406,\n","                        0.9434491395950317,\n","                        0.9446288347244263,\n","                        0.9441784024238586,\n","                        0.9461244940757751,\n","                        0.9461910724639893,\n","                        0.9467625617980957],\n","                       [0.8361712098121643,\n","                        0.9061655402183533,\n","                        0.9188437461853027,\n","                        0.9266958236694336,\n","                        0.9298147559165955,\n","                        0.9330427050590515,\n","                        0.933264970779419,\n","                        0.9344503283500671,\n","                        0.9374895691871643,\n","                        0.9389360547065735,\n","                        0.9402665495872498,\n","                        0.9393256306648254,\n","                        0.9415886998176575,\n","                        0.9431793093681335,\n","                        0.9447312951087952,\n","                        0.9440371990203857,\n","                        0.9451587200164795,\n","                        0.9429141283035278,\n","                        0.9472572207450867,\n","                        0.9478133916854858],\n","                       [0.8386805653572083,\n","                        0.9065219759941101,\n","                        0.9051651954650879,\n","                        0.9157885909080505,\n","                        0.9250046610832214,\n","                        0.9288021922111511,\n","                        0.9317204356193542,\n","                        0.9332402348518372,\n","                        0.9358203411102295,\n","                        0.9362218976020813,\n","                        0.9389587640762329,\n","                        0.939503014087677,\n","                        0.9412614703178406,\n","                        0.9402777552604675,\n","                        0.9429357647895813,\n","                        0.9437255859375,\n","                        0.9435247182846069,\n","                        0.9435204267501831,\n","                        0.9466902017593384,\n","                        0.945726752281189]],\n"," 'Training Loss': [[0.42278414964675903,\n","                    0.2363353669643402,\n","                    0.20669366419315338,\n","                    0.1892973780632019,\n","                    0.18185946345329285,\n","                    0.18019701540470123,\n","                    0.1751008927822113,\n","                    0.16361404955387115,\n","                    0.15848511457443237,\n","                    0.154788076877594,\n","                    0.15330812335014343,\n","                    0.14871250092983246,\n","                    0.15438343584537506,\n","                    0.14562079310417175,\n","                    0.14172260463237762,\n","                    0.14226312935352325,\n","                    0.14320684969425201,\n","                    0.13725458085536957,\n","                    0.13849642872810364,\n","                    0.13345061242580414],\n","                   [0.40462350845336914,\n","                    0.2691822648048401,\n","                    0.22258144617080688,\n","                    0.1976718157529831,\n","                    0.2107565551996231,\n","                    0.19077996909618378,\n","                    0.17700669169425964,\n","                    0.1673327386379242,\n","                    0.16636043787002563,\n","                    0.1596670001745224,\n","                    0.15839721262454987,\n","                    0.15287965536117554,\n","                    0.14955002069473267,\n","                    0.1491076797246933,\n","                    0.15238822996616364,\n","                    0.14261987805366516,\n","                    0.14071689546108246,\n","                    0.14072829484939575,\n","                    0.13942845165729523,\n","                    0.13693393766880035],\n","                   [0.3955526053905487,\n","                    0.2450847178697586,\n","                    0.20643633604049683,\n","                    0.18816770613193512,\n","                    0.18321117758750916,\n","                    0.16706883907318115,\n","                    0.16489562392234802,\n","                    0.15844422578811646,\n","                    0.15549945831298828,\n","                    0.15237553417682648,\n","                    0.1482781320810318,\n","                    0.1458340883255005,\n","                    0.1460035741329193,\n","                    0.14207640290260315,\n","                    0.13894253969192505,\n","                    0.13539712131023407,\n","                    0.13746081292629242,\n","                    0.13075651228427887,\n","                    0.1323235034942627,\n","                    0.13022616505622864],\n","                   [0.44031262397766113,\n","                    0.2511463761329651,\n","                    0.2106906920671463,\n","                    0.18759438395500183,\n","                    0.177464097738266,\n","                    0.16783547401428223,\n","                    0.16763974726200104,\n","                    0.16460727155208588,\n","                    0.15637364983558655,\n","                    0.15046794712543488,\n","                    0.1474027931690216,\n","                    0.15226086974143982,\n","                    0.14390569925308228,\n","                    0.1395752727985382,\n","                    0.13543500006198883,\n","                    0.13796210289001465,\n","                    0.13412733376026154,\n","                    0.14077967405319214,\n","                    0.1280342936515808,\n","                    0.12712375819683075],\n","                   [0.4399348497390747,\n","                    0.24972964823246002,\n","                    0.25669771432876587,\n","                    0.2219322919845581,\n","                    0.19148804247379303,\n","                    0.18067947030067444,\n","                    0.1711859405040741,\n","                    0.16766484081745148,\n","                    0.15992125868797302,\n","                    0.1585010290145874,\n","                    0.15188738703727722,\n","                    0.15009412169456482,\n","                    0.14549989998340607,\n","                    0.14807508885860443,\n","                    0.14063772559165955,\n","                    0.13915793597698212,\n","                    0.1388998031616211,\n","                    0.14019857347011566,\n","                    0.1313943862915039,\n","                    0.13373757898807526]],\n"," 'Validation Accuracy': [[0.9061324000358582,\n","                          0.9098033308982849,\n","                          0.9216535091400146,\n","                          0.928250789642334,\n","                          0.9315258860588074,\n","                          0.933321475982666,\n","                          0.9357874393463135,\n","                          0.9357772469520569,\n","                          0.9380908012390137,\n","                          0.9356459975242615,\n","                          0.9386634826660156,\n","                          0.940092146396637,\n","                          0.9412862062454224,\n","                          0.9401626586914062,\n","                          0.9415160417556763,\n","                          0.9366459250450134,\n","                          0.9402866363525391,\n","                          0.9432127475738525,\n","                          0.9412213563919067,\n","                          0.9448584914207458],\n","                         [0.8970043063163757,\n","                          0.9090919494628906,\n","                          0.9186606407165527,\n","                          0.9294195771217346,\n","                          0.9258253574371338,\n","                          0.9304983019828796,\n","                          0.9323732256889343,\n","                          0.931438148021698,\n","                          0.9352007508277893,\n","                          0.9387258291244507,\n","                          0.9375617504119873,\n","                          0.9374505281448364,\n","                          0.9413005113601685,\n","                          0.9410126805305481,\n","                          0.9404821991920471,\n","                          0.9433093070983887,\n","                          0.939372718334198,\n","                          0.9431502223014832,\n","                          0.9419552683830261,\n","                          0.9417093396186829],\n","                         [0.9072945713996887,\n","                          0.9177398085594177,\n","                          0.9224711060523987,\n","                          0.92181396484375,\n","                          0.9273607730865479,\n","                          0.9345932602882385,\n","                          0.9360609650611877,\n","                          0.935967206954956,\n","                          0.9378679990768433,\n","                          0.9290149211883545,\n","                          0.9361037015914917,\n","                          0.9380599856376648,\n","                          0.9423099756240845,\n","                          0.9398286938667297,\n","                          0.9425427913665771,\n","                          0.9425328969955444,\n","                          0.9442885518074036,\n","                          0.9405099153518677,\n","                          0.9443511366844177,\n","                          0.9390856027603149],\n","                         [0.8852120041847229,\n","                          0.9203203320503235,\n","                          0.9208465814590454,\n","                          0.9257781505584717,\n","                          0.9294484257698059,\n","                          0.9341830015182495,\n","                          0.9363470673561096,\n","                          0.936957836151123,\n","                          0.9366269707679749,\n","                          0.9392032623291016,\n","                          0.9388059377670288,\n","                          0.9359055161476135,\n","                          0.936119794845581,\n","                          0.9405677914619446,\n","                          0.9372371435165405,\n","                          0.9415225982666016,\n","                          0.9418009519577026,\n","                          0.942472517490387,\n","                          0.9416407346725464,\n","                          0.9423531889915466],\n","                         [0.8883873820304871,\n","                          0.9058318138122559,\n","                          0.9177079796791077,\n","                          0.9141606688499451,\n","                          0.9188510775566101,\n","                          0.9266170859336853,\n","                          0.930779218673706,\n","                          0.9340204000473022,\n","                          0.9299140572547913,\n","                          0.9328838586807251,\n","                          0.9332157373428345,\n","                          0.9369542598724365,\n","                          0.9377782940864563,\n","                          0.9340387582778931,\n","                          0.9365653395652771,\n","                          0.939146101474762,\n","                          0.9388409852981567,\n","                          0.9419341087341309,\n","                          0.9424166679382324,\n","                          0.9406799077987671]],\n"," 'Validation Loss': [[0.24780182540416718,\n","                      0.24070534110069275,\n","                      0.20057833194732666,\n","                      0.18224124610424042,\n","                      0.17329668998718262,\n","                      0.16478368639945984,\n","                      0.16024243831634521,\n","                      0.16035205125808716,\n","                      0.15396898984909058,\n","                      0.163848876953125,\n","                      0.15030509233474731,\n","                      0.14823700487613678,\n","                      0.14473138749599457,\n","                      0.14776238799095154,\n","                      0.14596976339817047,\n","                      0.15479835867881775,\n","                      0.14718709886074066,\n","                      0.1416272670030594,\n","                      0.144428551197052,\n","                      0.1353573054075241],\n","                     [0.285798043012619,\n","                      0.2545711100101471,\n","                      0.21240787208080292,\n","                      0.1781168282032013,\n","                      0.1912773698568344,\n","                      0.17536619305610657,\n","                      0.16807961463928223,\n","                      0.171098992228508,\n","                      0.1628832370042801,\n","                      0.15069420635700226,\n","                      0.15381623804569244,\n","                      0.15913625061511993,\n","                      0.1443900465965271,\n","                      0.1463100016117096,\n","                      0.15036876499652863,\n","                      0.13963626325130463,\n","                      0.14913775026798248,\n","                      0.14123043417930603,\n","                      0.1437661051750183,\n","                      0.1419605016708374],\n","                     [0.24580828845500946,\n","                      0.2154896855354309,\n","                      0.19770678877830505,\n","                      0.19940496981143951,\n","                      0.1834656000137329,\n","                      0.1626867800951004,\n","                      0.15954111516475677,\n","                      0.16063250601291656,\n","                      0.15489277243614197,\n","                      0.17491550743579865,\n","                      0.1624089628458023,\n","                      0.15217190980911255,\n","                      0.14216281473636627,\n","                      0.15160182118415833,\n","                      0.14153848588466644,\n","                      0.14155347645282745,\n","                      0.13833631575107574,\n","                      0.1473054140806198,\n","                      0.13694661855697632,\n","                      0.1504572629928589],\n","                     [0.30083519220352173,\n","                      0.20843017101287842,\n","                      0.2004864364862442,\n","                      0.1877714842557907,\n","                      0.18140935897827148,\n","                      0.1659591794013977,\n","                      0.1598108857870102,\n","                      0.15862496197223663,\n","                      0.15731103718280792,\n","                      0.15304115414619446,\n","                      0.15285047888755798,\n","                      0.16065570712089539,\n","                      0.16086460649967194,\n","                      0.1505095213651657,\n","                      0.15801750123500824,\n","                      0.14853687584400177,\n","                      0.14453384280204773,\n","                      0.14487074315547943,\n","                      0.14472094178199768,\n","                      0.14541399478912354],\n","                     [0.3111096918582916,\n","                      0.24060550332069397,\n","                      0.21387073397636414,\n","                      0.22703903913497925,\n","                      0.2086528092622757,\n","                      0.1851668357849121,\n","                      0.17390114068984985,\n","                      0.16446834802627563,\n","                      0.17486310005187988,\n","                      0.16864752769470215,\n","                      0.17563539743423462,\n","                      0.15401849150657654,\n","                      0.15428216755390167,\n","                      0.16403302550315857,\n","                      0.15887047350406647,\n","                      0.1488426774740219,\n","                      0.1499015837907791,\n","                      0.14269976317882538,\n","                      0.14054791629314423,\n","                      0.14602243900299072]],\n"," 'Validation MCC': [[np.float64(0.8562180072286244),\n","                     np.float64(0.8617442042092299),\n","                     np.float64(0.8804369068313281),\n","                     np.float64(0.8908882012038414),\n","                     np.float64(0.8958539625670168),\n","                     np.float64(0.8989551487383854),\n","                     np.float64(0.902242490133519),\n","                     np.float64(0.9022075444737213),\n","                     np.float64(0.9059476273858853),\n","                     np.float64(0.9023692161914256),\n","                     np.float64(0.906895593783342),\n","                     np.float64(0.9086468744881119),\n","                     np.float64(0.910781440114091),\n","                     np.float64(0.9092805635737989),\n","                     np.float64(0.9114794349254898),\n","                     np.float64(0.9035763237168077),\n","                     np.float64(0.9091574061780122),\n","                     np.float64(0.9135275802811437),\n","                     np.float64(0.911101639233195),\n","                     np.float64(0.9160973739729593)],\n","                    [np.float64(0.8414403879674748),\n","                     np.float64(0.8601176525959178),\n","                     np.float64(0.875245205657293),\n","                     np.float64(0.8917991971164445),\n","                     np.float64(0.8862594920280433),\n","                     np.float64(0.8934887028588377),\n","                     np.float64(0.8966058565217988),\n","                     np.float64(0.8951427880419577),\n","                     np.float64(0.900662256139254),\n","                     np.float64(0.9062711958107218),\n","                     np.float64(0.9044827993157464),\n","                     np.float64(0.9043306886913486),\n","                     np.float64(0.9103646273828057),\n","                     np.float64(0.9102868375629813),\n","                     np.float64(0.9089924403661026),\n","                     np.float64(0.913264841023139),\n","                     np.float64(0.9072312510380129),\n","                     np.float64(0.9130121171451454),\n","                     np.float64(0.9115756844091327),\n","                     np.float64(0.9108241160780668)],\n","                    [np.float64(0.8582187683502547),\n","                     np.float64(0.8742228619874063),\n","                     np.float64(0.8818086532733143),\n","                     np.float64(0.8817934483285571),\n","                     np.float64(0.8891213636187087),\n","                     np.float64(0.9004133503926653),\n","                     np.float64(0.9025272192928357),\n","                     np.float64(0.9023411367506368),\n","                     np.float64(0.905414770799762),\n","                     np.float64(0.8935832791157274),\n","                     np.float64(0.902601106966254),\n","                     np.float64(0.9057864763521684),\n","                     np.float64(0.9123649568487988),\n","                     np.float64(0.9084543236810162),\n","                     np.float64(0.9126798656125004),\n","                     np.float64(0.9126382205968685),\n","                     np.float64(0.9152533728631606),\n","                     np.float64(0.9097916312958003),\n","                     np.float64(0.9154223211642958),\n","                     np.float64(0.90718377490913)],\n","                    [np.float64(0.8300449959146162),\n","                     np.float64(0.8780217247571865),\n","                     np.float64(0.8785024143402484),\n","                     np.float64(0.8862223998009137),\n","                     np.float64(0.8922736912959064),\n","                     np.float64(0.8992618671240189),\n","                     np.float64(0.9027794536197676),\n","                     np.float64(0.9037600890414309),\n","                     np.float64(0.9032353931967105),\n","                     np.float64(0.9070253859317166),\n","                     np.float64(0.9071479700903288),\n","                     np.float64(0.9026217391129948),\n","                     np.float64(0.9025170716970566),\n","                     np.float64(0.9090333755345916),\n","                     np.float64(0.9045688847967459),\n","                     np.float64(0.9107877019631716),\n","                     np.float64(0.9110443196623579),\n","                     np.float64(0.9123722495780724),\n","                     np.float64(0.9109378459162516),\n","                     np.float64(0.9123068405341949)],\n","                    [np.float64(0.8297552309584457),\n","                     np.float64(0.8572697673511935),\n","                     np.float64(0.874617880497547),\n","                     np.float64(0.8688041092567064),\n","                     np.float64(0.8767643858459796),\n","                     np.float64(0.8879848822503985),\n","                     np.float64(0.8952469344236784),\n","                     np.float64(0.899322276769154),\n","                     np.float64(0.8934536845184161),\n","                     np.float64(0.8979377336049986),\n","                     np.float64(0.8984012766692844),\n","                     np.float64(0.9040440406458577),\n","                     np.float64(0.9051975124521084),\n","                     np.float64(0.8999613041757901),\n","                     np.float64(0.9037152449355093),\n","                     np.float64(0.9073610542423334),\n","                     np.float64(0.9071366856924197),\n","                     np.float64(0.9115982981740643),\n","                     np.float64(0.9124227398534162),\n","                     np.float64(0.909688535540807)]]}\n"]}]},{"cell_type":"markdown","source":["## Beta Binary Models"],"metadata":{"id":"R0O-6GsWvf7r"}},{"cell_type":"code","source":["data_vec_beta_trim = data_vec[:, 10:15, :]\n","label_vec_beta_trim = label_vec[:, 10:15, :]"],"metadata":{"id":"fWYCplQ8VpRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Class distribution\n","unique, counts = np.unique(label_vec_beta_trim.flatten(), return_counts=True)\n","print(f\"Class distribution: {dict(zip(unique, counts))}\")\n","print(f\"Noise  Percentage: {(counts[0]/(sum(counts))):.4f}\")\n","print(f\"Beta Percentage: {(counts[1]/(sum(counts))):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qrbtk08EV0Qc","executionInfo":{"status":"ok","timestamp":1740307167303,"user_tz":-60,"elapsed":17,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"a563f224-2937-4df7-efa7-cf689c316dd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class distribution: {0: 394387, 3: 355613}\n","Noise  Percentage: 0.5258\n","Beta Percentage: 0.4742\n"]}]},{"cell_type":"code","source":["data_vec_shaped, label_vec_shaped = preprocess_data(data_vector=data_vec_beta_trim, label_vector=label_vec_beta_trim)\n","\n","print(data_vec_shaped.shape)\n","print(label_vec_shaped.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtqgEJxeWHAe","executionInfo":{"status":"ok","timestamp":1740307169175,"user_tz":-60,"elapsed":65,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"e8414b38-4878-4209-c88b-cf8c5d1e496d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(100, 7500, 25)\n","(100, 7500, 1)\n"]}]},{"cell_type":"code","source":["new_seq_len = 500\n","\n","sample_increase_factor = int(data_vec_shaped.shape[1] / new_seq_len)\n","\n","data_vec_reshaped, label_vec_reshaped = split_sequences_keep_data(data_vec_shaped, label_vec_shaped, new_seq_len=new_seq_len)\n","\n","print(data_vec_reshaped.shape)\n","print(label_vec_reshaped.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4ZQLKz5Weej","executionInfo":{"status":"ok","timestamp":1740307170304,"user_tz":-60,"elapsed":35,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"35b66daf-cb57-4692-e0f2-5a1805a2df71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(500, 1500, 25)\n","(500, 1500, 1)\n"]}]},{"cell_type":"code","source":["beta_hilbert_incl_index = [0,3]\n","beta_hilbert_only_index = [3]\n","beta_wavelet_incl_index = [0,12,13,14,15,16]\n","beta_wavelet_only_index = [12,13,14,15,16]\n","beta_allF_index         = beta_hilbert_incl_index + beta_wavelet_only_index\n","\n","hilbert_data_vec = data_vec_reshaped[:,:,beta_hilbert_incl_index]\n","signal_data_vec = data_vec_reshaped[:,:, :1]\n","wavelet_data_vec = data_vec_reshaped[:,:, beta_wavelet_incl_index]\n","all_data_vec = data_vec_reshaped[:,:,beta_allF_index]\n","\n","label_multi = label_vec_reshaped\n","label_binary = label_vec_reshaped.copy()\n","label_binary[label_binary >= 1] = 1  # Convert to binary labels"],"metadata":{"id":"yv4GzMKRlhG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(signal_data_vec.shape)\n","print(hilbert_data_vec.shape)\n","print(wavelet_data_vec.shape)\n","print(all_data_vec.shape)\n","print(label_multi.shape)\n","print(label_binary.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJWrjlvumKtB","executionInfo":{"status":"ok","timestamp":1740307172750,"user_tz":-60,"elapsed":7,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"78cf3ef0-cb5d-457e-f392-e92b67dd873f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(500, 1500, 1)\n","(500, 1500, 2)\n","(500, 1500, 6)\n","(500, 1500, 7)\n","(500, 1500, 1)\n","(500, 1500, 1)\n"]}]},{"cell_type":"markdown","source":["All Features Beta Binary"],"metadata":{"id":"M80aQBSEmCh2"}},{"cell_type":"code","source":["allF_binary_results, trained_models = train_and_evaluate(simple_models_dict, X=all_data_vec, y=label_binary, epochs=n_epochs, dir_name=\"allF_binary\")\n","\n","basePath = \"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Model Comparisons/LSTM Models\"\n","\n","filePath = f\"{basePath}/30_AllF_Binary_Model_Results.json\"\n","\n","with open(filePath, 'w') as f:\n","        json.dump(allF_binary_results, f, indent=4)  # indent=4 for pretty formatting"],"metadata":{"id":"RTJ75uVR2WNq","executionInfo":{"status":"ok","timestamp":1740309740039,"user_tz":-60,"elapsed":2555843,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"360fd66a-2e00-42ad-a181-f4fe32d0e537","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 1\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5319 - loss: 0.6637\n","Epoch 1 - MCC: 0.6515\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.5358 - loss: 0.6614 - val_accuracy: 0.8115 - val_loss: 0.5591 - mcc: 0.6515\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8164 - loss: 0.5165\n","Epoch 2 - MCC: 0.7333\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8166 - loss: 0.5147 - val_accuracy: 0.8670 - val_loss: 0.3588 - mcc: 0.7333\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8676 - loss: 0.3346\n","Epoch 3 - MCC: 0.7777\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8677 - loss: 0.3341 - val_accuracy: 0.8878 - val_loss: 0.2850 - mcc: 0.7777\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8844 - loss: 0.2849\n","Epoch 4 - MCC: 0.8001\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8845 - loss: 0.2846 - val_accuracy: 0.8998 - val_loss: 0.2484 - mcc: 0.8001\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8978 - loss: 0.2499\n","Epoch 5 - MCC: 0.8212\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8978 - loss: 0.2498 - val_accuracy: 0.9107 - val_loss: 0.2206 - mcc: 0.8212\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9030 - loss: 0.2390\n","Epoch 6 - MCC: 0.8317\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9031 - loss: 0.2386 - val_accuracy: 0.9160 - val_loss: 0.2092 - mcc: 0.8317\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9094 - loss: 0.2227\n","Epoch 7 - MCC: 0.8346\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9095 - loss: 0.2227 - val_accuracy: 0.9168 - val_loss: 0.2002 - mcc: 0.8346\n","Epoch 8/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9166 - loss: 0.2081\n","Epoch 8 - MCC: 0.8373\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9164 - loss: 0.2083 - val_accuracy: 0.9189 - val_loss: 0.2018 - mcc: 0.8373\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9118 - loss: 0.2166\n","Epoch 9 - MCC: 0.8490\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.9119 - loss: 0.2162 - val_accuracy: 0.9248 - val_loss: 0.1863 - mcc: 0.8490\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9181 - loss: 0.2021\n","Epoch 10 - MCC: 0.8344\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9180 - loss: 0.2023 - val_accuracy: 0.9168 - val_loss: 0.1973 - mcc: 0.8344\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9105 - loss: 0.2165\n","Epoch 11 - MCC: 0.8385\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9107 - loss: 0.2161 - val_accuracy: 0.9187 - val_loss: 0.1923 - mcc: 0.8385\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9220 - loss: 0.1890\n","Epoch 12 - MCC: 0.8515\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9219 - loss: 0.1893 - val_accuracy: 0.9260 - val_loss: 0.1837 - mcc: 0.8515\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9232 - loss: 0.1901\n","Epoch 13 - MCC: 0.8523\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9230 - loss: 0.1902 - val_accuracy: 0.9262 - val_loss: 0.1823 - mcc: 0.8523\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9177 - loss: 0.2023\n","Epoch 14 - MCC: 0.8543\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9178 - loss: 0.2020 - val_accuracy: 0.9273 - val_loss: 0.1777 - mcc: 0.8543\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9223 - loss: 0.1889\n","Epoch 15 - MCC: 0.8529\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9223 - loss: 0.1888 - val_accuracy: 0.9266 - val_loss: 0.1781 - mcc: 0.8529\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9213 - loss: 0.1923\n","Epoch 16 - MCC: 0.8577\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9213 - loss: 0.1922 - val_accuracy: 0.9291 - val_loss: 0.1718 - mcc: 0.8577\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9227 - loss: 0.1869\n","Epoch 17 - MCC: 0.8549\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9227 - loss: 0.1869 - val_accuracy: 0.9276 - val_loss: 0.1765 - mcc: 0.8549\n","Epoch 18/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9204 - loss: 0.1934\n","Epoch 18 - MCC: 0.8605\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9206 - loss: 0.1927 - val_accuracy: 0.9305 - val_loss: 0.1688 - mcc: 0.8605\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9278 - loss: 0.1778\n","Epoch 19 - MCC: 0.8496\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9277 - loss: 0.1780 - val_accuracy: 0.9249 - val_loss: 0.1762 - mcc: 0.8496\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9232 - loss: 0.1866\n","Epoch 20 - MCC: 0.8581\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9232 - loss: 0.1865 - val_accuracy: 0.9291 - val_loss: 0.1703 - mcc: 0.8581\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9253 - loss: 0.1820\n","Epoch 21 - MCC: 0.8621\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9253 - loss: 0.1820 - val_accuracy: 0.9313 - val_loss: 0.1662 - mcc: 0.8621\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9267 - loss: 0.1800\n","Epoch 22 - MCC: 0.8587\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9266 - loss: 0.1800 - val_accuracy: 0.9295 - val_loss: 0.1688 - mcc: 0.8587\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9259 - loss: 0.1802\n","Epoch 23 - MCC: 0.8561\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9259 - loss: 0.1801 - val_accuracy: 0.9279 - val_loss: 0.1730 - mcc: 0.8561\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9233 - loss: 0.1838\n","Epoch 24 - MCC: 0.8608\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9234 - loss: 0.1838 - val_accuracy: 0.9307 - val_loss: 0.1677 - mcc: 0.8608\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9330 - loss: 0.1640\n","Epoch 25 - MCC: 0.8618\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9328 - loss: 0.1646 - val_accuracy: 0.9310 - val_loss: 0.1656 - mcc: 0.8618\n","Epoch 26/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9223 - loss: 0.1884\n","Epoch 26 - MCC: 0.8653\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.9226 - loss: 0.1877 - val_accuracy: 0.9329 - val_loss: 0.1622 - mcc: 0.8653\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9247 - loss: 0.1825\n","Epoch 27 - MCC: 0.8635\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.9248 - loss: 0.1823 - val_accuracy: 0.9320 - val_loss: 0.1637 - mcc: 0.8635\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9248 - loss: 0.1822\n","Epoch 28 - MCC: 0.8666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9249 - loss: 0.1820 - val_accuracy: 0.9335 - val_loss: 0.1610 - mcc: 0.8666\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9280 - loss: 0.1737\n","Epoch 29 - MCC: 0.8629\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9280 - loss: 0.1738 - val_accuracy: 0.9315 - val_loss: 0.1660 - mcc: 0.8629\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9291 - loss: 0.1730\n","Epoch 30 - MCC: 0.8621\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9290 - loss: 0.1733 - val_accuracy: 0.9312 - val_loss: 0.1645 - mcc: 0.8621\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5221 - loss: 0.6609\n","Epoch 1 - MCC: 0.5129\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.5233 - loss: 0.6599 - val_accuracy: 0.7282 - val_loss: 0.5836 - mcc: 0.5129\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7814 - loss: 0.5444\n","Epoch 2 - MCC: 0.6835\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.7826 - loss: 0.5426 - val_accuracy: 0.8406 - val_loss: 0.3997 - mcc: 0.6835\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8640 - loss: 0.3496\n","Epoch 3 - MCC: 0.7528\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8643 - loss: 0.3486 - val_accuracy: 0.8766 - val_loss: 0.3098 - mcc: 0.7528\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8889 - loss: 0.2795\n","Epoch 4 - MCC: 0.7795\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.8890 - loss: 0.2792 - val_accuracy: 0.8900 - val_loss: 0.2761 - mcc: 0.7795\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9014 - loss: 0.2492\n","Epoch 5 - MCC: 0.7923\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.9015 - loss: 0.2489 - val_accuracy: 0.8961 - val_loss: 0.2552 - mcc: 0.7923\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9127 - loss: 0.2181\n","Epoch 6 - MCC: 0.7886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9126 - loss: 0.2183 - val_accuracy: 0.8942 - val_loss: 0.2543 - mcc: 0.7886\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9109 - loss: 0.2168\n","Epoch 7 - MCC: 0.8123\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9109 - loss: 0.2168 - val_accuracy: 0.9064 - val_loss: 0.2299 - mcc: 0.8123\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9140 - loss: 0.2120\n","Epoch 8 - MCC: 0.8142\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9141 - loss: 0.2118 - val_accuracy: 0.9071 - val_loss: 0.2250 - mcc: 0.8142\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9175 - loss: 0.2026\n","Epoch 9 - MCC: 0.8172\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9175 - loss: 0.2025 - val_accuracy: 0.9088 - val_loss: 0.2194 - mcc: 0.8172\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9219 - loss: 0.1890\n","Epoch 10 - MCC: 0.8194\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9218 - loss: 0.1893 - val_accuracy: 0.9096 - val_loss: 0.2217 - mcc: 0.8194\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9174 - loss: 0.2031\n","Epoch 11 - MCC: 0.8123\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9174 - loss: 0.2030 - val_accuracy: 0.9054 - val_loss: 0.2288 - mcc: 0.8123\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9218 - loss: 0.1912\n","Epoch 12 - MCC: 0.8244\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9217 - loss: 0.1913 - val_accuracy: 0.9124 - val_loss: 0.2113 - mcc: 0.8244\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9228 - loss: 0.1895\n","Epoch 13 - MCC: 0.8232\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9229 - loss: 0.1895 - val_accuracy: 0.9118 - val_loss: 0.2110 - mcc: 0.8232\n","Epoch 14/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9189 - loss: 0.1953\n","Epoch 14 - MCC: 0.8274\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.9192 - loss: 0.1947 - val_accuracy: 0.9137 - val_loss: 0.2106 - mcc: 0.8274\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9231 - loss: 0.1866\n","Epoch 15 - MCC: 0.8225\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9231 - loss: 0.1865 - val_accuracy: 0.9113 - val_loss: 0.2080 - mcc: 0.8225\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9244 - loss: 0.1816\n","Epoch 16 - MCC: 0.8268\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9244 - loss: 0.1816 - val_accuracy: 0.9136 - val_loss: 0.2051 - mcc: 0.8268\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9265 - loss: 0.1776\n","Epoch 17 - MCC: 0.8289\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9264 - loss: 0.1778 - val_accuracy: 0.9146 - val_loss: 0.2063 - mcc: 0.8289\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9302 - loss: 0.1692\n","Epoch 18 - MCC: 0.8294\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9300 - loss: 0.1697 - val_accuracy: 0.9149 - val_loss: 0.2032 - mcc: 0.8294\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9308 - loss: 0.1681\n","Epoch 19 - MCC: 0.8296\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9306 - loss: 0.1686 - val_accuracy: 0.9149 - val_loss: 0.2030 - mcc: 0.8296\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9275 - loss: 0.1760\n","Epoch 20 - MCC: 0.8307\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9274 - loss: 0.1761 - val_accuracy: 0.9153 - val_loss: 0.2012 - mcc: 0.8307\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9287 - loss: 0.1731\n","Epoch 21 - MCC: 0.8285\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9286 - loss: 0.1732 - val_accuracy: 0.9144 - val_loss: 0.2012 - mcc: 0.8285\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9306 - loss: 0.1671\n","Epoch 22 - MCC: 0.8301\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.9305 - loss: 0.1674 - val_accuracy: 0.9152 - val_loss: 0.2032 - mcc: 0.8301\n","Epoch 23/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9305 - loss: 0.1677\n","Epoch 23 - MCC: 0.8271\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.9302 - loss: 0.1684 - val_accuracy: 0.9136 - val_loss: 0.2050 - mcc: 0.8271\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9263 - loss: 0.1793\n","Epoch 24 - MCC: 0.8329\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9264 - loss: 0.1791 - val_accuracy: 0.9166 - val_loss: 0.1988 - mcc: 0.8329\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9299 - loss: 0.1716\n","Epoch 25 - MCC: 0.8346\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9299 - loss: 0.1716 - val_accuracy: 0.9175 - val_loss: 0.1958 - mcc: 0.8346\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9276 - loss: 0.1742\n","Epoch 26 - MCC: 0.8299\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9276 - loss: 0.1741 - val_accuracy: 0.9152 - val_loss: 0.2019 - mcc: 0.8299\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9306 - loss: 0.1694\n","Epoch 27 - MCC: 0.8360\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9306 - loss: 0.1694 - val_accuracy: 0.9179 - val_loss: 0.1953 - mcc: 0.8360\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9295 - loss: 0.1695\n","Epoch 28 - MCC: 0.8353\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9296 - loss: 0.1695 - val_accuracy: 0.9178 - val_loss: 0.1957 - mcc: 0.8353\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9248 - loss: 0.1812\n","Epoch 29 - MCC: 0.8385\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9250 - loss: 0.1808 - val_accuracy: 0.9193 - val_loss: 0.1937 - mcc: 0.8385\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9322 - loss: 0.1658\n","Epoch 30 - MCC: 0.8346\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9322 - loss: 0.1659 - val_accuracy: 0.9175 - val_loss: 0.1956 - mcc: 0.8346\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 3\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4954 - loss: 0.6730\n","Epoch 1 - MCC: 0.4517\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.4987 - loss: 0.6708 - val_accuracy: 0.6651 - val_loss: 0.5894 - mcc: 0.4517\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7427 - loss: 0.5474\n","Epoch 2 - MCC: 0.7134\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.7445 - loss: 0.5456 - val_accuracy: 0.8568 - val_loss: 0.3874 - mcc: 0.7134\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8612 - loss: 0.3571\n","Epoch 3 - MCC: 0.7558\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8615 - loss: 0.3559 - val_accuracy: 0.8781 - val_loss: 0.2932 - mcc: 0.7558\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8872 - loss: 0.2800\n","Epoch 4 - MCC: 0.8053\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8872 - loss: 0.2798 - val_accuracy: 0.9031 - val_loss: 0.2426 - mcc: 0.8053\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9020 - loss: 0.2421\n","Epoch 5 - MCC: 0.8132\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9020 - loss: 0.2420 - val_accuracy: 0.9055 - val_loss: 0.2279 - mcc: 0.8132\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9084 - loss: 0.2266\n","Epoch 6 - MCC: 0.8148\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9083 - loss: 0.2267 - val_accuracy: 0.9070 - val_loss: 0.2209 - mcc: 0.8148\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9092 - loss: 0.2228\n","Epoch 7 - MCC: 0.8321\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9092 - loss: 0.2226 - val_accuracy: 0.9164 - val_loss: 0.2058 - mcc: 0.8321\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9119 - loss: 0.2142\n","Epoch 8 - MCC: 0.8359\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9120 - loss: 0.2140 - val_accuracy: 0.9183 - val_loss: 0.1999 - mcc: 0.8359\n","Epoch 9/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9152 - loss: 0.2050\n","Epoch 9 - MCC: 0.8370\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9152 - loss: 0.2050 - val_accuracy: 0.9187 - val_loss: 0.1965 - mcc: 0.8370\n","Epoch 10/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9174 - loss: 0.2013\n","Epoch 10 - MCC: 0.8336\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.9173 - loss: 0.2014 - val_accuracy: 0.9167 - val_loss: 0.2016 - mcc: 0.8336\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9169 - loss: 0.2004\n","Epoch 11 - MCC: 0.8412\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9169 - loss: 0.2005 - val_accuracy: 0.9209 - val_loss: 0.1901 - mcc: 0.8412\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9198 - loss: 0.1950\n","Epoch 12 - MCC: 0.8442\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9198 - loss: 0.1950 - val_accuracy: 0.9224 - val_loss: 0.1882 - mcc: 0.8442\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9203 - loss: 0.1928\n","Epoch 13 - MCC: 0.8391\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9203 - loss: 0.1930 - val_accuracy: 0.9199 - val_loss: 0.1897 - mcc: 0.8391\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9185 - loss: 0.1953\n","Epoch 14 - MCC: 0.8400\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9186 - loss: 0.1953 - val_accuracy: 0.9203 - val_loss: 0.1878 - mcc: 0.8400\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9211 - loss: 0.1902\n","Epoch 15 - MCC: 0.8457\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9210 - loss: 0.1904 - val_accuracy: 0.9231 - val_loss: 0.1842 - mcc: 0.8457\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9221 - loss: 0.1894\n","Epoch 16 - MCC: 0.8450\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9220 - loss: 0.1895 - val_accuracy: 0.9227 - val_loss: 0.1858 - mcc: 0.8450\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9194 - loss: 0.1938\n","Epoch 17 - MCC: 0.8474\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9195 - loss: 0.1936 - val_accuracy: 0.9240 - val_loss: 0.1809 - mcc: 0.8474\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9224 - loss: 0.1868\n","Epoch 18 - MCC: 0.8486\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.9224 - loss: 0.1868 - val_accuracy: 0.9246 - val_loss: 0.1798 - mcc: 0.8486\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9244 - loss: 0.1822\n","Epoch 19 - MCC: 0.8512\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9243 - loss: 0.1824 - val_accuracy: 0.9259 - val_loss: 0.1793 - mcc: 0.8512\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9258 - loss: 0.1809\n","Epoch 20 - MCC: 0.8522\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9257 - loss: 0.1810 - val_accuracy: 0.9264 - val_loss: 0.1764 - mcc: 0.8522\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9234 - loss: 0.1844\n","Epoch 21 - MCC: 0.8544\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9234 - loss: 0.1843 - val_accuracy: 0.9275 - val_loss: 0.1738 - mcc: 0.8544\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9285 - loss: 0.1737\n","Epoch 22 - MCC: 0.8547\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9284 - loss: 0.1740 - val_accuracy: 0.9277 - val_loss: 0.1750 - mcc: 0.8547\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9277 - loss: 0.1746\n","Epoch 23 - MCC: 0.8517\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9276 - loss: 0.1748 - val_accuracy: 0.9258 - val_loss: 0.1763 - mcc: 0.8517\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9216 - loss: 0.1877\n","Epoch 24 - MCC: 0.8551\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9217 - loss: 0.1875 - val_accuracy: 0.9277 - val_loss: 0.1748 - mcc: 0.8551\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9231 - loss: 0.1858\n","Epoch 25 - MCC: 0.8544\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9232 - loss: 0.1857 - val_accuracy: 0.9275 - val_loss: 0.1737 - mcc: 0.8544\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9250 - loss: 0.1782\n","Epoch 26 - MCC: 0.8557\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9250 - loss: 0.1782 - val_accuracy: 0.9280 - val_loss: 0.1731 - mcc: 0.8557\n","Epoch 27/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9248 - loss: 0.1794\n","Epoch 27 - MCC: 0.8577\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.9250 - loss: 0.1792 - val_accuracy: 0.9291 - val_loss: 0.1707 - mcc: 0.8577\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9304 - loss: 0.1672\n","Epoch 28 - MCC: 0.8610\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9303 - loss: 0.1675 - val_accuracy: 0.9308 - val_loss: 0.1673 - mcc: 0.8610\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9274 - loss: 0.1756\n","Epoch 29 - MCC: 0.8533\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9273 - loss: 0.1757 - val_accuracy: 0.9267 - val_loss: 0.1775 - mcc: 0.8533\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9324 - loss: 0.1646\n","Epoch 30 - MCC: 0.8619\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9322 - loss: 0.1651 - val_accuracy: 0.9312 - val_loss: 0.1674 - mcc: 0.8619\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5141 - loss: 0.6665\n","Epoch 1 - MCC: 0.5150\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.5153 - loss: 0.6655 - val_accuracy: 0.7057 - val_loss: 0.5770 - mcc: 0.5150\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7890 - loss: 0.5337\n","Epoch 2 - MCC: 0.7241\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.7899 - loss: 0.5321 - val_accuracy: 0.8623 - val_loss: 0.3728 - mcc: 0.7241\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8644 - loss: 0.3447\n","Epoch 3 - MCC: 0.7958\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8648 - loss: 0.3436 - val_accuracy: 0.8979 - val_loss: 0.2678 - mcc: 0.7958\n","Epoch 4/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8940 - loss: 0.2652\n","Epoch 4 - MCC: 0.8141\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.8939 - loss: 0.2652 - val_accuracy: 0.9071 - val_loss: 0.2415 - mcc: 0.8141\n","Epoch 5/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9004 - loss: 0.2499\n","Epoch 5 - MCC: 0.8242\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.9007 - loss: 0.2491 - val_accuracy: 0.9113 - val_loss: 0.2238 - mcc: 0.8242\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9092 - loss: 0.2238\n","Epoch 6 - MCC: 0.8346\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9091 - loss: 0.2239 - val_accuracy: 0.9173 - val_loss: 0.2109 - mcc: 0.8346\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9112 - loss: 0.2185\n","Epoch 7 - MCC: 0.8388\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9112 - loss: 0.2184 - val_accuracy: 0.9195 - val_loss: 0.2044 - mcc: 0.8388\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9162 - loss: 0.2072\n","Epoch 8 - MCC: 0.8441\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9162 - loss: 0.2072 - val_accuracy: 0.9222 - val_loss: 0.1955 - mcc: 0.8441\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9188 - loss: 0.1966\n","Epoch 9 - MCC: 0.8471\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9187 - loss: 0.1968 - val_accuracy: 0.9236 - val_loss: 0.1908 - mcc: 0.8471\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9173 - loss: 0.1995\n","Epoch 10 - MCC: 0.8511\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9173 - loss: 0.1994 - val_accuracy: 0.9256 - val_loss: 0.1876 - mcc: 0.8511\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9190 - loss: 0.1968\n","Epoch 11 - MCC: 0.8493\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9190 - loss: 0.1968 - val_accuracy: 0.9246 - val_loss: 0.1861 - mcc: 0.8493\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9213 - loss: 0.1885\n","Epoch 12 - MCC: 0.8520\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9212 - loss: 0.1888 - val_accuracy: 0.9260 - val_loss: 0.1842 - mcc: 0.8520\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9182 - loss: 0.1966\n","Epoch 13 - MCC: 0.8524\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9183 - loss: 0.1964 - val_accuracy: 0.9263 - val_loss: 0.1825 - mcc: 0.8524\n","Epoch 14/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9155 - loss: 0.2030\n","Epoch 14 - MCC: 0.8576\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9159 - loss: 0.2019 - val_accuracy: 0.9289 - val_loss: 0.1777 - mcc: 0.8576\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9181 - loss: 0.1951\n","Epoch 15 - MCC: 0.8552\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9181 - loss: 0.1949 - val_accuracy: 0.9278 - val_loss: 0.1791 - mcc: 0.8552\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9242 - loss: 0.1823\n","Epoch 16 - MCC: 0.8585\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9241 - loss: 0.1825 - val_accuracy: 0.9294 - val_loss: 0.1783 - mcc: 0.8585\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9228 - loss: 0.1865\n","Epoch 17 - MCC: 0.8523\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9228 - loss: 0.1865 - val_accuracy: 0.9257 - val_loss: 0.1828 - mcc: 0.8523\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9208 - loss: 0.1886\n","Epoch 18 - MCC: 0.8599\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9208 - loss: 0.1887 - val_accuracy: 0.9301 - val_loss: 0.1742 - mcc: 0.8599\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9216 - loss: 0.1893\n","Epoch 19 - MCC: 0.8603\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9215 - loss: 0.1894 - val_accuracy: 0.9303 - val_loss: 0.1748 - mcc: 0.8603\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9221 - loss: 0.1864\n","Epoch 20 - MCC: 0.8592\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9222 - loss: 0.1863 - val_accuracy: 0.9297 - val_loss: 0.1755 - mcc: 0.8592\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9271 - loss: 0.1753\n","Epoch 21 - MCC: 0.8625\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9270 - loss: 0.1755 - val_accuracy: 0.9314 - val_loss: 0.1715 - mcc: 0.8625\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9229 - loss: 0.1856\n","Epoch 22 - MCC: 0.8586\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9230 - loss: 0.1856 - val_accuracy: 0.9295 - val_loss: 0.1732 - mcc: 0.8586\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9234 - loss: 0.1833\n","Epoch 23 - MCC: 0.8614\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.9234 - loss: 0.1832 - val_accuracy: 0.9308 - val_loss: 0.1710 - mcc: 0.8614\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9283 - loss: 0.1706\n","Epoch 24 - MCC: 0.8604\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9282 - loss: 0.1710 - val_accuracy: 0.9304 - val_loss: 0.1712 - mcc: 0.8604\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9241 - loss: 0.1812\n","Epoch 25 - MCC: 0.8611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step - accuracy: 0.9242 - loss: 0.1810 - val_accuracy: 0.9307 - val_loss: 0.1687 - mcc: 0.8611\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9232 - loss: 0.1826\n","Epoch 26 - MCC: 0.8643\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9233 - loss: 0.1825 - val_accuracy: 0.9323 - val_loss: 0.1708 - mcc: 0.8643\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9253 - loss: 0.1802\n","Epoch 27 - MCC: 0.8591\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9254 - loss: 0.1801 - val_accuracy: 0.9293 - val_loss: 0.1730 - mcc: 0.8591\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9256 - loss: 0.1768\n","Epoch 28 - MCC: 0.8663\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9256 - loss: 0.1768 - val_accuracy: 0.9333 - val_loss: 0.1669 - mcc: 0.8663\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9288 - loss: 0.1723\n","Epoch 29 - MCC: 0.8647\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9286 - loss: 0.1724 - val_accuracy: 0.9323 - val_loss: 0.1675 - mcc: 0.8647\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9291 - loss: 0.1707\n","Epoch 30 - MCC: 0.8647\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9290 - loss: 0.1709 - val_accuracy: 0.9325 - val_loss: 0.1672 - mcc: 0.8647\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.4975 - loss: 0.6663\n","Epoch 1 - MCC: 0.5892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.4998 - loss: 0.6651 - val_accuracy: 0.7699 - val_loss: 0.5714 - mcc: 0.5892\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8160 - loss: 0.5239\n","Epoch 2 - MCC: 0.6957\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.8163 - loss: 0.5223 - val_accuracy: 0.8480 - val_loss: 0.3830 - mcc: 0.6957\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8648 - loss: 0.3415\n","Epoch 3 - MCC: 0.7494\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8651 - loss: 0.3407 - val_accuracy: 0.8750 - val_loss: 0.3025 - mcc: 0.7494\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8848 - loss: 0.2869\n","Epoch 4 - MCC: 0.7630\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8848 - loss: 0.2868 - val_accuracy: 0.8817 - val_loss: 0.2857 - mcc: 0.7630\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8883 - loss: 0.2722\n","Epoch 5 - MCC: 0.7842\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.8884 - loss: 0.2719 - val_accuracy: 0.8923 - val_loss: 0.2623 - mcc: 0.7842\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8984 - loss: 0.2479\n","Epoch 6 - MCC: 0.8084\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8985 - loss: 0.2475 - val_accuracy: 0.9043 - val_loss: 0.2335 - mcc: 0.8084\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9069 - loss: 0.2265\n","Epoch 7 - MCC: 0.8147\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9071 - loss: 0.2262 - val_accuracy: 0.9071 - val_loss: 0.2258 - mcc: 0.8147\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9126 - loss: 0.2150\n","Epoch 8 - MCC: 0.8211\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9126 - loss: 0.2149 - val_accuracy: 0.9105 - val_loss: 0.2164 - mcc: 0.8211\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9122 - loss: 0.2113\n","Epoch 9 - MCC: 0.8234\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9124 - loss: 0.2110 - val_accuracy: 0.9118 - val_loss: 0.2155 - mcc: 0.8234\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9172 - loss: 0.2018\n","Epoch 10 - MCC: 0.8060\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9173 - loss: 0.2017 - val_accuracy: 0.9028 - val_loss: 0.2219 - mcc: 0.8060\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9179 - loss: 0.1969\n","Epoch 11 - MCC: 0.8339\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.9179 - loss: 0.1969 - val_accuracy: 0.9171 - val_loss: 0.2028 - mcc: 0.8339\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9247 - loss: 0.1834\n","Epoch 12 - MCC: 0.8299\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9246 - loss: 0.1838 - val_accuracy: 0.9150 - val_loss: 0.2014 - mcc: 0.8299\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9291 - loss: 0.1755\n","Epoch 13 - MCC: 0.8304\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9289 - loss: 0.1761 - val_accuracy: 0.9149 - val_loss: 0.2045 - mcc: 0.8304\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9230 - loss: 0.1895\n","Epoch 14 - MCC: 0.8314\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9231 - loss: 0.1894 - val_accuracy: 0.9158 - val_loss: 0.2021 - mcc: 0.8314\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9205 - loss: 0.1927\n","Epoch 15 - MCC: 0.8410\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9206 - loss: 0.1925 - val_accuracy: 0.9206 - val_loss: 0.1944 - mcc: 0.8410\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9245 - loss: 0.1816\n","Epoch 16 - MCC: 0.8404\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9245 - loss: 0.1817 - val_accuracy: 0.9202 - val_loss: 0.1939 - mcc: 0.8404\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9195 - loss: 0.1944\n","Epoch 17 - MCC: 0.8247\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9196 - loss: 0.1941 - val_accuracy: 0.9119 - val_loss: 0.2068 - mcc: 0.8247\n","Epoch 18/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9269 - loss: 0.1792\n","Epoch 18 - MCC: 0.8402\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.9267 - loss: 0.1794 - val_accuracy: 0.9200 - val_loss: 0.1915 - mcc: 0.8402\n","Epoch 19/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9237 - loss: 0.1837\n","Epoch 19 - MCC: 0.8411\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9239 - loss: 0.1833 - val_accuracy: 0.9207 - val_loss: 0.1894 - mcc: 0.8411\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9263 - loss: 0.1770\n","Epoch 20 - MCC: 0.8386\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9263 - loss: 0.1771 - val_accuracy: 0.9192 - val_loss: 0.1913 - mcc: 0.8386\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9274 - loss: 0.1765\n","Epoch 21 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9274 - loss: 0.1765 - val_accuracy: 0.9230 - val_loss: 0.1858 - mcc: 0.8458\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9302 - loss: 0.1678\n","Epoch 22 - MCC: 0.8456\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9301 - loss: 0.1681 - val_accuracy: 0.9229 - val_loss: 0.1856 - mcc: 0.8456\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9261 - loss: 0.1791\n","Epoch 23 - MCC: 0.8417\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9261 - loss: 0.1790 - val_accuracy: 0.9209 - val_loss: 0.1873 - mcc: 0.8417\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9305 - loss: 0.1689\n","Epoch 24 - MCC: 0.8436\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9304 - loss: 0.1691 - val_accuracy: 0.9220 - val_loss: 0.1861 - mcc: 0.8436\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9287 - loss: 0.1711\n","Epoch 25 - MCC: 0.8422\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9286 - loss: 0.1712 - val_accuracy: 0.9212 - val_loss: 0.1867 - mcc: 0.8422\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9260 - loss: 0.1764\n","Epoch 26 - MCC: 0.8489\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9261 - loss: 0.1762 - val_accuracy: 0.9242 - val_loss: 0.1829 - mcc: 0.8489\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9326 - loss: 0.1635\n","Epoch 27 - MCC: 0.8476\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.9325 - loss: 0.1638 - val_accuracy: 0.9238 - val_loss: 0.1841 - mcc: 0.8476\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9246 - loss: 0.1797\n","Epoch 28 - MCC: 0.8485\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.9248 - loss: 0.1793 - val_accuracy: 0.9243 - val_loss: 0.1806 - mcc: 0.8485\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9309 - loss: 0.1651\n","Epoch 29 - MCC: 0.8501\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9309 - loss: 0.1652 - val_accuracy: 0.9248 - val_loss: 0.1813 - mcc: 0.8501\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9325 - loss: 0.1625\n","Epoch 30 - MCC: 0.8525\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9324 - loss: 0.1626 - val_accuracy: 0.9262 - val_loss: 0.1784 - mcc: 0.8525\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9325266666666666,\n","              'mean': 0.9277306666666666,\n","              'min': 0.9175,\n","              'std': 0.005547452828901644},\n"," 'Inference Time (s/sample)': {'max': 0.0019939184188842774,\n","                               'mean': 0.0016186923980712892,\n","                               'min': 0.0013416719436645508,\n","                               'std': 0.0002690051471599492},\n"," 'MCC': {'max': 0.8647491832268185,\n","         'mean': 0.8551646046711916,\n","         'min': 0.8346151841753867,\n","         'std': 0.01107951703050232},\n"," 'Parameters': 5153,\n"," 'Train Time (s)': {'max': 45.05508875846863,\n","                    'mean': 44.1069429397583,\n","                    'min': 42.80695629119873,\n","                    'std': 0.8403188174154053},\n"," 'Training Accuracy': [[0.5818666815757751,\n","                        0.8209049105644226,\n","                        0.8698366284370422,\n","                        0.8871016502380371,\n","                        0.8980000019073486,\n","                        0.9069100022315979,\n","                        0.9102548956871033,\n","                        0.9139867424964905,\n","                        0.9155516624450684,\n","                        0.9159767031669617,\n","                        0.9155883193016052,\n","                        0.9186716675758362,\n","                        0.9201183319091797,\n","                        0.9214465618133545,\n","                        0.9227033257484436,\n","                        0.9219866991043091,\n","                        0.9222714900970459,\n","                        0.9240016341209412,\n","                        0.9247183203697205,\n","                        0.923550009727478,\n","                        0.9243166446685791,\n","                        0.925153374671936,\n","                        0.926266610622406,\n","                        0.9247833490371704,\n","                        0.9260949492454529,\n","                        0.9259783029556274,\n","                        0.9267067909240723,\n","                        0.9267899990081787,\n","                        0.9277416467666626,\n","                        0.9264416694641113],\n","                       [0.5520966649055481,\n","                        0.8131248950958252,\n","                        0.8717933893203735,\n","                        0.890666663646698,\n","                        0.9039666056632996,\n","                        0.9088684320449829,\n","                        0.9119598865509033,\n","                        0.9157898426055908,\n","                        0.9186366200447083,\n","                        0.9198199510574341,\n","                        0.9185716509819031,\n","                        0.9207834005355835,\n","                        0.9228915572166443,\n","                        0.9230799674987793,\n","                        0.9237050414085388,\n","                        0.9242265820503235,\n","                        0.9245800971984863,\n","                        0.9252400398254395,\n","                        0.9248883128166199,\n","                        0.9261616468429565,\n","                        0.9274783134460449,\n","                        0.9275499582290649,\n","                        0.9265083074569702,\n","                        0.9286201000213623,\n","                        0.9293618202209473,\n","                        0.9284900426864624,\n","                        0.9297000765800476,\n","                        0.9304800629615784,\n","                        0.9293533563613892,\n","                        0.9312917590141296],\n","                       [0.538361668586731,\n","                        0.7908183336257935,\n","                        0.869303286075592,\n","                        0.8895748853683472,\n","                        0.9032432436943054,\n","                        0.9061615467071533,\n","                        0.910663366317749,\n","                        0.9142249822616577,\n","                        0.915293276309967,\n","                        0.9167734384536743,\n","                        0.9160683155059814,\n","                        0.9184783697128296,\n","                        0.9184333086013794,\n","                        0.9195183515548706,\n","                        0.9189132452011108,\n","                        0.9193633794784546,\n","                        0.92194002866745,\n","                        0.9233800768852234,\n","                        0.9216016530990601,\n","                        0.9240800738334656,\n","                        0.9241082668304443,\n","                        0.9255400896072388,\n","                        0.925441563129425,\n","                        0.9234349727630615,\n","                        0.9249100685119629,\n","                        0.9249365925788879,\n","                        0.9265583157539368,\n","                        0.9274051189422607,\n","                        0.9262499809265137,\n","                        0.9270967245101929],\n","                       [0.5461750030517578,\n","                        0.8122098445892334,\n","                        0.8725767731666565,\n","                        0.8926649689674377,\n","                        0.9035932421684265,\n","                        0.9088083505630493,\n","                        0.9111183285713196,\n","                        0.9154701232910156,\n","                        0.9162049293518066,\n","                        0.9186151027679443,\n","                        0.9192882776260376,\n","                        0.918596625328064,\n","                        0.9208783507347107,\n","                        0.9213216304779053,\n","                        0.9198998808860779,\n","                        0.9220666289329529,\n","                        0.9230515956878662,\n","                        0.9201534390449524,\n","                        0.9194515943527222,\n","                        0.923478364944458,\n","                        0.9246150255203247,\n","                        0.9232000112533569,\n","                        0.924704909324646,\n","                        0.9247549176216125,\n","                        0.9255567193031311,\n","                        0.9255300760269165,\n","                        0.9263899922370911,\n","                        0.9257383942604065,\n","                        0.9256833791732788,\n","                        0.9257633090019226],\n","                       [0.5581666827201843,\n","                        0.8243533372879028,\n","                        0.8711265325546265,\n","                        0.8835766315460205,\n","                        0.8913933634757996,\n","                        0.9031034111976624,\n","                        0.9105783700942993,\n","                        0.9138316512107849,\n","                        0.9168384075164795,\n","                        0.9187148809432983,\n","                        0.9182032942771912,\n","                        0.9213632941246033,\n","                        0.9220900535583496,\n","                        0.9235000014305115,\n","                        0.9236618280410767,\n","                        0.9239066243171692,\n","                        0.9225183725357056,\n","                        0.9248200058937073,\n","                        0.9261084794998169,\n","                        0.9259634613990784,\n","                        0.9271699786186218,\n","                        0.9274099469184875,\n","                        0.927060067653656,\n","                        0.9269798994064331,\n","                        0.928163468837738,\n","                        0.928576648235321,\n","                        0.9297149777412415,\n","                        0.9290665984153748,\n","                        0.9306865930557251,\n","                        0.9304817318916321]],\n"," 'Training Loss': [[0.6330676078796387,\n","                    0.4697817265987396,\n","                    0.3234179615974426,\n","                    0.2768726050853729,\n","                    0.2486298531293869,\n","                    0.23012594878673553,\n","                    0.22067919373512268,\n","                    0.2113020271062851,\n","                    0.20830616354942322,\n","                    0.20637328922748566,\n","                    0.20675194263458252,\n","                    0.19746924936771393,\n","                    0.19487260282039642,\n","                    0.19237037003040314,\n","                    0.1885337233543396,\n","                    0.19028179347515106,\n","                    0.18862658739089966,\n","                    0.1844712495803833,\n","                    0.1831672340631485,\n","                    0.184412881731987,\n","                    0.1830490082502365,\n","                    0.18218380212783813,\n","                    0.17971640825271606,\n","                    0.1812778115272522,\n","                    0.1787445843219757,\n","                    0.17939746379852295,\n","                    0.17722825706005096,\n","                    0.17759840190410614,\n","                    0.1751335710287094,\n","                    0.17857243120670319],\n","                   [0.6355169415473938,\n","                    0.4978314936161041,\n","                    0.32439926266670227,\n","                    0.27246731519699097,\n","                    0.24113762378692627,\n","                    0.22470833361148834,\n","                    0.21570010483264923,\n","                    0.2075575590133667,\n","                    0.19967857003211975,\n","                    0.1955556422472,\n","                    0.19943898916244507,\n","                    0.19323956966400146,\n","                    0.18806427717208862,\n","                    0.1869795024394989,\n","                    0.18450823426246643,\n","                    0.18357671797275543,\n","                    0.18143849074840546,\n","                    0.18107591569423676,\n","                    0.18145321309566498,\n","                    0.17831870913505554,\n","                    0.17535439133644104,\n","                    0.17501430213451385,\n","                    0.17709822952747345,\n","                    0.17359142005443573,\n","                    0.17237083613872528,\n","                    0.17323549091815948,\n","                    0.1708882451057434,\n","                    0.1682780683040619,\n","                    0.17072658240795135,\n","                    0.16773447394371033],\n","                   [0.6444273591041565,\n","                    0.4988119900226593,\n","                    0.32684317231178284,\n","                    0.2747275233268738,\n","                    0.23802141845226288,\n","                    0.23046685755252838,\n","                    0.2188357561826706,\n","                    0.2084578573703766,\n","                    0.20459318161010742,\n","                    0.20295307040214539,\n","                    0.20334726572036743,\n","                    0.19633273780345917,\n","                    0.19736787676811218,\n","                    0.19355568289756775,\n","                    0.19512447714805603,\n","                    0.1931697130203247,\n","                    0.18793880939483643,\n","                    0.18473222851753235,\n","                    0.1882990598678589,\n","                    0.18390949070453644,\n","                    0.18272942304611206,\n","                    0.1799401044845581,\n","                    0.18006780743598938,\n","                    0.18386082351207733,\n","                    0.18154437839984894,\n","                    0.17962230741977692,\n","                    0.17665694653987885,\n","                    0.1750181019306183,\n","                    0.17698383331298828,\n","                    0.17614111304283142],\n","                   [0.6395460367202759,\n","                    0.4920521676540375,\n","                    0.31752169132232666,\n","                    0.2653074562549591,\n","                    0.24006614089012146,\n","                    0.22571724653244019,\n","                    0.21672485768795013,\n","                    0.20733590424060822,\n","                    0.20238620042800903,\n","                    0.19715920090675354,\n","                    0.1957165002822876,\n","                    0.19499684870243073,\n","                    0.19190643727779388,\n","                    0.1894029676914215,\n","                    0.19096368551254272,\n","                    0.1871592402458191,\n","                    0.18574102222919464,\n","                    0.19042916595935822,\n","                    0.19284313917160034,\n","                    0.1838710755109787,\n","                    0.1806565821170807,\n","                    0.18313047289848328,\n","                    0.18004266917705536,\n","                    0.17984075844287872,\n","                    0.17803633213043213,\n","                    0.17806746065616608,\n","                    0.17661982774734497,\n","                    0.1769733875989914,\n","                    0.1770162731409073,\n","                    0.17650991678237915],\n","                   [0.6357806324958801,\n","                    0.4832233190536499,\n","                    0.32126981019973755,\n","                    0.285686194896698,\n","                    0.26526039838790894,\n","                    0.23730142414569855,\n","                    0.21775545179843903,\n","                    0.21112385392189026,\n","                    0.20315560698509216,\n","                    0.1980506181716919,\n","                    0.197652667760849,\n","                    0.19182799756526947,\n","                    0.19065290689468384,\n","                    0.18715089559555054,\n","                    0.18616096675395966,\n","                    0.184212327003479,\n","                    0.18680839240550995,\n","                    0.1823626458644867,\n","                    0.17928661406040192,\n","                    0.1784186214208603,\n","                    0.17678554356098175,\n","                    0.17500187456607819,\n","                    0.17602436244487762,\n","                    0.17620602250099182,\n","                    0.17292383313179016,\n","                    0.17218950390815735,\n","                    0.16937103867530823,\n","                    0.1704091578722,\n","                    0.16719074547290802,\n","                    0.16689042747020721]],\n"," 'Validation Accuracy': [[0.8115267157554626,\n","                          0.8669532537460327,\n","                          0.8877599835395813,\n","                          0.8997799754142761,\n","                          0.9106733202934265,\n","                          0.9160400629043579,\n","                          0.9168133735656738,\n","                          0.9189466834068298,\n","                          0.9247666001319885,\n","                          0.9167599678039551,\n","                          0.9187400341033936,\n","                          0.9260267019271851,\n","                          0.9262332916259766,\n","                          0.9273267388343811,\n","                          0.9266266822814941,\n","                          0.929099977016449,\n","                          0.9275733232498169,\n","                          0.9305399060249329,\n","                          0.9248867034912109,\n","                          0.9291066527366638,\n","                          0.9312599897384644,\n","                          0.9295399188995361,\n","                          0.9278599619865417,\n","                          0.9306600093841553,\n","                          0.9310399889945984,\n","                          0.9329200983047485,\n","                          0.9320200085639954,\n","                          0.9335067272186279,\n","                          0.931546688079834,\n","                          0.9311733245849609],\n","                         [0.7282000780105591,\n","                          0.8405866026878357,\n","                          0.8766400218009949,\n","                          0.8900200724601746,\n","                          0.8961133360862732,\n","                          0.8942266702651978,\n","                          0.9063732624053955,\n","                          0.9071199893951416,\n","                          0.9088467359542847,\n","                          0.9095733761787415,\n","                          0.9054067134857178,\n","                          0.9124133586883545,\n","                          0.9118266105651855,\n","                          0.9137066602706909,\n","                          0.911306619644165,\n","                          0.9136133790016174,\n","                          0.9146333336830139,\n","                          0.9148933291435242,\n","                          0.9148933291435242,\n","                          0.9153133630752563,\n","                          0.9143666625022888,\n","                          0.9152333736419678,\n","                          0.9136267304420471,\n","                          0.9165599942207336,\n","                          0.9174666404724121,\n","                          0.9151666164398193,\n","                          0.9179333448410034,\n","                          0.917793333530426,\n","                          0.9192599654197693,\n","                          0.9175000190734863],\n","                         [0.6651466488838196,\n","                          0.8568333983421326,\n","                          0.8780933618545532,\n","                          0.903053343296051,\n","                          0.9055067300796509,\n","                          0.9070000648498535,\n","                          0.9163599610328674,\n","                          0.9182733297348022,\n","                          0.91867995262146,\n","                          0.9167000651359558,\n","                          0.9209201335906982,\n","                          0.9223933219909668,\n","                          0.9198665618896484,\n","                          0.9202867150306702,\n","                          0.9231134057044983,\n","                          0.9226733446121216,\n","                          0.9240400195121765,\n","                          0.9246399402618408,\n","                          0.9258666038513184,\n","                          0.9263867139816284,\n","                          0.9275000095367432,\n","                          0.9276599287986755,\n","                          0.925819993019104,\n","                          0.9277066588401794,\n","                          0.9274533987045288,\n","                          0.928006649017334,\n","                          0.9291266798973083,\n","                          0.9307533502578735,\n","                          0.9267467260360718,\n","                          0.93121337890625],\n","                         [0.7056733965873718,\n","                          0.8623332977294922,\n","                          0.8978800177574158,\n","                          0.9071266651153564,\n","                          0.9112732410430908,\n","                          0.9173333644866943,\n","                          0.9195466637611389,\n","                          0.9222466349601746,\n","                          0.9236000180244446,\n","                          0.9256399273872375,\n","                          0.9245667457580566,\n","                          0.9260200262069702,\n","                          0.9262799620628357,\n","                          0.9289466738700867,\n","                          0.9277600049972534,\n","                          0.9293999671936035,\n","                          0.9256865978240967,\n","                          0.9301199913024902,\n","                          0.9302933216094971,\n","                          0.9297400712966919,\n","                          0.9314000010490417,\n","                          0.9294533729553223,\n","                          0.930840015411377,\n","                          0.9303666949272156,\n","                          0.9307067394256592,\n","                          0.9322733283042908,\n","                          0.9293400645256042,\n","                          0.9332800507545471,\n","                          0.9323466420173645,\n","                          0.932526707649231],\n","                         [0.7699466943740845,\n","                          0.8480200171470642,\n","                          0.8749533295631409,\n","                          0.8816599249839783,\n","                          0.8923200368881226,\n","                          0.9042667150497437,\n","                          0.9071000814437866,\n","                          0.9105333685874939,\n","                          0.9117666482925415,\n","                          0.9028266668319702,\n","                          0.9170534014701843,\n","                          0.9150466918945312,\n","                          0.9148932695388794,\n","                          0.9158400893211365,\n","                          0.9206467270851135,\n","                          0.9202467203140259,\n","                          0.9119333624839783,\n","                          0.9200132489204407,\n","                          0.9206534028053284,\n","                          0.9192267060279846,\n","                          0.9229933023452759,\n","                          0.9228532910346985,\n","                          0.9209199547767639,\n","                          0.9219533801078796,\n","                          0.9211933016777039,\n","                          0.9242399334907532,\n","                          0.9238000512123108,\n","                          0.9243333339691162,\n","                          0.9248266816139221,\n","                          0.9262400269508362]],\n"," 'Validation Loss': [[0.5591490864753723,\n","                      0.3587746024131775,\n","                      0.2850448191165924,\n","                      0.24837464094161987,\n","                      0.22060835361480713,\n","                      0.20920507609844208,\n","                      0.20023773610591888,\n","                      0.2018364667892456,\n","                      0.1863364428281784,\n","                      0.1973225623369217,\n","                      0.19231046736240387,\n","                      0.18373754620552063,\n","                      0.18233536183834076,\n","                      0.17774461209774017,\n","                      0.17806337773799896,\n","                      0.1718192845582962,\n","                      0.1764669269323349,\n","                      0.16877391934394836,\n","                      0.17615383863449097,\n","                      0.1702595204114914,\n","                      0.16616301238536835,\n","                      0.16880245506763458,\n","                      0.17300531268119812,\n","                      0.16773353517055511,\n","                      0.16564056277275085,\n","                      0.16217440366744995,\n","                      0.16367283463478088,\n","                      0.16098834574222565,\n","                      0.16597720980644226,\n","                      0.1644747108221054],\n","                     [0.5835900902748108,\n","                      0.399684339761734,\n","                      0.3098095655441284,\n","                      0.2761082351207733,\n","                      0.2551618814468384,\n","                      0.2543167769908905,\n","                      0.22985650599002838,\n","                      0.2250148057937622,\n","                      0.21942207217216492,\n","                      0.22172947227954865,\n","                      0.22875837981700897,\n","                      0.21125827729701996,\n","                      0.21102271974086761,\n","                      0.21060995757579803,\n","                      0.2080451250076294,\n","                      0.20506101846694946,\n","                      0.2062636911869049,\n","                      0.20321398973464966,\n","                      0.20304381847381592,\n","                      0.2012396603822708,\n","                      0.20120245218276978,\n","                      0.2032477706670761,\n","                      0.20495519042015076,\n","                      0.19884225726127625,\n","                      0.19583219289779663,\n","                      0.20190075039863586,\n","                      0.19526275992393494,\n","                      0.1956697255373001,\n","                      0.1936982125043869,\n","                      0.19560092687606812],\n","                     [0.5894062519073486,\n","                      0.38742339611053467,\n","                      0.2932031452655792,\n","                      0.2425868660211563,\n","                      0.22794181108474731,\n","                      0.22092953324317932,\n","                      0.20583191514015198,\n","                      0.19993773102760315,\n","                      0.196541428565979,\n","                      0.20155063271522522,\n","                      0.19012238085269928,\n","                      0.18821585178375244,\n","                      0.18967770040035248,\n","                      0.18784743547439575,\n","                      0.18415282666683197,\n","                      0.18581266701221466,\n","                      0.1808731108903885,\n","                      0.179836705327034,\n","                      0.17930541932582855,\n","                      0.17643192410469055,\n","                      0.1737651824951172,\n","                      0.17497868835926056,\n","                      0.17632833123207092,\n","                      0.17482703924179077,\n","                      0.17369061708450317,\n","                      0.17312172055244446,\n","                      0.17068932950496674,\n","                      0.1672622710466385,\n","                      0.1774524450302124,\n","                      0.167423814535141],\n","                     [0.5769978165626526,\n","                      0.3727942705154419,\n","                      0.2678237855434418,\n","                      0.24147360026836395,\n","                      0.22376006841659546,\n","                      0.21090611815452576,\n","                      0.2044285237789154,\n","                      0.19552695751190186,\n","                      0.19079726934432983,\n","                      0.18759214878082275,\n","                      0.1860819309949875,\n","                      0.18415459990501404,\n","                      0.18251977860927582,\n","                      0.1776801496744156,\n","                      0.1790565550327301,\n","                      0.17825499176979065,\n","                      0.18277481198310852,\n","                      0.17420916259288788,\n","                      0.1748274564743042,\n","                      0.17553871870040894,\n","                      0.17149482667446136,\n","                      0.1731879562139511,\n","                      0.17104065418243408,\n","                      0.17115728557109833,\n","                      0.16870906949043274,\n","                      0.17075084149837494,\n","                      0.17297764122486115,\n","                      0.16689559817314148,\n","                      0.16749261319637299,\n","                      0.16716471314430237],\n","                     [0.5713569521903992,\n","                      0.38297271728515625,\n","                      0.3025450110435486,\n","                      0.28568124771118164,\n","                      0.2623085081577301,\n","                      0.23350656032562256,\n","                      0.22583520412445068,\n","                      0.21643786132335663,\n","                      0.21551105380058289,\n","                      0.22191722691059113,\n","                      0.20281748473644257,\n","                      0.20135459303855896,\n","                      0.20453570783138275,\n","                      0.20205028355121613,\n","                      0.1944447159767151,\n","                      0.19394396245479584,\n","                      0.20683249831199646,\n","                      0.19153793156147003,\n","                      0.18940162658691406,\n","                      0.1913319230079651,\n","                      0.18575969338417053,\n","                      0.18555185198783875,\n","                      0.18730680644512177,\n","                      0.18610306084156036,\n","                      0.1866883635520935,\n","                      0.1829233020544052,\n","                      0.18405504524707794,\n","                      0.18056346476078033,\n","                      0.18133686482906342,\n","                      0.17842383682727814]],\n"," 'Validation MCC': [[0.6514695249455393,\n","                     0.7333192983945738,\n","                     0.7777050077540856,\n","                     0.800146397264295,\n","                     0.821220573198023,\n","                     0.8316778557000869,\n","                     0.8345746647409295,\n","                     0.8372754059808417,\n","                     0.8489558470119426,\n","                     0.8343899456233675,\n","                     0.8385141913588728,\n","                     0.8514820442375131,\n","                     0.8523012807077937,\n","                     0.8543434299865847,\n","                     0.8529314220580303,\n","                     0.8577182699713652,\n","                     0.8549435985314067,\n","                     0.8605467320364811,\n","                     0.8496234057189219,\n","                     0.8580703403645286,\n","                     0.8620630348774844,\n","                     0.8586715713474363,\n","                     0.8561265252118196,\n","                     0.8607859944698901,\n","                     0.861823545665669,\n","                     0.8653401418918677,\n","                     0.8635089412676505,\n","                     0.8666102586207116,\n","                     0.862908280383586,\n","                     0.8620662948378994],\n","                    [0.5129041207967033,\n","                     0.6834624761807857,\n","                     0.7528384059380787,\n","                     0.7795134047312988,\n","                     0.7922743445190741,\n","                     0.788618829735941,\n","                     0.812300971447177,\n","                     0.8141674998748386,\n","                     0.8172448674448599,\n","                     0.8194457378102512,\n","                     0.8123242163475642,\n","                     0.8243978100426014,\n","                     0.8232269696612363,\n","                     0.8274001200956806,\n","                     0.8224511138066544,\n","                     0.8268236074280629,\n","                     0.8289052658474408,\n","                     0.8293730040725006,\n","                     0.8295953763183755,\n","                     0.8306748776150734,\n","                     0.8285434719263182,\n","                     0.83013140114315,\n","                     0.8270576108431922,\n","                     0.8328940763685835,\n","                     0.8345613077478381,\n","                     0.8299200545903193,\n","                     0.8359840621893981,\n","                     0.8353452463878421,\n","                     0.8384678286473322,\n","                     0.8346151841753867],\n","                    [0.45169069926276223,\n","                     0.7134096422799994,\n","                     0.7557615586540087,\n","                     0.8052607505119371,\n","                     0.8131834883726732,\n","                     0.8148478359816477,\n","                     0.8320870679615764,\n","                     0.8359089703346527,\n","                     0.8369832643228738,\n","                     0.833606877443607,\n","                     0.8411691497893408,\n","                     0.844216966053732,\n","                     0.8390689603773005,\n","                     0.8399542886767648,\n","                     0.8456642881457884,\n","                     0.8449547021997861,\n","                     0.8474485496738758,\n","                     0.8486423543013303,\n","                     0.851184477942804,\n","                     0.8522308423092501,\n","                     0.8544002537540508,\n","                     0.8547458179199596,\n","                     0.8516742076779248,\n","                     0.8550671406726542,\n","                     0.8543559800675812,\n","                     0.8557263408193723,\n","                     0.8576636692036049,\n","                     0.8609520998900025,\n","                     0.853255281704644,\n","                     0.8618731310076967],\n","                    [0.5150278960752375,\n","                     0.7241483216023978,\n","                     0.7958348809434553,\n","                     0.8141189759553328,\n","                     0.8242296981211725,\n","                     0.8345560213619091,\n","                     0.8387856933090194,\n","                     0.8441134659583347,\n","                     0.8470606706452639,\n","                     0.851055014956112,\n","                     0.8492563492735778,\n","                     0.8520236034719417,\n","                     0.8524001721167922,\n","                     0.8576335376395049,\n","                     0.8552044363875397,\n","                     0.8584687250311462,\n","                     0.8523178834858514,\n","                     0.8599113976979478,\n","                     0.8603273074706022,\n","                     0.859226788184644,\n","                     0.8624825032734443,\n","                     0.8585591826438769,\n","                     0.8614015880613236,\n","                     0.8604072643508663,\n","                     0.8610715188696227,\n","                     0.8643192759722184,\n","                     0.8591020681413207,\n","                     0.8663026675795981,\n","                     0.864663759688723,\n","                     0.8647491832268185],\n","                    [0.5892170803809557,\n","                     0.695722353127343,\n","                     0.7494207696126918,\n","                     0.7630006275808584,\n","                     0.7842285409548853,\n","                     0.8084268145335478,\n","                     0.8147150618972316,\n","                     0.8210573127737104,\n","                     0.8233896039972485,\n","                     0.8059872907511011,\n","                     0.8339133887921824,\n","                     0.8298739942961105,\n","                     0.8303945284779456,\n","                     0.8313627748905682,\n","                     0.841002860413658,\n","                     0.8403979515768407,\n","                     0.8247291624996411,\n","                     0.8402130537178534,\n","                     0.8411207503338373,\n","                     0.8385658363600234,\n","                     0.8457875191881679,\n","                     0.8456303051941606,\n","                     0.8416768824544352,\n","                     0.8436141053901988,\n","                     0.8422380242459651,\n","                     0.8489087634905313,\n","                     0.8476358649111065,\n","                     0.8484863838346934,\n","                     0.8501481180040803,\n","                     0.8525192301081567]]}\n","Training Model: LSTM_Dense, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6336 - loss: 0.6794\n","Epoch 1 - MCC: 0.5396\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 68ms/step - accuracy: 0.6354 - loss: 0.6784 - val_accuracy: 0.7621 - val_loss: 0.5881 - mcc: 0.5396\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7817 - loss: 0.5382\n","Epoch 2 - MCC: 0.7054\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.7825 - loss: 0.5362 - val_accuracy: 0.8526 - val_loss: 0.3808 - mcc: 0.7054\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8479 - loss: 0.3658\n","Epoch 3 - MCC: 0.7582\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8483 - loss: 0.3649 - val_accuracy: 0.8785 - val_loss: 0.2897 - mcc: 0.7582\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8861 - loss: 0.2817\n","Epoch 4 - MCC: 0.8013\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8862 - loss: 0.2815 - val_accuracy: 0.9010 - val_loss: 0.2420 - mcc: 0.8013\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8950 - loss: 0.2543\n","Epoch 5 - MCC: 0.8183\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8952 - loss: 0.2540 - val_accuracy: 0.9090 - val_loss: 0.2254 - mcc: 0.8183\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9067 - loss: 0.2285\n","Epoch 6 - MCC: 0.8264\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9066 - loss: 0.2287 - val_accuracy: 0.9131 - val_loss: 0.2113 - mcc: 0.8264\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9043 - loss: 0.2304\n","Epoch 7 - MCC: 0.8297\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9044 - loss: 0.2301 - val_accuracy: 0.9149 - val_loss: 0.2055 - mcc: 0.8297\n","Epoch 8/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9117 - loss: 0.2132\n","Epoch 8 - MCC: 0.8323\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9117 - loss: 0.2133 - val_accuracy: 0.9161 - val_loss: 0.2003 - mcc: 0.8323\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9157 - loss: 0.2037\n","Epoch 9 - MCC: 0.8378\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9156 - loss: 0.2038 - val_accuracy: 0.9190 - val_loss: 0.1929 - mcc: 0.8378\n","Epoch 10/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9139 - loss: 0.2069\n","Epoch 10 - MCC: 0.8328\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9141 - loss: 0.2066 - val_accuracy: 0.9164 - val_loss: 0.1953 - mcc: 0.8328\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9141 - loss: 0.2068\n","Epoch 11 - MCC: 0.8233\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9142 - loss: 0.2065 - val_accuracy: 0.9111 - val_loss: 0.2024 - mcc: 0.8233\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9120 - loss: 0.2075\n","Epoch 12 - MCC: 0.8444\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.9122 - loss: 0.2073 - val_accuracy: 0.9223 - val_loss: 0.1868 - mcc: 0.8444\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9176 - loss: 0.1943\n","Epoch 13 - MCC: 0.8452\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9176 - loss: 0.1945 - val_accuracy: 0.9226 - val_loss: 0.1824 - mcc: 0.8452\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9231 - loss: 0.1825\n","Epoch 14 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9229 - loss: 0.1829 - val_accuracy: 0.9231 - val_loss: 0.1815 - mcc: 0.8458\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9202 - loss: 0.1921\n","Epoch 15 - MCC: 0.8469\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9202 - loss: 0.1921 - val_accuracy: 0.9236 - val_loss: 0.1824 - mcc: 0.8469\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9139 - loss: 0.2062\n","Epoch 16 - MCC: 0.8514\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9142 - loss: 0.2056 - val_accuracy: 0.9260 - val_loss: 0.1758 - mcc: 0.8514\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9221 - loss: 0.1880\n","Epoch 17 - MCC: 0.8522\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9221 - loss: 0.1880 - val_accuracy: 0.9263 - val_loss: 0.1756 - mcc: 0.8522\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9215 - loss: 0.1899\n","Epoch 18 - MCC: 0.8546\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9215 - loss: 0.1899 - val_accuracy: 0.9276 - val_loss: 0.1718 - mcc: 0.8546\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9221 - loss: 0.1889\n","Epoch 19 - MCC: 0.8573\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9221 - loss: 0.1888 - val_accuracy: 0.9289 - val_loss: 0.1704 - mcc: 0.8573\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9179 - loss: 0.1957\n","Epoch 20 - MCC: 0.8505\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9181 - loss: 0.1954 - val_accuracy: 0.9253 - val_loss: 0.1749 - mcc: 0.8505\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9233 - loss: 0.1849\n","Epoch 21 - MCC: 0.8490\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9233 - loss: 0.1849 - val_accuracy: 0.9244 - val_loss: 0.1767 - mcc: 0.8490\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9205 - loss: 0.1889\n","Epoch 22 - MCC: 0.8559\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9206 - loss: 0.1887 - val_accuracy: 0.9282 - val_loss: 0.1709 - mcc: 0.8559\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9227 - loss: 0.1860\n","Epoch 23 - MCC: 0.8513\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9227 - loss: 0.1860 - val_accuracy: 0.9255 - val_loss: 0.1746 - mcc: 0.8513\n","Epoch 24/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9243 - loss: 0.1810\n","Epoch 24 - MCC: 0.8554\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9243 - loss: 0.1811 - val_accuracy: 0.9277 - val_loss: 0.1714 - mcc: 0.8554\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9245 - loss: 0.1813\n","Epoch 25 - MCC: 0.8555\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9245 - loss: 0.1813 - val_accuracy: 0.9277 - val_loss: 0.1727 - mcc: 0.8555\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9218 - loss: 0.1863\n","Epoch 26 - MCC: 0.8553\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.9219 - loss: 0.1861 - val_accuracy: 0.9273 - val_loss: 0.1736 - mcc: 0.8553\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9249 - loss: 0.1806\n","Epoch 27 - MCC: 0.8614\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9249 - loss: 0.1807 - val_accuracy: 0.9310 - val_loss: 0.1655 - mcc: 0.8614\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9230 - loss: 0.1832\n","Epoch 28 - MCC: 0.8599\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9231 - loss: 0.1830 - val_accuracy: 0.9302 - val_loss: 0.1649 - mcc: 0.8599\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9238 - loss: 0.1814\n","Epoch 29 - MCC: 0.8595\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9239 - loss: 0.1813 - val_accuracy: 0.9300 - val_loss: 0.1655 - mcc: 0.8595\n","Epoch 30/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9262 - loss: 0.1760\n","Epoch 30 - MCC: 0.8625\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9262 - loss: 0.1762 - val_accuracy: 0.9315 - val_loss: 0.1631 - mcc: 0.8625\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 2\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.5510 - loss: 0.6876\n","Epoch 1 - MCC: 0.4606\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.5552 - loss: 0.6856 - val_accuracy: 0.7204 - val_loss: 0.6081 - mcc: 0.4606\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7819 - loss: 0.5563\n","Epoch 2 - MCC: 0.6713\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.7826 - loss: 0.5546 - val_accuracy: 0.8359 - val_loss: 0.4111 - mcc: 0.6713\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8576 - loss: 0.3607\n","Epoch 3 - MCC: 0.7533\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8580 - loss: 0.3595 - val_accuracy: 0.8769 - val_loss: 0.3046 - mcc: 0.7533\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8915 - loss: 0.2711\n","Epoch 4 - MCC: 0.7719\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8915 - loss: 0.2709 - val_accuracy: 0.8861 - val_loss: 0.2757 - mcc: 0.7719\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9027 - loss: 0.2393\n","Epoch 5 - MCC: 0.7851\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9027 - loss: 0.2394 - val_accuracy: 0.8922 - val_loss: 0.2622 - mcc: 0.7851\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9013 - loss: 0.2420\n","Epoch 6 - MCC: 0.8001\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9015 - loss: 0.2415 - val_accuracy: 0.9002 - val_loss: 0.2370 - mcc: 0.8001\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9103 - loss: 0.2179\n","Epoch 7 - MCC: 0.8092\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9105 - loss: 0.2175 - val_accuracy: 0.9048 - val_loss: 0.2238 - mcc: 0.8092\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9133 - loss: 0.2070\n","Epoch 8 - MCC: 0.8184\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9135 - loss: 0.2067 - val_accuracy: 0.9094 - val_loss: 0.2170 - mcc: 0.8184\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9207 - loss: 0.1914\n","Epoch 9 - MCC: 0.8201\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9207 - loss: 0.1914 - val_accuracy: 0.9102 - val_loss: 0.2141 - mcc: 0.8201\n","Epoch 10/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9242 - loss: 0.1844\n","Epoch 10 - MCC: 0.8235\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.9238 - loss: 0.1851 - val_accuracy: 0.9115 - val_loss: 0.2133 - mcc: 0.8235\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9194 - loss: 0.1930\n","Epoch 11 - MCC: 0.8235\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9195 - loss: 0.1928 - val_accuracy: 0.9120 - val_loss: 0.2076 - mcc: 0.8235\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9236 - loss: 0.1848\n","Epoch 12 - MCC: 0.8253\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9236 - loss: 0.1849 - val_accuracy: 0.9129 - val_loss: 0.2054 - mcc: 0.8253\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9221 - loss: 0.1856\n","Epoch 13 - MCC: 0.8282\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9222 - loss: 0.1855 - val_accuracy: 0.9142 - val_loss: 0.2068 - mcc: 0.8282\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9250 - loss: 0.1821\n","Epoch 14 - MCC: 0.8273\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9250 - loss: 0.1821 - val_accuracy: 0.9138 - val_loss: 0.2036 - mcc: 0.8273\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9218 - loss: 0.1862\n","Epoch 15 - MCC: 0.8270\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9219 - loss: 0.1860 - val_accuracy: 0.9135 - val_loss: 0.2060 - mcc: 0.8270\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9275 - loss: 0.1741\n","Epoch 16 - MCC: 0.8283\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9273 - loss: 0.1744 - val_accuracy: 0.9142 - val_loss: 0.2036 - mcc: 0.8283\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9263 - loss: 0.1769\n","Epoch 17 - MCC: 0.8281\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9263 - loss: 0.1769 - val_accuracy: 0.9142 - val_loss: 0.2013 - mcc: 0.8281\n","Epoch 18/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9316 - loss: 0.1635\n","Epoch 18 - MCC: 0.8221\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9311 - loss: 0.1646 - val_accuracy: 0.9107 - val_loss: 0.2102 - mcc: 0.8221\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9254 - loss: 0.1789\n","Epoch 19 - MCC: 0.8298\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9254 - loss: 0.1789 - val_accuracy: 0.9150 - val_loss: 0.2023 - mcc: 0.8298\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9242 - loss: 0.1804\n","Epoch 20 - MCC: 0.8241\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9243 - loss: 0.1802 - val_accuracy: 0.9121 - val_loss: 0.2058 - mcc: 0.8241\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9277 - loss: 0.1740\n","Epoch 21 - MCC: 0.8309\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9277 - loss: 0.1740 - val_accuracy: 0.9155 - val_loss: 0.1986 - mcc: 0.8309\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9273 - loss: 0.1730\n","Epoch 22 - MCC: 0.8320\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9274 - loss: 0.1730 - val_accuracy: 0.9161 - val_loss: 0.1983 - mcc: 0.8320\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9278 - loss: 0.1700\n","Epoch 23 - MCC: 0.8320\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9279 - loss: 0.1700 - val_accuracy: 0.9162 - val_loss: 0.1964 - mcc: 0.8320\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9274 - loss: 0.1744\n","Epoch 24 - MCC: 0.8346\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9275 - loss: 0.1742 - val_accuracy: 0.9174 - val_loss: 0.1965 - mcc: 0.8346\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9304 - loss: 0.1672\n","Epoch 25 - MCC: 0.8352\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9303 - loss: 0.1673 - val_accuracy: 0.9177 - val_loss: 0.1945 - mcc: 0.8352\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9296 - loss: 0.1671\n","Epoch 26 - MCC: 0.8355\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9295 - loss: 0.1672 - val_accuracy: 0.9177 - val_loss: 0.1943 - mcc: 0.8355\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9322 - loss: 0.1613\n","Epoch 27 - MCC: 0.8348\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9321 - loss: 0.1616 - val_accuracy: 0.9172 - val_loss: 0.1944 - mcc: 0.8348\n","Epoch 28/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9323 - loss: 0.1636\n","Epoch 28 - MCC: 0.8363\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9322 - loss: 0.1638 - val_accuracy: 0.9181 - val_loss: 0.1947 - mcc: 0.8363\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9327 - loss: 0.1613\n","Epoch 29 - MCC: 0.8394\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9327 - loss: 0.1614 - val_accuracy: 0.9199 - val_loss: 0.1896 - mcc: 0.8394\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9341 - loss: 0.1580\n","Epoch 30 - MCC: 0.8387\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9340 - loss: 0.1582 - val_accuracy: 0.9193 - val_loss: 0.1907 - mcc: 0.8387\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5802 - loss: 0.7125\n","Epoch 1 - MCC: 0.6758\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.5842 - loss: 0.7114 - val_accuracy: 0.8386 - val_loss: 0.6213 - mcc: 0.6758\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8345 - loss: 0.5755\n","Epoch 2 - MCC: 0.6414\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.8341 - loss: 0.5735 - val_accuracy: 0.8216 - val_loss: 0.4206 - mcc: 0.6414\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8382 - loss: 0.3922\n","Epoch 3 - MCC: 0.7449\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.8387 - loss: 0.3911 - val_accuracy: 0.8729 - val_loss: 0.3052 - mcc: 0.7449\n","Epoch 4/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8781 - loss: 0.3040\n","Epoch 4 - MCC: 0.7899\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.8786 - loss: 0.3030 - val_accuracy: 0.8933 - val_loss: 0.2634 - mcc: 0.7899\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8997 - loss: 0.2510\n","Epoch 5 - MCC: 0.8102\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8997 - loss: 0.2510 - val_accuracy: 0.9051 - val_loss: 0.2302 - mcc: 0.8102\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9019 - loss: 0.2390\n","Epoch 6 - MCC: 0.8205\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9020 - loss: 0.2388 - val_accuracy: 0.9106 - val_loss: 0.2197 - mcc: 0.8205\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9052 - loss: 0.2284\n","Epoch 7 - MCC: 0.8280\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9053 - loss: 0.2281 - val_accuracy: 0.9143 - val_loss: 0.2060 - mcc: 0.8280\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9086 - loss: 0.2188\n","Epoch 8 - MCC: 0.8342\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9087 - loss: 0.2185 - val_accuracy: 0.9174 - val_loss: 0.2000 - mcc: 0.8342\n","Epoch 9/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9108 - loss: 0.2131\n","Epoch 9 - MCC: 0.8272\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9110 - loss: 0.2128 - val_accuracy: 0.9140 - val_loss: 0.2006 - mcc: 0.8272\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9134 - loss: 0.2077\n","Epoch 10 - MCC: 0.8316\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9135 - loss: 0.2075 - val_accuracy: 0.9161 - val_loss: 0.1968 - mcc: 0.8316\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9153 - loss: 0.2029\n","Epoch 11 - MCC: 0.8389\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9153 - loss: 0.2028 - val_accuracy: 0.9197 - val_loss: 0.1906 - mcc: 0.8389\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9153 - loss: 0.2022\n","Epoch 12 - MCC: 0.8413\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9153 - loss: 0.2021 - val_accuracy: 0.9210 - val_loss: 0.1881 - mcc: 0.8413\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9176 - loss: 0.1965\n","Epoch 13 - MCC: 0.8425\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9176 - loss: 0.1966 - val_accuracy: 0.9215 - val_loss: 0.1861 - mcc: 0.8425\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9168 - loss: 0.1964\n","Epoch 14 - MCC: 0.8360\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.9169 - loss: 0.1964 - val_accuracy: 0.9179 - val_loss: 0.1916 - mcc: 0.8360\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9191 - loss: 0.1926\n","Epoch 15 - MCC: 0.8453\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9191 - loss: 0.1926 - val_accuracy: 0.9230 - val_loss: 0.1831 - mcc: 0.8453\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9194 - loss: 0.1912\n","Epoch 16 - MCC: 0.8463\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9194 - loss: 0.1912 - val_accuracy: 0.9233 - val_loss: 0.1828 - mcc: 0.8463\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9224 - loss: 0.1841\n","Epoch 17 - MCC: 0.8468\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9223 - loss: 0.1843 - val_accuracy: 0.9236 - val_loss: 0.1839 - mcc: 0.8468\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9137 - loss: 0.2021\n","Epoch 18 - MCC: 0.8469\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9140 - loss: 0.2016 - val_accuracy: 0.9237 - val_loss: 0.1798 - mcc: 0.8469\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9224 - loss: 0.1856\n","Epoch 19 - MCC: 0.8475\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9224 - loss: 0.1856 - val_accuracy: 0.9240 - val_loss: 0.1812 - mcc: 0.8475\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9223 - loss: 0.1840\n","Epoch 20 - MCC: 0.8484\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9223 - loss: 0.1841 - val_accuracy: 0.9245 - val_loss: 0.1785 - mcc: 0.8484\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9254 - loss: 0.1794\n","Epoch 21 - MCC: 0.8462\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9253 - loss: 0.1797 - val_accuracy: 0.9234 - val_loss: 0.1801 - mcc: 0.8462\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9209 - loss: 0.1874\n","Epoch 22 - MCC: 0.8492\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9209 - loss: 0.1874 - val_accuracy: 0.9248 - val_loss: 0.1786 - mcc: 0.8492\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9191 - loss: 0.1918\n","Epoch 23 - MCC: 0.8506\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9192 - loss: 0.1915 - val_accuracy: 0.9255 - val_loss: 0.1789 - mcc: 0.8506\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9226 - loss: 0.1847\n","Epoch 24 - MCC: 0.8481\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9226 - loss: 0.1847 - val_accuracy: 0.9236 - val_loss: 0.1827 - mcc: 0.8481\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9196 - loss: 0.1906\n","Epoch 25 - MCC: 0.8542\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9197 - loss: 0.1905 - val_accuracy: 0.9274 - val_loss: 0.1741 - mcc: 0.8542\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9222 - loss: 0.1859\n","Epoch 26 - MCC: 0.8543\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9223 - loss: 0.1858 - val_accuracy: 0.9275 - val_loss: 0.1718 - mcc: 0.8543\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9226 - loss: 0.1846\n","Epoch 27 - MCC: 0.8515\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9226 - loss: 0.1844 - val_accuracy: 0.9259 - val_loss: 0.1760 - mcc: 0.8515\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9182 - loss: 0.1933\n","Epoch 28 - MCC: 0.8544\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9184 - loss: 0.1929 - val_accuracy: 0.9274 - val_loss: 0.1725 - mcc: 0.8544\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9274 - loss: 0.1758\n","Epoch 29 - MCC: 0.8533\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9273 - loss: 0.1759 - val_accuracy: 0.9268 - val_loss: 0.1740 - mcc: 0.8533\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9218 - loss: 0.1864\n","Epoch 30 - MCC: 0.8567\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9219 - loss: 0.1861 - val_accuracy: 0.9285 - val_loss: 0.1684 - mcc: 0.8567\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5274 - loss: 0.7013\n","Epoch 1 - MCC: 0.3725\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.5290 - loss: 0.7001 - val_accuracy: 0.6373 - val_loss: 0.6060 - mcc: 0.3725\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7067 - loss: 0.5665\n","Epoch 2 - MCC: 0.7345\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.7091 - loss: 0.5647 - val_accuracy: 0.8671 - val_loss: 0.3966 - mcc: 0.7345\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8654 - loss: 0.3520\n","Epoch 3 - MCC: 0.7901\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8656 - loss: 0.3508 - val_accuracy: 0.8951 - val_loss: 0.2624 - mcc: 0.7901\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8855 - loss: 0.2765\n","Epoch 4 - MCC: 0.8123\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8857 - loss: 0.2760 - val_accuracy: 0.9064 - val_loss: 0.2273 - mcc: 0.8123\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8998 - loss: 0.2431\n","Epoch 5 - MCC: 0.8249\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9000 - loss: 0.2426 - val_accuracy: 0.9124 - val_loss: 0.2102 - mcc: 0.8249\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9130 - loss: 0.2079\n","Epoch 6 - MCC: 0.8339\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9129 - loss: 0.2082 - val_accuracy: 0.9169 - val_loss: 0.1982 - mcc: 0.8339\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9139 - loss: 0.2041\n","Epoch 7 - MCC: 0.8381\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9138 - loss: 0.2041 - val_accuracy: 0.9189 - val_loss: 0.1948 - mcc: 0.8381\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9167 - loss: 0.1990\n","Epoch 8 - MCC: 0.8462\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9167 - loss: 0.1991 - val_accuracy: 0.9233 - val_loss: 0.1868 - mcc: 0.8462\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9177 - loss: 0.1948\n","Epoch 9 - MCC: 0.8358\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9176 - loss: 0.1948 - val_accuracy: 0.9180 - val_loss: 0.1938 - mcc: 0.8358\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9190 - loss: 0.1918\n","Epoch 10 - MCC: 0.8486\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.9189 - loss: 0.1918 - val_accuracy: 0.9244 - val_loss: 0.1831 - mcc: 0.8486\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9187 - loss: 0.1916\n","Epoch 11 - MCC: 0.8514\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9187 - loss: 0.1915 - val_accuracy: 0.9258 - val_loss: 0.1789 - mcc: 0.8514\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9181 - loss: 0.1942\n","Epoch 12 - MCC: 0.8470\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9182 - loss: 0.1941 - val_accuracy: 0.9235 - val_loss: 0.1853 - mcc: 0.8470\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9198 - loss: 0.1887\n","Epoch 13 - MCC: 0.8560\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9198 - loss: 0.1887 - val_accuracy: 0.9282 - val_loss: 0.1754 - mcc: 0.8560\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9198 - loss: 0.1875\n","Epoch 14 - MCC: 0.8526\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9198 - loss: 0.1875 - val_accuracy: 0.9264 - val_loss: 0.1758 - mcc: 0.8526\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9160 - loss: 0.1985\n","Epoch 15 - MCC: 0.8507\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9162 - loss: 0.1980 - val_accuracy: 0.9255 - val_loss: 0.1776 - mcc: 0.8507\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9221 - loss: 0.1839\n","Epoch 16 - MCC: 0.8585\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9221 - loss: 0.1840 - val_accuracy: 0.9294 - val_loss: 0.1735 - mcc: 0.8585\n","Epoch 17/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9211 - loss: 0.1875\n","Epoch 17 - MCC: 0.8581\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9212 - loss: 0.1870 - val_accuracy: 0.9292 - val_loss: 0.1716 - mcc: 0.8581\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9213 - loss: 0.1871\n","Epoch 18 - MCC: 0.8584\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9213 - loss: 0.1869 - val_accuracy: 0.9292 - val_loss: 0.1718 - mcc: 0.8584\n","Epoch 19/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9271 - loss: 0.1720\n","Epoch 19 - MCC: 0.8587\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9268 - loss: 0.1726 - val_accuracy: 0.9295 - val_loss: 0.1729 - mcc: 0.8587\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9223 - loss: 0.1827\n","Epoch 20 - MCC: 0.8609\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9223 - loss: 0.1826 - val_accuracy: 0.9305 - val_loss: 0.1696 - mcc: 0.8609\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9267 - loss: 0.1734\n","Epoch 21 - MCC: 0.8626\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9266 - loss: 0.1737 - val_accuracy: 0.9314 - val_loss: 0.1666 - mcc: 0.8626\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9251 - loss: 0.1778\n","Epoch 22 - MCC: 0.8658\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9251 - loss: 0.1777 - val_accuracy: 0.9330 - val_loss: 0.1658 - mcc: 0.8658\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9281 - loss: 0.1706\n","Epoch 23 - MCC: 0.8627\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9280 - loss: 0.1709 - val_accuracy: 0.9315 - val_loss: 0.1650 - mcc: 0.8627\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9278 - loss: 0.1707\n","Epoch 24 - MCC: 0.8651\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9277 - loss: 0.1709 - val_accuracy: 0.9327 - val_loss: 0.1637 - mcc: 0.8651\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9289 - loss: 0.1699\n","Epoch 25 - MCC: 0.8670\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9289 - loss: 0.1700 - val_accuracy: 0.9336 - val_loss: 0.1618 - mcc: 0.8670\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9258 - loss: 0.1761\n","Epoch 26 - MCC: 0.8619\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.9259 - loss: 0.1760 - val_accuracy: 0.9310 - val_loss: 0.1682 - mcc: 0.8619\n","Epoch 27/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9283 - loss: 0.1712\n","Epoch 27 - MCC: 0.8690\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9283 - loss: 0.1713 - val_accuracy: 0.9346 - val_loss: 0.1602 - mcc: 0.8690\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9275 - loss: 0.1732\n","Epoch 28 - MCC: 0.8672\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9275 - loss: 0.1731 - val_accuracy: 0.9338 - val_loss: 0.1637 - mcc: 0.8672\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9286 - loss: 0.1713\n","Epoch 29 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9286 - loss: 0.1713 - val_accuracy: 0.9351 - val_loss: 0.1607 - mcc: 0.8701\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9293 - loss: 0.1687\n","Epoch 30 - MCC: 0.8730\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9293 - loss: 0.1687 - val_accuracy: 0.9366 - val_loss: 0.1580 - mcc: 0.8730\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 5\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5804 - loss: 0.6827\n","Epoch 1 - MCC: 0.5332\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.5856 - loss: 0.6810 - val_accuracy: 0.7531 - val_loss: 0.6085 - mcc: 0.5332\n","Epoch 2/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7949 - loss: 0.5620\n","Epoch 2 - MCC: 0.6623\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.7959 - loss: 0.5581 - val_accuracy: 0.8312 - val_loss: 0.4101 - mcc: 0.6623\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8581 - loss: 0.3594\n","Epoch 3 - MCC: 0.7640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8584 - loss: 0.3585 - val_accuracy: 0.8822 - val_loss: 0.2976 - mcc: 0.7640\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8873 - loss: 0.2820\n","Epoch 4 - MCC: 0.7905\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8875 - loss: 0.2815 - val_accuracy: 0.8952 - val_loss: 0.2589 - mcc: 0.7905\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9036 - loss: 0.2417\n","Epoch 5 - MCC: 0.8104\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9037 - loss: 0.2415 - val_accuracy: 0.9053 - val_loss: 0.2334 - mcc: 0.8104\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9101 - loss: 0.2234\n","Epoch 6 - MCC: 0.8135\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9102 - loss: 0.2233 - val_accuracy: 0.9069 - val_loss: 0.2238 - mcc: 0.8135\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9123 - loss: 0.2146\n","Epoch 7 - MCC: 0.8193\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9124 - loss: 0.2145 - val_accuracy: 0.9098 - val_loss: 0.2158 - mcc: 0.8193\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9168 - loss: 0.2045\n","Epoch 8 - MCC: 0.8278\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9168 - loss: 0.2044 - val_accuracy: 0.9140 - val_loss: 0.2091 - mcc: 0.8278\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9156 - loss: 0.2055\n","Epoch 9 - MCC: 0.8286\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9157 - loss: 0.2052 - val_accuracy: 0.9143 - val_loss: 0.2084 - mcc: 0.8286\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9192 - loss: 0.1966\n","Epoch 10 - MCC: 0.8250\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9192 - loss: 0.1965 - val_accuracy: 0.9115 - val_loss: 0.2109 - mcc: 0.8250\n","Epoch 11/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9231 - loss: 0.1848\n","Epoch 11 - MCC: 0.8306\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.9230 - loss: 0.1851 - val_accuracy: 0.9149 - val_loss: 0.2047 - mcc: 0.8306\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9200 - loss: 0.1918\n","Epoch 12 - MCC: 0.8321\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9200 - loss: 0.1917 - val_accuracy: 0.9161 - val_loss: 0.1976 - mcc: 0.8321\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9232 - loss: 0.1843\n","Epoch 13 - MCC: 0.8395\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9233 - loss: 0.1842 - val_accuracy: 0.9195 - val_loss: 0.1951 - mcc: 0.8395\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9245 - loss: 0.1828\n","Epoch 14 - MCC: 0.8396\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9245 - loss: 0.1828 - val_accuracy: 0.9198 - val_loss: 0.1945 - mcc: 0.8396\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9240 - loss: 0.1832\n","Epoch 15 - MCC: 0.8393\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9241 - loss: 0.1831 - val_accuracy: 0.9198 - val_loss: 0.1930 - mcc: 0.8393\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9283 - loss: 0.1724\n","Epoch 16 - MCC: 0.8441\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9282 - loss: 0.1727 - val_accuracy: 0.9219 - val_loss: 0.1907 - mcc: 0.8441\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9262 - loss: 0.1788\n","Epoch 17 - MCC: 0.8357\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9262 - loss: 0.1789 - val_accuracy: 0.9179 - val_loss: 0.1945 - mcc: 0.8357\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9243 - loss: 0.1832\n","Epoch 18 - MCC: 0.8455\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9244 - loss: 0.1829 - val_accuracy: 0.9226 - val_loss: 0.1893 - mcc: 0.8455\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9271 - loss: 0.1775\n","Epoch 19 - MCC: 0.8406\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9271 - loss: 0.1775 - val_accuracy: 0.9201 - val_loss: 0.1917 - mcc: 0.8406\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9297 - loss: 0.1693\n","Epoch 20 - MCC: 0.8472\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.9296 - loss: 0.1695 - val_accuracy: 0.9237 - val_loss: 0.1840 - mcc: 0.8472\n","Epoch 21/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9288 - loss: 0.1734\n","Epoch 21 - MCC: 0.8473\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9288 - loss: 0.1733 - val_accuracy: 0.9235 - val_loss: 0.1851 - mcc: 0.8473\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9277 - loss: 0.1737\n","Epoch 22 - MCC: 0.8420\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9277 - loss: 0.1736 - val_accuracy: 0.9197 - val_loss: 0.1959 - mcc: 0.8420\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9306 - loss: 0.1685\n","Epoch 23 - MCC: 0.8495\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9306 - loss: 0.1686 - val_accuracy: 0.9247 - val_loss: 0.1834 - mcc: 0.8495\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9306 - loss: 0.1679\n","Epoch 24 - MCC: 0.8517\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9306 - loss: 0.1678 - val_accuracy: 0.9257 - val_loss: 0.1821 - mcc: 0.8517\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9270 - loss: 0.1742\n","Epoch 25 - MCC: 0.8435\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9271 - loss: 0.1739 - val_accuracy: 0.9217 - val_loss: 0.1845 - mcc: 0.8435\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9292 - loss: 0.1708\n","Epoch 26 - MCC: 0.8512\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9292 - loss: 0.1707 - val_accuracy: 0.9257 - val_loss: 0.1791 - mcc: 0.8512\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9284 - loss: 0.1709\n","Epoch 27 - MCC: 0.8506\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9285 - loss: 0.1708 - val_accuracy: 0.9252 - val_loss: 0.1833 - mcc: 0.8506\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9273 - loss: 0.1759\n","Epoch 28 - MCC: 0.8550\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9274 - loss: 0.1755 - val_accuracy: 0.9274 - val_loss: 0.1774 - mcc: 0.8550\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9278 - loss: 0.1744\n","Epoch 29 - MCC: 0.8564\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.9280 - loss: 0.1740 - val_accuracy: 0.9282 - val_loss: 0.1756 - mcc: 0.8564\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9286 - loss: 0.1700\n","Epoch 30 - MCC: 0.8523\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.9287 - loss: 0.1697 - val_accuracy: 0.9263 - val_loss: 0.1773 - mcc: 0.8523\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.93664,\n","              'mean': 0.9284533333333334,\n","              'min': 0.9193333333333333,\n","              'std': 0.005725540440284514},\n"," 'Inference Time (s/sample)': {'max': 0.0019947195053100584,\n","                               'mean': 0.0018271417617797849,\n","                               'min': 0.001371786594390869,\n","                               'std': 0.00023027226595574357},\n"," 'MCC': {'max': 0.8729698560368481,\n","         'mean': 0.8566423913624528,\n","         'min': 0.838686216058784,\n","         'std': 0.011335950827610191},\n"," 'Parameters': 5665,\n"," 'Train Time (s)': {'max': 54.40808701515198,\n","                    'mean': 47.05475192070007,\n","                    'min': 43.35520339012146,\n","                    'std': 3.886735335338728},\n"," 'Training Accuracy': [[0.6791633367538452,\n","                        0.8026183247566223,\n","                        0.8577150106430054,\n","                        0.8879083395004272,\n","                        0.8994168043136597,\n","                        0.9043516516685486,\n","                        0.9079132676124573,\n","                        0.9114416241645813,\n","                        0.9138132333755493,\n","                        0.9159700274467468,\n","                        0.9163618683815002,\n","                        0.9152367115020752,\n","                        0.9164832234382629,\n","                        0.9194284081459045,\n","                        0.9203383922576904,\n","                        0.9202182292938232,\n","                        0.9216450452804565,\n","                        0.9217566847801208,\n","                        0.9223299622535706,\n","                        0.9222033023834229,\n","                        0.9233232140541077,\n","                        0.9229816198348999,\n","                        0.9224733114242554,\n","                        0.9239883422851562,\n","                        0.9247332811355591,\n","                        0.9242250323295593,\n","                        0.9252982139587402,\n","                        0.9256449937820435,\n","                        0.9260733127593994,\n","                        0.9258366227149963],\n","                       [0.6067365407943726,\n","                        0.8017499446868896,\n","                        0.869795024394989,\n","                        0.8922800421714783,\n","                        0.9007217288017273,\n","                        0.9060666561126709,\n","                        0.9138799905776978,\n","                        0.9177350401878357,\n","                        0.9205915927886963,\n","                        0.9192050099372864,\n","                        0.92180335521698,\n","                        0.9223916530609131,\n","                        0.9228700399398804,\n","                        0.9245349764823914,\n","                        0.9244550466537476,\n","                        0.9239014983177185,\n","                        0.9257434010505676,\n","                        0.9257815480232239,\n","                        0.9255700707435608,\n","                        0.9265398979187012,\n","                        0.9276301860809326,\n","                        0.9281365871429443,\n","                        0.9280883073806763,\n","                        0.9293400645256042,\n","                        0.9297749996185303,\n","                        0.9292299747467041,\n","                        0.9290984869003296,\n","                        0.9305633306503296,\n","                        0.9316617846488953,\n","                        0.9321800470352173],\n","                       [0.6848834156990051,\n","                        0.8224432468414307,\n","                        0.8502984046936035,\n","                        0.8839383125305176,\n","                        0.8988233208656311,\n","                        0.9046032428741455,\n","                        0.9078882336616516,\n","                        0.9114699363708496,\n","                        0.9132382869720459,\n","                        0.9155816435813904,\n","                        0.9167883396148682,\n","                        0.916561484336853,\n","                        0.9173064231872559,\n","                        0.9177801012992859,\n","                        0.9193199872970581,\n","                        0.9197816252708435,\n","                        0.9203367829322815,\n","                        0.9199501276016235,\n","                        0.9215333461761475,\n","                        0.9221316576004028,\n","                        0.9224750995635986,\n","                        0.9213199615478516,\n","                        0.9227200150489807,\n","                        0.9216667413711548,\n","                        0.9213167428970337,\n","                        0.9238132238388062,\n","                        0.9241650104522705,\n","                        0.9235067963600159,\n","                        0.9251381754875183,\n","                        0.9252099394798279],\n","                       [0.5692416429519653,\n","                        0.7678250074386597,\n","                        0.8715016841888428,\n","                        0.8905283212661743,\n","                        0.9035566449165344,\n","                        0.9089715480804443,\n","                        0.9135666489601135,\n","                        0.9154565930366516,\n","                        0.9165249466896057,\n","                        0.918459951877594,\n","                        0.9192167520523071,\n","                        0.9198049902915955,\n","                        0.920078456401825,\n","                        0.9201916456222534,\n","                        0.9212499856948853,\n","                        0.9210766553878784,\n","                        0.9231283664703369,\n","                        0.9225117564201355,\n","                        0.9238733649253845,\n","                        0.9241133332252502,\n","                        0.9241066575050354,\n","                        0.9254134297370911,\n","                        0.9255916476249695,\n","                        0.925320029258728,\n","                        0.9273150563240051,\n","                        0.9274299740791321,\n","                        0.9280501008033752,\n","                        0.9294431805610657,\n","                        0.9284082055091858,\n","                        0.9292849898338318],\n","                       [0.64784836769104,\n","                        0.8085049390792847,\n","                        0.8663366436958313,\n","                        0.8922083973884583,\n","                        0.9057933688163757,\n","                        0.9109416007995605,\n","                        0.9143567085266113,\n","                        0.917548418045044,\n","                        0.9187449812889099,\n","                        0.919313371181488,\n","                        0.9217099547386169,\n","                        0.9218966960906982,\n","                        0.9247100353240967,\n","                        0.9241183400154114,\n","                        0.9248014688491821,\n","                        0.9251550436019897,\n","                        0.9247599840164185,\n","                        0.9273132085800171,\n","                        0.9270867109298706,\n","                        0.9284467101097107,\n","                        0.9282750487327576,\n","                        0.9293866753578186,\n","                        0.9299984574317932,\n","                        0.9305134415626526,\n","                        0.9303250312805176,\n","                        0.9296084046363831,\n","                        0.930234968662262,\n","                        0.9314233660697937,\n","                        0.932518482208252,\n","                        0.9319650530815125]],\n"," 'Training Loss': [[0.6545220017433167,\n","                    0.4861018657684326,\n","                    0.34190911054611206,\n","                    0.2749132812023163,\n","                    0.24537134170532227,\n","                    0.23368363082408905,\n","                    0.22294044494628906,\n","                    0.21465711295604706,\n","                    0.20782333612442017,\n","                    0.20290715992450714,\n","                    0.2011331170797348,\n","                    0.20218291878700256,\n","                    0.19875702261924744,\n","                    0.19268321990966797,\n","                    0.19148379564285278,\n","                    0.19138002395629883,\n","                    0.1886116862297058,\n","                    0.18842840194702148,\n","                    0.18757383525371552,\n","                    0.18780352175235748,\n","                    0.18482734262943268,\n","                    0.18481475114822388,\n","                    0.1863686442375183,\n","                    0.18261924386024475,\n","                    0.18154960870742798,\n","                    0.1816720962524414,\n","                    0.1809401959180832,\n","                    0.1786210685968399,\n","                    0.1777876913547516,\n","                    0.1783166080713272],\n","                   [0.661998450756073,\n","                    0.5098970532417297,\n","                    0.33005669713020325,\n","                    0.26609304547309875,\n","                    0.24181140959262848,\n","                    0.2293621450662613,\n","                    0.20885412395000458,\n","                    0.19893038272857666,\n","                    0.19114193320274353,\n","                    0.19435986876487732,\n","                    0.18794246017932892,\n","                    0.18705321848392487,\n","                    0.18413929641246796,\n","                    0.18155977129936218,\n","                    0.18147136270999908,\n","                    0.18139411509037018,\n","                    0.17780564725399017,\n","                    0.17741701006889343,\n","                    0.17833000421524048,\n","                    0.17539142072200775,\n","                    0.1734011471271515,\n","                    0.17230626940727234,\n","                    0.17113067209720612,\n","                    0.16955159604549408,\n","                    0.1689104437828064,\n","                    0.16917243599891663,\n","                    0.16934673488140106,\n","                    0.16720789670944214,\n","                    0.16440622508525848,\n","                    0.16315799951553345],\n","                   [0.6832807064056396,\n","                    0.5232757329940796,\n","                    0.3646107614040375,\n","                    0.2897906005382538,\n","                    0.24993732571601868,\n","                    0.2322946935892105,\n","                    0.22171461582183838,\n","                    0.2129596471786499,\n","                    0.2092178910970688,\n","                    0.20249323546886444,\n","                    0.20027099549770355,\n","                    0.19971539080142975,\n","                    0.19730813801288605,\n","                    0.19569343328475952,\n","                    0.19292782247066498,\n","                    0.1910894364118576,\n","                    0.19029401242733002,\n","                    0.18954503536224365,\n","                    0.18730129301548004,\n","                    0.1863173544406891,\n","                    0.1857140213251114,\n","                    0.1876346468925476,\n","                    0.18477380275726318,\n","                    0.18606753647327423,\n","                    0.18756219744682312,\n","                    0.18189115822315216,\n","                    0.1808987557888031,\n","                    0.18182435631752014,\n","                    0.1792530119419098,\n","                    0.1794406771659851],\n","                   [0.6697838306427002,\n","                    0.5189924836158752,\n","                    0.32155078649520874,\n","                    0.26429426670074463,\n","                    0.22979626059532166,\n","                    0.21520580351352692,\n","                    0.20415237545967102,\n","                    0.20117446780204773,\n","                    0.19625787436962128,\n","                    0.19339172542095184,\n","                    0.19108028709888458,\n","                    0.18989667296409607,\n","                    0.18875502049922943,\n","                    0.18746180832386017,\n","                    0.18562670052051544,\n","                    0.18699705600738525,\n","                    0.18176180124282837,\n","                    0.183304563164711,\n","                    0.17971524596214294,\n","                    0.17946772277355194,\n","                    0.17925509810447693,\n","                    0.17664460837841034,\n","                    0.1760392040014267,\n","                    0.17658375203609467,\n","                    0.17411941289901733,\n","                    0.1729496717453003,\n","                    0.17217281460762024,\n","                    0.16905362904071808,\n","                    0.17097854614257812,\n","                    0.16873569786548615],\n","                   [0.6611480116844177,\n","                    0.5115626454353333,\n","                    0.3362377882003784,\n","                    0.26836350560188293,\n","                    0.23423567414283752,\n","                    0.2202635556459427,\n","                    0.21080146729946136,\n","                    0.20270416140556335,\n","                    0.1980249285697937,\n","                    0.19463130831718445,\n","                    0.18916292488574982,\n","                    0.18898999691009521,\n","                    0.1824178695678711,\n","                    0.18338562548160553,\n","                    0.18139530718326569,\n","                    0.17946772277355194,\n","                    0.18093883991241455,\n","                    0.17509004473686218,\n","                    0.1771198809146881,\n","                    0.1725197285413742,\n","                    0.1730480045080185,\n","                    0.17026546597480774,\n","                    0.16908809542655945,\n","                    0.16728156805038452,\n","                    0.1669377088546753,\n","                    0.16949081420898438,\n","                    0.167695090174675,\n","                    0.16504104435443878,\n","                    0.1626138836145401,\n","                    0.16345570981502533]],\n"," 'Validation Accuracy': [[0.7621467113494873,\n","                          0.8525866866111755,\n","                          0.8784798979759216,\n","                          0.9009599089622498,\n","                          0.909006655216217,\n","                          0.9130867123603821,\n","                          0.9149133563041687,\n","                          0.9161133766174316,\n","                          0.9189733266830444,\n","                          0.9164466857910156,\n","                          0.9110599756240845,\n","                          0.9223200082778931,\n","                          0.9225932955741882,\n","                          0.923133373260498,\n","                          0.9235799312591553,\n","                          0.9259933233261108,\n","                          0.92632657289505,\n","                          0.9276000261306763,\n","                          0.9289133548736572,\n","                          0.9252667427062988,\n","                          0.9243733286857605,\n","                          0.9281866550445557,\n","                          0.9254799485206604,\n","                          0.9276599884033203,\n","                          0.9277265667915344,\n","                          0.9273000359535217,\n","                          0.9309667348861694,\n","                          0.9302200078964233,\n","                          0.9299733638763428,\n","                          0.9314600229263306],\n","                         [0.7203733921051025,\n","                          0.8359200358390808,\n","                          0.8769267201423645,\n","                          0.8860733509063721,\n","                          0.8921999931335449,\n","                          0.9001799821853638,\n","                          0.9048066735267639,\n","                          0.9094333052635193,\n","                          0.9101733565330505,\n","                          0.9114733338356018,\n","                          0.9119666814804077,\n","                          0.9128533005714417,\n","                          0.9141533374786377,\n","                          0.9138267636299133,\n","                          0.9135066270828247,\n","                          0.914246678352356,\n","                          0.9142199754714966,\n","                          0.9107133746147156,\n","                          0.9150400757789612,\n","                          0.9121267199516296,\n","                          0.9155266284942627,\n","                          0.9161133766174316,\n","                          0.916179895401001,\n","                          0.917419970035553,\n","                          0.9177399277687073,\n","                          0.9177066683769226,\n","                          0.9171666502952576,\n","                          0.9181333184242249,\n","                          0.9198600053787231,\n","                          0.9193333387374878],\n","                         [0.8386467099189758,\n","                          0.8215933442115784,\n","                          0.8728932738304138,\n","                          0.8932666778564453,\n","                          0.9051401019096375,\n","                          0.9106399416923523,\n","                          0.9143267273902893,\n","                          0.9174200296401978,\n","                          0.9139666557312012,\n","                          0.9161267280578613,\n","                          0.9197466373443604,\n","                          0.920960009098053,\n","                          0.9215199947357178,\n","                          0.9178733229637146,\n","                          0.9229866862297058,\n","                          0.9233067035675049,\n","                          0.9235532879829407,\n","                          0.923706591129303,\n","                          0.9240466952323914,\n","                          0.9244733452796936,\n","                          0.9234000444412231,\n","                          0.9248466491699219,\n","                          0.9254999756813049,\n","                          0.9235666394233704,\n","                          0.9273933172225952,\n","                          0.9274599552154541,\n","                          0.925926685333252,\n","                          0.9273932576179504,\n","                          0.926766574382782,\n","                          0.928540050983429],\n","                         [0.6372733116149902,\n","                          0.8671066164970398,\n","                          0.8951266407966614,\n","                          0.9063666462898254,\n","                          0.9124399423599243,\n","                          0.9168933033943176,\n","                          0.9188733100891113,\n","                          0.923266589641571,\n","                          0.9179599285125732,\n","                          0.9244199395179749,\n","                          0.9258333444595337,\n","                          0.9234799742698669,\n","                          0.9281933307647705,\n","                          0.9264066815376282,\n","                          0.9255266785621643,\n","                          0.9293599724769592,\n","                          0.9291733503341675,\n","                          0.9291865825653076,\n","                          0.9294666051864624,\n","                          0.9305466413497925,\n","                          0.9314066171646118,\n","                          0.9330199956893921,\n","                          0.9314666986465454,\n","                          0.9327201247215271,\n","                          0.9336466193199158,\n","                          0.9309999942779541,\n","                          0.9346334338188171,\n","                          0.933753252029419,\n","                          0.9350600242614746,\n","                          0.9366400241851807],\n","                         [0.7530800700187683,\n","                          0.8312332630157471,\n","                          0.8822000026702881,\n","                          0.8952066898345947,\n","                          0.9053400158882141,\n","                          0.9068999290466309,\n","                          0.9098132848739624,\n","                          0.9139666557312012,\n","                          0.914306640625,\n","                          0.9114733338356018,\n","                          0.9149399399757385,\n","                          0.9161199927330017,\n","                          0.9194866418838501,\n","                          0.91975998878479,\n","                          0.9197800159454346,\n","                          0.921886682510376,\n","                          0.9179333448410034,\n","                          0.9226133227348328,\n","                          0.9200533032417297,\n","                          0.9237332940101624,\n","                          0.9235267043113708,\n","                          0.9197132587432861,\n","                          0.9247400760650635,\n","                          0.9256532788276672,\n","                          0.9217333793640137,\n","                          0.9256999492645264,\n","                          0.925173282623291,\n","                          0.9274466633796692,\n","                          0.9282000064849854,\n","                          0.9262934327125549]],\n"," 'Validation Loss': [[0.5880751609802246,\n","                      0.3808329403400421,\n","                      0.2896922826766968,\n","                      0.24196568131446838,\n","                      0.22538046538829803,\n","                      0.21127380430698395,\n","                      0.2054804414510727,\n","                      0.2003195881843567,\n","                      0.19294866919517517,\n","                      0.1953292042016983,\n","                      0.202397421002388,\n","                      0.18677091598510742,\n","                      0.18243558704853058,\n","                      0.18148374557495117,\n","                      0.18239536881446838,\n","                      0.17584526538848877,\n","                      0.17560283839702606,\n","                      0.17179729044437408,\n","                      0.17044474184513092,\n","                      0.17486706376075745,\n","                      0.1766906976699829,\n","                      0.1709233671426773,\n","                      0.1745566576719284,\n","                      0.17142480611801147,\n","                      0.17265674471855164,\n","                      0.17364297807216644,\n","                      0.16554565727710724,\n","                      0.1649104505777359,\n","                      0.1655261367559433,\n","                      0.16314317286014557],\n","                     [0.6080747842788696,\n","                      0.4111262857913971,\n","                      0.30457010865211487,\n","                      0.2756883502006531,\n","                      0.26218175888061523,\n","                      0.23696744441986084,\n","                      0.22384518384933472,\n","                      0.21697567403316498,\n","                      0.21412082016468048,\n","                      0.21328964829444885,\n","                      0.20762518048286438,\n","                      0.20538988709449768,\n","                      0.20681510865688324,\n","                      0.2035907804965973,\n","                      0.20595529675483704,\n","                      0.20364058017730713,\n","                      0.20131397247314453,\n","                      0.2101694494485855,\n","                      0.20229698717594147,\n","                      0.2058347761631012,\n","                      0.198568195104599,\n","                      0.19830560684204102,\n","                      0.19643661379814148,\n","                      0.19654609262943268,\n","                      0.19453462958335876,\n","                      0.19425253570079803,\n","                      0.19437673687934875,\n","                      0.19472317397594452,\n","                      0.1896238923072815,\n","                      0.19066883623600006],\n","                     [0.6213412880897522,\n","                      0.4206336736679077,\n","                      0.3051735758781433,\n","                      0.2633711099624634,\n","                      0.23016977310180664,\n","                      0.21970489621162415,\n","                      0.20601128041744232,\n","                      0.1999722123146057,\n","                      0.2005707323551178,\n","                      0.19678635895252228,\n","                      0.19059351086616516,\n","                      0.18809132277965546,\n","                      0.18608903884887695,\n","                      0.1915852427482605,\n","                      0.18309295177459717,\n","                      0.18284152448177338,\n","                      0.18385523557662964,\n","                      0.1798357367515564,\n","                      0.18122665584087372,\n","                      0.1784691959619522,\n","                      0.1801319271326065,\n","                      0.17856352031230927,\n","                      0.17886216938495636,\n","                      0.18271413445472717,\n","                      0.17410583794116974,\n","                      0.1717938780784607,\n","                      0.17604541778564453,\n","                      0.1724989116191864,\n","                      0.17402660846710205,\n","                      0.16835449635982513],\n","                     [0.6060231924057007,\n","                      0.3966272473335266,\n","                      0.26237645745277405,\n","                      0.22729162871837616,\n","                      0.21021930873394012,\n","                      0.19817794859409332,\n","                      0.19478850066661835,\n","                      0.18679681420326233,\n","                      0.19383272528648376,\n","                      0.1831303834915161,\n","                      0.17893600463867188,\n","                      0.1853102296590805,\n","                      0.17540408670902252,\n","                      0.17580761015415192,\n","                      0.17762011289596558,\n","                      0.17352014780044556,\n","                      0.17156387865543365,\n","                      0.1718207746744156,\n","                      0.1728672832250595,\n","                      0.16959498822689056,\n","                      0.16663026809692383,\n","                      0.16576823592185974,\n","                      0.16495464742183685,\n","                      0.16366013884544373,\n","                      0.1618461012840271,\n","                      0.1681773066520691,\n","                      0.1601821780204773,\n","                      0.16366459429264069,\n","                      0.1606544852256775,\n","                      0.15798555314540863],\n","                     [0.6085224151611328,\n","                      0.41014742851257324,\n","                      0.2976177930831909,\n","                      0.25890061259269714,\n","                      0.23342174291610718,\n","                      0.2238031029701233,\n","                      0.21583467721939087,\n","                      0.20911537110805511,\n","                      0.20842482149600983,\n","                      0.21085502207279205,\n","                      0.20474138855934143,\n","                      0.19759933650493622,\n","                      0.19505934417247772,\n","                      0.1944572776556015,\n","                      0.19295017421245575,\n","                      0.1907443255186081,\n","                      0.19448204338550568,\n","                      0.1893111616373062,\n","                      0.19165872037410736,\n","                      0.18398310244083405,\n","                      0.1850975602865219,\n","                      0.19589294493198395,\n","                      0.18342216312885284,\n","                      0.1820756494998932,\n","                      0.18449676036834717,\n","                      0.17910268902778625,\n","                      0.18329209089279175,\n","                      0.17740356922149658,\n","                      0.1755644828081131,\n","                      0.1773391216993332]],\n"," 'Validation MCC': [[0.539597636723373,\n","                     0.7053797206237651,\n","                     0.7582212298520421,\n","                     0.8013063253323633,\n","                     0.8183429571161001,\n","                     0.8263963136665902,\n","                     0.8297336804084942,\n","                     0.8322617096803011,\n","                     0.8378476255016138,\n","                     0.8327502745672258,\n","                     0.8232514495811087,\n","                     0.8443680954998263,\n","                     0.845207954927734,\n","                     0.8457915533709829,\n","                     0.8468932307872913,\n","                     0.8514035291682089,\n","                     0.8522197828889367,\n","                     0.8546301426646297,\n","                     0.8572751107590301,\n","                     0.8505173085822603,\n","                     0.8489760012841817,\n","                     0.8558988841918204,\n","                     0.8513076049949498,\n","                     0.8553736936026543,\n","                     0.8554586631251704,\n","                     0.8553253916889194,\n","                     0.8613923384595255,\n","                     0.8599485030588542,\n","                     0.8595028624519111,\n","                     0.862539066625353],\n","                    [0.4605959723390269,\n","                     0.6713375044631552,\n","                     0.7532780205713725,\n","                     0.7719418388209248,\n","                     0.7851018844161132,\n","                     0.8000660735889118,\n","                     0.8091826312877948,\n","                     0.8184296845533102,\n","                     0.8201429727978716,\n","                     0.8234620284555264,\n","                     0.8235221154868781,\n","                     0.8253008201488664,\n","                     0.828172394139112,\n","                     0.8272954890739147,\n","                     0.8269527123707464,\n","                     0.828274166094553,\n","                     0.8280610801117627,\n","                     0.8220860940986493,\n","                     0.8298321237058904,\n","                     0.8240869702729663,\n","                     0.8308841789100451,\n","                     0.8320425716104732,\n","                     0.8319624459103305,\n","                     0.8345853521691536,\n","                     0.8351514691002571,\n","                     0.8355386356545138,\n","                     0.8347990941570348,\n","                     0.8363152532835679,\n","                     0.8393827599866237,\n","                     0.838686216058784],\n","                    [0.6757598371420008,\n","                     0.6414311719152102,\n","                     0.7449134477704238,\n","                     0.7899360870501788,\n","                     0.810197269696952,\n","                     0.820532809431594,\n","                     0.827982867250378,\n","                     0.8341547685051802,\n","                     0.8272333049094958,\n","                     0.8315506415906717,\n","                     0.8389190000924102,\n","                     0.8413178456372081,\n","                     0.8424980474290641,\n","                     0.8360411361364268,\n","                     0.8453231497668938,\n","                     0.8462618040701003,\n","                     0.8467672350312898,\n","                     0.8468778935966975,\n","                     0.8475431913527558,\n","                     0.8483888343825591,\n","                     0.8462443230160261,\n","                     0.84924531720915,\n","                     0.8505881135820414,\n","                     0.8480960074787925,\n","                     0.8541867887892537,\n","                     0.8543373134155292,\n","                     0.8514562239495914,\n","                     0.8543759148957016,\n","                     0.8532663193205606,\n","                     0.8567027234473638],\n","                    [0.37245254194477534,\n","                     0.7344958200229763,\n","                     0.7900912950550603,\n","                     0.8123101947875049,\n","                     0.824938693556398,\n","                     0.8339070354258833,\n","                     0.8381049172619253,\n","                     0.8462468917561282,\n","                     0.835793512120465,\n","                     0.8486081220592572,\n","                     0.8514161899406932,\n","                     0.8469501891786435,\n","                     0.8560360030553416,\n","                     0.8525664385972896,\n","                     0.8506836062221897,\n","                     0.8584852895624336,\n","                     0.8580864261926575,\n","                     0.8583615796202996,\n","                     0.8587297803168535,\n","                     0.860857921595344,\n","                     0.8625865438523092,\n","                     0.865779549745013,\n","                     0.8627406055824405,\n","                     0.8651104680762443,\n","                     0.8669800844159041,\n","                     0.8618885614495632,\n","                     0.8689615825574503,\n","                     0.8671839572262359,\n","                     0.8700617633526003,\n","                     0.8729698560368481],\n","                    [0.533187703741845,\n","                     0.6622970851688903,\n","                     0.7639908996547468,\n","                     0.7905116606809126,\n","                     0.81041110355009,\n","                     0.8135010390344234,\n","                     0.8193120352369119,\n","                     0.8277571658981183,\n","                     0.82857686071557,\n","                     0.8250364813109421,\n","                     0.8306441585644384,\n","                     0.8320531019764785,\n","                     0.8395046914689767,\n","                     0.8395578171863773,\n","                     0.8392727391336284,\n","                     0.8440816648570213,\n","                     0.8356620936767555,\n","                     0.8455421627913562,\n","                     0.8405580271799193,\n","                     0.8472470662710091,\n","                     0.8472919869928583,\n","                     0.8420345738242128,\n","                     0.8494907706045023,\n","                     0.8517111933497917,\n","                     0.8434630285593211,\n","                     0.8512157414874459,\n","                     0.8505512701749746,\n","                     0.8550041374936751,\n","                     0.8564482177047904,\n","                     0.8523140946439153]]}\n","Training Model: LSTM_Deep, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6384 - loss: 0.6449\n","Epoch 1 - MCC: 0.6576\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.6410 - loss: 0.6424 - val_accuracy: 0.8294 - val_loss: 0.4090 - mcc: 0.6576\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8530 - loss: 0.3589\n","Epoch 2 - MCC: 0.7964\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8536 - loss: 0.3575 - val_accuracy: 0.8985 - val_loss: 0.2496 - mcc: 0.7964\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8923 - loss: 0.2587\n","Epoch 3 - MCC: 0.8269\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.8926 - loss: 0.2581 - val_accuracy: 0.9138 - val_loss: 0.2076 - mcc: 0.8269\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9050 - loss: 0.2233\n","Epoch 4 - MCC: 0.8369\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9052 - loss: 0.2230 - val_accuracy: 0.9184 - val_loss: 0.1944 - mcc: 0.8369\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9107 - loss: 0.2112\n","Epoch 5 - MCC: 0.8418\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9108 - loss: 0.2111 - val_accuracy: 0.9208 - val_loss: 0.1885 - mcc: 0.8418\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9095 - loss: 0.2129\n","Epoch 6 - MCC: 0.8445\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9096 - loss: 0.2126 - val_accuracy: 0.9225 - val_loss: 0.1846 - mcc: 0.8445\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9119 - loss: 0.2108\n","Epoch 7 - MCC: 0.8391\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9120 - loss: 0.2105 - val_accuracy: 0.9189 - val_loss: 0.1914 - mcc: 0.8391\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9184 - loss: 0.1945\n","Epoch 8 - MCC: 0.8471\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9184 - loss: 0.1944 - val_accuracy: 0.9236 - val_loss: 0.1795 - mcc: 0.8471\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9236 - loss: 0.1809\n","Epoch 9 - MCC: 0.8487\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9234 - loss: 0.1814 - val_accuracy: 0.9246 - val_loss: 0.1807 - mcc: 0.8487\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9202 - loss: 0.1870\n","Epoch 10 - MCC: 0.8549\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9201 - loss: 0.1872 - val_accuracy: 0.9276 - val_loss: 0.1725 - mcc: 0.8549\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9198 - loss: 0.1888\n","Epoch 11 - MCC: 0.8466\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9198 - loss: 0.1887 - val_accuracy: 0.9227 - val_loss: 0.1818 - mcc: 0.8466\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9183 - loss: 0.1916\n","Epoch 12 - MCC: 0.8496\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9182 - loss: 0.1917 - val_accuracy: 0.9250 - val_loss: 0.1781 - mcc: 0.8496\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9216 - loss: 0.1874\n","Epoch 13 - MCC: 0.8540\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9216 - loss: 0.1872 - val_accuracy: 0.9269 - val_loss: 0.1708 - mcc: 0.8540\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9224 - loss: 0.1857\n","Epoch 14 - MCC: 0.8518\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9224 - loss: 0.1855 - val_accuracy: 0.9261 - val_loss: 0.1725 - mcc: 0.8518\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9251 - loss: 0.1784\n","Epoch 15 - MCC: 0.8659\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9251 - loss: 0.1785 - val_accuracy: 0.9331 - val_loss: 0.1603 - mcc: 0.8659\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9233 - loss: 0.1836\n","Epoch 16 - MCC: 0.8597\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9233 - loss: 0.1834 - val_accuracy: 0.9298 - val_loss: 0.1642 - mcc: 0.8597\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9249 - loss: 0.1795\n","Epoch 17 - MCC: 0.8622\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9250 - loss: 0.1794 - val_accuracy: 0.9310 - val_loss: 0.1635 - mcc: 0.8622\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9298 - loss: 0.1663\n","Epoch 18 - MCC: 0.8615\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9297 - loss: 0.1667 - val_accuracy: 0.9308 - val_loss: 0.1612 - mcc: 0.8615\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9243 - loss: 0.1790\n","Epoch 19 - MCC: 0.8691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9244 - loss: 0.1787 - val_accuracy: 0.9343 - val_loss: 0.1552 - mcc: 0.8691\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9283 - loss: 0.1718\n","Epoch 20 - MCC: 0.8769\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9284 - loss: 0.1717 - val_accuracy: 0.9387 - val_loss: 0.1482 - mcc: 0.8769\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9325 - loss: 0.1616\n","Epoch 21 - MCC: 0.8705\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9324 - loss: 0.1619 - val_accuracy: 0.9355 - val_loss: 0.1560 - mcc: 0.8705\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9312 - loss: 0.1660\n","Epoch 22 - MCC: 0.8747\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9311 - loss: 0.1661 - val_accuracy: 0.9374 - val_loss: 0.1510 - mcc: 0.8747\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9305 - loss: 0.1667\n","Epoch 23 - MCC: 0.8739\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9306 - loss: 0.1666 - val_accuracy: 0.9372 - val_loss: 0.1499 - mcc: 0.8739\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9359 - loss: 0.1546\n","Epoch 24 - MCC: 0.8776\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9358 - loss: 0.1549 - val_accuracy: 0.9386 - val_loss: 0.1466 - mcc: 0.8776\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9312 - loss: 0.1637\n","Epoch 25 - MCC: 0.8765\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9312 - loss: 0.1637 - val_accuracy: 0.9384 - val_loss: 0.1468 - mcc: 0.8765\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9325 - loss: 0.1626\n","Epoch 26 - MCC: 0.8785\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.9326 - loss: 0.1625 - val_accuracy: 0.9395 - val_loss: 0.1460 - mcc: 0.8785\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9360 - loss: 0.1560\n","Epoch 27 - MCC: 0.8834\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9360 - loss: 0.1562 - val_accuracy: 0.9419 - val_loss: 0.1398 - mcc: 0.8834\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9368 - loss: 0.1519\n","Epoch 28 - MCC: 0.8640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9367 - loss: 0.1521 - val_accuracy: 0.9313 - val_loss: 0.1630 - mcc: 0.8640\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9322 - loss: 0.1629\n","Epoch 29 - MCC: 0.8830\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9323 - loss: 0.1628 - val_accuracy: 0.9417 - val_loss: 0.1413 - mcc: 0.8830\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9368 - loss: 0.1519\n","Epoch 30 - MCC: 0.8851\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9367 - loss: 0.1521 - val_accuracy: 0.9427 - val_loss: 0.1386 - mcc: 0.8851\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6042 - loss: 0.6204\n","Epoch 1 - MCC: 0.6237\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.6089 - loss: 0.6175 - val_accuracy: 0.8032 - val_loss: 0.4185 - mcc: 0.6237\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8614 - loss: 0.3367\n","Epoch 2 - MCC: 0.7739\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - accuracy: 0.8620 - loss: 0.3354 - val_accuracy: 0.8866 - val_loss: 0.2778 - mcc: 0.7739\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8997 - loss: 0.2455\n","Epoch 3 - MCC: 0.8002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8998 - loss: 0.2452 - val_accuracy: 0.9003 - val_loss: 0.2394 - mcc: 0.8002\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9131 - loss: 0.2093\n","Epoch 4 - MCC: 0.8103\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9130 - loss: 0.2096 - val_accuracy: 0.9053 - val_loss: 0.2276 - mcc: 0.8103\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9128 - loss: 0.2071\n","Epoch 5 - MCC: 0.8149\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9129 - loss: 0.2070 - val_accuracy: 0.9077 - val_loss: 0.2199 - mcc: 0.8149\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9146 - loss: 0.2040\n","Epoch 6 - MCC: 0.8127\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9148 - loss: 0.2037 - val_accuracy: 0.9060 - val_loss: 0.2217 - mcc: 0.8127\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9187 - loss: 0.1946\n","Epoch 7 - MCC: 0.8091\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9187 - loss: 0.1946 - val_accuracy: 0.9029 - val_loss: 0.2261 - mcc: 0.8091\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9194 - loss: 0.1931\n","Epoch 8 - MCC: 0.8230\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9195 - loss: 0.1928 - val_accuracy: 0.9110 - val_loss: 0.2113 - mcc: 0.8230\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9174 - loss: 0.1960\n","Epoch 9 - MCC: 0.8202\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9176 - loss: 0.1956 - val_accuracy: 0.9097 - val_loss: 0.2081 - mcc: 0.8202\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9293 - loss: 0.1700\n","Epoch 10 - MCC: 0.8331\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9292 - loss: 0.1702 - val_accuracy: 0.9168 - val_loss: 0.1963 - mcc: 0.8331\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9310 - loss: 0.1659\n","Epoch 11 - MCC: 0.8336\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - accuracy: 0.9309 - loss: 0.1662 - val_accuracy: 0.9170 - val_loss: 0.1966 - mcc: 0.8336\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9248 - loss: 0.1803\n","Epoch 12 - MCC: 0.8280\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9248 - loss: 0.1803 - val_accuracy: 0.9131 - val_loss: 0.2034 - mcc: 0.8280\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9245 - loss: 0.1806\n","Epoch 13 - MCC: 0.8387\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9246 - loss: 0.1803 - val_accuracy: 0.9195 - val_loss: 0.1940 - mcc: 0.8387\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9261 - loss: 0.1757\n","Epoch 14 - MCC: 0.8298\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9262 - loss: 0.1754 - val_accuracy: 0.9136 - val_loss: 0.2045 - mcc: 0.8298\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9245 - loss: 0.1789\n","Epoch 15 - MCC: 0.8388\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9246 - loss: 0.1787 - val_accuracy: 0.9191 - val_loss: 0.1919 - mcc: 0.8388\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9272 - loss: 0.1734\n","Epoch 16 - MCC: 0.8314\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9273 - loss: 0.1731 - val_accuracy: 0.9158 - val_loss: 0.1967 - mcc: 0.8314\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9274 - loss: 0.1713\n","Epoch 17 - MCC: 0.8417\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9275 - loss: 0.1710 - val_accuracy: 0.9210 - val_loss: 0.1874 - mcc: 0.8417\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9320 - loss: 0.1626\n","Epoch 18 - MCC: 0.8442\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9319 - loss: 0.1627 - val_accuracy: 0.9222 - val_loss: 0.1853 - mcc: 0.8442\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9277 - loss: 0.1725\n","Epoch 19 - MCC: 0.8297\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9279 - loss: 0.1721 - val_accuracy: 0.9143 - val_loss: 0.1999 - mcc: 0.8297\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9280 - loss: 0.1722\n","Epoch 20 - MCC: 0.8427\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9282 - loss: 0.1718 - val_accuracy: 0.9215 - val_loss: 0.1859 - mcc: 0.8427\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9345 - loss: 0.1591\n","Epoch 21 - MCC: 0.8466\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9344 - loss: 0.1591 - val_accuracy: 0.9235 - val_loss: 0.1828 - mcc: 0.8466\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9340 - loss: 0.1588\n","Epoch 22 - MCC: 0.8449\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9341 - loss: 0.1587 - val_accuracy: 0.9222 - val_loss: 0.1839 - mcc: 0.8449\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9377 - loss: 0.1522\n","Epoch 23 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.9377 - loss: 0.1522 - val_accuracy: 0.9230 - val_loss: 0.1810 - mcc: 0.8458\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9384 - loss: 0.1507\n","Epoch 24 - MCC: 0.8532\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9384 - loss: 0.1508 - val_accuracy: 0.9267 - val_loss: 0.1776 - mcc: 0.8532\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9377 - loss: 0.1511\n","Epoch 25 - MCC: 0.8539\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9377 - loss: 0.1510 - val_accuracy: 0.9270 - val_loss: 0.1757 - mcc: 0.8539\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9375 - loss: 0.1505\n","Epoch 26 - MCC: 0.8401\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9375 - loss: 0.1505 - val_accuracy: 0.9201 - val_loss: 0.1857 - mcc: 0.8401\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9290 - loss: 0.1686\n","Epoch 27 - MCC: 0.8498\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9293 - loss: 0.1680 - val_accuracy: 0.9246 - val_loss: 0.1827 - mcc: 0.8498\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9410 - loss: 0.1458\n","Epoch 28 - MCC: 0.8522\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9409 - loss: 0.1459 - val_accuracy: 0.9262 - val_loss: 0.1769 - mcc: 0.8522\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9343 - loss: 0.1590\n","Epoch 29 - MCC: 0.8542\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9344 - loss: 0.1587 - val_accuracy: 0.9269 - val_loss: 0.1767 - mcc: 0.8542\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9358 - loss: 0.1558\n","Epoch 30 - MCC: 0.8538\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9359 - loss: 0.1556 - val_accuracy: 0.9270 - val_loss: 0.1751 - mcc: 0.8538\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6418 - loss: 0.6233\n","Epoch 1 - MCC: 0.6761\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.6456 - loss: 0.6205 - val_accuracy: 0.8384 - val_loss: 0.3885 - mcc: 0.6761\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8354 - loss: 0.3921\n","Epoch 2 - MCC: 0.7622\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.8361 - loss: 0.3908 - val_accuracy: 0.8769 - val_loss: 0.3053 - mcc: 0.7622\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8872 - loss: 0.2820\n","Epoch 3 - MCC: 0.8003\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.8873 - loss: 0.2817 - val_accuracy: 0.9006 - val_loss: 0.2421 - mcc: 0.8003\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8992 - loss: 0.2443\n","Epoch 4 - MCC: 0.8254\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.8994 - loss: 0.2439 - val_accuracy: 0.9131 - val_loss: 0.2129 - mcc: 0.8254\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9117 - loss: 0.2139\n","Epoch 5 - MCC: 0.8357\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9116 - loss: 0.2140 - val_accuracy: 0.9180 - val_loss: 0.1987 - mcc: 0.8357\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9105 - loss: 0.2140\n","Epoch 6 - MCC: 0.8221\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9105 - loss: 0.2140 - val_accuracy: 0.9105 - val_loss: 0.2088 - mcc: 0.8221\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9138 - loss: 0.2053\n","Epoch 7 - MCC: 0.8445\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - accuracy: 0.9139 - loss: 0.2051 - val_accuracy: 0.9226 - val_loss: 0.1878 - mcc: 0.8445\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9170 - loss: 0.1997\n","Epoch 8 - MCC: 0.8414\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9170 - loss: 0.1996 - val_accuracy: 0.9210 - val_loss: 0.1875 - mcc: 0.8414\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9194 - loss: 0.1929\n","Epoch 9 - MCC: 0.8481\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9193 - loss: 0.1931 - val_accuracy: 0.9244 - val_loss: 0.1814 - mcc: 0.8481\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9179 - loss: 0.1952\n","Epoch 10 - MCC: 0.8459\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.9179 - loss: 0.1951 - val_accuracy: 0.9232 - val_loss: 0.1845 - mcc: 0.8459\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9191 - loss: 0.1925\n","Epoch 11 - MCC: 0.8488\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9191 - loss: 0.1925 - val_accuracy: 0.9247 - val_loss: 0.1799 - mcc: 0.8488\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9264 - loss: 0.1774\n","Epoch 12 - MCC: 0.8487\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9262 - loss: 0.1778 - val_accuracy: 0.9245 - val_loss: 0.1832 - mcc: 0.8487\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9291 - loss: 0.1717\n","Epoch 13 - MCC: 0.8475\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9288 - loss: 0.1722 - val_accuracy: 0.9236 - val_loss: 0.1803 - mcc: 0.8475\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9151 - loss: 0.2003\n","Epoch 14 - MCC: 0.8513\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.9153 - loss: 0.1997 - val_accuracy: 0.9259 - val_loss: 0.1792 - mcc: 0.8513\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9232 - loss: 0.1837\n","Epoch 15 - MCC: 0.8548\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9232 - loss: 0.1836 - val_accuracy: 0.9273 - val_loss: 0.1729 - mcc: 0.8548\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9235 - loss: 0.1814\n","Epoch 16 - MCC: 0.8533\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9235 - loss: 0.1814 - val_accuracy: 0.9268 - val_loss: 0.1759 - mcc: 0.8533\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9277 - loss: 0.1740\n","Epoch 17 - MCC: 0.8521\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9276 - loss: 0.1743 - val_accuracy: 0.9263 - val_loss: 0.1738 - mcc: 0.8521\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9260 - loss: 0.1779\n","Epoch 18 - MCC: 0.8599\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9260 - loss: 0.1779 - val_accuracy: 0.9302 - val_loss: 0.1670 - mcc: 0.8599\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9270 - loss: 0.1733\n","Epoch 19 - MCC: 0.8575\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9270 - loss: 0.1733 - val_accuracy: 0.9290 - val_loss: 0.1679 - mcc: 0.8575\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9274 - loss: 0.1739\n","Epoch 20 - MCC: 0.8628\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.9274 - loss: 0.1738 - val_accuracy: 0.9316 - val_loss: 0.1679 - mcc: 0.8628\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9308 - loss: 0.1646\n","Epoch 21 - MCC: 0.8648\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9307 - loss: 0.1648 - val_accuracy: 0.9327 - val_loss: 0.1625 - mcc: 0.8648\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9328 - loss: 0.1624\n","Epoch 22 - MCC: 0.8674\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9326 - loss: 0.1627 - val_accuracy: 0.9340 - val_loss: 0.1583 - mcc: 0.8674\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9285 - loss: 0.1700\n","Epoch 23 - MCC: 0.8629\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9285 - loss: 0.1700 - val_accuracy: 0.9317 - val_loss: 0.1653 - mcc: 0.8629\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9328 - loss: 0.1638\n","Epoch 24 - MCC: 0.8685\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9327 - loss: 0.1641 - val_accuracy: 0.9345 - val_loss: 0.1573 - mcc: 0.8685\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9261 - loss: 0.1774\n","Epoch 25 - MCC: 0.8660\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9262 - loss: 0.1770 - val_accuracy: 0.9332 - val_loss: 0.1620 - mcc: 0.8660\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9299 - loss: 0.1656\n","Epoch 26 - MCC: 0.8706\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.9299 - loss: 0.1657 - val_accuracy: 0.9354 - val_loss: 0.1552 - mcc: 0.8706\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9343 - loss: 0.1566\n","Epoch 27 - MCC: 0.8717\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9341 - loss: 0.1569 - val_accuracy: 0.9361 - val_loss: 0.1542 - mcc: 0.8717\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9382 - loss: 0.1475\n","Epoch 28 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9380 - loss: 0.1481 - val_accuracy: 0.9353 - val_loss: 0.1560 - mcc: 0.8701\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9296 - loss: 0.1673\n","Epoch 29 - MCC: 0.8729\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9297 - loss: 0.1670 - val_accuracy: 0.9366 - val_loss: 0.1525 - mcc: 0.8729\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9321 - loss: 0.1642\n","Epoch 30 - MCC: 0.8683\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9322 - loss: 0.1640 - val_accuracy: 0.9344 - val_loss: 0.1552 - mcc: 0.8683\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6952 - loss: 0.6483\n","Epoch 1 - MCC: 0.6561\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - accuracy: 0.6977 - loss: 0.6460 - val_accuracy: 0.8285 - val_loss: 0.4222 - mcc: 0.6561\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8391 - loss: 0.3902\n","Epoch 2 - MCC: 0.7891\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8400 - loss: 0.3884 - val_accuracy: 0.8940 - val_loss: 0.2640 - mcc: 0.7891\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8945 - loss: 0.2626\n","Epoch 3 - MCC: 0.8244\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.8946 - loss: 0.2621 - val_accuracy: 0.9124 - val_loss: 0.2155 - mcc: 0.8244\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9074 - loss: 0.2229\n","Epoch 4 - MCC: 0.8418\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9075 - loss: 0.2226 - val_accuracy: 0.9210 - val_loss: 0.1938 - mcc: 0.8418\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9119 - loss: 0.2094\n","Epoch 5 - MCC: 0.8477\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9120 - loss: 0.2091 - val_accuracy: 0.9241 - val_loss: 0.1858 - mcc: 0.8477\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9174 - loss: 0.1966\n","Epoch 6 - MCC: 0.8510\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9174 - loss: 0.1965 - val_accuracy: 0.9256 - val_loss: 0.1811 - mcc: 0.8510\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9165 - loss: 0.1986\n","Epoch 7 - MCC: 0.8538\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9166 - loss: 0.1984 - val_accuracy: 0.9271 - val_loss: 0.1799 - mcc: 0.8538\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9192 - loss: 0.1912\n","Epoch 8 - MCC: 0.8502\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9192 - loss: 0.1910 - val_accuracy: 0.9253 - val_loss: 0.1807 - mcc: 0.8502\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9180 - loss: 0.1939\n","Epoch 9 - MCC: 0.8572\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9181 - loss: 0.1936 - val_accuracy: 0.9286 - val_loss: 0.1745 - mcc: 0.8572\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9214 - loss: 0.1858\n","Epoch 10 - MCC: 0.8498\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9214 - loss: 0.1858 - val_accuracy: 0.9250 - val_loss: 0.1828 - mcc: 0.8498\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9191 - loss: 0.1917\n","Epoch 11 - MCC: 0.8604\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9191 - loss: 0.1915 - val_accuracy: 0.9303 - val_loss: 0.1708 - mcc: 0.8604\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9241 - loss: 0.1795\n","Epoch 12 - MCC: 0.8606\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.9241 - loss: 0.1794 - val_accuracy: 0.9304 - val_loss: 0.1721 - mcc: 0.8606\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9243 - loss: 0.1804\n","Epoch 13 - MCC: 0.8585\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9243 - loss: 0.1804 - val_accuracy: 0.9291 - val_loss: 0.1697 - mcc: 0.8585\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9250 - loss: 0.1784\n","Epoch 14 - MCC: 0.8612\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9250 - loss: 0.1784 - val_accuracy: 0.9307 - val_loss: 0.1673 - mcc: 0.8612\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9291 - loss: 0.1698\n","Epoch 15 - MCC: 0.8531\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9290 - loss: 0.1700 - val_accuracy: 0.9265 - val_loss: 0.1787 - mcc: 0.8531\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9247 - loss: 0.1787\n","Epoch 16 - MCC: 0.8586\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9247 - loss: 0.1787 - val_accuracy: 0.9290 - val_loss: 0.1737 - mcc: 0.8586\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9251 - loss: 0.1776\n","Epoch 17 - MCC: 0.8659\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9252 - loss: 0.1774 - val_accuracy: 0.9331 - val_loss: 0.1648 - mcc: 0.8659\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9261 - loss: 0.1752\n","Epoch 18 - MCC: 0.8669\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9262 - loss: 0.1750 - val_accuracy: 0.9335 - val_loss: 0.1606 - mcc: 0.8669\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9233 - loss: 0.1788\n","Epoch 19 - MCC: 0.8611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9235 - loss: 0.1784 - val_accuracy: 0.9305 - val_loss: 0.1684 - mcc: 0.8611\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9276 - loss: 0.1704\n","Epoch 20 - MCC: 0.8663\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9276 - loss: 0.1704 - val_accuracy: 0.9333 - val_loss: 0.1643 - mcc: 0.8663\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9301 - loss: 0.1646\n","Epoch 21 - MCC: 0.8721\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9301 - loss: 0.1647 - val_accuracy: 0.9362 - val_loss: 0.1578 - mcc: 0.8721\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9296 - loss: 0.1691\n","Epoch 22 - MCC: 0.8691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9296 - loss: 0.1691 - val_accuracy: 0.9345 - val_loss: 0.1601 - mcc: 0.8691\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9285 - loss: 0.1703\n","Epoch 23 - MCC: 0.8745\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9286 - loss: 0.1700 - val_accuracy: 0.9374 - val_loss: 0.1536 - mcc: 0.8745\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9349 - loss: 0.1550\n","Epoch 24 - MCC: 0.8755\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9348 - loss: 0.1552 - val_accuracy: 0.9379 - val_loss: 0.1528 - mcc: 0.8755\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9366 - loss: 0.1521\n","Epoch 25 - MCC: 0.8761\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9364 - loss: 0.1524 - val_accuracy: 0.9381 - val_loss: 0.1520 - mcc: 0.8761\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9345 - loss: 0.1556\n","Epoch 26 - MCC: 0.8760\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9345 - loss: 0.1557 - val_accuracy: 0.9381 - val_loss: 0.1508 - mcc: 0.8760\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9308 - loss: 0.1650\n","Epoch 27 - MCC: 0.8764\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9309 - loss: 0.1647 - val_accuracy: 0.9382 - val_loss: 0.1525 - mcc: 0.8764\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9291 - loss: 0.1669\n","Epoch 28 - MCC: 0.8769\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9292 - loss: 0.1666 - val_accuracy: 0.9385 - val_loss: 0.1509 - mcc: 0.8769\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9380 - loss: 0.1493\n","Epoch 29 - MCC: 0.8739\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9379 - loss: 0.1496 - val_accuracy: 0.9370 - val_loss: 0.1531 - mcc: 0.8739\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9383 - loss: 0.1477\n","Epoch 30 - MCC: 0.8789\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9381 - loss: 0.1481 - val_accuracy: 0.9396 - val_loss: 0.1480 - mcc: 0.8789\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6037 - loss: 0.6400\n","Epoch 1 - MCC: 0.6323\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - accuracy: 0.6069 - loss: 0.6376 - val_accuracy: 0.8160 - val_loss: 0.4201 - mcc: 0.6323\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8519 - loss: 0.3639\n","Epoch 2 - MCC: 0.7792\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8525 - loss: 0.3625 - val_accuracy: 0.8895 - val_loss: 0.2697 - mcc: 0.7792\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9028 - loss: 0.2422\n","Epoch 3 - MCC: 0.7737\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9027 - loss: 0.2421 - val_accuracy: 0.8826 - val_loss: 0.2674 - mcc: 0.7737\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9008 - loss: 0.2359\n","Epoch 4 - MCC: 0.8171\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9011 - loss: 0.2355 - val_accuracy: 0.9087 - val_loss: 0.2168 - mcc: 0.8171\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9134 - loss: 0.2085\n","Epoch 5 - MCC: 0.8224\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9135 - loss: 0.2083 - val_accuracy: 0.9113 - val_loss: 0.2087 - mcc: 0.8224\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9165 - loss: 0.1999\n","Epoch 6 - MCC: 0.8291\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - accuracy: 0.9166 - loss: 0.1997 - val_accuracy: 0.9147 - val_loss: 0.2012 - mcc: 0.8291\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9178 - loss: 0.1973\n","Epoch 7 - MCC: 0.8334\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.9179 - loss: 0.1970 - val_accuracy: 0.9168 - val_loss: 0.1970 - mcc: 0.8334\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9253 - loss: 0.1796\n","Epoch 8 - MCC: 0.8362\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.9252 - loss: 0.1798 - val_accuracy: 0.9182 - val_loss: 0.1927 - mcc: 0.8362\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9306 - loss: 0.1686\n","Epoch 9 - MCC: 0.8365\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9304 - loss: 0.1692 - val_accuracy: 0.9184 - val_loss: 0.1921 - mcc: 0.8365\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9224 - loss: 0.1842\n","Epoch 10 - MCC: 0.8335\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.9224 - loss: 0.1841 - val_accuracy: 0.9166 - val_loss: 0.1965 - mcc: 0.8335\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9195 - loss: 0.1897\n","Epoch 11 - MCC: 0.8406\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 189ms/step - accuracy: 0.9196 - loss: 0.1894 - val_accuracy: 0.9201 - val_loss: 0.1882 - mcc: 0.8406\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9272 - loss: 0.1735\n","Epoch 12 - MCC: 0.8446\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9272 - loss: 0.1735 - val_accuracy: 0.9224 - val_loss: 0.1840 - mcc: 0.8446\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9255 - loss: 0.1775\n","Epoch 13 - MCC: 0.8480\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 183ms/step - accuracy: 0.9256 - loss: 0.1773 - val_accuracy: 0.9241 - val_loss: 0.1807 - mcc: 0.8480\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9285 - loss: 0.1703\n","Epoch 14 - MCC: 0.8412\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9284 - loss: 0.1705 - val_accuracy: 0.9204 - val_loss: 0.1870 - mcc: 0.8412\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9319 - loss: 0.1640\n","Epoch 15 - MCC: 0.8472\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9318 - loss: 0.1642 - val_accuracy: 0.9236 - val_loss: 0.1828 - mcc: 0.8472\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9294 - loss: 0.1695\n","Epoch 16 - MCC: 0.8479\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.9294 - loss: 0.1695 - val_accuracy: 0.9238 - val_loss: 0.1818 - mcc: 0.8479\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9292 - loss: 0.1719\n","Epoch 17 - MCC: 0.8552\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.9293 - loss: 0.1716 - val_accuracy: 0.9278 - val_loss: 0.1726 - mcc: 0.8552\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9324 - loss: 0.1619\n","Epoch 18 - MCC: 0.8572\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 126ms/step - accuracy: 0.9325 - loss: 0.1619 - val_accuracy: 0.9283 - val_loss: 0.1731 - mcc: 0.8572\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9349 - loss: 0.1565\n","Epoch 19 - MCC: 0.8552\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 0.9348 - loss: 0.1567 - val_accuracy: 0.9276 - val_loss: 0.1720 - mcc: 0.8552\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9367 - loss: 0.1543\n","Epoch 20 - MCC: 0.8598\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - accuracy: 0.9366 - loss: 0.1544 - val_accuracy: 0.9300 - val_loss: 0.1669 - mcc: 0.8598\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9356 - loss: 0.1542\n","Epoch 21 - MCC: 0.8599\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9356 - loss: 0.1542 - val_accuracy: 0.9300 - val_loss: 0.1681 - mcc: 0.8599\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9344 - loss: 0.1589\n","Epoch 22 - MCC: 0.8598\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9344 - loss: 0.1589 - val_accuracy: 0.9295 - val_loss: 0.1700 - mcc: 0.8598\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9381 - loss: 0.1497\n","Epoch 23 - MCC: 0.8433\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9380 - loss: 0.1501 - val_accuracy: 0.9207 - val_loss: 0.1855 - mcc: 0.8433\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9337 - loss: 0.1586\n","Epoch 24 - MCC: 0.8613\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.9337 - loss: 0.1586 - val_accuracy: 0.9307 - val_loss: 0.1662 - mcc: 0.8613\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9320 - loss: 0.1635\n","Epoch 25 - MCC: 0.8668\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9322 - loss: 0.1631 - val_accuracy: 0.9334 - val_loss: 0.1616 - mcc: 0.8668\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9378 - loss: 0.1516\n","Epoch 26 - MCC: 0.8673\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9379 - loss: 0.1514 - val_accuracy: 0.9337 - val_loss: 0.1593 - mcc: 0.8673\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9379 - loss: 0.1483\n","Epoch 27 - MCC: 0.8674\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9379 - loss: 0.1483 - val_accuracy: 0.9338 - val_loss: 0.1598 - mcc: 0.8674\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9403 - loss: 0.1441\n","Epoch 28 - MCC: 0.8640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9403 - loss: 0.1443 - val_accuracy: 0.9321 - val_loss: 0.1635 - mcc: 0.8640\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9372 - loss: 0.1503\n","Epoch 29 - MCC: 0.8638\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9372 - loss: 0.1503 - val_accuracy: 0.9320 - val_loss: 0.1613 - mcc: 0.8638\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9392 - loss: 0.1461\n","Epoch 30 - MCC: 0.8658\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9392 - loss: 0.1461 - val_accuracy: 0.9330 - val_loss: 0.1606 - mcc: 0.8658\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9427266666666667,\n","              'mean': 0.9353493333333335,\n","              'min': 0.9270266666666667,\n","              'std': 0.00543736308639891},\n"," 'Inference Time (s/sample)': {'max': 0.0036596393585205077,\n","                               'mean': 0.0030783491134643553,\n","                               'min': 0.0019434881210327148,\n","                               'std': 0.000613192154407897},\n"," 'MCC': {'max': 0.885123847003017,\n","         'mean': 0.8703700348868795,\n","         'min': 0.8537678736212269,\n","         'std': 0.010869492914844242},\n"," 'Parameters': 31393,\n"," 'Train Time (s)': {'max': 103.11069893836975,\n","                    'mean': 87.76944661140442,\n","                    'min': 79.38773894309998,\n","                    'std': 8.321322768042759},\n"," 'Training Accuracy': [[0.7047233581542969,\n","                        0.867431640625,\n","                        0.8987900018692017,\n","                        0.9091333746910095,\n","                        0.9127750992774963,\n","                        0.9136734008789062,\n","                        0.9149900078773499,\n","                        0.9190066456794739,\n","                        0.9182850122451782,\n","                        0.9189499616622925,\n","                        0.9206016063690186,\n","                        0.9173816442489624,\n","                        0.9224650859832764,\n","                        0.9239399433135986,\n","                        0.9245166778564453,\n","                        0.9253948926925659,\n","                        0.9258415699005127,\n","                        0.9268766641616821,\n","                        0.9290081858634949,\n","                        0.9299783110618591,\n","                        0.928860068321228,\n","                        0.9293134212493896,\n","                        0.9319416880607605,\n","                        0.9327600002288818,\n","                        0.9316367506980896,\n","                        0.9339967370033264,\n","                        0.9336983561515808,\n","                        0.9343550801277161,\n","                        0.9332833886146545,\n","                        0.9351884722709656],\n","                       [0.7259483337402344,\n","                        0.8767315149307251,\n","                        0.9025698304176331,\n","                        0.9101065993309021,\n","                        0.9148616790771484,\n","                        0.9189034104347229,\n","                        0.918541669845581,\n","                        0.9228883981704712,\n","                        0.9219300150871277,\n","                        0.9262731671333313,\n","                        0.9271950721740723,\n","                        0.9249951243400574,\n","                        0.9273698329925537,\n","                        0.9293899536132812,\n","                        0.9270350933074951,\n","                        0.93086838722229,\n","                        0.9309017062187195,\n","                        0.9304383993148804,\n","                        0.9322634339332581,\n","                        0.9321783185005188,\n","                        0.9335583448410034,\n","                        0.9355749487876892,\n","                        0.9367433786392212,\n","                        0.9370449781417847,\n","                        0.9376617670059204,\n","                        0.9374200701713562,\n","                        0.9356899261474609,\n","                        0.9394749402999878,\n","                        0.9377300143241882,\n","                        0.9381349682807922],\n","                       [0.741641640663147,\n","                        0.8536667823791504,\n","                        0.8888200521469116,\n","                        0.9039750695228577,\n","                        0.9110917448997498,\n","                        0.9105932712554932,\n","                        0.9158366918563843,\n","                        0.917055070400238,\n","                        0.9174983501434326,\n","                        0.9184199571609497,\n","                        0.919029951095581,\n","                        0.9213700294494629,\n","                        0.9230817556381226,\n","                        0.9220216870307922,\n","                        0.9246633052825928,\n","                        0.9241166710853577,\n","                        0.9252133369445801,\n","                        0.92624831199646,\n","                        0.9265065789222717,\n","                        0.9275266528129578,\n","                        0.9283766746520996,\n","                        0.9290766716003418,\n","                        0.9281332492828369,\n","                        0.9290833473205566,\n","                        0.930014967918396,\n","                        0.9295833706855774,\n","                        0.930959939956665,\n","                        0.9323816895484924,\n","                        0.9322567582130432,\n","                        0.9332934021949768],\n","                       [0.7595617771148682,\n","                        0.8614667654037476,\n","                        0.8982317447662354,\n","                        0.910871684551239,\n","                        0.9148717522621155,\n","                        0.917773425579071,\n","                        0.9192981719970703,\n","                        0.9211233258247375,\n","                        0.9221633076667786,\n","                        0.9222599267959595,\n","                        0.9211750626564026,\n","                        0.9245733022689819,\n","                        0.9239033460617065,\n","                        0.9252501130104065,\n","                        0.9258066415786743,\n","                        0.9237250685691833,\n","                        0.9274218082427979,\n","                        0.9284433126449585,\n","                        0.9284666180610657,\n","                        0.9277333617210388,\n","                        0.9289765954017639,\n","                        0.9299300909042358,\n","                        0.9313350915908813,\n","                        0.9327934384346008,\n","                        0.9327467083930969,\n","                        0.9334999918937683,\n","                        0.9336332082748413,\n","                        0.9322183132171631,\n","                        0.9340050220489502,\n","                        0.9337000846862793],\n","                       [0.6878100037574768,\n","                        0.8675450086593628,\n","                        0.9023200273513794,\n","                        0.9070833325386047,\n","                        0.9165633320808411,\n","                        0.9191181659698486,\n","                        0.9210749864578247,\n","                        0.9224017262458801,\n","                        0.9231583476066589,\n","                        0.9233483076095581,\n","                        0.9232099652290344,\n","                        0.9269582629203796,\n","                        0.9279818534851074,\n","                        0.9263449907302856,\n","                        0.9290117025375366,\n","                        0.9291766285896301,\n","                        0.9324665665626526,\n","                        0.9331349730491638,\n","                        0.9327983856201172,\n","                        0.9355400800704956,\n","                        0.935678243637085,\n","                        0.9341434240341187,\n","                        0.9343750476837158,\n","                        0.9333983659744263,\n","                        0.9366133809089661,\n","                        0.9383717179298401,\n","                        0.9380767941474915,\n","                        0.9381865859031677,\n","                        0.9370999932289124,\n","                        0.9386432766914368]],\n"," 'Training Loss': [[0.5790603160858154,\n","                    0.32275477051734924,\n","                    0.243995800614357,\n","                    0.21573981642723083,\n","                    0.20737840235233307,\n","                    0.2037772685289383,\n","                    0.20293942093849182,\n","                    0.19293220341205597,\n","                    0.1935221254825592,\n","                    0.19152241945266724,\n","                    0.18738418817520142,\n","                    0.19565659761428833,\n","                    0.18454642593860626,\n","                    0.18142028152942657,\n","                    0.17902646958827972,\n","                    0.17782463133335114,\n","                    0.1771559864282608,\n","                    0.17488589882850647,\n","                    0.17020221054553986,\n","                    0.16845375299453735,\n","                    0.16991974413394928,\n","                    0.16926562786102295,\n","                    0.163950115442276,\n","                    0.16192571818828583,\n","                    0.16318552196025848,\n","                    0.15926629304885864,\n","                    0.16003872454166412,\n","                    0.15840616822242737,\n","                    0.15992926061153412,\n","                    0.15545260906219482],\n","                   [0.5437817573547363,\n","                    0.30444782972335815,\n","                    0.23807020485401154,\n","                    0.21634899079799652,\n","                    0.20423339307308197,\n","                    0.19471943378448486,\n","                    0.19477646052837372,\n","                    0.18559518456459045,\n","                    0.18647165596485138,\n","                    0.17654173076152802,\n","                    0.17409111559391022,\n","                    0.17917320132255554,\n","                    0.17408335208892822,\n","                    0.16848915815353394,\n","                    0.1732904016971588,\n","                    0.16574299335479736,\n","                    0.16549500823020935,\n","                    0.16576796770095825,\n","                    0.1625603884458542,\n","                    0.16348567605018616,\n","                    0.15967263281345367,\n","                    0.1561109870672226,\n","                    0.153572678565979,\n","                    0.15296265482902527,\n","                    0.15017548203468323,\n","                    0.1516263782978058,\n","                    0.153630793094635,\n","                    0.14734473824501038,\n","                    0.15029025077819824,\n","                    0.1500379890203476],\n","                   [0.548703134059906,\n","                    0.35900112986564636,\n","                    0.2749978303909302,\n","                    0.23486314713954926,\n","                    0.2156267762184143,\n","                    0.21452903747558594,\n","                    0.20143555104732513,\n","                    0.1980222761631012,\n","                    0.19643676280975342,\n","                    0.1933421492576599,\n","                    0.1926814615726471,\n","                    0.18779781460762024,\n","                    0.18361349403858185,\n","                    0.18569448590278625,\n","                    0.18069881200790405,\n","                    0.18065349757671356,\n","                    0.17976529896259308,\n","                    0.17706096172332764,\n","                    0.1745835244655609,\n","                    0.17293566465377808,\n","                    0.17072241008281708,\n","                    0.16911132633686066,\n","                    0.17095404863357544,\n","                    0.169246107339859,\n","                    0.16695545613765717,\n","                    0.1667148470878601,\n","                    0.16345219314098358,\n","                    0.16208428144454956,\n","                    0.16121041774749756,\n","                    0.15932905673980713],\n","                   [0.5867580771446228,\n","                    0.34239205718040466,\n","                    0.24944858253002167,\n","                    0.21471795439720154,\n","                    0.20227950811386108,\n","                    0.19521582126617432,\n","                    0.19299064576625824,\n","                    0.18695734441280365,\n","                    0.18464550375938416,\n","                    0.18423865735530853,\n","                    0.18642383813858032,\n","                    0.17864976823329926,\n","                    0.1809815764427185,\n","                    0.17665034532546997,\n","                    0.17615218460559845,\n","                    0.18046024441719055,\n","                    0.17318134009838104,\n","                    0.17042112350463867,\n","                    0.17016157507896423,\n","                    0.17092855274677277,\n","                    0.16734115779399872,\n","                    0.167567640542984,\n","                    0.16400405764579773,\n","                    0.1605866551399231,\n","                    0.16088180243968964,\n","                    0.1590484082698822,\n","                    0.1578785926103592,\n","                    0.16141103208065033,\n","                    0.1574745923280716,\n","                    0.15844014286994934],\n","                   [0.5752457976341248,\n","                    0.3254028260707855,\n","                    0.23971861600875854,\n","                    0.2243318408727646,\n","                    0.20096804201602936,\n","                    0.1945742815732956,\n","                    0.18945041298866272,\n","                    0.18527241051197052,\n","                    0.18398168683052063,\n","                    0.18174996972084045,\n","                    0.18187068402767181,\n","                    0.17331553995609283,\n","                    0.17177341878414154,\n","                    0.17568491399288177,\n","                    0.16960786283016205,\n","                    0.1698262095451355,\n","                    0.16274097561836243,\n","                    0.1608988642692566,\n","                    0.16051019728183746,\n","                    0.15659800171852112,\n","                    0.15424777567386627,\n","                    0.15857306122779846,\n","                    0.15891942381858826,\n","                    0.15871979296207428,\n","                    0.15264268219470978,\n","                    0.14857080578804016,\n","                    0.1481628566980362,\n","                    0.14947324991226196,\n","                    0.1502324491739273,\n","                    0.14722400903701782]],\n"," 'Validation Accuracy': [[0.829426646232605,\n","                          0.8985199928283691,\n","                          0.9137933254241943,\n","                          0.9184333086013794,\n","                          0.9208133816719055,\n","                          0.9225332736968994,\n","                          0.9188534021377563,\n","                          0.9235534071922302,\n","                          0.9245532751083374,\n","                          0.9276201128959656,\n","                          0.9226866960525513,\n","                          0.9249599575996399,\n","                          0.9269400238990784,\n","                          0.9261200428009033,\n","                          0.9330733418464661,\n","                          0.9297999739646912,\n","                          0.931006669998169,\n","                          0.9307733178138733,\n","                          0.9343066215515137,\n","                          0.9386866092681885,\n","                          0.9354866147041321,\n","                          0.9374266266822815,\n","                          0.9371799230575562,\n","                          0.9386066198348999,\n","                          0.9383866190910339,\n","                          0.9394933581352234,\n","                          0.9418734908103943,\n","                          0.9312866926193237,\n","                          0.9416800141334534,\n","                          0.9427266716957092],\n","                         [0.8031932711601257,\n","                          0.886573314666748,\n","                          0.9003199934959412,\n","                          0.9053400158882141,\n","                          0.9076733589172363,\n","                          0.9059933423995972,\n","                          0.9029399752616882,\n","                          0.9109731912612915,\n","                          0.9096933007240295,\n","                          0.9167600870132446,\n","                          0.9170133471488953,\n","                          0.9131466746330261,\n","                          0.9195266962051392,\n","                          0.9136333465576172,\n","                          0.9190799593925476,\n","                          0.9158200621604919,\n","                          0.9210066795349121,\n","                          0.9222466945648193,\n","                          0.9143133759498596,\n","                          0.9214666485786438,\n","                          0.9234599471092224,\n","                          0.9221866726875305,\n","                          0.9230067729949951,\n","                          0.9266600012779236,\n","                          0.9270200133323669,\n","                          0.9201133251190186,\n","                          0.9245733618736267,\n","                          0.9262332916259766,\n","                          0.9268665909767151,\n","                          0.9270266890525818],\n","                         [0.8383600115776062,\n","                          0.8769466280937195,\n","                          0.9005866050720215,\n","                          0.9130600690841675,\n","                          0.9179867506027222,\n","                          0.9105000495910645,\n","                          0.9225866794586182,\n","                          0.9209733605384827,\n","                          0.9243799448013306,\n","                          0.9232000112533569,\n","                          0.9247133731842041,\n","                          0.9244934320449829,\n","                          0.9235933423042297,\n","                          0.925926685333252,\n","                          0.9272733330726624,\n","                          0.926806628704071,\n","                          0.9263401031494141,\n","                          0.9302467107772827,\n","                          0.9290000200271606,\n","                          0.9316200017929077,\n","                          0.9326533675193787,\n","                          0.9339599609375,\n","                          0.9317466616630554,\n","                          0.9345199465751648,\n","                          0.9332133531570435,\n","                          0.935366690158844,\n","                          0.9361200928688049,\n","                          0.9353333115577698,\n","                          0.9366000294685364,\n","                          0.934386670589447],\n","                         [0.8285133242607117,\n","                          0.8940266966819763,\n","                          0.912433385848999,\n","                          0.921019971370697,\n","                          0.9240532517433167,\n","                          0.9255599975585938,\n","                          0.9270600080490112,\n","                          0.9252666234970093,\n","                          0.9285600781440735,\n","                          0.925000011920929,\n","                          0.9303133487701416,\n","                          0.9304400682449341,\n","                          0.9290599822998047,\n","                          0.9306932687759399,\n","                          0.9264600276947021,\n","                          0.9290199875831604,\n","                          0.9331066608428955,\n","                          0.9334999918937683,\n","                          0.9305465817451477,\n","                          0.9332866668701172,\n","                          0.9361533522605896,\n","                          0.9345332384109497,\n","                          0.9373667240142822,\n","                          0.9378666877746582,\n","                          0.9381067156791687,\n","                          0.9381199479103088,\n","                          0.9381733536720276,\n","                          0.9384733438491821,\n","                          0.9369733333587646,\n","                          0.9395732879638672],\n","                         [0.8159732818603516,\n","                          0.889533281326294,\n","                          0.8825867176055908,\n","                          0.9087200164794922,\n","                          0.9112600088119507,\n","                          0.9147067070007324,\n","                          0.916806697845459,\n","                          0.918233335018158,\n","                          0.9183533191680908,\n","                          0.9165599942207336,\n","                          0.9201065897941589,\n","                          0.9224199652671814,\n","                          0.9240866303443909,\n","                          0.9204066395759583,\n","                          0.9235665798187256,\n","                          0.9238466620445251,\n","                          0.9277532696723938,\n","                          0.9283400177955627,\n","                          0.927619993686676,\n","                          0.9299866557121277,\n","                          0.9299600124359131,\n","                          0.9294732809066772,\n","                          0.9206733703613281,\n","                          0.9307332634925842,\n","                          0.9333667159080505,\n","                          0.9337266683578491,\n","                          0.9337800741195679,\n","                          0.9321267008781433,\n","                          0.9320066571235657,\n","                          0.9330333471298218]],\n"," 'Validation Loss': [[0.40896672010421753,\n","                      0.2496379017829895,\n","                      0.20761841535568237,\n","                      0.19436074793338776,\n","                      0.18849271535873413,\n","                      0.18456637859344482,\n","                      0.19144760072231293,\n","                      0.17954105138778687,\n","                      0.18070818483829498,\n","                      0.1724640130996704,\n","                      0.1817592829465866,\n","                      0.17807132005691528,\n","                      0.17081274092197418,\n","                      0.17245571315288544,\n","                      0.16034357249736786,\n","                      0.16423696279525757,\n","                      0.16348962485790253,\n","                      0.16120237112045288,\n","                      0.15515118837356567,\n","                      0.14823997020721436,\n","                      0.15600107610225677,\n","                      0.1510399878025055,\n","                      0.14988689124584198,\n","                      0.14657533168792725,\n","                      0.14681725203990936,\n","                      0.14599519968032837,\n","                      0.13977164030075073,\n","                      0.16297844052314758,\n","                      0.14134074747562408,\n","                      0.13858085870742798],\n","                     [0.4184665381908417,\n","                      0.27775898575782776,\n","                      0.23935672640800476,\n","                      0.22758783400058746,\n","                      0.21993643045425415,\n","                      0.22166387736797333,\n","                      0.22613190114498138,\n","                      0.2112956941127777,\n","                      0.2081088125705719,\n","                      0.1963271200656891,\n","                      0.19661352038383484,\n","                      0.2034108191728592,\n","                      0.1939508467912674,\n","                      0.20446868240833282,\n","                      0.19185292720794678,\n","                      0.19673101603984833,\n","                      0.187400683760643,\n","                      0.18529319763183594,\n","                      0.19986997544765472,\n","                      0.18589147925376892,\n","                      0.18282903730869293,\n","                      0.18394462764263153,\n","                      0.18104428052902222,\n","                      0.17760097980499268,\n","                      0.17573805153369904,\n","                      0.18571443855762482,\n","                      0.18267427384853363,\n","                      0.17692193388938904,\n","                      0.17666961252689362,\n","                      0.17506767809391022],\n","                     [0.388526052236557,\n","                      0.3052700161933899,\n","                      0.2421102076768875,\n","                      0.21294280886650085,\n","                      0.19867858290672302,\n","                      0.20879937708377838,\n","                      0.18776476383209229,\n","                      0.18746276199817657,\n","                      0.18136022984981537,\n","                      0.18453359603881836,\n","                      0.17989514768123627,\n","                      0.18324413895606995,\n","                      0.18028520047664642,\n","                      0.1791563183069229,\n","                      0.17288854718208313,\n","                      0.17587552964687347,\n","                      0.17376558482646942,\n","                      0.16696196794509888,\n","                      0.167851984500885,\n","                      0.1678965538740158,\n","                      0.16252465546131134,\n","                      0.1582634598016739,\n","                      0.16532637178897858,\n","                      0.1572742909193039,\n","                      0.16200625896453857,\n","                      0.15516377985477448,\n","                      0.15423822402954102,\n","                      0.1560257524251938,\n","                      0.1525014042854309,\n","                      0.15521997213363647],\n","                     [0.4222020208835602,\n","                      0.26401984691619873,\n","                      0.2155158668756485,\n","                      0.19375115633010864,\n","                      0.18583902716636658,\n","                      0.18110555410385132,\n","                      0.17986871302127838,\n","                      0.18073196709156036,\n","                      0.17451655864715576,\n","                      0.18284669518470764,\n","                      0.17082691192626953,\n","                      0.172096386551857,\n","                      0.16971687972545624,\n","                      0.16732630133628845,\n","                      0.17872686684131622,\n","                      0.17368368804454803,\n","                      0.16484428942203522,\n","                      0.16060477495193481,\n","                      0.1683884561061859,\n","                      0.1643255650997162,\n","                      0.15775613486766815,\n","                      0.16005051136016846,\n","                      0.15362264215946198,\n","                      0.15282076597213745,\n","                      0.15200333297252655,\n","                      0.1508137732744217,\n","                      0.15246786177158356,\n","                      0.15085552632808685,\n","                      0.15308400988578796,\n","                      0.14801985025405884],\n","                     [0.42006781697273254,\n","                      0.26969006657600403,\n","                      0.26739582419395447,\n","                      0.21678772568702698,\n","                      0.20866398513317108,\n","                      0.20118720829486847,\n","                      0.19696666300296783,\n","                      0.19274859130382538,\n","                      0.1921369731426239,\n","                      0.19645702838897705,\n","                      0.18815945088863373,\n","                      0.18402732908725739,\n","                      0.1807328760623932,\n","                      0.18697375059127808,\n","                      0.18283312022686005,\n","                      0.18182025849819183,\n","                      0.17255014181137085,\n","                      0.17306959629058838,\n","                      0.1720212996006012,\n","                      0.1668584793806076,\n","                      0.16814321279525757,\n","                      0.1700206995010376,\n","                      0.1854720264673233,\n","                      0.16621841490268707,\n","                      0.16157737374305725,\n","                      0.1593388319015503,\n","                      0.1598006784915924,\n","                      0.16349071264266968,\n","                      0.16132177412509918,\n","                      0.16062766313552856]],\n"," 'Validation MCC': [[0.6575922712981602,\n","                     0.7963732639748027,\n","                     0.8269435488107718,\n","                     0.8369464735534161,\n","                     0.8418336145532548,\n","                     0.8444688380563494,\n","                     0.8390552946462513,\n","                     0.847124621942251,\n","                     0.8486575904771502,\n","                     0.8548647310169746,\n","                     0.8466498197146934,\n","                     0.8495720643633705,\n","                     0.8540076259873113,\n","                     0.8517571606555608,\n","                     0.8659104578778476,\n","                     0.8597263674693035,\n","                     0.8621556790800877,\n","                     0.8614922576780384,\n","                     0.8690715805475181,\n","                     0.8768972868277147,\n","                     0.8705054835094997,\n","                     0.8747206477769482,\n","                     0.8738944478204357,\n","                     0.8775592465599672,\n","                     0.8764749823322864,\n","                     0.8785412331196342,\n","                     0.8833826383402168,\n","                     0.8640254985465023,\n","                     0.8829502339299757,\n","                     0.885123847003017],\n","                    [0.6236760626871287,\n","                     0.7738975821882202,\n","                     0.8002027608908214,\n","                     0.8103237841975793,\n","                     0.8149127437778362,\n","                     0.8127398998747142,\n","                     0.8090898042358525,\n","                     0.8229997554718296,\n","                     0.8201835557881758,\n","                     0.8331255036551883,\n","                     0.8336241549358987,\n","                     0.828024425305079,\n","                     0.8387463003631097,\n","                     0.8298393643740561,\n","                     0.8388050200239148,\n","                     0.8314064010547768,\n","                     0.8416943912586853,\n","                     0.8441922201190337,\n","                     0.8297258461587542,\n","                     0.8427402934907101,\n","                     0.8465913446557158,\n","                     0.8449041274343377,\n","                     0.8458008446878656,\n","                     0.8531606659489089,\n","                     0.8538659165168027,\n","                     0.8400993106853647,\n","                     0.8498334205161863,\n","                     0.8521908739006778,\n","                     0.8542337481016767,\n","                     0.8537678736212269],\n","                    [0.6761396513638458,\n","                     0.7621736560347884,\n","                     0.8003494227403516,\n","                     0.8254302078549833,\n","                     0.8357291891735705,\n","                     0.8220810667477845,\n","                     0.844515900356675,\n","                     0.8413515222209849,\n","                     0.8481233199876632,\n","                     0.8458963137133132,\n","                     0.8488043362027625,\n","                     0.8487371423653615,\n","                     0.847464425533857,\n","                     0.8512811562685797,\n","                     0.8547616872185803,\n","                     0.853270857137629,\n","                     0.8520808941656429,\n","                     0.8599157638381085,\n","                     0.857537652242898,\n","                     0.8627733102179587,\n","                     0.8648417515107486,\n","                     0.8674162701432022,\n","                     0.8629315112845244,\n","                     0.8685054626670484,\n","                     0.8660302573862025,\n","                     0.8706027621367257,\n","                     0.8717381620513249,\n","                     0.8701255360428402,\n","                     0.8728679585361282,\n","                     0.8682576417672064],\n","                    [0.6560874533939556,\n","                     0.7891057124463986,\n","                     0.8244484763406493,\n","                     0.8417596425234788,\n","                     0.8477290701935606,\n","                     0.8510128134510188,\n","                     0.8538399585105007,\n","                     0.8502056624052681,\n","                     0.8572299562026582,\n","                     0.8497519670942447,\n","                     0.860403476301854,\n","                     0.8606056752749334,\n","                     0.8585221921700974,\n","                     0.8611622153638838,\n","                     0.8531203001023723,\n","                     0.8585953540607408,\n","                     0.8659237710162476,\n","                     0.8668690652880525,\n","                     0.8610694076845339,\n","                     0.8662587053472645,\n","                     0.8720717903307001,\n","                     0.8690757361919766,\n","                     0.8744540014986556,\n","                     0.875528985835662,\n","                     0.876123679801067,\n","                     0.8759501016427342,\n","                     0.8764150919609445,\n","                     0.8768576708479101,\n","                     0.8739315612100629,\n","                     0.8788740823671666],\n","                    [0.6323181633554666,\n","                     0.7791556313332849,\n","                     0.7737360524257348,\n","                     0.8171236187535172,\n","                     0.8224410812259347,\n","                     0.8290904847895547,\n","                     0.8333871407493793,\n","                     0.8361726786601225,\n","                     0.8364891817581421,\n","                     0.8334958354795308,\n","                     0.8406154208438863,\n","                     0.8445911837964492,\n","                     0.8479862153386737,\n","                     0.8412022839527556,\n","                     0.8472367381392761,\n","                     0.8479372120410794,\n","                     0.8552428030306982,\n","                     0.8572459258638555,\n","                     0.8552212923047259,\n","                     0.8597916871395068,\n","                     0.8598711828262402,\n","                     0.859789810667174,\n","                     0.8433406905758504,\n","                     0.8613246503864096,\n","                     0.8668247540742327,\n","                     0.8673083681162116,\n","                     0.8674043752425789,\n","                     0.8640201950667693,\n","                     0.8638223062174964,\n","                     0.8658267296757801]]}\n","Training Model: BiLSTM, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5044 - loss: 0.6648\n","Epoch 1 - MCC: 0.5513\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.5061 - loss: 0.6636 - val_accuracy: 0.7356 - val_loss: 0.5608 - mcc: 0.5513\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7949 - loss: 0.5164\n","Epoch 2 - MCC: 0.7323\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.7959 - loss: 0.5140 - val_accuracy: 0.8667 - val_loss: 0.3283 - mcc: 0.7323\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8789 - loss: 0.2885\n","Epoch 3 - MCC: 0.7963\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8792 - loss: 0.2879 - val_accuracy: 0.8967 - val_loss: 0.2469 - mcc: 0.7963\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8981 - loss: 0.2433\n","Epoch 4 - MCC: 0.8261\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.8982 - loss: 0.2430 - val_accuracy: 0.9131 - val_loss: 0.2133 - mcc: 0.8261\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9066 - loss: 0.2224\n","Epoch 5 - MCC: 0.8321\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9067 - loss: 0.2223 - val_accuracy: 0.9163 - val_loss: 0.2014 - mcc: 0.8321\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9117 - loss: 0.2131\n","Epoch 6 - MCC: 0.8379\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9117 - loss: 0.2130 - val_accuracy: 0.9192 - val_loss: 0.1925 - mcc: 0.8379\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9172 - loss: 0.2004\n","Epoch 7 - MCC: 0.8491\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9173 - loss: 0.2003 - val_accuracy: 0.9247 - val_loss: 0.1812 - mcc: 0.8491\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9240 - loss: 0.1847\n","Epoch 8 - MCC: 0.8537\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9239 - loss: 0.1849 - val_accuracy: 0.9271 - val_loss: 0.1723 - mcc: 0.8537\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9259 - loss: 0.1788\n","Epoch 9 - MCC: 0.8588\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9258 - loss: 0.1790 - val_accuracy: 0.9297 - val_loss: 0.1670 - mcc: 0.8588\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9287 - loss: 0.1738\n","Epoch 10 - MCC: 0.8650\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9286 - loss: 0.1740 - val_accuracy: 0.9328 - val_loss: 0.1626 - mcc: 0.8650\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9262 - loss: 0.1774\n","Epoch 11 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9263 - loss: 0.1773 - val_accuracy: 0.9353 - val_loss: 0.1567 - mcc: 0.8701\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9302 - loss: 0.1705\n","Epoch 12 - MCC: 0.8631\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9302 - loss: 0.1706 - val_accuracy: 0.9309 - val_loss: 0.1642 - mcc: 0.8631\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9311 - loss: 0.1692\n","Epoch 13 - MCC: 0.8754\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9311 - loss: 0.1692 - val_accuracy: 0.9379 - val_loss: 0.1521 - mcc: 0.8754\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9317 - loss: 0.1673\n","Epoch 14 - MCC: 0.8782\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9318 - loss: 0.1672 - val_accuracy: 0.9393 - val_loss: 0.1480 - mcc: 0.8782\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9335 - loss: 0.1625\n","Epoch 15 - MCC: 0.8802\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9335 - loss: 0.1624 - val_accuracy: 0.9403 - val_loss: 0.1454 - mcc: 0.8802\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9338 - loss: 0.1610\n","Epoch 16 - MCC: 0.8780\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.9339 - loss: 0.1609 - val_accuracy: 0.9392 - val_loss: 0.1476 - mcc: 0.8780\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9355 - loss: 0.1590\n","Epoch 17 - MCC: 0.8801\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9355 - loss: 0.1589 - val_accuracy: 0.9402 - val_loss: 0.1428 - mcc: 0.8801\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9354 - loss: 0.1590\n","Epoch 18 - MCC: 0.8882\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9355 - loss: 0.1588 - val_accuracy: 0.9443 - val_loss: 0.1360 - mcc: 0.8882\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9371 - loss: 0.1534\n","Epoch 19 - MCC: 0.8892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9372 - loss: 0.1532 - val_accuracy: 0.9448 - val_loss: 0.1346 - mcc: 0.8892\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9398 - loss: 0.1470\n","Epoch 20 - MCC: 0.8907\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9397 - loss: 0.1470 - val_accuracy: 0.9456 - val_loss: 0.1319 - mcc: 0.8907\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9444 - loss: 0.1372\n","Epoch 21 - MCC: 0.8860\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9442 - loss: 0.1375 - val_accuracy: 0.9431 - val_loss: 0.1382 - mcc: 0.8860\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9385 - loss: 0.1519\n","Epoch 22 - MCC: 0.8919\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9386 - loss: 0.1516 - val_accuracy: 0.9461 - val_loss: 0.1308 - mcc: 0.8919\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9467 - loss: 0.1323\n","Epoch 23 - MCC: 0.8956\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9465 - loss: 0.1327 - val_accuracy: 0.9480 - val_loss: 0.1284 - mcc: 0.8956\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9380 - loss: 0.1526\n","Epoch 24 - MCC: 0.8929\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9382 - loss: 0.1522 - val_accuracy: 0.9466 - val_loss: 0.1290 - mcc: 0.8929\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9392 - loss: 0.1466\n","Epoch 25 - MCC: 0.8953\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.9393 - loss: 0.1464 - val_accuracy: 0.9478 - val_loss: 0.1297 - mcc: 0.8953\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9397 - loss: 0.1497\n","Epoch 26 - MCC: 0.8934\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.9398 - loss: 0.1494 - val_accuracy: 0.9469 - val_loss: 0.1265 - mcc: 0.8934\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9395 - loss: 0.1484\n","Epoch 27 - MCC: 0.8919\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9397 - loss: 0.1480 - val_accuracy: 0.9460 - val_loss: 0.1291 - mcc: 0.8919\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9416 - loss: 0.1421\n","Epoch 28 - MCC: 0.8992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9417 - loss: 0.1419 - val_accuracy: 0.9498 - val_loss: 0.1224 - mcc: 0.8992\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9438 - loss: 0.1373\n","Epoch 29 - MCC: 0.8975\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9438 - loss: 0.1373 - val_accuracy: 0.9489 - val_loss: 0.1243 - mcc: 0.8975\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9441 - loss: 0.1374\n","Epoch 30 - MCC: 0.9001\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9441 - loss: 0.1373 - val_accuracy: 0.9502 - val_loss: 0.1217 - mcc: 0.9001\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5790 - loss: 0.6280\n","Epoch 1 - MCC: 0.5803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.5820 - loss: 0.6264 - val_accuracy: 0.7899 - val_loss: 0.4962 - mcc: 0.5803\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8189 - loss: 0.4325\n","Epoch 2 - MCC: 0.7161\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.8197 - loss: 0.4306 - val_accuracy: 0.8538 - val_loss: 0.3396 - mcc: 0.7161\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8833 - loss: 0.2829\n","Epoch 3 - MCC: 0.7838\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.8836 - loss: 0.2822 - val_accuracy: 0.8921 - val_loss: 0.2604 - mcc: 0.7838\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9054 - loss: 0.2309\n","Epoch 4 - MCC: 0.7972\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9054 - loss: 0.2309 - val_accuracy: 0.8988 - val_loss: 0.2439 - mcc: 0.7972\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9116 - loss: 0.2173\n","Epoch 5 - MCC: 0.8011\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9115 - loss: 0.2175 - val_accuracy: 0.9007 - val_loss: 0.2344 - mcc: 0.8011\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9131 - loss: 0.2098\n","Epoch 6 - MCC: 0.8041\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9132 - loss: 0.2097 - val_accuracy: 0.9022 - val_loss: 0.2328 - mcc: 0.8041\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9159 - loss: 0.2004\n","Epoch 7 - MCC: 0.8146\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9160 - loss: 0.2003 - val_accuracy: 0.9075 - val_loss: 0.2188 - mcc: 0.8146\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9211 - loss: 0.1887\n","Epoch 8 - MCC: 0.8140\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9211 - loss: 0.1889 - val_accuracy: 0.9069 - val_loss: 0.2174 - mcc: 0.8140\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9184 - loss: 0.1939\n","Epoch 9 - MCC: 0.8179\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9185 - loss: 0.1937 - val_accuracy: 0.9090 - val_loss: 0.2125 - mcc: 0.8179\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9250 - loss: 0.1785\n","Epoch 10 - MCC: 0.8246\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9249 - loss: 0.1787 - val_accuracy: 0.9125 - val_loss: 0.2047 - mcc: 0.8246\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9263 - loss: 0.1749\n","Epoch 11 - MCC: 0.8300\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9263 - loss: 0.1750 - val_accuracy: 0.9151 - val_loss: 0.2034 - mcc: 0.8300\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9244 - loss: 0.1781\n","Epoch 12 - MCC: 0.8350\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9245 - loss: 0.1779 - val_accuracy: 0.9177 - val_loss: 0.1943 - mcc: 0.8350\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9252 - loss: 0.1781\n","Epoch 13 - MCC: 0.8323\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.9253 - loss: 0.1778 - val_accuracy: 0.9163 - val_loss: 0.1974 - mcc: 0.8323\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9296 - loss: 0.1669\n","Epoch 14 - MCC: 0.8391\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9296 - loss: 0.1669 - val_accuracy: 0.9197 - val_loss: 0.1920 - mcc: 0.8391\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9332 - loss: 0.1609\n","Epoch 15 - MCC: 0.8451\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9331 - loss: 0.1610 - val_accuracy: 0.9227 - val_loss: 0.1856 - mcc: 0.8451\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9335 - loss: 0.1599\n","Epoch 16 - MCC: 0.8479\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9335 - loss: 0.1599 - val_accuracy: 0.9241 - val_loss: 0.1840 - mcc: 0.8479\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9322 - loss: 0.1633\n","Epoch 17 - MCC: 0.8505\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9323 - loss: 0.1630 - val_accuracy: 0.9254 - val_loss: 0.1812 - mcc: 0.8505\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9366 - loss: 0.1522\n","Epoch 18 - MCC: 0.8508\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9366 - loss: 0.1524 - val_accuracy: 0.9255 - val_loss: 0.1800 - mcc: 0.8508\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9344 - loss: 0.1585\n","Epoch 19 - MCC: 0.8544\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9345 - loss: 0.1582 - val_accuracy: 0.9274 - val_loss: 0.1777 - mcc: 0.8544\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9374 - loss: 0.1520\n","Epoch 20 - MCC: 0.8503\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9374 - loss: 0.1518 - val_accuracy: 0.9252 - val_loss: 0.1831 - mcc: 0.8503\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9383 - loss: 0.1509\n","Epoch 21 - MCC: 0.8522\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9382 - loss: 0.1510 - val_accuracy: 0.9260 - val_loss: 0.1771 - mcc: 0.8522\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9380 - loss: 0.1500\n","Epoch 22 - MCC: 0.8577\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step - accuracy: 0.9380 - loss: 0.1500 - val_accuracy: 0.9290 - val_loss: 0.1749 - mcc: 0.8577\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9398 - loss: 0.1473\n","Epoch 23 - MCC: 0.8589\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9398 - loss: 0.1472 - val_accuracy: 0.9293 - val_loss: 0.1745 - mcc: 0.8589\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9414 - loss: 0.1422\n","Epoch 24 - MCC: 0.8605\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9414 - loss: 0.1422 - val_accuracy: 0.9304 - val_loss: 0.1693 - mcc: 0.8605\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9433 - loss: 0.1380\n","Epoch 25 - MCC: 0.8630\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9433 - loss: 0.1381 - val_accuracy: 0.9316 - val_loss: 0.1679 - mcc: 0.8630\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9443 - loss: 0.1366\n","Epoch 26 - MCC: 0.8635\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9443 - loss: 0.1368 - val_accuracy: 0.9319 - val_loss: 0.1688 - mcc: 0.8635\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9445 - loss: 0.1362\n","Epoch 27 - MCC: 0.8674\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9445 - loss: 0.1362 - val_accuracy: 0.9339 - val_loss: 0.1648 - mcc: 0.8674\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9425 - loss: 0.1408\n","Epoch 28 - MCC: 0.8592\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.9426 - loss: 0.1407 - val_accuracy: 0.9295 - val_loss: 0.1741 - mcc: 0.8592\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9443 - loss: 0.1396\n","Epoch 29 - MCC: 0.8666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9443 - loss: 0.1394 - val_accuracy: 0.9333 - val_loss: 0.1655 - mcc: 0.8666\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9431 - loss: 0.1393\n","Epoch 30 - MCC: 0.8635\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9431 - loss: 0.1391 - val_accuracy: 0.9317 - val_loss: 0.1658 - mcc: 0.8635\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5371 - loss: 0.7006\n","Epoch 1 - MCC: 0.4745\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 0.5378 - loss: 0.6991 - val_accuracy: 0.6814 - val_loss: 0.5878 - mcc: 0.4745\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7696 - loss: 0.5365\n","Epoch 2 - MCC: 0.6945\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.7710 - loss: 0.5344 - val_accuracy: 0.8479 - val_loss: 0.3533 - mcc: 0.6945\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8666 - loss: 0.3197\n","Epoch 3 - MCC: 0.8024\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.8671 - loss: 0.3185 - val_accuracy: 0.9013 - val_loss: 0.2405 - mcc: 0.8024\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9024 - loss: 0.2388\n","Epoch 4 - MCC: 0.8203\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.9024 - loss: 0.2387 - val_accuracy: 0.9103 - val_loss: 0.2188 - mcc: 0.8203\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9049 - loss: 0.2290\n","Epoch 5 - MCC: 0.8212\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9049 - loss: 0.2290 - val_accuracy: 0.9105 - val_loss: 0.2177 - mcc: 0.8212\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9060 - loss: 0.2264\n","Epoch 6 - MCC: 0.8301\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9061 - loss: 0.2260 - val_accuracy: 0.9153 - val_loss: 0.2046 - mcc: 0.8301\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9148 - loss: 0.2060\n","Epoch 7 - MCC: 0.8302\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9147 - loss: 0.2060 - val_accuracy: 0.9146 - val_loss: 0.2038 - mcc: 0.8302\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9131 - loss: 0.2088\n","Epoch 8 - MCC: 0.8426\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9132 - loss: 0.2086 - val_accuracy: 0.9216 - val_loss: 0.1882 - mcc: 0.8426\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9198 - loss: 0.1929\n","Epoch 9 - MCC: 0.8479\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9198 - loss: 0.1930 - val_accuracy: 0.9243 - val_loss: 0.1829 - mcc: 0.8479\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9209 - loss: 0.1900\n","Epoch 10 - MCC: 0.8501\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9209 - loss: 0.1899 - val_accuracy: 0.9253 - val_loss: 0.1777 - mcc: 0.8501\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9231 - loss: 0.1852\n","Epoch 11 - MCC: 0.8580\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9231 - loss: 0.1850 - val_accuracy: 0.9293 - val_loss: 0.1707 - mcc: 0.8580\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9286 - loss: 0.1713\n","Epoch 12 - MCC: 0.8592\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9285 - loss: 0.1715 - val_accuracy: 0.9298 - val_loss: 0.1686 - mcc: 0.8592\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9302 - loss: 0.1705\n","Epoch 13 - MCC: 0.8654\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9302 - loss: 0.1704 - val_accuracy: 0.9330 - val_loss: 0.1633 - mcc: 0.8654\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9295 - loss: 0.1709\n","Epoch 14 - MCC: 0.8614\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9295 - loss: 0.1708 - val_accuracy: 0.9308 - val_loss: 0.1685 - mcc: 0.8614\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9351 - loss: 0.1602\n","Epoch 15 - MCC: 0.8730\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9350 - loss: 0.1603 - val_accuracy: 0.9367 - val_loss: 0.1556 - mcc: 0.8730\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9341 - loss: 0.1608\n","Epoch 16 - MCC: 0.8724\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9341 - loss: 0.1609 - val_accuracy: 0.9362 - val_loss: 0.1584 - mcc: 0.8724\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9354 - loss: 0.1580\n","Epoch 17 - MCC: 0.8747\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.9354 - loss: 0.1580 - val_accuracy: 0.9375 - val_loss: 0.1522 - mcc: 0.8747\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9389 - loss: 0.1494\n","Epoch 18 - MCC: 0.8774\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9388 - loss: 0.1496 - val_accuracy: 0.9389 - val_loss: 0.1508 - mcc: 0.8774\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9351 - loss: 0.1586\n","Epoch 19 - MCC: 0.8769\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9352 - loss: 0.1584 - val_accuracy: 0.9386 - val_loss: 0.1488 - mcc: 0.8769\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9367 - loss: 0.1537\n","Epoch 20 - MCC: 0.8762\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9368 - loss: 0.1535 - val_accuracy: 0.9383 - val_loss: 0.1512 - mcc: 0.8762\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9380 - loss: 0.1515\n","Epoch 21 - MCC: 0.8835\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9380 - loss: 0.1514 - val_accuracy: 0.9420 - val_loss: 0.1435 - mcc: 0.8835\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9407 - loss: 0.1450\n","Epoch 22 - MCC: 0.8810\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9407 - loss: 0.1451 - val_accuracy: 0.9406 - val_loss: 0.1466 - mcc: 0.8810\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9384 - loss: 0.1513\n","Epoch 23 - MCC: 0.8791\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9384 - loss: 0.1510 - val_accuracy: 0.9395 - val_loss: 0.1455 - mcc: 0.8791\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9404 - loss: 0.1457\n","Epoch 24 - MCC: 0.8827\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9404 - loss: 0.1458 - val_accuracy: 0.9416 - val_loss: 0.1441 - mcc: 0.8827\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9411 - loss: 0.1441\n","Epoch 25 - MCC: 0.8823\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.9411 - loss: 0.1441 - val_accuracy: 0.9414 - val_loss: 0.1427 - mcc: 0.8823\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9421 - loss: 0.1422\n","Epoch 26 - MCC: 0.8879\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9420 - loss: 0.1423 - val_accuracy: 0.9442 - val_loss: 0.1378 - mcc: 0.8879\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9435 - loss: 0.1388\n","Epoch 27 - MCC: 0.8864\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9434 - loss: 0.1389 - val_accuracy: 0.9433 - val_loss: 0.1382 - mcc: 0.8864\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9447 - loss: 0.1364\n","Epoch 28 - MCC: 0.8886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9446 - loss: 0.1365 - val_accuracy: 0.9445 - val_loss: 0.1378 - mcc: 0.8886\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9423 - loss: 0.1425\n","Epoch 29 - MCC: 0.8832\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9423 - loss: 0.1424 - val_accuracy: 0.9418 - val_loss: 0.1423 - mcc: 0.8832\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9397 - loss: 0.1474\n","Epoch 30 - MCC: 0.8888\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9399 - loss: 0.1470 - val_accuracy: 0.9446 - val_loss: 0.1360 - mcc: 0.8888\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6014 - loss: 0.6364\n","Epoch 1 - MCC: 0.7065\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 138ms/step - accuracy: 0.6047 - loss: 0.6347 - val_accuracy: 0.8536 - val_loss: 0.4611 - mcc: 0.7065\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8441 - loss: 0.4061\n","Epoch 2 - MCC: 0.7556\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.8441 - loss: 0.4048 - val_accuracy: 0.8745 - val_loss: 0.3040 - mcc: 0.7556\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8817 - loss: 0.2862\n","Epoch 3 - MCC: 0.8045\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.8819 - loss: 0.2856 - val_accuracy: 0.9012 - val_loss: 0.2362 - mcc: 0.8045\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9037 - loss: 0.2321\n","Epoch 4 - MCC: 0.8220\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9037 - loss: 0.2322 - val_accuracy: 0.9108 - val_loss: 0.2154 - mcc: 0.8220\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9089 - loss: 0.2167\n","Epoch 5 - MCC: 0.8298\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - accuracy: 0.9088 - loss: 0.2168 - val_accuracy: 0.9146 - val_loss: 0.2047 - mcc: 0.8298\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9103 - loss: 0.2113\n","Epoch 6 - MCC: 0.8423\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9103 - loss: 0.2112 - val_accuracy: 0.9213 - val_loss: 0.1926 - mcc: 0.8423\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9112 - loss: 0.2097\n","Epoch 7 - MCC: 0.8470\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9113 - loss: 0.2094 - val_accuracy: 0.9237 - val_loss: 0.1872 - mcc: 0.8470\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9123 - loss: 0.2041\n","Epoch 8 - MCC: 0.8462\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9125 - loss: 0.2037 - val_accuracy: 0.9232 - val_loss: 0.1866 - mcc: 0.8462\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9196 - loss: 0.1903\n","Epoch 9 - MCC: 0.8496\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9196 - loss: 0.1904 - val_accuracy: 0.9248 - val_loss: 0.1826 - mcc: 0.8496\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9217 - loss: 0.1840\n","Epoch 10 - MCC: 0.8566\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.9216 - loss: 0.1841 - val_accuracy: 0.9285 - val_loss: 0.1779 - mcc: 0.8566\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9195 - loss: 0.1903\n","Epoch 11 - MCC: 0.8577\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9196 - loss: 0.1900 - val_accuracy: 0.9290 - val_loss: 0.1756 - mcc: 0.8577\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9233 - loss: 0.1832\n","Epoch 12 - MCC: 0.8592\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9233 - loss: 0.1831 - val_accuracy: 0.9294 - val_loss: 0.1778 - mcc: 0.8592\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9211 - loss: 0.1873\n","Epoch 13 - MCC: 0.8595\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9212 - loss: 0.1870 - val_accuracy: 0.9295 - val_loss: 0.1703 - mcc: 0.8595\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9228 - loss: 0.1834\n","Epoch 14 - MCC: 0.8619\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9230 - loss: 0.1831 - val_accuracy: 0.9310 - val_loss: 0.1726 - mcc: 0.8619\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9207 - loss: 0.1883\n","Epoch 15 - MCC: 0.8695\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9209 - loss: 0.1878 - val_accuracy: 0.9349 - val_loss: 0.1642 - mcc: 0.8695\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9276 - loss: 0.1731\n","Epoch 16 - MCC: 0.8680\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9277 - loss: 0.1729 - val_accuracy: 0.9341 - val_loss: 0.1615 - mcc: 0.8680\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9315 - loss: 0.1644\n","Epoch 17 - MCC: 0.8725\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9315 - loss: 0.1645 - val_accuracy: 0.9364 - val_loss: 0.1585 - mcc: 0.8725\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9374 - loss: 0.1530\n","Epoch 18 - MCC: 0.8790\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9372 - loss: 0.1533 - val_accuracy: 0.9396 - val_loss: 0.1546 - mcc: 0.8790\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9330 - loss: 0.1617\n","Epoch 19 - MCC: 0.8782\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9330 - loss: 0.1616 - val_accuracy: 0.9392 - val_loss: 0.1521 - mcc: 0.8782\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9326 - loss: 0.1611\n","Epoch 20 - MCC: 0.8811\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9327 - loss: 0.1610 - val_accuracy: 0.9405 - val_loss: 0.1521 - mcc: 0.8811\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9386 - loss: 0.1507\n","Epoch 21 - MCC: 0.8791\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9386 - loss: 0.1508 - val_accuracy: 0.9396 - val_loss: 0.1502 - mcc: 0.8791\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9337 - loss: 0.1613\n","Epoch 22 - MCC: 0.8833\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9338 - loss: 0.1610 - val_accuracy: 0.9418 - val_loss: 0.1466 - mcc: 0.8833\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9404 - loss: 0.1468\n","Epoch 23 - MCC: 0.8814\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9403 - loss: 0.1470 - val_accuracy: 0.9407 - val_loss: 0.1464 - mcc: 0.8814\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9383 - loss: 0.1496\n","Epoch 24 - MCC: 0.8839\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9383 - loss: 0.1496 - val_accuracy: 0.9418 - val_loss: 0.1466 - mcc: 0.8839\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9387 - loss: 0.1478\n","Epoch 25 - MCC: 0.8876\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.9388 - loss: 0.1477 - val_accuracy: 0.9440 - val_loss: 0.1422 - mcc: 0.8876\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9404 - loss: 0.1461\n","Epoch 26 - MCC: 0.8875\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.9404 - loss: 0.1461 - val_accuracy: 0.9438 - val_loss: 0.1405 - mcc: 0.8875\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9376 - loss: 0.1512\n","Epoch 27 - MCC: 0.8844\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9377 - loss: 0.1511 - val_accuracy: 0.9422 - val_loss: 0.1441 - mcc: 0.8844\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9389 - loss: 0.1484\n","Epoch 28 - MCC: 0.8884\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9390 - loss: 0.1483 - val_accuracy: 0.9443 - val_loss: 0.1396 - mcc: 0.8884\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9420 - loss: 0.1414\n","Epoch 29 - MCC: 0.8906\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9420 - loss: 0.1415 - val_accuracy: 0.9454 - val_loss: 0.1364 - mcc: 0.8906\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9374 - loss: 0.1527\n","Epoch 30 - MCC: 0.8860\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9376 - loss: 0.1523 - val_accuracy: 0.9431 - val_loss: 0.1396 - mcc: 0.8860\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5093 - loss: 0.6521\n","Epoch 1 - MCC: 0.6628\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.5114 - loss: 0.6510 - val_accuracy: 0.8185 - val_loss: 0.5410 - mcc: 0.6628\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8372 - loss: 0.4727\n","Epoch 2 - MCC: 0.7176\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.8372 - loss: 0.4707 - val_accuracy: 0.8584 - val_loss: 0.3225 - mcc: 0.7176\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8729 - loss: 0.2971\n","Epoch 3 - MCC: 0.7882\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.8735 - loss: 0.2959 - val_accuracy: 0.8939 - val_loss: 0.2533 - mcc: 0.7882\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9079 - loss: 0.2211\n","Epoch 4 - MCC: 0.8122\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.9079 - loss: 0.2212 - val_accuracy: 0.9061 - val_loss: 0.2239 - mcc: 0.8122\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9069 - loss: 0.2214\n","Epoch 5 - MCC: 0.8102\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9070 - loss: 0.2212 - val_accuracy: 0.9035 - val_loss: 0.2344 - mcc: 0.8102\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9104 - loss: 0.2128\n","Epoch 6 - MCC: 0.8233\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9105 - loss: 0.2126 - val_accuracy: 0.9115 - val_loss: 0.2092 - mcc: 0.8233\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9201 - loss: 0.1914\n","Epoch 7 - MCC: 0.8356\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9200 - loss: 0.1916 - val_accuracy: 0.9179 - val_loss: 0.1978 - mcc: 0.8356\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9197 - loss: 0.1911\n","Epoch 8 - MCC: 0.8403\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9199 - loss: 0.1909 - val_accuracy: 0.9203 - val_loss: 0.1908 - mcc: 0.8403\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9264 - loss: 0.1785\n","Epoch 9 - MCC: 0.8381\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9263 - loss: 0.1787 - val_accuracy: 0.9191 - val_loss: 0.1912 - mcc: 0.8381\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9259 - loss: 0.1758\n","Epoch 10 - MCC: 0.8453\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9259 - loss: 0.1759 - val_accuracy: 0.9227 - val_loss: 0.1837 - mcc: 0.8453\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9313 - loss: 0.1664\n","Epoch 11 - MCC: 0.8481\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9312 - loss: 0.1667 - val_accuracy: 0.9242 - val_loss: 0.1800 - mcc: 0.8481\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9297 - loss: 0.1682\n","Epoch 12 - MCC: 0.8490\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9297 - loss: 0.1682 - val_accuracy: 0.9245 - val_loss: 0.1792 - mcc: 0.8490\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9304 - loss: 0.1686\n","Epoch 13 - MCC: 0.8491\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9304 - loss: 0.1685 - val_accuracy: 0.9247 - val_loss: 0.1783 - mcc: 0.8491\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9327 - loss: 0.1653\n","Epoch 14 - MCC: 0.8489\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9327 - loss: 0.1652 - val_accuracy: 0.9246 - val_loss: 0.1762 - mcc: 0.8489\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9325 - loss: 0.1641\n","Epoch 15 - MCC: 0.8579\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9326 - loss: 0.1639 - val_accuracy: 0.9291 - val_loss: 0.1683 - mcc: 0.8579\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9386 - loss: 0.1489\n","Epoch 16 - MCC: 0.8558\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9384 - loss: 0.1493 - val_accuracy: 0.9280 - val_loss: 0.1716 - mcc: 0.8558\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9345 - loss: 0.1593\n","Epoch 17 - MCC: 0.8593\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9345 - loss: 0.1592 - val_accuracy: 0.9296 - val_loss: 0.1684 - mcc: 0.8593\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9379 - loss: 0.1518\n","Epoch 18 - MCC: 0.8611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9379 - loss: 0.1518 - val_accuracy: 0.9307 - val_loss: 0.1644 - mcc: 0.8611\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9361 - loss: 0.1555\n","Epoch 19 - MCC: 0.8591\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9361 - loss: 0.1554 - val_accuracy: 0.9296 - val_loss: 0.1661 - mcc: 0.8591\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9366 - loss: 0.1532\n","Epoch 20 - MCC: 0.8660\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.9367 - loss: 0.1531 - val_accuracy: 0.9331 - val_loss: 0.1596 - mcc: 0.8660\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9391 - loss: 0.1481\n","Epoch 21 - MCC: 0.8666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9391 - loss: 0.1481 - val_accuracy: 0.9333 - val_loss: 0.1589 - mcc: 0.8666\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9432 - loss: 0.1409\n","Epoch 22 - MCC: 0.8694\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9431 - loss: 0.1410 - val_accuracy: 0.9348 - val_loss: 0.1554 - mcc: 0.8694\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9437 - loss: 0.1382\n","Epoch 23 - MCC: 0.8691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9436 - loss: 0.1384 - val_accuracy: 0.9347 - val_loss: 0.1544 - mcc: 0.8691\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9425 - loss: 0.1410\n","Epoch 24 - MCC: 0.8715\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9425 - loss: 0.1410 - val_accuracy: 0.9357 - val_loss: 0.1541 - mcc: 0.8715\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9454 - loss: 0.1343\n","Epoch 25 - MCC: 0.8702\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9453 - loss: 0.1346 - val_accuracy: 0.9351 - val_loss: 0.1559 - mcc: 0.8702\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9453 - loss: 0.1353\n","Epoch 26 - MCC: 0.8726\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9452 - loss: 0.1355 - val_accuracy: 0.9363 - val_loss: 0.1511 - mcc: 0.8726\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9444 - loss: 0.1366\n","Epoch 27 - MCC: 0.8696\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9444 - loss: 0.1366 - val_accuracy: 0.9348 - val_loss: 0.1536 - mcc: 0.8696\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9447 - loss: 0.1344\n","Epoch 28 - MCC: 0.8680\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9447 - loss: 0.1346 - val_accuracy: 0.9338 - val_loss: 0.1594 - mcc: 0.8680\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9429 - loss: 0.1417\n","Epoch 29 - MCC: 0.8746\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9429 - loss: 0.1416 - val_accuracy: 0.9371 - val_loss: 0.1515 - mcc: 0.8746\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9412 - loss: 0.1436\n","Epoch 30 - MCC: 0.8697\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9414 - loss: 0.1433 - val_accuracy: 0.9349 - val_loss: 0.1549 - mcc: 0.8697\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9502333333333334,\n","              'mean': 0.9409093333333333,\n","              'min': 0.9316866666666667,\n","              'std': 0.0067201357129153365},\n"," 'Inference Time (s/sample)': {'max': 0.003573880195617676,\n","                               'mean': 0.0033998241424560553,\n","                               'min': 0.002778909206390381,\n","                               'std': 0.00031065486827278624},\n"," 'MCC': {'max': 0.9001358094935016,\n","         'mean': 0.8816127836583736,\n","         'min': 0.8634928123875674,\n","         'std': 0.013299310912691605},\n"," 'Parameters': 10305,\n"," 'Train Time (s)': {'max': 95.36140060424805,\n","                    'mean': 91.49701089859009,\n","                    'min': 86.45194149017334,\n","                    'std': 3.318074131938586},\n"," 'Training Accuracy': [[0.5490817427635193,\n","                        0.8215234279632568,\n","                        0.8856783509254456,\n","                        0.9026516675949097,\n","                        0.908769965171814,\n","                        0.9124283790588379,\n","                        0.9184499979019165,\n","                        0.9217132329940796,\n","                        0.9239950776100159,\n","                        0.926746666431427,\n","                        0.9280200004577637,\n","                        0.9298317432403564,\n","                        0.9299665689468384,\n","                        0.933138370513916,\n","                        0.9351733326911926,\n","                        0.935253381729126,\n","                        0.9364166855812073,\n","                        0.9375084042549133,\n","                        0.938418447971344,\n","                        0.9396016001701355,\n","                        0.940706729888916,\n","                        0.9411983489990234,\n","                        0.942080020904541,\n","                        0.942288339138031,\n","                        0.9417566657066345,\n","                        0.9424350261688232,\n","                        0.9438501000404358,\n","                        0.9443516731262207,\n","                        0.9438449144363403,\n","                        0.9448184370994568],\n","                       [0.6577566266059875,\n","                        0.8396917581558228,\n","                        0.8929200172424316,\n","                        0.9050366282463074,\n","                        0.909508466720581,\n","                        0.9144516587257385,\n","                        0.916888415813446,\n","                        0.9193316698074341,\n","                        0.9207750558853149,\n","                        0.9226749539375305,\n","                        0.9248049855232239,\n","                        0.927276611328125,\n","                        0.9293681979179382,\n","                        0.9300233721733093,\n","                        0.9315184950828552,\n","                        0.9338300228118896,\n","                        0.9353266358375549,\n","                        0.9355617761611938,\n","                        0.937116801738739,\n","                        0.9383065700531006,\n","                        0.9375048875808716,\n","                        0.9388449192047119,\n","                        0.9397867321968079,\n","                        0.9419350624084473,\n","                        0.9423699378967285,\n","                        0.9426000118255615,\n","                        0.9443283677101135,\n","                        0.9440699815750122,\n","                        0.9450232982635498,\n","                        0.9453151226043701],\n","                       [0.5558099150657654,\n","                        0.8042817115783691,\n","                        0.8818017840385437,\n","                        0.9033699631690979,\n","                        0.9065199494361877,\n","                        0.9102932810783386,\n","                        0.9138915538787842,\n","                        0.9154149889945984,\n","                        0.9179749488830566,\n","                        0.9214316606521606,\n","                        0.9240666031837463,\n","                        0.927155077457428,\n","                        0.930229902267456,\n","                        0.9300466179847717,\n","                        0.9329733848571777,\n","                        0.9336499571800232,\n","                        0.9360950589179993,\n","                        0.9360683560371399,\n","                        0.9376782774925232,\n","                        0.9387867450714111,\n","                        0.9389033317565918,\n","                        0.9387367367744446,\n","                        0.9404966235160828,\n","                        0.9398150444030762,\n","                        0.9407849907875061,\n","                        0.9409950375556946,\n","                        0.9418149590492249,\n","                        0.9432699084281921,\n","                        0.9432583451271057,\n","                        0.9434700012207031],\n","                       [0.6871317028999329,\n","                        0.846530020236969,\n","                        0.8869783282279968,\n","                        0.9024317264556885,\n","                        0.9078717231750488,\n","                        0.9114099144935608,\n","                        0.9149015545845032,\n","                        0.9176099896430969,\n","                        0.918678343296051,\n","                        0.92132169008255,\n","                        0.9234899878501892,\n","                        0.9238048791885376,\n","                        0.9245266914367676,\n","                        0.9268799424171448,\n","                        0.9257832169532776,\n","                        0.9293367862701416,\n","                        0.931088387966156,\n","                        0.9327684044837952,\n","                        0.9343616366386414,\n","                        0.9345033764839172,\n","                        0.9364667534828186,\n","                        0.9370982646942139,\n","                        0.9375867247581482,\n","                        0.938528299331665,\n","                        0.9391666650772095,\n","                        0.9405699968338013,\n","                        0.9386166930198669,\n","                        0.9403433799743652,\n","                        0.9411782622337341,\n","                        0.9410483837127686],\n","                       [0.5631166696548462,\n","                        0.8371116518974304,\n","                        0.888273298740387,\n","                        0.906663179397583,\n","                        0.9090050458908081,\n","                        0.91318678855896,\n","                        0.9189417958259583,\n","                        0.9225550293922424,\n","                        0.923703134059906,\n","                        0.9256983399391174,\n","                        0.9287166595458984,\n","                        0.9300549030303955,\n","                        0.9318516254425049,\n","                        0.9337266683578491,\n","                        0.9350549578666687,\n","                        0.9347999691963196,\n","                        0.9355983734130859,\n","                        0.9376398324966431,\n","                        0.9375350475311279,\n","                        0.9382882714271545,\n","                        0.9397751092910767,\n","                        0.9412150382995605,\n","                        0.9410284161567688,\n","                        0.9425784349441528,\n","                        0.9423368573188782,\n","                        0.9428166747093201,\n","                        0.9432933330535889,\n","                        0.942943274974823,\n","                        0.9434151649475098,\n","                        0.9448949694633484]],\n"," 'Training Loss': [[0.6344183087348938,\n","                    0.45504188537597656,\n","                    0.27225956320762634,\n","                    0.23450836539268494,\n","                    0.2199392467737198,\n","                    0.21106742322444916,\n","                    0.19866541028022766,\n","                    0.18942376971244812,\n","                    0.18334250152111053,\n","                    0.1778011918067932,\n","                    0.17436747252941132,\n","                    0.1718142181634903,\n","                    0.17038844525814056,\n","                    0.16324637830257416,\n","                    0.15891876816749573,\n","                    0.15809163451194763,\n","                    0.1560882031917572,\n","                    0.15332593023777008,\n","                    0.15019163489341736,\n","                    0.14771990478038788,\n","                    0.14512436091899872,\n","                    0.14442652463912964,\n","                    0.14239630103111267,\n","                    0.14232689142227173,\n","                    0.1424909383058548,\n","                    0.1418028622865677,\n","                    0.13828513026237488,\n","                    0.13701890408992767,\n","                    0.1385464072227478,\n","                    0.13575401902198792],\n","                   [0.5859997272491455,\n","                    0.3838684856891632,\n","                    0.2631188631057739,\n","                    0.23083242774009705,\n","                    0.220680370926857,\n","                    0.20647674798965454,\n","                    0.1984248012304306,\n","                    0.19237737357616425,\n","                    0.1880328208208084,\n","                    0.18365560472011566,\n","                    0.17802006006240845,\n","                    0.17322205007076263,\n","                    0.16946063935756683,\n","                    0.16704483330249786,\n","                    0.1640922874212265,\n","                    0.15939049422740936,\n","                    0.15630920231342316,\n","                    0.15550239384174347,\n","                    0.15249180793762207,\n","                    0.14915324747562408,\n","                    0.15271702408790588,\n","                    0.14878727495670319,\n","                    0.14627592265605927,\n","                    0.14254525303840637,\n","                    0.14004561305046082,\n","                    0.14021463692188263,\n","                    0.13697248697280884,\n","                    0.13738466799259186,\n","                    0.13622432947158813,\n","                    0.13509105145931244],\n","                   [0.6595795154571533,\n","                    0.48145365715026855,\n","                    0.2878623902797699,\n","                    0.23633553087711334,\n","                    0.22705896198749542,\n","                    0.21751099824905396,\n","                    0.20815670490264893,\n","                    0.20310074090957642,\n","                    0.19658298790454865,\n","                    0.18821130692958832,\n","                    0.18197980523109436,\n","                    0.17582716047763824,\n","                    0.1695661097764969,\n","                    0.16955053806304932,\n","                    0.16439715027809143,\n","                    0.16299831867218018,\n","                    0.15647511184215546,\n","                    0.15553398430347443,\n","                    0.15352532267570496,\n","                    0.1495046615600586,\n","                    0.14874237775802612,\n","                    0.14851228892803192,\n","                    0.1456536054611206,\n","                    0.14690335094928741,\n","                    0.14452208578586578,\n","                    0.144648477435112,\n","                    0.14269475638866425,\n","                    0.13934558629989624,\n","                    0.13989375531673431,\n","                    0.1385429948568344],\n","                   [0.5926844477653503,\n","                    0.3718750774860382,\n","                    0.2713976800441742,\n","                    0.2343560755252838,\n","                    0.21895627677440643,\n","                    0.20917843282222748,\n","                    0.2008160948753357,\n","                    0.19470445811748505,\n","                    0.1914038509130478,\n","                    0.18670125305652618,\n","                    0.18268504738807678,\n","                    0.18141403794288635,\n","                    0.180068239569664,\n","                    0.1745399832725525,\n","                    0.17720040678977966,\n","                    0.1690560281276703,\n","                    0.16578979790210724,\n","                    0.16269417107105255,\n","                    0.15991388261318207,\n","                    0.1584957093000412,\n","                    0.1554114669561386,\n","                    0.1530081331729889,\n","                    0.15223735570907593,\n","                    0.1497265100479126,\n","                    0.14765052497386932,\n","                    0.1451331079006195,\n","                    0.1492197960615158,\n","                    0.14545269310474396,\n","                    0.14310695230960846,\n","                    0.14336057007312775],\n","                   [0.6224567294120789,\n","                    0.4217880666255951,\n","                    0.2656112313270569,\n","                    0.22460010647773743,\n","                    0.21701431274414062,\n","                    0.20826925337314606,\n","                    0.1946527659893036,\n","                    0.18550097942352295,\n","                    0.18342339992523193,\n","                    0.1782257854938507,\n","                    0.17254401743412018,\n","                    0.16831642389297485,\n","                    0.16558519005775452,\n","                    0.1610395461320877,\n","                    0.15771599113941193,\n","                    0.15824511647224426,\n","                    0.15716513991355896,\n","                    0.15229015052318573,\n","                    0.15287864208221436,\n","                    0.1499868929386139,\n","                    0.14806823432445526,\n","                    0.14433136582374573,\n","                    0.14415721595287323,\n","                    0.14098477363586426,\n","                    0.14153224229812622,\n","                    0.13980868458747864,\n","                    0.1391288787126541,\n","                    0.13975025713443756,\n","                    0.1390911042690277,\n","                    0.13554728031158447]],\n"," 'Validation Accuracy': [[0.735633373260498,\n","                          0.8667133450508118,\n","                          0.8967333436012268,\n","                          0.9131066203117371,\n","                          0.916266679763794,\n","                          0.9192399382591248,\n","                          0.9247333407402039,\n","                          0.9271466732025146,\n","                          0.9296599626541138,\n","                          0.9327666759490967,\n","                          0.9352733492851257,\n","                          0.9308866858482361,\n","                          0.937873363494873,\n","                          0.9393333196640015,\n","                          0.9402733445167542,\n","                          0.9392133355140686,\n","                          0.9401666522026062,\n","                          0.9442800879478455,\n","                          0.9448267221450806,\n","                          0.9455599784851074,\n","                          0.9431334137916565,\n","                          0.9461332559585571,\n","                          0.9479799866676331,\n","                          0.9465999603271484,\n","                          0.9478399753570557,\n","                          0.9468799829483032,\n","                          0.9460399746894836,\n","                          0.9497599601745605,\n","                          0.9489133358001709,\n","                          0.9502333998680115],\n","                         [0.7899200320243835,\n","                          0.8538000583648682,\n","                          0.892133355140686,\n","                          0.8988200426101685,\n","                          0.900713324546814,\n","                          0.902180016040802,\n","                          0.9075333476066589,\n","                          0.9069467186927795,\n","                          0.908966600894928,\n","                          0.9124733209609985,\n","                          0.9150799512863159,\n","                          0.9176733493804932,\n","                          0.9162867069244385,\n","                          0.9197199940681458,\n","                          0.922700047492981,\n","                          0.9240533709526062,\n","                          0.9253799915313721,\n","                          0.9254600405693054,\n","                          0.9273600578308105,\n","                          0.9251866936683655,\n","                          0.9260199666023254,\n","                          0.9290133714675903,\n","                          0.9293332695960999,\n","                          0.9303799271583557,\n","                          0.9316266775131226,\n","                          0.931886613368988,\n","                          0.9338667392730713,\n","                          0.9294933080673218,\n","                          0.9332666993141174,\n","                          0.9316866993904114],\n","                         [0.6813533306121826,\n","                          0.8479199409484863,\n","                          0.9012733697891235,\n","                          0.910319983959198,\n","                          0.9104600548744202,\n","                          0.9153466820716858,\n","                          0.9146400690078735,\n","                          0.9216266870498657,\n","                          0.9242667555809021,\n","                          0.9253333210945129,\n","                          0.9293066263198853,\n","                          0.9297600388526917,\n","                          0.9329599738121033,\n","                          0.9308199882507324,\n","                          0.936739981174469,\n","                          0.936186671257019,\n","                          0.9375466704368591,\n","                          0.9388800263404846,\n","                          0.9386333227157593,\n","                          0.9383066296577454,\n","                          0.941986620426178,\n","                          0.9405866861343384,\n","                          0.9394533634185791,\n","                          0.9415600299835205,\n","                          0.9413599967956543,\n","                          0.9441933631896973,\n","                          0.9433266520500183,\n","                          0.9445400238037109,\n","                          0.9418333172798157,\n","                          0.9445733428001404],\n","                         [0.8536266088485718,\n","                          0.8745332956314087,\n","                          0.9012333154678345,\n","                          0.9107866883277893,\n","                          0.9146133661270142,\n","                          0.9213333129882812,\n","                          0.9236533641815186,\n","                          0.9231933355331421,\n","                          0.9248133301734924,\n","                          0.928453266620636,\n","                          0.9290332794189453,\n","                          0.9294133186340332,\n","                          0.9295333027839661,\n","                          0.9310266971588135,\n","                          0.9348933696746826,\n","                          0.9340866208076477,\n","                          0.9363601207733154,\n","                          0.939633309841156,\n","                          0.9392067193984985,\n","                          0.9405465722084045,\n","                          0.9396133422851562,\n","                          0.9418067336082458,\n","                          0.9407466053962708,\n","                          0.9418333172798157,\n","                          0.9439533352851868,\n","                          0.9438400268554688,\n","                          0.9422067403793335,\n","                          0.9443200826644897,\n","                          0.9454200267791748,\n","                          0.9431065917015076],\n","                         [0.8184800744056702,\n","                          0.8583999872207642,\n","                          0.8939200639724731,\n","                          0.9061200618743896,\n","                          0.9034732580184937,\n","                          0.9115200042724609,\n","                          0.9179133772850037,\n","                          0.9202866554260254,\n","                          0.9191000461578369,\n","                          0.9227199554443359,\n","                          0.9241933226585388,\n","                          0.9245400428771973,\n","                          0.9246866703033447,\n","                          0.9245667457580566,\n","                          0.9290733337402344,\n","                          0.9280000329017639,\n","                          0.9296466708183289,\n","                          0.9306667447090149,\n","                          0.9296200275421143,\n","                          0.9331133365631104,\n","                          0.9332733154296875,\n","                          0.9348400235176086,\n","                          0.9346666932106018,\n","                          0.9356799125671387,\n","                          0.9350732564926147,\n","                          0.9363133311271667,\n","                          0.934779942035675,\n","                          0.9338200092315674,\n","                          0.9370867013931274,\n","                          0.9349467754364014]],\n"," 'Validation Loss': [[0.5608279705047607,\n","                      0.32827094197273254,\n","                      0.24689485132694244,\n","                      0.21334974467754364,\n","                      0.20143190026283264,\n","                      0.19253009557724,\n","                      0.18119190633296967,\n","                      0.17229203879833221,\n","                      0.16703195869922638,\n","                      0.1625789850950241,\n","                      0.156727597117424,\n","                      0.16424933075904846,\n","                      0.15213045477867126,\n","                      0.1480204313993454,\n","                      0.1454375684261322,\n","                      0.14759019017219543,\n","                      0.14275367558002472,\n","                      0.13601216673851013,\n","                      0.13461093604564667,\n","                      0.13191190361976624,\n","                      0.1382482647895813,\n","                      0.13075123727321625,\n","                      0.12840811908245087,\n","                      0.1290062665939331,\n","                      0.1297074556350708,\n","                      0.1265033334493637,\n","                      0.12912343442440033,\n","                      0.12237975746393204,\n","                      0.124277763068676,\n","                      0.12168806791305542],\n","                     [0.4962099492549896,\n","                      0.3396274149417877,\n","                      0.2604409158229828,\n","                      0.2439495325088501,\n","                      0.234401136636734,\n","                      0.23279358446598053,\n","                      0.21880456805229187,\n","                      0.21738654375076294,\n","                      0.21248897910118103,\n","                      0.2046547383069992,\n","                      0.20343270897865295,\n","                      0.19427993893623352,\n","                      0.1974271535873413,\n","                      0.19196546077728271,\n","                      0.18564294278621674,\n","                      0.18403153121471405,\n","                      0.18124860525131226,\n","                      0.18002624809741974,\n","                      0.17765936255455017,\n","                      0.18311788141727448,\n","                      0.17712771892547607,\n","                      0.17492900788784027,\n","                      0.17449826002120972,\n","                      0.16929656267166138,\n","                      0.16789008677005768,\n","                      0.16883598268032074,\n","                      0.16483701765537262,\n","                      0.17414359748363495,\n","                      0.16546934843063354,\n","                      0.16575875878334045],\n","                     [0.5877618193626404,\n","                      0.35332542657852173,\n","                      0.24045288562774658,\n","                      0.2188059687614441,\n","                      0.21774223446846008,\n","                      0.20461684465408325,\n","                      0.20378275215625763,\n","                      0.18822331726551056,\n","                      0.18286682665348053,\n","                      0.17771823704242706,\n","                      0.17066626250743866,\n","                      0.1685928851366043,\n","                      0.1632796674966812,\n","                      0.16854700446128845,\n","                      0.15555676817893982,\n","                      0.15841124951839447,\n","                      0.1521771103143692,\n","                      0.15083876252174377,\n","                      0.14884530007839203,\n","                      0.15122926235198975,\n","                      0.14347979426383972,\n","                      0.14664271473884583,\n","                      0.14551453292369843,\n","                      0.14414313435554504,\n","                      0.1426772177219391,\n","                      0.13781975209712982,\n","                      0.13821910321712494,\n","                      0.13775402307510376,\n","                      0.14225181937217712,\n","                      0.13600444793701172],\n","                     [0.46106070280075073,\n","                      0.3039908707141876,\n","                      0.23618881404399872,\n","                      0.21541297435760498,\n","                      0.20471078157424927,\n","                      0.19257602095603943,\n","                      0.18721219897270203,\n","                      0.186630517244339,\n","                      0.18261754512786865,\n","                      0.17786283791065216,\n","                      0.1756412386894226,\n","                      0.17780373990535736,\n","                      0.17033469676971436,\n","                      0.17255279421806335,\n","                      0.16417863965034485,\n","                      0.16153649985790253,\n","                      0.15851956605911255,\n","                      0.15463639795780182,\n","                      0.15213121473789215,\n","                      0.15207114815711975,\n","                      0.1501823365688324,\n","                      0.1465516984462738,\n","                      0.14638292789459229,\n","                      0.1466057300567627,\n","                      0.14216624200344086,\n","                      0.14047564566135406,\n","                      0.14410749077796936,\n","                      0.13961058855056763,\n","                      0.13643980026245117,\n","                      0.13957656919956207],\n","                     [0.5410182476043701,\n","                      0.3224784731864929,\n","                      0.25325676798820496,\n","                      0.22389444708824158,\n","                      0.23440313339233398,\n","                      0.20919273793697357,\n","                      0.19776730239391327,\n","                      0.19080248475074768,\n","                      0.19124668836593628,\n","                      0.1837231069803238,\n","                      0.17996633052825928,\n","                      0.17915214598178864,\n","                      0.1782558411359787,\n","                      0.17624534666538239,\n","                      0.1683201938867569,\n","                      0.1715674251317978,\n","                      0.16835948824882507,\n","                      0.16443347930908203,\n","                      0.16608189046382904,\n","                      0.15961778163909912,\n","                      0.15893879532814026,\n","                      0.15544839203357697,\n","                      0.15436981618404388,\n","                      0.1540907919406891,\n","                      0.15587709844112396,\n","                      0.15105415880680084,\n","                      0.1536368727684021,\n","                      0.1593664288520813,\n","                      0.15145011246204376,\n","                      0.15488964319229126]],\n"," 'Validation MCC': [[0.5513063805318397,\n","                     0.7323324117070404,\n","                     0.7962592378045584,\n","                     0.8261103368056346,\n","                     0.8320815679777215,\n","                     0.8379147192683778,\n","                     0.8491092960629523,\n","                     0.8537485304832485,\n","                     0.8587695781830411,\n","                     0.8650075841701055,\n","                     0.8700533222491512,\n","                     0.8630918866712096,\n","                     0.875358591030847,\n","                     0.8781965081620334,\n","                     0.880201641037002,\n","                     0.8780404008308063,\n","                     0.8800578109323343,\n","                     0.8882032219456927,\n","                     0.8892379584689089,\n","                     0.8907054430613759,\n","                     0.8859704582784148,\n","                     0.8918546610022143,\n","                     0.8955635690710319,\n","                     0.8929081143096674,\n","                     0.8952849154410693,\n","                     0.8933929821756057,\n","                     0.8919410931411663,\n","                     0.8991763199873659,\n","                     0.8974604750985662,\n","                     0.9001358094935016],\n","                    [0.5802712107986734,\n","                     0.7160860429533944,\n","                     0.7838384646128657,\n","                     0.7971524865542357,\n","                     0.8011186225910106,\n","                     0.8040530453194518,\n","                     0.8146477614004382,\n","                     0.8140326597009289,\n","                     0.8179173996157101,\n","                     0.824609995864012,\n","                     0.8299818630588256,\n","                     0.8349509261775827,\n","                     0.8322885944414089,\n","                     0.8390610640852059,\n","                     0.8451112779418696,\n","                     0.8478600466852112,\n","                     0.8504826436405774,\n","                     0.8507895018774017,\n","                     0.8543845816838024,\n","                     0.8502959842482476,\n","                     0.8521501119156042,\n","                     0.8577318747517162,\n","                     0.8588526694143036,\n","                     0.8605200133559879,\n","                     0.8630240087077019,\n","                     0.8635177891137853,\n","                     0.8674248776710027,\n","                     0.8591881324605294,\n","                     0.8665837602374071,\n","                     0.8634928123875674],\n","                    [0.47445705161298896,\n","                     0.6944898970302384,\n","                     0.8024030997813285,\n","                     0.8202980381295798,\n","                     0.8211906085279093,\n","                     0.8300986258209405,\n","                     0.8302412279191438,\n","                     0.842586030169011,\n","                     0.8479053765360252,\n","                     0.8501044567188939,\n","                     0.8580330668708904,\n","                     0.8591662105764,\n","                     0.8653611596013954,\n","                     0.8613751469392829,\n","                     0.872977808062452,\n","                     0.8723807583058542,\n","                     0.8746758726794943,\n","                     0.8774218052374172,\n","                     0.8768884316000358,\n","                     0.8762122338338746,\n","                     0.8834930329097846,\n","                     0.8810216554489717,\n","                     0.8790801068356848,\n","                     0.8827221480632776,\n","                     0.8823136031571527,\n","                     0.8879271539816104,\n","                     0.8863598446448364,\n","                     0.8886210890662425,\n","                     0.8831843697090982,\n","                     0.888762976392817],\n","                    [0.7065423105643776,\n","                     0.7556407023786172,\n","                     0.8045102607328478,\n","                     0.8219927345699599,\n","                     0.8297760758456668,\n","                     0.8422804753913347,\n","                     0.8470187030218964,\n","                     0.8461683954757866,\n","                     0.8496466420348374,\n","                     0.8565564242237342,\n","                     0.8577382909631135,\n","                     0.8592056563828612,\n","                     0.8594509412289655,\n","                     0.861920906993141,\n","                     0.8694674797225351,\n","                     0.8680234803717352,\n","                     0.8724974437165851,\n","                     0.8789756560547523,\n","                     0.8781680991230315,\n","                     0.8810626689108998,\n","                     0.8791475347820809,\n","                     0.8833319204602025,\n","                     0.8813948140852628,\n","                     0.8838922142814795,\n","                     0.8876350363683879,\n","                     0.8874531566190487,\n","                     0.884405326902468,\n","                     0.8883707837559303,\n","                     0.8905786230789194,\n","                     0.8859992317439653],\n","                    [0.6627810049763462,\n","                     0.717563541138442,\n","                     0.7881573295125491,\n","                     0.8122174313506696,\n","                     0.8101782040693556,\n","                     0.8233266432260069,\n","                     0.8355898183355922,\n","                     0.8402741004597095,\n","                     0.838072950334876,\n","                     0.8453487534426612,\n","                     0.8481141978890259,\n","                     0.8489943103060371,\n","                     0.8491129394870355,\n","                     0.8488555312631004,\n","                     0.8579085479419631,\n","                     0.8557820304610881,\n","                     0.8593088970788672,\n","                     0.8611326682957244,\n","                     0.8591185654587536,\n","                     0.8659996373604818,\n","                     0.866637391714489,\n","                     0.8694401475808521,\n","                     0.8691031980845842,\n","                     0.8715380014611794,\n","                     0.8702027820375494,\n","                     0.8726016273302836,\n","                     0.8696438725196587,\n","                     0.8679565836123033,\n","                     0.8745600959133043,\n","                     0.869673088274016]]}\n","Training Model: BiLSTM_Dense, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5460 - loss: 0.6585\n","Epoch 1 - MCC: 0.5143\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 147ms/step - accuracy: 0.5477 - loss: 0.6571 - val_accuracy: 0.7370 - val_loss: 0.5349 - mcc: 0.5143\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7878 - loss: 0.4850\n","Epoch 2 - MCC: 0.7410\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.7890 - loss: 0.4829 - val_accuracy: 0.8702 - val_loss: 0.3196 - mcc: 0.7410\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8636 - loss: 0.3178\n","Epoch 3 - MCC: 0.7924\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8640 - loss: 0.3170 - val_accuracy: 0.8966 - val_loss: 0.2450 - mcc: 0.7924\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8938 - loss: 0.2537\n","Epoch 4 - MCC: 0.8228\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8939 - loss: 0.2534 - val_accuracy: 0.9115 - val_loss: 0.2116 - mcc: 0.8228\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9058 - loss: 0.2261\n","Epoch 5 - MCC: 0.8341\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9059 - loss: 0.2260 - val_accuracy: 0.9171 - val_loss: 0.1990 - mcc: 0.8341\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9156 - loss: 0.2043\n","Epoch 6 - MCC: 0.8499\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.9155 - loss: 0.2044 - val_accuracy: 0.9252 - val_loss: 0.1797 - mcc: 0.8499\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9202 - loss: 0.1945\n","Epoch 7 - MCC: 0.8615\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9203 - loss: 0.1944 - val_accuracy: 0.9309 - val_loss: 0.1680 - mcc: 0.8615\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9232 - loss: 0.1891\n","Epoch 8 - MCC: 0.8674\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9234 - loss: 0.1887 - val_accuracy: 0.9339 - val_loss: 0.1582 - mcc: 0.8674\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9293 - loss: 0.1726\n","Epoch 9 - MCC: 0.8745\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9293 - loss: 0.1726 - val_accuracy: 0.9374 - val_loss: 0.1543 - mcc: 0.8745\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9263 - loss: 0.1797\n","Epoch 10 - MCC: 0.8772\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9265 - loss: 0.1791 - val_accuracy: 0.9387 - val_loss: 0.1479 - mcc: 0.8772\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9379 - loss: 0.1537\n","Epoch 11 - MCC: 0.8779\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9378 - loss: 0.1539 - val_accuracy: 0.9391 - val_loss: 0.1471 - mcc: 0.8779\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9336 - loss: 0.1643\n","Epoch 12 - MCC: 0.8854\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9336 - loss: 0.1642 - val_accuracy: 0.9428 - val_loss: 0.1411 - mcc: 0.8854\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9376 - loss: 0.1539\n","Epoch 13 - MCC: 0.8905\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9376 - loss: 0.1539 - val_accuracy: 0.9454 - val_loss: 0.1368 - mcc: 0.8905\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9407 - loss: 0.1490\n","Epoch 14 - MCC: 0.8911\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9406 - loss: 0.1491 - val_accuracy: 0.9457 - val_loss: 0.1354 - mcc: 0.8911\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9387 - loss: 0.1539\n","Epoch 15 - MCC: 0.8880\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9387 - loss: 0.1537 - val_accuracy: 0.9441 - val_loss: 0.1364 - mcc: 0.8880\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9421 - loss: 0.1447\n","Epoch 16 - MCC: 0.8967\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9421 - loss: 0.1447 - val_accuracy: 0.9485 - val_loss: 0.1285 - mcc: 0.8967\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9418 - loss: 0.1453\n","Epoch 17 - MCC: 0.8940\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9418 - loss: 0.1453 - val_accuracy: 0.9471 - val_loss: 0.1304 - mcc: 0.8940\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9444 - loss: 0.1394\n","Epoch 18 - MCC: 0.8965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9443 - loss: 0.1395 - val_accuracy: 0.9484 - val_loss: 0.1277 - mcc: 0.8965\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9453 - loss: 0.1371\n","Epoch 19 - MCC: 0.8989\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9452 - loss: 0.1372 - val_accuracy: 0.9496 - val_loss: 0.1249 - mcc: 0.8989\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9431 - loss: 0.1413\n","Epoch 20 - MCC: 0.9015\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9431 - loss: 0.1412 - val_accuracy: 0.9509 - val_loss: 0.1223 - mcc: 0.9015\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9450 - loss: 0.1383\n","Epoch 21 - MCC: 0.9020\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9450 - loss: 0.1383 - val_accuracy: 0.9511 - val_loss: 0.1210 - mcc: 0.9020\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9458 - loss: 0.1353\n","Epoch 22 - MCC: 0.9020\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9458 - loss: 0.1354 - val_accuracy: 0.9512 - val_loss: 0.1219 - mcc: 0.9020\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9501 - loss: 0.1245\n","Epoch 23 - MCC: 0.9041\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9499 - loss: 0.1249 - val_accuracy: 0.9521 - val_loss: 0.1196 - mcc: 0.9041\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9441 - loss: 0.1371\n","Epoch 24 - MCC: 0.9039\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9441 - loss: 0.1371 - val_accuracy: 0.9521 - val_loss: 0.1188 - mcc: 0.9039\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9475 - loss: 0.1300\n","Epoch 25 - MCC: 0.9055\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9474 - loss: 0.1302 - val_accuracy: 0.9529 - val_loss: 0.1175 - mcc: 0.9055\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9482 - loss: 0.1300\n","Epoch 26 - MCC: 0.9059\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9481 - loss: 0.1301 - val_accuracy: 0.9531 - val_loss: 0.1155 - mcc: 0.9059\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9496 - loss: 0.1247\n","Epoch 27 - MCC: 0.9070\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9495 - loss: 0.1249 - val_accuracy: 0.9537 - val_loss: 0.1148 - mcc: 0.9070\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9479 - loss: 0.1303\n","Epoch 28 - MCC: 0.9058\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9479 - loss: 0.1302 - val_accuracy: 0.9530 - val_loss: 0.1158 - mcc: 0.9058\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9498 - loss: 0.1256\n","Epoch 29 - MCC: 0.9084\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.9497 - loss: 0.1257 - val_accuracy: 0.9542 - val_loss: 0.1156 - mcc: 0.9084\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9444 - loss: 0.1383\n","Epoch 30 - MCC: 0.9021\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9445 - loss: 0.1380 - val_accuracy: 0.9512 - val_loss: 0.1220 - mcc: 0.9021\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5591 - loss: 0.6741\n","Epoch 1 - MCC: 0.5256\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.5607 - loss: 0.6729 - val_accuracy: 0.7271 - val_loss: 0.5739 - mcc: 0.5256\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7928 - loss: 0.5254\n","Epoch 2 - MCC: 0.7053\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.7939 - loss: 0.5235 - val_accuracy: 0.8517 - val_loss: 0.3781 - mcc: 0.7053\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8886 - loss: 0.2949\n","Epoch 3 - MCC: 0.7864\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step - accuracy: 0.8888 - loss: 0.2938 - val_accuracy: 0.8935 - val_loss: 0.2561 - mcc: 0.7864\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9090 - loss: 0.2199\n","Epoch 4 - MCC: 0.8068\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9092 - loss: 0.2197 - val_accuracy: 0.9035 - val_loss: 0.2298 - mcc: 0.8068\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9142 - loss: 0.2051\n","Epoch 5 - MCC: 0.8144\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9144 - loss: 0.2048 - val_accuracy: 0.9065 - val_loss: 0.2216 - mcc: 0.8144\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9165 - loss: 0.1981\n","Epoch 6 - MCC: 0.8247\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9167 - loss: 0.1977 - val_accuracy: 0.9118 - val_loss: 0.2107 - mcc: 0.8247\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9273 - loss: 0.1770\n","Epoch 7 - MCC: 0.8337\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9272 - loss: 0.1771 - val_accuracy: 0.9170 - val_loss: 0.1968 - mcc: 0.8337\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9278 - loss: 0.1739\n","Epoch 8 - MCC: 0.8373\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.9278 - loss: 0.1738 - val_accuracy: 0.9188 - val_loss: 0.1922 - mcc: 0.8373\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9275 - loss: 0.1725\n","Epoch 9 - MCC: 0.8388\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9276 - loss: 0.1723 - val_accuracy: 0.9193 - val_loss: 0.1943 - mcc: 0.8388\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9299 - loss: 0.1698\n","Epoch 10 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9300 - loss: 0.1695 - val_accuracy: 0.9230 - val_loss: 0.1849 - mcc: 0.8458\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9325 - loss: 0.1619\n","Epoch 11 - MCC: 0.8480\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9326 - loss: 0.1617 - val_accuracy: 0.9237 - val_loss: 0.1841 - mcc: 0.8480\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9399 - loss: 0.1470\n","Epoch 12 - MCC: 0.8539\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9397 - loss: 0.1473 - val_accuracy: 0.9270 - val_loss: 0.1788 - mcc: 0.8539\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9347 - loss: 0.1572\n","Epoch 13 - MCC: 0.8517\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9347 - loss: 0.1571 - val_accuracy: 0.9254 - val_loss: 0.1815 - mcc: 0.8517\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9392 - loss: 0.1480\n","Epoch 14 - MCC: 0.8579\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9392 - loss: 0.1481 - val_accuracy: 0.9291 - val_loss: 0.1718 - mcc: 0.8579\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9372 - loss: 0.1526\n","Epoch 15 - MCC: 0.8547\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9373 - loss: 0.1524 - val_accuracy: 0.9275 - val_loss: 0.1744 - mcc: 0.8547\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9398 - loss: 0.1456\n","Epoch 16 - MCC: 0.8585\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9398 - loss: 0.1455 - val_accuracy: 0.9294 - val_loss: 0.1706 - mcc: 0.8585\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9400 - loss: 0.1448\n","Epoch 17 - MCC: 0.8635\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9400 - loss: 0.1447 - val_accuracy: 0.9319 - val_loss: 0.1681 - mcc: 0.8635\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9421 - loss: 0.1415\n","Epoch 18 - MCC: 0.8614\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.9421 - loss: 0.1415 - val_accuracy: 0.9308 - val_loss: 0.1710 - mcc: 0.8614\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9427 - loss: 0.1399\n","Epoch 19 - MCC: 0.8603\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9427 - loss: 0.1399 - val_accuracy: 0.9303 - val_loss: 0.1679 - mcc: 0.8603\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9461 - loss: 0.1333\n","Epoch 20 - MCC: 0.8590\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9460 - loss: 0.1336 - val_accuracy: 0.9297 - val_loss: 0.1710 - mcc: 0.8590\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9444 - loss: 0.1377\n","Epoch 21 - MCC: 0.8708\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9444 - loss: 0.1376 - val_accuracy: 0.9355 - val_loss: 0.1613 - mcc: 0.8708\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9444 - loss: 0.1344\n","Epoch 22 - MCC: 0.8725\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.9444 - loss: 0.1344 - val_accuracy: 0.9364 - val_loss: 0.1616 - mcc: 0.8725\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9426 - loss: 0.1397\n","Epoch 23 - MCC: 0.8658\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9426 - loss: 0.1395 - val_accuracy: 0.9329 - val_loss: 0.1647 - mcc: 0.8658\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9452 - loss: 0.1343\n","Epoch 24 - MCC: 0.8700\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9452 - loss: 0.1342 - val_accuracy: 0.9352 - val_loss: 0.1600 - mcc: 0.8700\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9482 - loss: 0.1271\n","Epoch 25 - MCC: 0.8705\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9481 - loss: 0.1273 - val_accuracy: 0.9352 - val_loss: 0.1600 - mcc: 0.8705\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9454 - loss: 0.1333\n","Epoch 26 - MCC: 0.8736\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9455 - loss: 0.1332 - val_accuracy: 0.9369 - val_loss: 0.1572 - mcc: 0.8736\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9453 - loss: 0.1338\n","Epoch 27 - MCC: 0.8673\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9453 - loss: 0.1337 - val_accuracy: 0.9338 - val_loss: 0.1648 - mcc: 0.8673\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9457 - loss: 0.1321\n","Epoch 28 - MCC: 0.8630\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9458 - loss: 0.1320 - val_accuracy: 0.9314 - val_loss: 0.1670 - mcc: 0.8630\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9461 - loss: 0.1315\n","Epoch 29 - MCC: 0.8686\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9461 - loss: 0.1314 - val_accuracy: 0.9345 - val_loss: 0.1598 - mcc: 0.8686\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9447 - loss: 0.1328\n","Epoch 30 - MCC: 0.8705\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9448 - loss: 0.1327 - val_accuracy: 0.9354 - val_loss: 0.1576 - mcc: 0.8705\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5424 - loss: 0.6419\n","Epoch 1 - MCC: 0.6988\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 123ms/step - accuracy: 0.5454 - loss: 0.6404 - val_accuracy: 0.8407 - val_loss: 0.5042 - mcc: 0.6988\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8499 - loss: 0.4489\n","Epoch 2 - MCC: 0.7516\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8502 - loss: 0.4470 - val_accuracy: 0.8757 - val_loss: 0.3125 - mcc: 0.7516\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8789 - loss: 0.3019\n","Epoch 3 - MCC: 0.8079\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8792 - loss: 0.3010 - val_accuracy: 0.9044 - val_loss: 0.2361 - mcc: 0.8079\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9002 - loss: 0.2433\n","Epoch 4 - MCC: 0.8173\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9004 - loss: 0.2428 - val_accuracy: 0.9081 - val_loss: 0.2206 - mcc: 0.8173\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9080 - loss: 0.2229\n","Epoch 5 - MCC: 0.8343\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9081 - loss: 0.2227 - val_accuracy: 0.9173 - val_loss: 0.2030 - mcc: 0.8343\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9155 - loss: 0.2040\n","Epoch 6 - MCC: 0.8433\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9155 - loss: 0.2040 - val_accuracy: 0.9214 - val_loss: 0.1936 - mcc: 0.8433\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9166 - loss: 0.2000\n","Epoch 7 - MCC: 0.8500\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9167 - loss: 0.1998 - val_accuracy: 0.9253 - val_loss: 0.1828 - mcc: 0.8500\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9219 - loss: 0.1887\n","Epoch 8 - MCC: 0.8551\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9220 - loss: 0.1886 - val_accuracy: 0.9277 - val_loss: 0.1749 - mcc: 0.8551\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9279 - loss: 0.1772\n","Epoch 9 - MCC: 0.8548\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.9278 - loss: 0.1773 - val_accuracy: 0.9276 - val_loss: 0.1724 - mcc: 0.8548\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9286 - loss: 0.1720\n","Epoch 10 - MCC: 0.8620\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9286 - loss: 0.1721 - val_accuracy: 0.9313 - val_loss: 0.1684 - mcc: 0.8620\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9285 - loss: 0.1706\n","Epoch 11 - MCC: 0.8647\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9286 - loss: 0.1706 - val_accuracy: 0.9326 - val_loss: 0.1623 - mcc: 0.8647\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9335 - loss: 0.1610\n","Epoch 12 - MCC: 0.8676\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9334 - loss: 0.1613 - val_accuracy: 0.9341 - val_loss: 0.1598 - mcc: 0.8676\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9299 - loss: 0.1683\n","Epoch 13 - MCC: 0.8666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9300 - loss: 0.1682 - val_accuracy: 0.9335 - val_loss: 0.1610 - mcc: 0.8666\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9293 - loss: 0.1690\n","Epoch 14 - MCC: 0.8724\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9295 - loss: 0.1687 - val_accuracy: 0.9363 - val_loss: 0.1579 - mcc: 0.8724\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9368 - loss: 0.1531\n","Epoch 15 - MCC: 0.8731\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9367 - loss: 0.1534 - val_accuracy: 0.9368 - val_loss: 0.1537 - mcc: 0.8731\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9388 - loss: 0.1494\n","Epoch 16 - MCC: 0.8730\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9387 - loss: 0.1497 - val_accuracy: 0.9365 - val_loss: 0.1561 - mcc: 0.8730\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9336 - loss: 0.1606\n","Epoch 17 - MCC: 0.8741\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9337 - loss: 0.1604 - val_accuracy: 0.9373 - val_loss: 0.1513 - mcc: 0.8741\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9396 - loss: 0.1478\n","Epoch 18 - MCC: 0.8770\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9396 - loss: 0.1480 - val_accuracy: 0.9387 - val_loss: 0.1507 - mcc: 0.8770\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9404 - loss: 0.1454\n","Epoch 19 - MCC: 0.8745\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9403 - loss: 0.1456 - val_accuracy: 0.9375 - val_loss: 0.1532 - mcc: 0.8745\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9331 - loss: 0.1620\n","Epoch 20 - MCC: 0.8809\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9333 - loss: 0.1616 - val_accuracy: 0.9406 - val_loss: 0.1462 - mcc: 0.8809\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9417 - loss: 0.1438\n","Epoch 21 - MCC: 0.8796\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9416 - loss: 0.1440 - val_accuracy: 0.9400 - val_loss: 0.1460 - mcc: 0.8796\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9431 - loss: 0.1392\n","Epoch 22 - MCC: 0.8815\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9430 - loss: 0.1394 - val_accuracy: 0.9410 - val_loss: 0.1451 - mcc: 0.8815\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9438 - loss: 0.1380\n","Epoch 23 - MCC: 0.8831\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9437 - loss: 0.1381 - val_accuracy: 0.9418 - val_loss: 0.1429 - mcc: 0.8831\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9426 - loss: 0.1413\n","Epoch 24 - MCC: 0.8836\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9426 - loss: 0.1413 - val_accuracy: 0.9420 - val_loss: 0.1417 - mcc: 0.8836\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9447 - loss: 0.1369\n","Epoch 25 - MCC: 0.8867\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9447 - loss: 0.1370 - val_accuracy: 0.9436 - val_loss: 0.1403 - mcc: 0.8867\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9467 - loss: 0.1318\n","Epoch 26 - MCC: 0.8774\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9466 - loss: 0.1321 - val_accuracy: 0.9389 - val_loss: 0.1487 - mcc: 0.8774\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9392 - loss: 0.1476\n","Epoch 27 - MCC: 0.8892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9393 - loss: 0.1473 - val_accuracy: 0.9448 - val_loss: 0.1371 - mcc: 0.8892\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9419 - loss: 0.1408\n","Epoch 28 - MCC: 0.8895\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9420 - loss: 0.1406 - val_accuracy: 0.9450 - val_loss: 0.1354 - mcc: 0.8895\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9433 - loss: 0.1381\n","Epoch 29 - MCC: 0.8886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9434 - loss: 0.1380 - val_accuracy: 0.9445 - val_loss: 0.1364 - mcc: 0.8886\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9419 - loss: 0.1398\n","Epoch 30 - MCC: 0.8869\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9421 - loss: 0.1396 - val_accuracy: 0.9437 - val_loss: 0.1368 - mcc: 0.8869\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5992 - loss: 0.6576\n","Epoch 1 - MCC: 0.6790\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.6026 - loss: 0.6559 - val_accuracy: 0.8393 - val_loss: 0.5011 - mcc: 0.6790\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8486 - loss: 0.4314\n","Epoch 2 - MCC: 0.7738\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.8488 - loss: 0.4295 - val_accuracy: 0.8872 - val_loss: 0.2798 - mcc: 0.7738\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8836 - loss: 0.2824\n","Epoch 3 - MCC: 0.8091\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8838 - loss: 0.2820 - val_accuracy: 0.9048 - val_loss: 0.2332 - mcc: 0.8091\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8971 - loss: 0.2481\n","Epoch 4 - MCC: 0.8287\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.8973 - loss: 0.2476 - val_accuracy: 0.9145 - val_loss: 0.2105 - mcc: 0.8287\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9065 - loss: 0.2225\n","Epoch 5 - MCC: 0.8423\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9066 - loss: 0.2223 - val_accuracy: 0.9213 - val_loss: 0.1944 - mcc: 0.8423\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9143 - loss: 0.2040\n","Epoch 6 - MCC: 0.8524\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9144 - loss: 0.2038 - val_accuracy: 0.9263 - val_loss: 0.1844 - mcc: 0.8524\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9206 - loss: 0.1891\n","Epoch 7 - MCC: 0.8571\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.9206 - loss: 0.1891 - val_accuracy: 0.9287 - val_loss: 0.1768 - mcc: 0.8571\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9221 - loss: 0.1857\n","Epoch 8 - MCC: 0.8629\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9222 - loss: 0.1855 - val_accuracy: 0.9316 - val_loss: 0.1713 - mcc: 0.8629\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9217 - loss: 0.1862\n","Epoch 9 - MCC: 0.8646\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9218 - loss: 0.1859 - val_accuracy: 0.9324 - val_loss: 0.1686 - mcc: 0.8646\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9285 - loss: 0.1710\n","Epoch 10 - MCC: 0.8697\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9285 - loss: 0.1710 - val_accuracy: 0.9350 - val_loss: 0.1607 - mcc: 0.8697\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9285 - loss: 0.1706\n","Epoch 11 - MCC: 0.8739\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9286 - loss: 0.1706 - val_accuracy: 0.9370 - val_loss: 0.1576 - mcc: 0.8739\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9322 - loss: 0.1648\n","Epoch 12 - MCC: 0.8721\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9321 - loss: 0.1649 - val_accuracy: 0.9361 - val_loss: 0.1558 - mcc: 0.8721\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9348 - loss: 0.1579\n","Epoch 13 - MCC: 0.8795\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9348 - loss: 0.1580 - val_accuracy: 0.9399 - val_loss: 0.1498 - mcc: 0.8795\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9397 - loss: 0.1461\n","Epoch 14 - MCC: 0.8814\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9396 - loss: 0.1465 - val_accuracy: 0.9408 - val_loss: 0.1483 - mcc: 0.8814\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9372 - loss: 0.1527\n","Epoch 15 - MCC: 0.8754\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9371 - loss: 0.1529 - val_accuracy: 0.9377 - val_loss: 0.1501 - mcc: 0.8754\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9399 - loss: 0.1450\n","Epoch 16 - MCC: 0.8806\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9398 - loss: 0.1453 - val_accuracy: 0.9405 - val_loss: 0.1457 - mcc: 0.8806\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9380 - loss: 0.1494\n","Epoch 17 - MCC: 0.8822\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9380 - loss: 0.1495 - val_accuracy: 0.9412 - val_loss: 0.1425 - mcc: 0.8822\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9338 - loss: 0.1588\n","Epoch 18 - MCC: 0.8826\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9340 - loss: 0.1585 - val_accuracy: 0.9414 - val_loss: 0.1437 - mcc: 0.8826\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9396 - loss: 0.1456\n","Epoch 19 - MCC: 0.8854\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9395 - loss: 0.1457 - val_accuracy: 0.9428 - val_loss: 0.1436 - mcc: 0.8854\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9372 - loss: 0.1519\n","Epoch 20 - MCC: 0.8890\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9372 - loss: 0.1517 - val_accuracy: 0.9445 - val_loss: 0.1372 - mcc: 0.8890\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9425 - loss: 0.1403\n","Epoch 21 - MCC: 0.8893\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9424 - loss: 0.1405 - val_accuracy: 0.9447 - val_loss: 0.1368 - mcc: 0.8893\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9432 - loss: 0.1403\n","Epoch 22 - MCC: 0.8836\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9432 - loss: 0.1404 - val_accuracy: 0.9419 - val_loss: 0.1426 - mcc: 0.8836\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9424 - loss: 0.1426\n","Epoch 23 - MCC: 0.8920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9424 - loss: 0.1425 - val_accuracy: 0.9461 - val_loss: 0.1326 - mcc: 0.8920\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9435 - loss: 0.1393\n","Epoch 24 - MCC: 0.8921\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9435 - loss: 0.1393 - val_accuracy: 0.9461 - val_loss: 0.1367 - mcc: 0.8921\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9442 - loss: 0.1390\n","Epoch 25 - MCC: 0.8943\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9442 - loss: 0.1390 - val_accuracy: 0.9473 - val_loss: 0.1308 - mcc: 0.8943\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9438 - loss: 0.1378\n","Epoch 26 - MCC: 0.8969\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9437 - loss: 0.1378 - val_accuracy: 0.9486 - val_loss: 0.1291 - mcc: 0.8969\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9506 - loss: 0.1228\n","Epoch 27 - MCC: 0.8951\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9503 - loss: 0.1233 - val_accuracy: 0.9476 - val_loss: 0.1304 - mcc: 0.8951\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9478 - loss: 0.1284\n","Epoch 28 - MCC: 0.8928\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9477 - loss: 0.1287 - val_accuracy: 0.9465 - val_loss: 0.1312 - mcc: 0.8928\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9435 - loss: 0.1380\n","Epoch 29 - MCC: 0.8969\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9435 - loss: 0.1378 - val_accuracy: 0.9485 - val_loss: 0.1274 - mcc: 0.8969\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9469 - loss: 0.1296\n","Epoch 30 - MCC: 0.9008\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9469 - loss: 0.1297 - val_accuracy: 0.9505 - val_loss: 0.1239 - mcc: 0.9008\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5505 - loss: 0.6389\n","Epoch 1 - MCC: 0.6080\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.5538 - loss: 0.6375 - val_accuracy: 0.7984 - val_loss: 0.5195 - mcc: 0.6080\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8244 - loss: 0.4625\n","Epoch 2 - MCC: 0.7306\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8250 - loss: 0.4605 - val_accuracy: 0.8648 - val_loss: 0.3165 - mcc: 0.7306\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8857 - loss: 0.2716\n","Epoch 3 - MCC: 0.7895\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.8859 - loss: 0.2713 - val_accuracy: 0.8949 - val_loss: 0.2512 - mcc: 0.7895\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9060 - loss: 0.2267\n","Epoch 4 - MCC: 0.8126\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9060 - loss: 0.2265 - val_accuracy: 0.9064 - val_loss: 0.2235 - mcc: 0.8126\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9069 - loss: 0.2196\n","Epoch 5 - MCC: 0.8300\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9073 - loss: 0.2190 - val_accuracy: 0.9152 - val_loss: 0.2034 - mcc: 0.8300\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9238 - loss: 0.1819\n","Epoch 6 - MCC: 0.8332\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9237 - loss: 0.1821 - val_accuracy: 0.9166 - val_loss: 0.1971 - mcc: 0.8332\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9228 - loss: 0.1845\n","Epoch 7 - MCC: 0.8404\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9229 - loss: 0.1843 - val_accuracy: 0.9203 - val_loss: 0.1886 - mcc: 0.8404\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9302 - loss: 0.1704\n","Epoch 8 - MCC: 0.8444\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9302 - loss: 0.1704 - val_accuracy: 0.9219 - val_loss: 0.1866 - mcc: 0.8444\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9278 - loss: 0.1729\n","Epoch 9 - MCC: 0.8524\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9279 - loss: 0.1728 - val_accuracy: 0.9262 - val_loss: 0.1771 - mcc: 0.8524\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9345 - loss: 0.1602\n","Epoch 10 - MCC: 0.8462\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9345 - loss: 0.1602 - val_accuracy: 0.9231 - val_loss: 0.1810 - mcc: 0.8462\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9316 - loss: 0.1662\n","Epoch 11 - MCC: 0.8565\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9317 - loss: 0.1659 - val_accuracy: 0.9284 - val_loss: 0.1700 - mcc: 0.8565\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9355 - loss: 0.1576\n","Epoch 12 - MCC: 0.8626\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9356 - loss: 0.1574 - val_accuracy: 0.9313 - val_loss: 0.1648 - mcc: 0.8626\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9356 - loss: 0.1566\n","Epoch 13 - MCC: 0.8681\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9357 - loss: 0.1564 - val_accuracy: 0.9341 - val_loss: 0.1611 - mcc: 0.8681\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9421 - loss: 0.1430\n","Epoch 14 - MCC: 0.8657\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9420 - loss: 0.1432 - val_accuracy: 0.9326 - val_loss: 0.1651 - mcc: 0.8657\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9385 - loss: 0.1541\n","Epoch 15 - MCC: 0.8653\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9386 - loss: 0.1538 - val_accuracy: 0.9319 - val_loss: 0.1700 - mcc: 0.8653\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9424 - loss: 0.1420\n","Epoch 16 - MCC: 0.8739\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9423 - loss: 0.1421 - val_accuracy: 0.9366 - val_loss: 0.1578 - mcc: 0.8739\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9406 - loss: 0.1462\n","Epoch 17 - MCC: 0.8718\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9407 - loss: 0.1461 - val_accuracy: 0.9360 - val_loss: 0.1538 - mcc: 0.8718\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9435 - loss: 0.1397\n","Epoch 18 - MCC: 0.8793\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9435 - loss: 0.1397 - val_accuracy: 0.9397 - val_loss: 0.1497 - mcc: 0.8793\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9455 - loss: 0.1334\n","Epoch 19 - MCC: 0.8799\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9455 - loss: 0.1335 - val_accuracy: 0.9398 - val_loss: 0.1498 - mcc: 0.8799\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9435 - loss: 0.1402\n","Epoch 20 - MCC: 0.8798\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9436 - loss: 0.1400 - val_accuracy: 0.9398 - val_loss: 0.1473 - mcc: 0.8798\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9490 - loss: 0.1287\n","Epoch 21 - MCC: 0.8754\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9489 - loss: 0.1289 - val_accuracy: 0.9378 - val_loss: 0.1494 - mcc: 0.8754\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9478 - loss: 0.1287\n","Epoch 22 - MCC: 0.8787\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9477 - loss: 0.1289 - val_accuracy: 0.9392 - val_loss: 0.1481 - mcc: 0.8787\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9491 - loss: 0.1287\n","Epoch 23 - MCC: 0.8803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9490 - loss: 0.1288 - val_accuracy: 0.9402 - val_loss: 0.1441 - mcc: 0.8803\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9486 - loss: 0.1290\n","Epoch 24 - MCC: 0.8794\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9486 - loss: 0.1291 - val_accuracy: 0.9398 - val_loss: 0.1472 - mcc: 0.8794\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9492 - loss: 0.1272\n","Epoch 25 - MCC: 0.8822\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9491 - loss: 0.1273 - val_accuracy: 0.9407 - val_loss: 0.1455 - mcc: 0.8822\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9485 - loss: 0.1289\n","Epoch 26 - MCC: 0.8757\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9485 - loss: 0.1290 - val_accuracy: 0.9369 - val_loss: 0.1554 - mcc: 0.8757\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9505 - loss: 0.1239\n","Epoch 27 - MCC: 0.8812\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9504 - loss: 0.1241 - val_accuracy: 0.9406 - val_loss: 0.1419 - mcc: 0.8812\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9479 - loss: 0.1278\n","Epoch 28 - MCC: 0.8856\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9480 - loss: 0.1277 - val_accuracy: 0.9429 - val_loss: 0.1374 - mcc: 0.8856\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9529 - loss: 0.1161\n","Epoch 29 - MCC: 0.8860\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9528 - loss: 0.1165 - val_accuracy: 0.9431 - val_loss: 0.1387 - mcc: 0.8860\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9524 - loss: 0.1189\n","Epoch 30 - MCC: 0.8814\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9523 - loss: 0.1191 - val_accuracy: 0.9408 - val_loss: 0.1390 - mcc: 0.8814\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9512466666666667,\n","              'mean': 0.9443253333333335,\n","              'min': 0.93542,\n","              'std': 0.005967634781794951},\n"," 'Inference Time (s/sample)': {'max': 0.0037422752380371093,\n","                               'mean': 0.00359860610961914,\n","                               'min': 0.0035293126106262208,\n","                               'std': 7.869711437962122e-05},\n"," 'MCC': {'max': 0.902131877038278,\n","         'mean': 0.8883548209419013,\n","         'min': 0.8705347285447775,\n","         'std': 0.011925881602494099},\n"," 'Parameters': 11297,\n"," 'Train Time (s)': {'max': 88.03604531288147,\n","                    'mean': 84.75726900100707,\n","                    'min': 79.86635851860046,\n","                    'std': 2.7559340594319313},\n"," 'Training Accuracy': [[0.5908049941062927,\n","                        0.819546639919281,\n","                        0.8737199902534485,\n","                        0.8975182175636292,\n","                        0.9071033000946045,\n","                        0.9139050245285034,\n","                        0.9207967519760132,\n","                        0.9264650940895081,\n","                        0.9286350011825562,\n","                        0.9325000047683716,\n","                        0.9352648854255676,\n","                        0.9351784586906433,\n","                        0.937084972858429,\n","                        0.939228355884552,\n","                        0.9401084184646606,\n","                        0.9413484334945679,\n","                        0.9417850375175476,\n","                        0.9434983134269714,\n","                        0.9434550404548645,\n","                        0.9442867040634155,\n","                        0.9452615976333618,\n","                        0.9451165795326233,\n","                        0.9454250931739807,\n","                        0.9442816376686096,\n","                        0.9453366994857788,\n","                        0.9472882747650146,\n","                        0.9469299912452698,\n","                        0.9482517242431641,\n","                        0.9482266902923584,\n","                        0.947866678237915],\n","                       [0.6006850004196167,\n","                        0.8217399716377258,\n","                        0.8937382698059082,\n","                        0.9119465351104736,\n","                        0.9179316759109497,\n","                        0.9217032790184021,\n","                        0.9251651167869568,\n","                        0.9284499287605286,\n","                        0.9305083751678467,\n","                        0.9318898916244507,\n","                        0.9341183304786682,\n","                        0.9358433485031128,\n","                        0.9360401034355164,\n","                        0.9385316371917725,\n","                        0.9399784207344055,\n","                        0.940915048122406,\n","                        0.9414700269699097,\n","                        0.9422166347503662,\n","                        0.9428249597549438,\n","                        0.943238377571106,\n","                        0.9447650909423828,\n","                        0.9443985223770142,\n","                        0.9445565938949585,\n","                        0.9459400177001953,\n","                        0.9459666609764099,\n","                        0.9461699724197388,\n","                        0.9466517567634583,\n","                        0.9472900629043579,\n","                        0.9466516375541687,\n","                        0.9458833932876587],\n","                       [0.6212150454521179,\n","                        0.8572115302085876,\n","                        0.8884716033935547,\n","                        0.905660092830658,\n","                        0.9108515977859497,\n","                        0.9154998660087585,\n","                        0.919283390045166,\n","                        0.9232349991798401,\n","                        0.9256400465965271,\n","                        0.9272883534431458,\n","                        0.9295966625213623,\n","                        0.9305333495140076,\n","                        0.9322882890701294,\n","                        0.9337934851646423,\n","                        0.933138370513916,\n","                        0.9358548521995544,\n","                        0.9356516003608704,\n","                        0.9377332925796509,\n","                        0.9379916191101074,\n","                        0.9380966424942017,\n","                        0.9399400353431702,\n","                        0.9408617615699768,\n","                        0.941994845867157,\n","                        0.9418299198150635,\n","                        0.9433199167251587,\n","                        0.9434349536895752,\n","                        0.9428533315658569,\n","                        0.9443967342376709,\n","                        0.9454098343849182,\n","                        0.9452300071716309],\n","                       [0.6880682110786438,\n","                        0.8527200222015381,\n","                        0.8891449570655823,\n","                        0.9024933576583862,\n","                        0.9100683331489563,\n","                        0.9164767265319824,\n","                        0.920708417892456,\n","                        0.9241383075714111,\n","                        0.925571620464325,\n","                        0.9285016059875488,\n","                        0.9298032522201538,\n","                        0.93049156665802,\n","                        0.9335101246833801,\n","                        0.9355100393295288,\n","                        0.9355383515357971,\n","                        0.937043309211731,\n","                        0.937613308429718,\n","                        0.9372700452804565,\n","                        0.938373327255249,\n","                        0.9394365549087524,\n","                        0.9406683444976807,\n","                        0.9419333934783936,\n","                        0.9421767592430115,\n","                        0.9434133172035217,\n","                        0.9434533715248108,\n","                        0.9434800148010254,\n","                        0.9434917569160461,\n","                        0.9440051913261414,\n","                        0.9445549249649048,\n","                        0.9449833035469055],\n","                       [0.6366150379180908,\n","                        0.8392349481582642,\n","                        0.8910598754882812,\n","                        0.9072383046150208,\n","                        0.9152216911315918,\n","                        0.9215866923332214,\n","                        0.9252533912658691,\n","                        0.9289233684539795,\n","                        0.9300766587257385,\n","                        0.9345800876617432,\n","                        0.9349648952484131,\n","                        0.93805330991745,\n","                        0.9392850399017334,\n","                        0.939746618270874,\n","                        0.9408334493637085,\n","                        0.9409531950950623,\n","                        0.9422498941421509,\n","                        0.943816602230072,\n","                        0.9443451166152954,\n","                        0.9454064965248108,\n","                        0.9463250041007996,\n","                        0.9452633857727051,\n","                        0.9468966126441956,\n","                        0.947608232498169,\n","                        0.9478683471679688,\n","                        0.9475248456001282,\n","                        0.9484666585922241,\n","                        0.949156641960144,\n","                        0.9496232867240906,\n","                        0.9504300951957703]],\n"," 'Training Loss': [[0.620698869228363,\n","                    0.4286797344684601,\n","                    0.29594549536705017,\n","                    0.24583911895751953,\n","                    0.22392383217811584,\n","                    0.2070925086736679,\n","                    0.19148096442222595,\n","                    0.17974889278411865,\n","                    0.17384956777095795,\n","                    0.16605377197265625,\n","                    0.16030043363571167,\n","                    0.16052383184432983,\n","                    0.1549624502658844,\n","                    0.15199333429336548,\n","                    0.1498643457889557,\n","                    0.14538496732711792,\n","                    0.1448667198419571,\n","                    0.1411292999982834,\n","                    0.1407511830329895,\n","                    0.13902179896831512,\n","                    0.13704410195350647,\n","                    0.13649657368659973,\n","                    0.134987473487854,\n","                    0.13738471269607544,\n","                    0.13517044484615326,\n","                    0.13226166367530823,\n","                    0.13057272136211395,\n","                    0.12875215709209442,\n","                    0.12772850692272186,\n","                    0.12939652800559998],\n","                   [0.6434085369110107,\n","                    0.47681549191474915,\n","                    0.26679322123527527,\n","                    0.21355074644088745,\n","                    0.1968657523393631,\n","                    0.18823444843292236,\n","                    0.17955276370048523,\n","                    0.1715720295906067,\n","                    0.1660158783197403,\n","                    0.16292455792427063,\n","                    0.15816286206245422,\n","                    0.1546717882156372,\n","                    0.15452821552753448,\n","                    0.14946281909942627,\n","                    0.14621283113956451,\n","                    0.14346706867218018,\n","                    0.1425982415676117,\n","                    0.14168189465999603,\n","                    0.13967543840408325,\n","                    0.13924701511859894,\n","                    0.13622772693634033,\n","                    0.13518185913562775,\n","                    0.1352241486310959,\n","                    0.13196223974227905,\n","                    0.1319822371006012,\n","                    0.13078081607818604,\n","                    0.13077802956104279,\n","                    0.1287696361541748,\n","                    0.13005290925502777,\n","                    0.13158808648586273],\n","                   [0.6020656228065491,\n","                    0.3983365297317505,\n","                    0.2787456810474396,\n","                    0.23197761178016663,\n","                    0.2168758064508438,\n","                    0.20525091886520386,\n","                    0.19554920494556427,\n","                    0.18610763549804688,\n","                    0.18084776401519775,\n","                    0.17474466562271118,\n","                    0.1698388308286667,\n","                    0.16753873229026794,\n","                    0.1639515459537506,\n","                    0.15981128811836243,\n","                    0.16113415360450745,\n","                    0.1558975726366043,\n","                    0.15649519860744476,\n","                    0.1513902246952057,\n","                    0.15054593980312347,\n","                    0.15061791241168976,\n","                    0.14731845259666443,\n","                    0.14494623243808746,\n","                    0.14182013273239136,\n","                    0.14188094437122345,\n","                    0.13943618535995483,\n","                    0.13883469998836517,\n","                    0.1398647278547287,\n","                    0.13661542534828186,\n","                    0.13459639251232147,\n","                    0.13424170017242432],\n","                   [0.6143160462379456,\n","                    0.3818671405315399,\n","                    0.27134236693382263,\n","                    0.234598770737648,\n","                    0.21512317657470703,\n","                    0.1992773562669754,\n","                    0.18933382630348206,\n","                    0.1815679371356964,\n","                    0.17824847996234894,\n","                    0.17170673608779907,\n","                    0.16833902895450592,\n","                    0.16761161386966705,\n","                    0.16140159964561462,\n","                    0.15660396218299866,\n","                    0.15602022409439087,\n","                    0.15260456502437592,\n","                    0.15166758000850677,\n","                    0.15205159783363342,\n","                    0.14867961406707764,\n","                    0.14732949435710907,\n","                    0.14454202353954315,\n","                    0.142610564827919,\n","                    0.14144191145896912,\n","                    0.13884247839450836,\n","                    0.1391962319612503,\n","                    0.13911278545856476,\n","                    0.13772088289260864,\n","                    0.13671857118606567,\n","                    0.13532038033008575,\n","                    0.13392433524131775],\n","                   [0.6036750674247742,\n","                    0.40977951884269714,\n","                    0.26165926456451416,\n","                    0.22249649465084076,\n","                    0.20222890377044678,\n","                    0.18718045949935913,\n","                    0.17922864854335785,\n","                    0.17279456555843353,\n","                    0.1690155416727066,\n","                    0.16008567810058594,\n","                    0.159170463681221,\n","                    0.1527150571346283,\n","                    0.149417445063591,\n","                    0.14749851822853088,\n","                    0.14675261080265045,\n","                    0.14498671889305115,\n","                    0.14213652908802032,\n","                    0.13895809650421143,\n","                    0.1365646868944168,\n","                    0.13526387512683868,\n","                    0.13381271064281464,\n","                    0.13528123497962952,\n","                    0.132507786154747,\n","                    0.13068895041942596,\n","                    0.12928549945354462,\n","                    0.13072778284549713,\n","                    0.1286582052707672,\n","                    0.12571382522583008,\n","                    0.12425515800714493,\n","                    0.12386191636323929]],\n"," 'Validation Accuracy': [[0.7369533777236938,\n","                          0.8701799511909485,\n","                          0.8966333270072937,\n","                          0.9115000367164612,\n","                          0.9171066284179688,\n","                          0.9252200126647949,\n","                          0.9309332966804504,\n","                          0.9339333176612854,\n","                          0.937386691570282,\n","                          0.9387466311454773,\n","                          0.9391332864761353,\n","                          0.9428333044052124,\n","                          0.9454399943351746,\n","                          0.9456533193588257,\n","                          0.9440867900848389,\n","                          0.9485399723052979,\n","                          0.9470800757408142,\n","                          0.9483999609947205,\n","                          0.9496066570281982,\n","                          0.9509133100509644,\n","                          0.9510933756828308,\n","                          0.9511666893959045,\n","                          0.9521000385284424,\n","                          0.9521467089653015,\n","                          0.9529199004173279,\n","                          0.9531333446502686,\n","                          0.9536533355712891,\n","                          0.9530266523361206,\n","                          0.9542400240898132,\n","                          0.9512467384338379],\n","                         [0.7271400690078735,\n","                          0.8516533374786377,\n","                          0.8934667110443115,\n","                          0.9035066962242126,\n","                          0.9065133929252625,\n","                          0.911786675453186,\n","                          0.9170466661453247,\n","                          0.9187999963760376,\n","                          0.9193066358566284,\n","                          0.9230400323867798,\n","                          0.923746645450592,\n","                          0.9270333051681519,\n","                          0.9254133701324463,\n","                          0.9290667176246643,\n","                          0.9275267124176025,\n","                          0.929360032081604,\n","                          0.9319133162498474,\n","                          0.9308466911315918,\n","                          0.9303066730499268,\n","                          0.9296600222587585,\n","                          0.9355133175849915,\n","                          0.9363932609558105,\n","                          0.9328932762145996,\n","                          0.935166597366333,\n","                          0.9351999759674072,\n","                          0.9369201064109802,\n","                          0.933793306350708,\n","                          0.9314000010490417,\n","                          0.9344599843025208,\n","                          0.9354200959205627],\n","                         [0.840739905834198,\n","                          0.8756599426269531,\n","                          0.9043867588043213,\n","                          0.9080800414085388,\n","                          0.9173466563224792,\n","                          0.921446681022644,\n","                          0.9252933263778687,\n","                          0.9277400374412537,\n","                          0.9276199340820312,\n","                          0.9313066601753235,\n","                          0.9325600266456604,\n","                          0.9340667128562927,\n","                          0.9335200786590576,\n","                          0.9362866878509521,\n","                          0.9368133544921875,\n","                          0.9364666938781738,\n","                          0.9373133778572083,\n","                          0.9386932849884033,\n","                          0.9374600052833557,\n","                          0.9406334161758423,\n","                          0.9400333166122437,\n","                          0.9409933686256409,\n","                          0.9417533874511719,\n","                          0.942026674747467,\n","                          0.9435867071151733,\n","                          0.938926637172699,\n","                          0.9447733163833618,\n","                          0.9449665546417236,\n","                          0.9445000290870667,\n","                          0.9436800479888916],\n","                         [0.8392866253852844,\n","                          0.8871734142303467,\n","                          0.9047867059707642,\n","                          0.9145466685295105,\n","                          0.9212532639503479,\n","                          0.92631995677948,\n","                          0.9286999702453613,\n","                          0.9315999746322632,\n","                          0.9323599338531494,\n","                          0.9350066184997559,\n","                          0.9370266795158386,\n","                          0.9361400008201599,\n","                          0.9398800134658813,\n","                          0.9407733678817749,\n","                          0.9377334117889404,\n","                          0.9404666423797607,\n","                          0.9412065744400024,\n","                          0.9414332509040833,\n","                          0.9428199529647827,\n","                          0.9445332884788513,\n","                          0.9447067379951477,\n","                          0.9419400095939636,\n","                          0.9461200833320618,\n","                          0.9460933208465576,\n","                          0.9472599625587463,\n","                          0.9485599994659424,\n","                          0.9476332664489746,\n","                          0.9465200901031494,\n","                          0.9484933614730835,\n","                          0.9504866003990173],\n","                         [0.7983999848365784,\n","                          0.8647533655166626,\n","                          0.8949466943740845,\n","                          0.9063532948493958,\n","                          0.9151600003242493,\n","                          0.9165732860565186,\n","                          0.9203466773033142,\n","                          0.921893298625946,\n","                          0.9261667728424072,\n","                          0.9231333136558533,\n","                          0.9283933043479919,\n","                          0.931273341178894,\n","                          0.9340733289718628,\n","                          0.9325733184814453,\n","                          0.9319466948509216,\n","                          0.9366466403007507,\n","                          0.9360133409500122,\n","                          0.939740002155304,\n","                          0.9397733211517334,\n","                          0.9397866725921631,\n","                          0.9378200769424438,\n","                          0.9391800165176392,\n","                          0.9402400255203247,\n","                          0.9397933483123779,\n","                          0.9407066106796265,\n","                          0.9368733763694763,\n","                          0.940613329410553,\n","                          0.9429066181182861,\n","                          0.9430999755859375,\n","                          0.9407933950424194]],\n"," 'Validation Loss': [[0.5348650217056274,\n","                      0.31958335638046265,\n","                      0.2449689358472824,\n","                      0.21160000562667847,\n","                      0.19900569319725037,\n","                      0.17971237003803253,\n","                      0.1679757833480835,\n","                      0.1582409143447876,\n","                      0.15431565046310425,\n","                      0.14792436361312866,\n","                      0.14713448286056519,\n","                      0.1410863846540451,\n","                      0.13682861626148224,\n","                      0.13540804386138916,\n","                      0.1364026516675949,\n","                      0.12852385640144348,\n","                      0.13036464154720306,\n","                      0.1277034729719162,\n","                      0.12488331645727158,\n","                      0.1223292350769043,\n","                      0.12104464322328568,\n","                      0.12187383323907852,\n","                      0.119649738073349,\n","                      0.11884736269712448,\n","                      0.11752425134181976,\n","                      0.11548268049955368,\n","                      0.11478263884782791,\n","                      0.11576928198337555,\n","                      0.1155899316072464,\n","                      0.122024305164814],\n","                     [0.5738580822944641,\n","                      0.37812960147857666,\n","                      0.25608718395233154,\n","                      0.22981345653533936,\n","                      0.22158795595169067,\n","                      0.21066871285438538,\n","                      0.19684678316116333,\n","                      0.19223880767822266,\n","                      0.19433225691318512,\n","                      0.18494850397109985,\n","                      0.18410779535770416,\n","                      0.17875832319259644,\n","                      0.18153338134288788,\n","                      0.1718050241470337,\n","                      0.1743534654378891,\n","                      0.17063166201114655,\n","                      0.1681104153394699,\n","                      0.17100316286087036,\n","                      0.16789136826992035,\n","                      0.1710229516029358,\n","                      0.16131453216075897,\n","                      0.1616339534521103,\n","                      0.16465725004673004,\n","                      0.16002988815307617,\n","                      0.16001197695732117,\n","                      0.15718960762023926,\n","                      0.16484323143959045,\n","                      0.16700293123722076,\n","                      0.1597619205713272,\n","                      0.157552570104599],\n","                     [0.5042209625244141,\n","                      0.31252339482307434,\n","                      0.2360539436340332,\n","                      0.22061052918434143,\n","                      0.2030005007982254,\n","                      0.1935501992702484,\n","                      0.18277885019779205,\n","                      0.1749216467142105,\n","                      0.17236851155757904,\n","                      0.1683691143989563,\n","                      0.16229620575904846,\n","                      0.15983237326145172,\n","                      0.16096848249435425,\n","                      0.15793928503990173,\n","                      0.1537085473537445,\n","                      0.15607395768165588,\n","                      0.151323601603508,\n","                      0.1506912261247635,\n","                      0.15323564410209656,\n","                      0.14615926146507263,\n","                      0.14603812992572784,\n","                      0.14513379335403442,\n","                      0.14293652772903442,\n","                      0.14171938598155975,\n","                      0.14026598632335663,\n","                      0.1486501693725586,\n","                      0.13707953691482544,\n","                      0.13542994856834412,\n","                      0.13638760149478912,\n","                      0.1367781013250351],\n","                     [0.5010895729064941,\n","                      0.2798364758491516,\n","                      0.23318509757518768,\n","                      0.2104836255311966,\n","                      0.19444018602371216,\n","                      0.1843758076429367,\n","                      0.17679430544376373,\n","                      0.1712852418422699,\n","                      0.16860796511173248,\n","                      0.1606501191854477,\n","                      0.15757495164871216,\n","                      0.1558479517698288,\n","                      0.14979001879692078,\n","                      0.14829297363758087,\n","                      0.15005570650100708,\n","                      0.1457487940788269,\n","                      0.14253874123096466,\n","                      0.14365333318710327,\n","                      0.14363223314285278,\n","                      0.13718050718307495,\n","                      0.13680139183998108,\n","                      0.14255981147289276,\n","                      0.1326243132352829,\n","                      0.1366889923810959,\n","                      0.13077060878276825,\n","                      0.12907914817333221,\n","                      0.13038316369056702,\n","                      0.13119558990001678,\n","                      0.1273840218782425,\n","                      0.12393540143966675],\n","                     [0.519496500492096,\n","                      0.3165050745010376,\n","                      0.2511521279811859,\n","                      0.22345630824565887,\n","                      0.2034044861793518,\n","                      0.19714584946632385,\n","                      0.18862365186214447,\n","                      0.18657493591308594,\n","                      0.17708244919776917,\n","                      0.1810469627380371,\n","                      0.17002052068710327,\n","                      0.16482408344745636,\n","                      0.16114678978919983,\n","                      0.1650848537683487,\n","                      0.17002560198307037,\n","                      0.15778066217899323,\n","                      0.15378928184509277,\n","                      0.14971885085105896,\n","                      0.1497822105884552,\n","                      0.1472891867160797,\n","                      0.14942960441112518,\n","                      0.14807696640491486,\n","                      0.14407223463058472,\n","                      0.14724282920360565,\n","                      0.14554983377456665,\n","                      0.1554267257452011,\n","                      0.14193195104599,\n","                      0.13740815222263336,\n","                      0.1387351006269455,\n","                      0.13904725015163422]],\n"," 'Validation MCC': [[0.514298400276022,\n","                     0.74099274944588,\n","                     0.7924203886370149,\n","                     0.8227541975829648,\n","                     0.8341051350297973,\n","                     0.8498605650432646,\n","                     0.8614922250445878,\n","                     0.8674000237774927,\n","                     0.8745113312199074,\n","                     0.8771889286329285,\n","                     0.8778700131343982,\n","                     0.8853823603587088,\n","                     0.8905216291354091,\n","                     0.8911211277964205,\n","                     0.8880301048141356,\n","                     0.8967343078471582,\n","                     0.8940087463135282,\n","                     0.8964988202061965,\n","                     0.8988942666671358,\n","                     0.9015087802620759,\n","                     0.9019972884163411,\n","                     0.9019861460874914,\n","                     0.9040744230763982,\n","                     0.9039488788820352,\n","                     0.905494029924905,\n","                     0.9059164151195396,\n","                     0.9069640373777894,\n","                     0.9058320982757792,\n","                     0.9083519753620746,\n","                     0.902131877038278],\n","                    [0.5256035544649635,\n","                     0.7053046003285696,\n","                     0.786406815679906,\n","                     0.8067669600092503,\n","                     0.8144172270387389,\n","                     0.824711524615756,\n","                     0.8336983528427542,\n","                     0.8373488454862058,\n","                     0.8388400607600939,\n","                     0.8458022275828468,\n","                     0.8480206188471175,\n","                     0.853945514371224,\n","                     0.8516846726529592,\n","                     0.8578761124657968,\n","                     0.8547465499897616,\n","                     0.8585372452833753,\n","                     0.8635372799919898,\n","                     0.8613759868723218,\n","                     0.8603250613112436,\n","                     0.8590098907374416,\n","                     0.8707912520761218,\n","                     0.8724940416486172,\n","                     0.8658019875553014,\n","                     0.8700322878680593,\n","                     0.8705011860134644,\n","                     0.8736016602197307,\n","                     0.8672753430856514,\n","                     0.8630245340901376,\n","                     0.868624934239017,\n","                     0.8705347285447775],\n","                    [0.6987690386060781,\n","                     0.7515928915288521,\n","                     0.8079402821391457,\n","                     0.8172632672008268,\n","                     0.8342817682031037,\n","                     0.8433127632631189,\n","                     0.850024154567537,\n","                     0.8550541337031048,\n","                     0.8547675008558009,\n","                     0.8620403255750492,\n","                     0.8646968010400835,\n","                     0.8676284683140266,\n","                     0.8665830220127514,\n","                     0.8723879893816942,\n","                     0.8731167859698669,\n","                     0.8729804742975213,\n","                     0.8741346637917216,\n","                     0.8769560748023313,\n","                     0.8744618694308206,\n","                     0.8808905942741054,\n","                     0.8796361933138487,\n","                     0.8815260574578206,\n","                     0.8830758404048468,\n","                     0.8835774500715995,\n","                     0.8867365701447131,\n","                     0.8773512249549404,\n","                     0.8891574307042597,\n","                     0.8895339256841641,\n","                     0.8885885047631753,\n","                     0.8869254900553789],\n","                    [0.6790484893259692,\n","                     0.7737563692703918,\n","                     0.8091224725109081,\n","                     0.8287266523328268,\n","                     0.8422791683627523,\n","                     0.8524289427280967,\n","                     0.857137377651963,\n","                     0.8628631160826341,\n","                     0.8646148154605466,\n","                     0.869698116447181,\n","                     0.8738504905834459,\n","                     0.8720897147392784,\n","                     0.8794672262318403,\n","                     0.881357308931222,\n","                     0.8754205545780374,\n","                     0.8806441813875635,\n","                     0.8822484826968735,\n","                     0.8826125104148514,\n","                     0.8853649136116205,\n","                     0.8890304966395947,\n","                     0.8892655033670869,\n","                     0.8836437234054321,\n","                     0.8919815664587544,\n","                     0.892102074833647,\n","                     0.8942656840294286,\n","                     0.8968717260966765,\n","                     0.8951446163920105,\n","                     0.8927848263377077,\n","                     0.896862478784212,\n","                     0.9007541795872331],\n","                    [0.6079692153813989,\n","                     0.7306074069094128,\n","                     0.7895243447616765,\n","                     0.8125609410222895,\n","                     0.8300437162379444,\n","                     0.8331666478655618,\n","                     0.8403942459982845,\n","                     0.8444381634083657,\n","                     0.8523957180219559,\n","                     0.8461633624108775,\n","                     0.8565213901580777,\n","                     0.8625615629715009,\n","                     0.8680921524658595,\n","                     0.8657166276301167,\n","                     0.8653245120046751,\n","                     0.8739152401619158,\n","                     0.8717965162799992,\n","                     0.8793082671254443,\n","                     0.8799096222680618,\n","                     0.8797814407551363,\n","                     0.8754375222069052,\n","                     0.8787337730213758,\n","                     0.8802612345215027,\n","                     0.8793898394190803,\n","                     0.8822212637552775,\n","                     0.8757237719854779,\n","                     0.881214925650814,\n","                     0.8856264742824237,\n","                     0.8860470512482193,\n","                     0.8814278294838392]]}\n","Training Model: BiLSTM_Deep, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.6202 - loss: 0.6172\n","Epoch 1 - MCC: 0.7570\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 241ms/step - accuracy: 0.6244 - loss: 0.6136 - val_accuracy: 0.8784 - val_loss: 0.2947 - mcc: 0.7570\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8828 - loss: 0.2845\n","Epoch 2 - MCC: 0.8305\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 184ms/step - accuracy: 0.8832 - loss: 0.2837 - val_accuracy: 0.9153 - val_loss: 0.2075 - mcc: 0.8305\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9079 - loss: 0.2226\n","Epoch 3 - MCC: 0.8554\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.9081 - loss: 0.2222 - val_accuracy: 0.9280 - val_loss: 0.1771 - mcc: 0.8554\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9185 - loss: 0.1975\n","Epoch 4 - MCC: 0.8635\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 165ms/step - accuracy: 0.9186 - loss: 0.1971 - val_accuracy: 0.9313 - val_loss: 0.1695 - mcc: 0.8635\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9279 - loss: 0.1766\n","Epoch 5 - MCC: 0.8617\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - accuracy: 0.9279 - loss: 0.1766 - val_accuracy: 0.9308 - val_loss: 0.1622 - mcc: 0.8617\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9298 - loss: 0.1682\n","Epoch 6 - MCC: 0.8774\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 165ms/step - accuracy: 0.9298 - loss: 0.1683 - val_accuracy: 0.9386 - val_loss: 0.1508 - mcc: 0.8774\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9329 - loss: 0.1626\n","Epoch 7 - MCC: 0.8697\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.9327 - loss: 0.1628 - val_accuracy: 0.9339 - val_loss: 0.1586 - mcc: 0.8697\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9314 - loss: 0.1657\n","Epoch 8 - MCC: 0.8876\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 201ms/step - accuracy: 0.9314 - loss: 0.1657 - val_accuracy: 0.9439 - val_loss: 0.1389 - mcc: 0.8876\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9356 - loss: 0.1584\n","Epoch 9 - MCC: 0.8867\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9356 - loss: 0.1582 - val_accuracy: 0.9433 - val_loss: 0.1386 - mcc: 0.8867\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9410 - loss: 0.1446\n","Epoch 10 - MCC: 0.8809\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - accuracy: 0.9408 - loss: 0.1449 - val_accuracy: 0.9407 - val_loss: 0.1437 - mcc: 0.8809\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9409 - loss: 0.1442\n","Epoch 11 - MCC: 0.8907\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9408 - loss: 0.1445 - val_accuracy: 0.9454 - val_loss: 0.1329 - mcc: 0.8907\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9396 - loss: 0.1475\n","Epoch 12 - MCC: 0.8979\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9397 - loss: 0.1474 - val_accuracy: 0.9491 - val_loss: 0.1257 - mcc: 0.8979\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9342 - loss: 0.1585\n","Epoch 13 - MCC: 0.8942\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 201ms/step - accuracy: 0.9344 - loss: 0.1581 - val_accuracy: 0.9472 - val_loss: 0.1287 - mcc: 0.8942\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9421 - loss: 0.1405\n","Epoch 14 - MCC: 0.8967\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.9421 - loss: 0.1405 - val_accuracy: 0.9486 - val_loss: 0.1258 - mcc: 0.8967\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9447 - loss: 0.1358\n","Epoch 15 - MCC: 0.9012\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9446 - loss: 0.1359 - val_accuracy: 0.9508 - val_loss: 0.1188 - mcc: 0.9012\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9477 - loss: 0.1264\n","Epoch 16 - MCC: 0.8983\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 193ms/step - accuracy: 0.9476 - loss: 0.1267 - val_accuracy: 0.9493 - val_loss: 0.1210 - mcc: 0.8983\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9461 - loss: 0.1314\n","Epoch 17 - MCC: 0.9005\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9461 - loss: 0.1315 - val_accuracy: 0.9503 - val_loss: 0.1191 - mcc: 0.9005\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9514 - loss: 0.1185\n","Epoch 18 - MCC: 0.9032\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 180ms/step - accuracy: 0.9512 - loss: 0.1189 - val_accuracy: 0.9517 - val_loss: 0.1170 - mcc: 0.9032\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9434 - loss: 0.1351\n","Epoch 19 - MCC: 0.9071\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 179ms/step - accuracy: 0.9434 - loss: 0.1349 - val_accuracy: 0.9537 - val_loss: 0.1119 - mcc: 0.9071\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9521 - loss: 0.1184\n","Epoch 20 - MCC: 0.9053\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9520 - loss: 0.1187 - val_accuracy: 0.9528 - val_loss: 0.1161 - mcc: 0.9053\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9499 - loss: 0.1230\n","Epoch 21 - MCC: 0.9063\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.9498 - loss: 0.1231 - val_accuracy: 0.9533 - val_loss: 0.1125 - mcc: 0.9063\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9547 - loss: 0.1118\n","Epoch 22 - MCC: 0.9114\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9544 - loss: 0.1123 - val_accuracy: 0.9558 - val_loss: 0.1071 - mcc: 0.9114\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9473 - loss: 0.1309\n","Epoch 23 - MCC: 0.9080\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9473 - loss: 0.1307 - val_accuracy: 0.9542 - val_loss: 0.1115 - mcc: 0.9080\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9487 - loss: 0.1244\n","Epoch 24 - MCC: 0.9145\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.9487 - loss: 0.1242 - val_accuracy: 0.9574 - val_loss: 0.1016 - mcc: 0.9145\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9497 - loss: 0.1231\n","Epoch 25 - MCC: 0.9155\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9498 - loss: 0.1229 - val_accuracy: 0.9579 - val_loss: 0.1019 - mcc: 0.9155\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9510 - loss: 0.1190\n","Epoch 26 - MCC: 0.9116\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9510 - loss: 0.1189 - val_accuracy: 0.9559 - val_loss: 0.1045 - mcc: 0.9116\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9550 - loss: 0.1110\n","Epoch 27 - MCC: 0.9131\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.9549 - loss: 0.1112 - val_accuracy: 0.9567 - val_loss: 0.1048 - mcc: 0.9131\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9551 - loss: 0.1104\n","Epoch 28 - MCC: 0.9189\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 165ms/step - accuracy: 0.9550 - loss: 0.1105 - val_accuracy: 0.9595 - val_loss: 0.1005 - mcc: 0.9189\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9496 - loss: 0.1206\n","Epoch 29 - MCC: 0.9179\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - accuracy: 0.9498 - loss: 0.1203 - val_accuracy: 0.9591 - val_loss: 0.0996 - mcc: 0.9179\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9553 - loss: 0.1086\n","Epoch 30 - MCC: 0.9165\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9553 - loss: 0.1087 - val_accuracy: 0.9584 - val_loss: 0.1006 - mcc: 0.9165\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.6607 - loss: 0.5930\n","Epoch 1 - MCC: 0.7311\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 241ms/step - accuracy: 0.6642 - loss: 0.5894 - val_accuracy: 0.8659 - val_loss: 0.3264 - mcc: 0.7311\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8834 - loss: 0.2813\n","Epoch 2 - MCC: 0.8071\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.8839 - loss: 0.2802 - val_accuracy: 0.9034 - val_loss: 0.2294 - mcc: 0.8071\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9128 - loss: 0.2086\n","Epoch 3 - MCC: 0.8242\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9132 - loss: 0.2078 - val_accuracy: 0.9110 - val_loss: 0.2179 - mcc: 0.8242\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9237 - loss: 0.1864\n","Epoch 4 - MCC: 0.8421\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 164ms/step - accuracy: 0.9238 - loss: 0.1861 - val_accuracy: 0.9212 - val_loss: 0.1916 - mcc: 0.8421\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9300 - loss: 0.1693\n","Epoch 5 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 155ms/step - accuracy: 0.9301 - loss: 0.1690 - val_accuracy: 0.9231 - val_loss: 0.1859 - mcc: 0.8458\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9379 - loss: 0.1498\n","Epoch 6 - MCC: 0.8592\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - accuracy: 0.9378 - loss: 0.1500 - val_accuracy: 0.9297 - val_loss: 0.1729 - mcc: 0.8592\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9336 - loss: 0.1584\n","Epoch 7 - MCC: 0.8640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9338 - loss: 0.1581 - val_accuracy: 0.9321 - val_loss: 0.1669 - mcc: 0.8640\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9400 - loss: 0.1438\n","Epoch 8 - MCC: 0.8585\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9399 - loss: 0.1440 - val_accuracy: 0.9294 - val_loss: 0.1720 - mcc: 0.8585\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9392 - loss: 0.1498\n","Epoch 9 - MCC: 0.8619\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 189ms/step - accuracy: 0.9392 - loss: 0.1499 - val_accuracy: 0.9311 - val_loss: 0.1692 - mcc: 0.8619\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9399 - loss: 0.1455\n","Epoch 10 - MCC: 0.8624\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9399 - loss: 0.1454 - val_accuracy: 0.9309 - val_loss: 0.1658 - mcc: 0.8624\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9477 - loss: 0.1283\n","Epoch 11 - MCC: 0.8702\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.9476 - loss: 0.1284 - val_accuracy: 0.9350 - val_loss: 0.1566 - mcc: 0.8702\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9497 - loss: 0.1230\n","Epoch 12 - MCC: 0.8729\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9496 - loss: 0.1233 - val_accuracy: 0.9365 - val_loss: 0.1522 - mcc: 0.8729\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9492 - loss: 0.1247\n","Epoch 13 - MCC: 0.8797\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9492 - loss: 0.1247 - val_accuracy: 0.9399 - val_loss: 0.1468 - mcc: 0.8797\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9465 - loss: 0.1314\n","Epoch 14 - MCC: 0.8781\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 206ms/step - accuracy: 0.9467 - loss: 0.1310 - val_accuracy: 0.9392 - val_loss: 0.1466 - mcc: 0.8781\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9495 - loss: 0.1221\n","Epoch 15 - MCC: 0.8823\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.9495 - loss: 0.1221 - val_accuracy: 0.9413 - val_loss: 0.1438 - mcc: 0.8823\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9557 - loss: 0.1085\n","Epoch 16 - MCC: 0.8844\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - accuracy: 0.9556 - loss: 0.1088 - val_accuracy: 0.9422 - val_loss: 0.1435 - mcc: 0.8844\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9522 - loss: 0.1183\n","Epoch 17 - MCC: 0.8827\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 192ms/step - accuracy: 0.9522 - loss: 0.1181 - val_accuracy: 0.9415 - val_loss: 0.1423 - mcc: 0.8827\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9538 - loss: 0.1146\n","Epoch 18 - MCC: 0.8716\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9536 - loss: 0.1149 - val_accuracy: 0.9355 - val_loss: 0.1530 - mcc: 0.8716\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9519 - loss: 0.1169\n","Epoch 19 - MCC: 0.8810\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - accuracy: 0.9518 - loss: 0.1170 - val_accuracy: 0.9406 - val_loss: 0.1452 - mcc: 0.8810\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9557 - loss: 0.1101\n","Epoch 20 - MCC: 0.8821\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 165ms/step - accuracy: 0.9555 - loss: 0.1104 - val_accuracy: 0.9410 - val_loss: 0.1431 - mcc: 0.8821\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9497 - loss: 0.1241\n","Epoch 21 - MCC: 0.8832\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9499 - loss: 0.1237 - val_accuracy: 0.9417 - val_loss: 0.1415 - mcc: 0.8832\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9572 - loss: 0.1057\n","Epoch 22 - MCC: 0.8832\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 0.9571 - loss: 0.1059 - val_accuracy: 0.9416 - val_loss: 0.1406 - mcc: 0.8832\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9564 - loss: 0.1063\n","Epoch 23 - MCC: 0.8841\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.9563 - loss: 0.1064 - val_accuracy: 0.9422 - val_loss: 0.1392 - mcc: 0.8841\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9532 - loss: 0.1132\n","Epoch 24 - MCC: 0.8851\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9532 - loss: 0.1130 - val_accuracy: 0.9426 - val_loss: 0.1409 - mcc: 0.8851\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9546 - loss: 0.1115\n","Epoch 25 - MCC: 0.8878\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.9546 - loss: 0.1113 - val_accuracy: 0.9440 - val_loss: 0.1345 - mcc: 0.8878\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9554 - loss: 0.1118\n","Epoch 26 - MCC: 0.8843\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.9555 - loss: 0.1115 - val_accuracy: 0.9423 - val_loss: 0.1379 - mcc: 0.8843\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9605 - loss: 0.0981\n","Epoch 27 - MCC: 0.8883\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9604 - loss: 0.0982 - val_accuracy: 0.9442 - val_loss: 0.1338 - mcc: 0.8883\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9597 - loss: 0.0999\n","Epoch 28 - MCC: 0.8858\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 196ms/step - accuracy: 0.9597 - loss: 0.1000 - val_accuracy: 0.9430 - val_loss: 0.1364 - mcc: 0.8858\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9622 - loss: 0.0920\n","Epoch 29 - MCC: 0.8826\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9620 - loss: 0.0924 - val_accuracy: 0.9414 - val_loss: 0.1399 - mcc: 0.8826\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9608 - loss: 0.0966\n","Epoch 30 - MCC: 0.8865\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 169ms/step - accuracy: 0.9607 - loss: 0.0968 - val_accuracy: 0.9432 - val_loss: 0.1406 - mcc: 0.8865\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.5865 - loss: 0.6331\n","Epoch 1 - MCC: 0.7005\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 226ms/step - accuracy: 0.5905 - loss: 0.6304 - val_accuracy: 0.8504 - val_loss: 0.3999 - mcc: 0.7005\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8656 - loss: 0.3454\n","Epoch 2 - MCC: 0.8265\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 166ms/step - accuracy: 0.8662 - loss: 0.3435 - val_accuracy: 0.9134 - val_loss: 0.2164 - mcc: 0.8265\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9182 - loss: 0.2020\n","Epoch 3 - MCC: 0.8421\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 0.9181 - loss: 0.2021 - val_accuracy: 0.9213 - val_loss: 0.1895 - mcc: 0.8421\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9254 - loss: 0.1823\n","Epoch 4 - MCC: 0.8538\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9253 - loss: 0.1823 - val_accuracy: 0.9268 - val_loss: 0.1741 - mcc: 0.8538\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9284 - loss: 0.1736\n","Epoch 5 - MCC: 0.8445\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - accuracy: 0.9284 - loss: 0.1738 - val_accuracy: 0.9209 - val_loss: 0.1836 - mcc: 0.8445\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9251 - loss: 0.1767\n","Epoch 6 - MCC: 0.8672\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 205ms/step - accuracy: 0.9253 - loss: 0.1764 - val_accuracy: 0.9338 - val_loss: 0.1596 - mcc: 0.8672\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9385 - loss: 0.1505\n","Epoch 7 - MCC: 0.8783\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9384 - loss: 0.1506 - val_accuracy: 0.9394 - val_loss: 0.1485 - mcc: 0.8783\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9338 - loss: 0.1584\n","Epoch 8 - MCC: 0.8757\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9340 - loss: 0.1581 - val_accuracy: 0.9380 - val_loss: 0.1477 - mcc: 0.8757\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9340 - loss: 0.1569\n","Epoch 9 - MCC: 0.8841\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 202ms/step - accuracy: 0.9342 - loss: 0.1564 - val_accuracy: 0.9422 - val_loss: 0.1404 - mcc: 0.8841\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9428 - loss: 0.1393\n","Epoch 10 - MCC: 0.8923\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9428 - loss: 0.1394 - val_accuracy: 0.9464 - val_loss: 0.1298 - mcc: 0.8923\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9367 - loss: 0.1502\n","Epoch 11 - MCC: 0.8944\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 172ms/step - accuracy: 0.9369 - loss: 0.1499 - val_accuracy: 0.9474 - val_loss: 0.1288 - mcc: 0.8944\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9462 - loss: 0.1297\n","Epoch 12 - MCC: 0.8991\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 175ms/step - accuracy: 0.9462 - loss: 0.1296 - val_accuracy: 0.9497 - val_loss: 0.1228 - mcc: 0.8991\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9444 - loss: 0.1332\n","Epoch 13 - MCC: 0.9011\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9445 - loss: 0.1330 - val_accuracy: 0.9507 - val_loss: 0.1227 - mcc: 0.9011\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9488 - loss: 0.1250\n","Epoch 14 - MCC: 0.9040\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.9488 - loss: 0.1249 - val_accuracy: 0.9522 - val_loss: 0.1184 - mcc: 0.9040\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9501 - loss: 0.1219\n","Epoch 15 - MCC: 0.8909\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9501 - loss: 0.1219 - val_accuracy: 0.9450 - val_loss: 0.1311 - mcc: 0.8909\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9450 - loss: 0.1330\n","Epoch 16 - MCC: 0.9035\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.9451 - loss: 0.1326 - val_accuracy: 0.9519 - val_loss: 0.1205 - mcc: 0.9035\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9470 - loss: 0.1264\n","Epoch 17 - MCC: 0.9056\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.9471 - loss: 0.1263 - val_accuracy: 0.9530 - val_loss: 0.1176 - mcc: 0.9056\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9504 - loss: 0.1191\n","Epoch 18 - MCC: 0.9047\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9505 - loss: 0.1190 - val_accuracy: 0.9525 - val_loss: 0.1151 - mcc: 0.9047\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9572 - loss: 0.1038\n","Epoch 19 - MCC: 0.9070\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9571 - loss: 0.1041 - val_accuracy: 0.9537 - val_loss: 0.1160 - mcc: 0.9070\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9562 - loss: 0.1076\n","Epoch 20 - MCC: 0.9058\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.9560 - loss: 0.1078 - val_accuracy: 0.9531 - val_loss: 0.1173 - mcc: 0.9058\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9562 - loss: 0.1072\n","Epoch 21 - MCC: 0.9024\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9560 - loss: 0.1075 - val_accuracy: 0.9507 - val_loss: 0.1229 - mcc: 0.9024\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9533 - loss: 0.1139\n","Epoch 22 - MCC: 0.9043\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.9534 - loss: 0.1138 - val_accuracy: 0.9522 - val_loss: 0.1143 - mcc: 0.9043\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9586 - loss: 0.1025\n","Epoch 23 - MCC: 0.9117\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 174ms/step - accuracy: 0.9585 - loss: 0.1026 - val_accuracy: 0.9560 - val_loss: 0.1098 - mcc: 0.9117\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9545 - loss: 0.1096\n","Epoch 24 - MCC: 0.9055\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - accuracy: 0.9544 - loss: 0.1097 - val_accuracy: 0.9529 - val_loss: 0.1144 - mcc: 0.9055\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9562 - loss: 0.1058\n","Epoch 25 - MCC: 0.9113\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 182ms/step - accuracy: 0.9562 - loss: 0.1058 - val_accuracy: 0.9558 - val_loss: 0.1115 - mcc: 0.9113\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9570 - loss: 0.1053\n","Epoch 26 - MCC: 0.9139\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.9569 - loss: 0.1054 - val_accuracy: 0.9571 - val_loss: 0.1066 - mcc: 0.9139\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9585 - loss: 0.1019\n","Epoch 27 - MCC: 0.9042\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9584 - loss: 0.1022 - val_accuracy: 0.9523 - val_loss: 0.1161 - mcc: 0.9042\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9580 - loss: 0.1006\n","Epoch 28 - MCC: 0.9157\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - accuracy: 0.9580 - loss: 0.1007 - val_accuracy: 0.9580 - val_loss: 0.1049 - mcc: 0.9157\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9579 - loss: 0.1025\n","Epoch 29 - MCC: 0.9113\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9579 - loss: 0.1024 - val_accuracy: 0.9558 - val_loss: 0.1077 - mcc: 0.9113\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9604 - loss: 0.0981\n","Epoch 30 - MCC: 0.9146\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9604 - loss: 0.0982 - val_accuracy: 0.9575 - val_loss: 0.1062 - mcc: 0.9146\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.6236 - loss: 0.5833\n","Epoch 1 - MCC: 0.7407\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 227ms/step - accuracy: 0.6282 - loss: 0.5798 - val_accuracy: 0.8681 - val_loss: 0.3260 - mcc: 0.7407\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8634 - loss: 0.3217\n","Epoch 2 - MCC: 0.8135\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 196ms/step - accuracy: 0.8639 - loss: 0.3207 - val_accuracy: 0.9070 - val_loss: 0.2294 - mcc: 0.8135\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9070 - loss: 0.2267\n","Epoch 3 - MCC: 0.8517\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9071 - loss: 0.2263 - val_accuracy: 0.9259 - val_loss: 0.1848 - mcc: 0.8517\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9179 - loss: 0.1974\n","Epoch 4 - MCC: 0.8619\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.9181 - loss: 0.1969 - val_accuracy: 0.9311 - val_loss: 0.1670 - mcc: 0.8619\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9251 - loss: 0.1797\n","Epoch 5 - MCC: 0.8571\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - accuracy: 0.9251 - loss: 0.1797 - val_accuracy: 0.9281 - val_loss: 0.1757 - mcc: 0.8571\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9275 - loss: 0.1762\n","Epoch 6 - MCC: 0.8754\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9275 - loss: 0.1761 - val_accuracy: 0.9378 - val_loss: 0.1535 - mcc: 0.8754\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9344 - loss: 0.1617\n","Epoch 7 - MCC: 0.8836\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 165ms/step - accuracy: 0.9344 - loss: 0.1616 - val_accuracy: 0.9419 - val_loss: 0.1446 - mcc: 0.8836\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9384 - loss: 0.1504\n","Epoch 8 - MCC: 0.8835\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - accuracy: 0.9384 - loss: 0.1505 - val_accuracy: 0.9419 - val_loss: 0.1423 - mcc: 0.8835\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9372 - loss: 0.1521\n","Epoch 9 - MCC: 0.8816\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9372 - loss: 0.1521 - val_accuracy: 0.9408 - val_loss: 0.1411 - mcc: 0.8816\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9408 - loss: 0.1452\n","Epoch 10 - MCC: 0.8863\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 187ms/step - accuracy: 0.9407 - loss: 0.1454 - val_accuracy: 0.9432 - val_loss: 0.1406 - mcc: 0.8863\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9304 - loss: 0.1658\n","Epoch 11 - MCC: 0.8910\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 169ms/step - accuracy: 0.9307 - loss: 0.1651 - val_accuracy: 0.9454 - val_loss: 0.1334 - mcc: 0.8910\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9399 - loss: 0.1451\n","Epoch 12 - MCC: 0.8956\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9400 - loss: 0.1450 - val_accuracy: 0.9478 - val_loss: 0.1301 - mcc: 0.8956\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9395 - loss: 0.1469\n","Epoch 13 - MCC: 0.8955\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - accuracy: 0.9396 - loss: 0.1467 - val_accuracy: 0.9479 - val_loss: 0.1295 - mcc: 0.8955\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9447 - loss: 0.1338\n","Epoch 14 - MCC: 0.8977\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9447 - loss: 0.1340 - val_accuracy: 0.9487 - val_loss: 0.1306 - mcc: 0.8977\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9414 - loss: 0.1417\n","Epoch 15 - MCC: 0.8990\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9414 - loss: 0.1415 - val_accuracy: 0.9496 - val_loss: 0.1229 - mcc: 0.8990\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9459 - loss: 0.1295\n","Epoch 16 - MCC: 0.8997\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.9459 - loss: 0.1295 - val_accuracy: 0.9500 - val_loss: 0.1230 - mcc: 0.8997\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9491 - loss: 0.1229\n","Epoch 17 - MCC: 0.8991\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9491 - loss: 0.1231 - val_accuracy: 0.9495 - val_loss: 0.1239 - mcc: 0.8991\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9492 - loss: 0.1238\n","Epoch 18 - MCC: 0.9005\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 178ms/step - accuracy: 0.9491 - loss: 0.1240 - val_accuracy: 0.9503 - val_loss: 0.1220 - mcc: 0.9005\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9495 - loss: 0.1210\n","Epoch 19 - MCC: 0.9076\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - accuracy: 0.9493 - loss: 0.1212 - val_accuracy: 0.9538 - val_loss: 0.1150 - mcc: 0.9076\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9489 - loss: 0.1229\n","Epoch 20 - MCC: 0.9113\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.9489 - loss: 0.1228 - val_accuracy: 0.9557 - val_loss: 0.1105 - mcc: 0.9113\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.9492 - loss: 0.1222\n","Epoch 21 - MCC: 0.9087\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 192ms/step - accuracy: 0.9492 - loss: 0.1222 - val_accuracy: 0.9544 - val_loss: 0.1130 - mcc: 0.9087\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9457 - loss: 0.1303\n","Epoch 22 - MCC: 0.9061\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.9459 - loss: 0.1299 - val_accuracy: 0.9532 - val_loss: 0.1150 - mcc: 0.9061\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9528 - loss: 0.1139\n","Epoch 23 - MCC: 0.9121\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 163ms/step - accuracy: 0.9528 - loss: 0.1140 - val_accuracy: 0.9561 - val_loss: 0.1105 - mcc: 0.9121\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9501 - loss: 0.1189\n","Epoch 24 - MCC: 0.9126\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 193ms/step - accuracy: 0.9502 - loss: 0.1189 - val_accuracy: 0.9564 - val_loss: 0.1095 - mcc: 0.9126\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9523 - loss: 0.1120\n","Epoch 25 - MCC: 0.9115\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9523 - loss: 0.1121 - val_accuracy: 0.9558 - val_loss: 0.1098 - mcc: 0.9115\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9523 - loss: 0.1147\n","Epoch 26 - MCC: 0.9102\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - accuracy: 0.9523 - loss: 0.1146 - val_accuracy: 0.9551 - val_loss: 0.1124 - mcc: 0.9102\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9534 - loss: 0.1120\n","Epoch 27 - MCC: 0.9140\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.9535 - loss: 0.1119 - val_accuracy: 0.9570 - val_loss: 0.1087 - mcc: 0.9140\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9526 - loss: 0.1145\n","Epoch 28 - MCC: 0.9158\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9527 - loss: 0.1143 - val_accuracy: 0.9580 - val_loss: 0.1057 - mcc: 0.9158\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9565 - loss: 0.1062\n","Epoch 29 - MCC: 0.9159\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 180ms/step - accuracy: 0.9565 - loss: 0.1063 - val_accuracy: 0.9581 - val_loss: 0.1072 - mcc: 0.9159\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9561 - loss: 0.1054\n","Epoch 30 - MCC: 0.9149\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 0.9560 - loss: 0.1056 - val_accuracy: 0.9575 - val_loss: 0.1071 - mcc: 0.9149\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6441 - loss: 0.6130\n","Epoch 1 - MCC: 0.7308\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 243ms/step - accuracy: 0.6479 - loss: 0.6095 - val_accuracy: 0.8655 - val_loss: 0.3238 - mcc: 0.7308\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8811 - loss: 0.2912\n","Epoch 2 - MCC: 0.7997\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.8814 - loss: 0.2905 - val_accuracy: 0.9000 - val_loss: 0.2479 - mcc: 0.7997\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9059 - loss: 0.2296\n","Epoch 3 - MCC: 0.8240\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9061 - loss: 0.2293 - val_accuracy: 0.9115 - val_loss: 0.2145 - mcc: 0.8240\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9152 - loss: 0.2052\n","Epoch 4 - MCC: 0.8363\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9154 - loss: 0.2048 - val_accuracy: 0.9183 - val_loss: 0.1955 - mcc: 0.8363\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9275 - loss: 0.1769\n","Epoch 5 - MCC: 0.8437\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - accuracy: 0.9275 - loss: 0.1769 - val_accuracy: 0.9218 - val_loss: 0.1867 - mcc: 0.8437\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9273 - loss: 0.1763\n","Epoch 6 - MCC: 0.8466\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - accuracy: 0.9274 - loss: 0.1761 - val_accuracy: 0.9233 - val_loss: 0.1850 - mcc: 0.8466\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9351 - loss: 0.1598\n","Epoch 7 - MCC: 0.8573\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9350 - loss: 0.1599 - val_accuracy: 0.9287 - val_loss: 0.1726 - mcc: 0.8573\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9359 - loss: 0.1578\n","Epoch 8 - MCC: 0.8604\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 160ms/step - accuracy: 0.9359 - loss: 0.1577 - val_accuracy: 0.9299 - val_loss: 0.1685 - mcc: 0.8604\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9377 - loss: 0.1530\n","Epoch 9 - MCC: 0.8517\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.9377 - loss: 0.1531 - val_accuracy: 0.9253 - val_loss: 0.1759 - mcc: 0.8517\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9343 - loss: 0.1589\n","Epoch 10 - MCC: 0.8592\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9343 - loss: 0.1588 - val_accuracy: 0.9292 - val_loss: 0.1698 - mcc: 0.8592\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9386 - loss: 0.1498\n","Epoch 11 - MCC: 0.8679\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - accuracy: 0.9386 - loss: 0.1497 - val_accuracy: 0.9337 - val_loss: 0.1628 - mcc: 0.8679\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9390 - loss: 0.1494\n","Epoch 12 - MCC: 0.8672\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9391 - loss: 0.1491 - val_accuracy: 0.9337 - val_loss: 0.1584 - mcc: 0.8672\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9433 - loss: 0.1378\n","Epoch 13 - MCC: 0.8793\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9433 - loss: 0.1379 - val_accuracy: 0.9398 - val_loss: 0.1483 - mcc: 0.8793\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9477 - loss: 0.1273\n","Epoch 14 - MCC: 0.8809\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 206ms/step - accuracy: 0.9477 - loss: 0.1274 - val_accuracy: 0.9406 - val_loss: 0.1436 - mcc: 0.8809\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9449 - loss: 0.1324\n","Epoch 15 - MCC: 0.8858\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9450 - loss: 0.1322 - val_accuracy: 0.9430 - val_loss: 0.1376 - mcc: 0.8858\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9494 - loss: 0.1236\n","Epoch 16 - MCC: 0.8743\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 172ms/step - accuracy: 0.9493 - loss: 0.1239 - val_accuracy: 0.9368 - val_loss: 0.1525 - mcc: 0.8743\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.9467 - loss: 0.1290\n","Epoch 17 - MCC: 0.8871\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - accuracy: 0.9468 - loss: 0.1289 - val_accuracy: 0.9436 - val_loss: 0.1377 - mcc: 0.8871\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9472 - loss: 0.1261\n","Epoch 18 - MCC: 0.8862\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9474 - loss: 0.1259 - val_accuracy: 0.9429 - val_loss: 0.1371 - mcc: 0.8862\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9518 - loss: 0.1170\n","Epoch 19 - MCC: 0.8878\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.9518 - loss: 0.1172 - val_accuracy: 0.9437 - val_loss: 0.1366 - mcc: 0.8878\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9545 - loss: 0.1128\n","Epoch 20 - MCC: 0.8888\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9544 - loss: 0.1131 - val_accuracy: 0.9443 - val_loss: 0.1384 - mcc: 0.8888\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9501 - loss: 0.1217\n","Epoch 21 - MCC: 0.8911\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.9502 - loss: 0.1215 - val_accuracy: 0.9456 - val_loss: 0.1304 - mcc: 0.8911\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.9520 - loss: 0.1161\n","Epoch 22 - MCC: 0.8965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.9521 - loss: 0.1159 - val_accuracy: 0.9483 - val_loss: 0.1244 - mcc: 0.8965\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9549 - loss: 0.1093\n","Epoch 23 - MCC: 0.8851\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9549 - loss: 0.1094 - val_accuracy: 0.9422 - val_loss: 0.1375 - mcc: 0.8851\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9522 - loss: 0.1180\n","Epoch 24 - MCC: 0.8899\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9523 - loss: 0.1179 - val_accuracy: 0.9449 - val_loss: 0.1309 - mcc: 0.8899\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9548 - loss: 0.1112\n","Epoch 25 - MCC: 0.8937\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 198ms/step - accuracy: 0.9549 - loss: 0.1111 - val_accuracy: 0.9466 - val_loss: 0.1295 - mcc: 0.8937\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9572 - loss: 0.1054\n","Epoch 26 - MCC: 0.8873\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9571 - loss: 0.1055 - val_accuracy: 0.9437 - val_loss: 0.1334 - mcc: 0.8873\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9540 - loss: 0.1149\n","Epoch 27 - MCC: 0.8981\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.9540 - loss: 0.1146 - val_accuracy: 0.9489 - val_loss: 0.1269 - mcc: 0.8981\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9560 - loss: 0.1079\n","Epoch 28 - MCC: 0.9010\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.9560 - loss: 0.1079 - val_accuracy: 0.9506 - val_loss: 0.1214 - mcc: 0.9010\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9579 - loss: 0.1031\n","Epoch 29 - MCC: 0.8981\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9579 - loss: 0.1031 - val_accuracy: 0.9491 - val_loss: 0.1235 - mcc: 0.8981\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9579 - loss: 0.1037\n","Epoch 30 - MCC: 0.9014\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 199ms/step - accuracy: 0.9579 - loss: 0.1036 - val_accuracy: 0.9508 - val_loss: 0.1193 - mcc: 0.9014\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.95838,\n","              'mean': 0.9534866666666666,\n","              'min': 0.94322,\n","              'std': 0.005816187372803225},\n"," 'Inference Time (s/sample)': {'max': 0.004210281372070313,\n","                               'mean': 0.0036422080993652344,\n","                               'min': 0.0032938408851623537,\n","                               'std': 0.0003049650722213964},\n"," 'MCC': {'max': 0.9164583828547461,\n","         'mean': 0.9067859029470968,\n","         'min': 0.8865045557787847,\n","         'std': 0.01150249994389627},\n"," 'Parameters': 79137,\n"," 'Train Time (s)': {'max': 153.23102378845215,\n","                    'mean': 149.74760856628419,\n","                    'min': 147.28850412368774,\n","                    'std': 2.2713180443047642},\n"," 'Training Accuracy': [[0.7286699414253235,\n","                        0.8924383521080017,\n","                        0.9125199317932129,\n","                        0.9221832156181335,\n","                        0.9275615811347961,\n","                        0.9288449883460999,\n","                        0.9293698072433472,\n","                        0.9313417077064514,\n","                        0.9366816878318787,\n","                        0.9372615218162537,\n","                        0.9375183582305908,\n","                        0.940833330154419,\n","                        0.9388367533683777,\n","                        0.9423165917396545,\n","                        0.9433749914169312,\n","                        0.9445933699607849,\n","                        0.9456199407577515,\n","                        0.9468650221824646,\n","                        0.9457416534423828,\n","                        0.9482350945472717,\n","                        0.947713315486908,\n","                        0.9486933946609497,\n","                        0.9493250846862793,\n","                        0.9503017663955688,\n","                        0.9512298703193665,\n","                        0.9517499804496765,\n","                        0.951978325843811,\n","                        0.9538233280181885,\n","                        0.9542999863624573,\n","                        0.9539149403572083],\n","                       [0.7521184682846069,\n","                        0.8961317539215088,\n","                        0.922458291053772,\n","                        0.9264034032821655,\n","                        0.9336566925048828,\n","                        0.9357882738113403,\n","                        0.9381932616233826,\n","                        0.9384499192237854,\n","                        0.9382765889167786,\n","                        0.94111168384552,\n","                        0.9461316466331482,\n","                        0.947119951248169,\n","                        0.9490749835968018,\n","                        0.9505884051322937,\n","                        0.9502667784690857,\n","                        0.9521549344062805,\n","                        0.9535816311836243,\n","                        0.9500882625579834,\n","                        0.9507915377616882,\n","                        0.9521114826202393,\n","                        0.953991711139679,\n","                        0.9554483890533447,\n","                        0.9553515911102295,\n","                        0.9545882344245911,\n","                        0.9556384086608887,\n","                        0.957683265209198,\n","                        0.9578784108161926,\n","                        0.9583867788314819,\n","                        0.958320140838623,\n","                        0.9586532115936279],\n","                       [0.6888399720191956,\n","                        0.8828067183494568,\n","                        0.9156699180603027,\n","                        0.9242866635322571,\n","                        0.926281750202179,\n","                        0.9296616315841675,\n","                        0.9362832903862,\n","                        0.9381700754165649,\n","                        0.9394800066947937,\n","                        0.9410383105278015,\n","                        0.9414250254631042,\n","                        0.9468217492103577,\n","                        0.9473716616630554,\n","                        0.9497750997543335,\n","                        0.9497748613357544,\n","                        0.9488683342933655,\n","                        0.9491798877716064,\n","                        0.9518900513648987,\n","                        0.9541432857513428,\n","                        0.9525933861732483,\n","                        0.9523066878318787,\n","                        0.9544284343719482,\n","                        0.9567617774009705,\n","                        0.9538348913192749,\n","                        0.9561284780502319,\n","                        0.956226646900177,\n","                        0.9561300873756409,\n","                        0.9580400586128235,\n","                        0.9590249061584473,\n","                        0.9588765501976013],\n","                       [0.7442632913589478,\n","                        0.8760749697685242,\n","                        0.9106433987617493,\n","                        0.922688364982605,\n","                        0.9259099364280701,\n","                        0.9281733632087708,\n","                        0.9346200823783875,\n","                        0.9369249939918518,\n","                        0.9372683167457581,\n","                        0.9387916326522827,\n","                        0.9385132789611816,\n","                        0.9414799213409424,\n","                        0.9411866664886475,\n","                        0.9432249665260315,\n","                        0.942984938621521,\n","                        0.9463484287261963,\n","                        0.9474617838859558,\n","                        0.9471117258071899,\n","                        0.9462484121322632,\n","                        0.9496034979820251,\n","                        0.9490500688552856,\n","                        0.9508533477783203,\n","                        0.9514567852020264,\n","                        0.9506782293319702,\n","                        0.9521766901016235,\n","                        0.9530917406082153,\n","                        0.954801619052887,\n","                        0.9547767639160156,\n","                        0.9556598663330078,\n","                        0.9545566439628601],\n","                       [0.7428016066551208,\n","                        0.8874050378799438,\n","                        0.9098498821258545,\n","                        0.9202717542648315,\n","                        0.9267566800117493,\n","                        0.930368185043335,\n","                        0.9337083697319031,\n","                        0.9361249804496765,\n","                        0.9355217218399048,\n","                        0.9357516765594482,\n","                        0.9391000270843506,\n","                        0.9417615532875061,\n","                        0.9426133632659912,\n","                        0.9465917348861694,\n","                        0.9470150470733643,\n","                        0.9466050863265991,\n","                        0.948223352432251,\n","                        0.9503183960914612,\n","                        0.9497765898704529,\n","                        0.9507633447647095,\n","                        0.9527416229248047,\n","                        0.9546215534210205,\n","                        0.9542100429534912,\n","                        0.9529049396514893,\n","                        0.9558384418487549,\n","                        0.9555915594100952,\n","                        0.955821692943573,\n","                        0.956309974193573,\n","                        0.957984983921051,\n","                        0.9584448337554932]],\n"," 'Training Loss': [[0.5236255526542664,\n","                    0.26373380422592163,\n","                    0.21198348701000214,\n","                    0.18816566467285156,\n","                    0.1773105412721634,\n","                    0.17125529050827026,\n","                    0.16951408982276917,\n","                    0.16530515253543854,\n","                    0.1549118459224701,\n","                    0.15228742361068726,\n","                    0.15151843428611755,\n","                    0.1443922072649002,\n","                    0.14820556342601776,\n","                    0.13976991176605225,\n","                    0.13798323273658752,\n","                    0.13390123844146729,\n","                    0.13189485669136047,\n","                    0.12875649333000183,\n","                    0.1300704926252365,\n","                    0.12561306357383728,\n","                    0.1265711784362793,\n","                    0.12470109015703201,\n","                    0.12483590841293335,\n","                    0.12068885564804077,\n","                    0.11910102516412735,\n","                    0.11627186834812164,\n","                    0.11711175739765167,\n","                    0.11281225830316544,\n","                    0.11067920923233032,\n","                    0.1124042198061943],\n","                   [0.4986499845981598,\n","                    0.2527044713497162,\n","                    0.18917648494243622,\n","                    0.17954862117767334,\n","                    0.16150535643100739,\n","                    0.15504620969295502,\n","                    0.14957404136657715,\n","                    0.1483246088027954,\n","                    0.1520572453737259,\n","                    0.14253056049346924,\n","                    0.13079838454723358,\n","                    0.12883582711219788,\n","                    0.12454820424318314,\n","                    0.12129025161266327,\n","                    0.12120409309864044,\n","                    0.11640656739473343,\n","                    0.1140969768166542,\n","                    0.1229754090309143,\n","                    0.12006062269210815,\n","                    0.11776512861251831,\n","                    0.11302848905324936,\n","                    0.1091027483344078,\n","                    0.10884455591440201,\n","                    0.1099485382437706,\n","                    0.10821311175823212,\n","                    0.10523674637079239,\n","                    0.10280897468328476,\n","                    0.1021786630153656,\n","                    0.1014784649014473,\n","                    0.1012071967124939],\n","                   [0.5625996589660645,\n","                    0.2963990271091461,\n","                    0.20570988953113556,\n","                    0.1836334615945816,\n","                    0.17796511948108673,\n","                    0.1677751988172531,\n","                    0.1547912061214447,\n","                    0.14937429130077362,\n","                    0.14570698142051697,\n","                    0.14165812730789185,\n","                    0.14156889915466309,\n","                    0.12861382961273193,\n","                    0.1270885467529297,\n","                    0.12266188859939575,\n","                    0.12218719720840454,\n","                    0.12433326989412308,\n","                    0.12336166203022003,\n","                    0.11572322994470596,\n","                    0.11132688820362091,\n","                    0.11442795395851135,\n","                    0.11536207795143127,\n","                    0.11171317100524902,\n","                    0.10670927166938782,\n","                    0.11243472993373871,\n","                    0.10709188133478165,\n","                    0.10731658339500427,\n","                    0.10745684802532196,\n","                    0.1027839407324791,\n","                    0.10055463016033173,\n","                    0.1008809506893158],\n","                   [0.49267661571502686,\n","                    0.29749852418899536,\n","                    0.2167493999004364,\n","                    0.1864679753780365,\n","                    0.17838174104690552,\n","                    0.1725725382566452,\n","                    0.16004830598831177,\n","                    0.15338511765003204,\n","                    0.15175630152225494,\n","                    0.14998972415924072,\n","                    0.14903931319713593,\n","                    0.14209885895252228,\n","                    0.1425720751285553,\n","                    0.137388214468956,\n","                    0.1364676058292389,\n","                    0.12976185977458954,\n","                    0.12789365649223328,\n","                    0.12799996137619019,\n","                    0.12794138491153717,\n","                    0.12237362563610077,\n","                    0.12212368100881577,\n","                    0.11871137470006943,\n","                    0.11745994538068771,\n","                    0.11768817901611328,\n","                    0.11351633071899414,\n","                    0.11263790726661682,\n","                    0.10956495255231857,\n","                    0.109130859375,\n","                    0.10767726600170135,\n","                    0.10884127765893936],\n","                   [0.5205742120742798,\n","                    0.27367186546325684,\n","                    0.22041060030460358,\n","                    0.19455555081367493,\n","                    0.17717941105365753,\n","                    0.168791726231575,\n","                    0.16276489198207855,\n","                    0.15684735774993896,\n","                    0.15668314695358276,\n","                    0.1573065221309662,\n","                    0.14860595762729645,\n","                    0.14263243973255157,\n","                    0.14015763998031616,\n","                    0.13061432540416718,\n","                    0.12801973521709442,\n","                    0.131164088845253,\n","                    0.12666714191436768,\n","                    0.12086068093776703,\n","                    0.12194767594337463,\n","                    0.12105939537286758,\n","                    0.11599797010421753,\n","                    0.11091449856758118,\n","                    0.1109909936785698,\n","                    0.1158037856221199,\n","                    0.1081223115324974,\n","                    0.10878773778676987,\n","                    0.1092534139752388,\n","                    0.10669868439435959,\n","                    0.10330326855182648,\n","                    0.1029534712433815]],\n"," 'Validation Accuracy': [[0.8783599734306335,\n","                          0.9153399467468262,\n","                          0.9279733896255493,\n","                          0.9312933087348938,\n","                          0.930806577205658,\n","                          0.9385799169540405,\n","                          0.9339000582695007,\n","                          0.9439466595649719,\n","                          0.9432933330535889,\n","                          0.9406599998474121,\n","                          0.9454333782196045,\n","                          0.9491199254989624,\n","                          0.9472467303276062,\n","                          0.9485666155815125,\n","                          0.9507666826248169,\n","                          0.9493266940116882,\n","                          0.9503400921821594,\n","                          0.9516932964324951,\n","                          0.9536865949630737,\n","                          0.9527601003646851,\n","                          0.9533200263977051,\n","                          0.9558266997337341,\n","                          0.9541533589363098,\n","                          0.9574333429336548,\n","                          0.9578999876976013,\n","                          0.955946683883667,\n","                          0.9566665887832642,\n","                          0.9595333337783813,\n","                          0.9591132998466492,\n","                          0.9583800435066223],\n","                         [0.8658866882324219,\n","                          0.9034199714660645,\n","                          0.9109799861907959,\n","                          0.9212267398834229,\n","                          0.9230599999427795,\n","                          0.9297333359718323,\n","                          0.9321334362030029,\n","                          0.9294066429138184,\n","                          0.9310667514801025,\n","                          0.930899977684021,\n","                          0.9350266456604004,\n","                          0.9364999532699585,\n","                          0.9399266242980957,\n","                          0.9391599893569946,\n","                          0.9412732720375061,\n","                          0.9422333240509033,\n","                          0.9414932727813721,\n","                          0.9354667067527771,\n","                          0.9405866861343384,\n","                          0.9410133361816406,\n","                          0.9416999816894531,\n","                          0.9415934085845947,\n","                          0.9421933889389038,\n","                          0.9425933361053467,\n","                          0.9440133571624756,\n","                          0.9422533512115479,\n","                          0.944246768951416,\n","                          0.9430333971977234,\n","                          0.9413800239562988,\n","                          0.9432199597358704],\n","                         [0.8503799438476562,\n","                          0.9133666157722473,\n","                          0.9212732911109924,\n","                          0.92684006690979,\n","                          0.9208599925041199,\n","                          0.933846652507782,\n","                          0.9394133687019348,\n","                          0.9380466341972351,\n","                          0.9422199726104736,\n","                          0.9463599324226379,\n","                          0.9474199414253235,\n","                          0.949720025062561,\n","                          0.9507266879081726,\n","                          0.9521666765213013,\n","                          0.9449666738510132,\n","                          0.9518733024597168,\n","                          0.9529666900634766,\n","                          0.9525200724601746,\n","                          0.9536734223365784,\n","                          0.95305997133255,\n","                          0.9506999850273132,\n","                          0.952239990234375,\n","                          0.9560266733169556,\n","                          0.9528866410255432,\n","                          0.9557799696922302,\n","                          0.9571332335472107,\n","                          0.9522866606712341,\n","                          0.9580333232879639,\n","                          0.9558467268943787,\n","                          0.9574933648109436],\n","                         [0.8680933117866516,\n","                          0.9069932699203491,\n","                          0.9259399175643921,\n","                          0.9310799241065979,\n","                          0.928119957447052,\n","                          0.9377999901771545,\n","                          0.941860020160675,\n","                          0.94187331199646,\n","                          0.9408332705497742,\n","                          0.943246603012085,\n","                          0.9453799724578857,\n","                          0.9477866291999817,\n","                          0.9478533864021301,\n","                          0.9487199187278748,\n","                          0.9496000409126282,\n","                          0.9499533176422119,\n","                          0.949459969997406,\n","                          0.9503333568572998,\n","                          0.9538067579269409,\n","                          0.9557133316993713,\n","                          0.9544399976730347,\n","                          0.9531800746917725,\n","                          0.9561465978622437,\n","                          0.9563999176025391,\n","                          0.9558066725730896,\n","                          0.9551332592964172,\n","                          0.9569733142852783,\n","                          0.9579666256904602,\n","                          0.9580667018890381,\n","                          0.9575467109680176],\n","                         [0.8654932975769043,\n","                          0.8999999165534973,\n","                          0.9115399122238159,\n","                          0.9183067083358765,\n","                          0.9218267202377319,\n","                          0.9233066439628601,\n","                          0.9286733269691467,\n","                          0.9298533797264099,\n","                          0.9253333210945129,\n","                          0.9292333126068115,\n","                          0.9336665868759155,\n","                          0.933733344078064,\n","                          0.9397533535957336,\n","                          0.9405732750892639,\n","                          0.9429934024810791,\n","                          0.9368066191673279,\n","                          0.9435667395591736,\n","                          0.9429399371147156,\n","                          0.9437399506568909,\n","                          0.9442732334136963,\n","                          0.945620059967041,\n","                          0.9482799768447876,\n","                          0.942246675491333,\n","                          0.9448932409286499,\n","                          0.9466133713722229,\n","                          0.9436732530593872,\n","                          0.9489200711250305,\n","                          0.9505599737167358,\n","                          0.949120044708252,\n","                          0.9507933259010315]],\n"," 'Validation Loss': [[0.29473641514778137,\n","                      0.20746038854122162,\n","                      0.17711128294467926,\n","                      0.16954226791858673,\n","                      0.16222353279590607,\n","                      0.15077394247055054,\n","                      0.1585979014635086,\n","                      0.13885870575904846,\n","                      0.13863036036491394,\n","                      0.14366792142391205,\n","                      0.13291051983833313,\n","                      0.12567399442195892,\n","                      0.1287052482366562,\n","                      0.12578700482845306,\n","                      0.118833988904953,\n","                      0.12095015496015549,\n","                      0.11906307935714722,\n","                      0.11695894598960876,\n","                      0.11194342374801636,\n","                      0.11610323935747147,\n","                      0.11250077188014984,\n","                      0.10707096755504608,\n","                      0.11150948703289032,\n","                      0.1015661358833313,\n","                      0.10185695439577103,\n","                      0.10451922565698624,\n","                      0.10481029748916626,\n","                      0.10052885115146637,\n","                      0.0995965227484703,\n","                      0.10057685524225235],\n","                     [0.3264453411102295,\n","                      0.22944298386573792,\n","                      0.2178928405046463,\n","                      0.1916377991437912,\n","                      0.18587364256381989,\n","                      0.17292247712612152,\n","                      0.16690444946289062,\n","                      0.17202532291412354,\n","                      0.16920961439609528,\n","                      0.16580480337142944,\n","                      0.15664678812026978,\n","                      0.15223370492458344,\n","                      0.146792471408844,\n","                      0.14663392305374146,\n","                      0.14376692473888397,\n","                      0.1434861123561859,\n","                      0.14231958985328674,\n","                      0.15304625034332275,\n","                      0.14520762860774994,\n","                      0.14309312403202057,\n","                      0.14145620167255402,\n","                      0.14057990908622742,\n","                      0.13918735086917877,\n","                      0.14088523387908936,\n","                      0.1345449984073639,\n","                      0.13792818784713745,\n","                      0.13375140726566315,\n","                      0.13643231987953186,\n","                      0.13990935683250427,\n","                      0.1405600905418396],\n","                     [0.39987099170684814,\n","                      0.2163977473974228,\n","                      0.18947003781795502,\n","                      0.1740599423646927,\n","                      0.18358495831489563,\n","                      0.159605011343956,\n","                      0.14849558472633362,\n","                      0.147692009806633,\n","                      0.14043202996253967,\n","                      0.12980598211288452,\n","                      0.12881937623023987,\n","                      0.12282872945070267,\n","                      0.12273111194372177,\n","                      0.11839433759450912,\n","                      0.13106657564640045,\n","                      0.12054219096899033,\n","                      0.11755340546369553,\n","                      0.11513974517583847,\n","                      0.11602698266506195,\n","                      0.11732448637485504,\n","                      0.12292077392339706,\n","                      0.11430307477712631,\n","                      0.1098114401102066,\n","                      0.11439016461372375,\n","                      0.11146793514490128,\n","                      0.10657742619514465,\n","                      0.11609924584627151,\n","                      0.10485327988862991,\n","                      0.10765596479177475,\n","                      0.10623249411582947],\n","                     [0.3259753882884979,\n","                      0.22937645018100739,\n","                      0.18480461835861206,\n","                      0.16700851917266846,\n","                      0.17568163573741913,\n","                      0.15349498391151428,\n","                      0.14460106194019318,\n","                      0.14234934747219086,\n","                      0.14107172191143036,\n","                      0.14063239097595215,\n","                      0.1333901286125183,\n","                      0.13013023138046265,\n","                      0.1295163780450821,\n","                      0.13060401380062103,\n","                      0.12294577807188034,\n","                      0.12299466878175735,\n","                      0.12386146187782288,\n","                      0.12201714515686035,\n","                      0.11500152945518494,\n","                      0.11053130775690079,\n","                      0.11300767213106155,\n","                      0.1149953082203865,\n","                      0.11046580970287323,\n","                      0.10945554822683334,\n","                      0.10979368537664413,\n","                      0.11243986338376999,\n","                      0.10866273194551468,\n","                      0.10574701428413391,\n","                      0.1072375550866127,\n","                      0.1071491464972496],\n","                     [0.32384684681892395,\n","                      0.24790333211421967,\n","                      0.2144840657711029,\n","                      0.19550207257270813,\n","                      0.186716690659523,\n","                      0.1850184202194214,\n","                      0.17255844175815582,\n","                      0.16849744319915771,\n","                      0.17594651877880096,\n","                      0.1698419153690338,\n","                      0.16278408467769623,\n","                      0.15840230882167816,\n","                      0.14831796288490295,\n","                      0.14356230199337006,\n","                      0.1375870257616043,\n","                      0.15252962708473206,\n","                      0.13766171038150787,\n","                      0.1370593011379242,\n","                      0.13655973970890045,\n","                      0.13842560350894928,\n","                      0.13038696348667145,\n","                      0.12443290650844574,\n","                      0.1374831199645996,\n","                      0.13085298240184784,\n","                      0.12950919568538666,\n","                      0.13336700201034546,\n","                      0.12690719962120056,\n","                      0.12141034007072449,\n","                      0.12348027527332306,\n","                      0.11933698505163193]],\n"," 'Validation MCC': [[0.7569922137267449,\n","                     0.8305188461089972,\n","                     0.8554446608532845,\n","                     0.8634590203831881,\n","                     0.8616860424643614,\n","                     0.8774253038931316,\n","                     0.8696802711708974,\n","                     0.887588485107037,\n","                     0.8866706313005307,\n","                     0.8809333230702381,\n","                     0.890741643848725,\n","                     0.8978658418495294,\n","                     0.8942141953528654,\n","                     0.8967435711933079,\n","                     0.9011599028759413,\n","                     0.8983150454818168,\n","                     0.9005186650319653,\n","                     0.9031971709106387,\n","                     0.9070522842615558,\n","                     0.9053205830018954,\n","                     0.9062842451792482,\n","                     0.9114136673694846,\n","                     0.9079780338296449,\n","                     0.9145481194562183,\n","                     0.915485781607862,\n","                     0.9116070496513536,\n","                     0.9130767901169472,\n","                     0.9188912478401061,\n","                     0.9179211053706628,\n","                     0.9164583828547461],\n","                    [0.7311022743029799,\n","                     0.807119217522645,\n","                     0.824165307513596,\n","                     0.8421319364538585,\n","                     0.845755552814123,\n","                     0.8591536399179955,\n","                     0.863958808262875,\n","                     0.858499605854686,\n","                     0.861872383379456,\n","                     0.8624437600782795,\n","                     0.870166905243563,\n","                     0.8729173268865671,\n","                     0.879684984510047,\n","                     0.8780503103727797,\n","                     0.8822969291696154,\n","                     0.884445095991303,\n","                     0.8827184940834323,\n","                     0.8715999090472577,\n","                     0.8809645369551032,\n","                     0.88207622894602,\n","                     0.8832237647972639,\n","                     0.8832014137055588,\n","                     0.8841159283210563,\n","                     0.8850692719276235,\n","                     0.8877632988429808,\n","                     0.8843491719675824,\n","                     0.8882666212630221,\n","                     0.8858010685166591,\n","                     0.8825939285572967,\n","                     0.8865045557787847],\n","                    [0.7004869857802872,\n","                     0.8264564494000883,\n","                     0.8420929577636199,\n","                     0.8537507838802584,\n","                     0.8444515748914687,\n","                     0.8672424278751617,\n","                     0.878334781493666,\n","                     0.8756777739298078,\n","                     0.8841398630673085,\n","                     0.8922999722601521,\n","                     0.8944231716954513,\n","                     0.8990680142576573,\n","                     0.9011033119245636,\n","                     0.9039618886655122,\n","                     0.8908620680704968,\n","                     0.9034886576199659,\n","                     0.9056429317438633,\n","                     0.9047166110918448,\n","                     0.9070106957626868,\n","                     0.9057858455879956,\n","                     0.902378553713324,\n","                     0.9043047131118122,\n","                     0.9116975802498288,\n","                     0.9055196879865335,\n","                     0.9113259560915025,\n","                     0.9139208663638931,\n","                     0.9042106426406948,\n","                     0.9157396551226575,\n","                     0.9113496619679703,\n","                     0.9146472778102781],\n","                    [0.7406792055848825,\n","                     0.8135486860228855,\n","                     0.8516584231919158,\n","                     0.8618560163719104,\n","                     0.8570747063459662,\n","                     0.8753902915194255,\n","                     0.8836258982508587,\n","                     0.8835202779248147,\n","                     0.8815571323071177,\n","                     0.8863096562803288,\n","                     0.8909574150723368,\n","                     0.8955842620012971,\n","                     0.8954825555832919,\n","                     0.8977176171755452,\n","                     0.8990360147368437,\n","                     0.8996656124633734,\n","                     0.8990964930468075,\n","                     0.9004734593999952,\n","                     0.9076047784829092,\n","                     0.9113391568645617,\n","                     0.9086708956989694,\n","                     0.906145205372867,\n","                     0.9120933780408038,\n","                     0.912593483947395,\n","                     0.911496614951369,\n","                     0.9101948813333662,\n","                     0.9139903783398156,\n","                     0.9157822932629396,\n","                     0.9159333089475729,\n","                     0.9149076874642904],\n","                    [0.7307543322863406,\n","                     0.7997322679442667,\n","                     0.8239925529533022,\n","                     0.8363161315989884,\n","                     0.8436994697258707,\n","                     0.8465611316942042,\n","                     0.8573129229729669,\n","                     0.8604392297196781,\n","                     0.8517260169006587,\n","                     0.8591599711819004,\n","                     0.8678981103570664,\n","                     0.8672259644735968,\n","                     0.879306957279907,\n","                     0.8809289941526922,\n","                     0.8857834696480762,\n","                     0.8743396552660933,\n","                     0.8870951069652905,\n","                     0.8862087765531074,\n","                     0.8878113297818051,\n","                     0.8887505081099758,\n","                     0.8910803532787251,\n","                     0.8965119179044142,\n","                     0.8851122164390507,\n","                     0.8899230885625492,\n","                     0.8936917518071869,\n","                     0.8873447875922555,\n","                     0.8981113814462847,\n","                     0.9010227451083616,\n","                     0.8981068991079841,\n","                     0.9014116108273843]]}\n"]}]},{"cell_type":"markdown","source":["Hilbert Beta Binary"],"metadata":{"id":"t2wLpIqNmVzw"}},{"cell_type":"code","source":["hilbert_binary_results, trained_models = train_and_evaluate(simple_models_dict, X=hilbert_data_vec, y=label_binary, epochs=n_epochs, dir_name=\"hilbert_binary\")\n","\n","basePath = \"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Model Comparisons/LSTM Models\"\n","\n","filePath = f\"{basePath}/30_Hilbert_Binary_Model_Results.json\"\n","\n","with open(filePath, 'w') as f:\n","        json.dump(hilbert_binary_results, f, indent=4)  # indent=4 for pretty formatting"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jFqLKgIcMsr","executionInfo":{"status":"ok","timestamp":1740316769352,"user_tz":-60,"elapsed":2588193,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"08ecbdc7-95b5-47db-94df-0394a0739618"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5203 - loss: 0.6932\n","Epoch 1 - MCC: 0.3685\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.5217 - loss: 0.6924 - val_accuracy: 0.6268 - val_loss: 0.6248 - mcc: 0.3685\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6801 - loss: 0.5991\n","Epoch 2 - MCC: 0.6672\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.6823 - loss: 0.5978 - val_accuracy: 0.8339 - val_loss: 0.4623 - mcc: 0.6672\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8354 - loss: 0.4176\n","Epoch 3 - MCC: 0.7355\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8355 - loss: 0.4165 - val_accuracy: 0.8680 - val_loss: 0.3484 - mcc: 0.7355\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8573 - loss: 0.3505\n","Epoch 4 - MCC: 0.7505\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8574 - loss: 0.3501 - val_accuracy: 0.8757 - val_loss: 0.3071 - mcc: 0.7505\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8747 - loss: 0.3050\n","Epoch 5 - MCC: 0.7874\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8749 - loss: 0.3046 - val_accuracy: 0.8941 - val_loss: 0.2577 - mcc: 0.7874\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8918 - loss: 0.2629\n","Epoch 6 - MCC: 0.8002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8919 - loss: 0.2627 - val_accuracy: 0.8998 - val_loss: 0.2432 - mcc: 0.8002\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8983 - loss: 0.2497\n","Epoch 7 - MCC: 0.8080\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8982 - loss: 0.2497 - val_accuracy: 0.9044 - val_loss: 0.2320 - mcc: 0.8080\n","Epoch 8/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9002 - loss: 0.2384\n","Epoch 8 - MCC: 0.8114\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9003 - loss: 0.2384 - val_accuracy: 0.9058 - val_loss: 0.2261 - mcc: 0.8114\n","Epoch 9/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9013 - loss: 0.2375\n","Epoch 9 - MCC: 0.8164\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9014 - loss: 0.2372 - val_accuracy: 0.9084 - val_loss: 0.2176 - mcc: 0.8164\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9048 - loss: 0.2261\n","Epoch 10 - MCC: 0.8161\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9049 - loss: 0.2260 - val_accuracy: 0.9081 - val_loss: 0.2152 - mcc: 0.8161\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9060 - loss: 0.2225\n","Epoch 11 - MCC: 0.8134\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9060 - loss: 0.2226 - val_accuracy: 0.9067 - val_loss: 0.2157 - mcc: 0.8134\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9050 - loss: 0.2231\n","Epoch 12 - MCC: 0.8229\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9050 - loss: 0.2232 - val_accuracy: 0.9113 - val_loss: 0.2076 - mcc: 0.8229\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9046 - loss: 0.2272\n","Epoch 13 - MCC: 0.8254\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9047 - loss: 0.2269 - val_accuracy: 0.9127 - val_loss: 0.2050 - mcc: 0.8254\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9049 - loss: 0.2227\n","Epoch 14 - MCC: 0.8289\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9050 - loss: 0.2225 - val_accuracy: 0.9145 - val_loss: 0.2022 - mcc: 0.8289\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9136 - loss: 0.2062\n","Epoch 15 - MCC: 0.8327\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9135 - loss: 0.2064 - val_accuracy: 0.9167 - val_loss: 0.1967 - mcc: 0.8327\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9100 - loss: 0.2127\n","Epoch 16 - MCC: 0.8257\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9101 - loss: 0.2126 - val_accuracy: 0.9128 - val_loss: 0.2003 - mcc: 0.8257\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9126 - loss: 0.2074\n","Epoch 17 - MCC: 0.8324\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9126 - loss: 0.2074 - val_accuracy: 0.9163 - val_loss: 0.1965 - mcc: 0.8324\n","Epoch 18/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9149 - loss: 0.2036\n","Epoch 18 - MCC: 0.8380\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.9148 - loss: 0.2038 - val_accuracy: 0.9193 - val_loss: 0.1921 - mcc: 0.8380\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9155 - loss: 0.1996\n","Epoch 19 - MCC: 0.8354\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9154 - loss: 0.1998 - val_accuracy: 0.9180 - val_loss: 0.1950 - mcc: 0.8354\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9129 - loss: 0.2069\n","Epoch 20 - MCC: 0.8278\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9129 - loss: 0.2068 - val_accuracy: 0.9136 - val_loss: 0.1976 - mcc: 0.8278\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9138 - loss: 0.2034\n","Epoch 21 - MCC: 0.8365\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9138 - loss: 0.2034 - val_accuracy: 0.9186 - val_loss: 0.1892 - mcc: 0.8365\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9155 - loss: 0.1993\n","Epoch 22 - MCC: 0.8393\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9154 - loss: 0.1995 - val_accuracy: 0.9199 - val_loss: 0.1874 - mcc: 0.8393\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9143 - loss: 0.2019\n","Epoch 23 - MCC: 0.8409\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9144 - loss: 0.2019 - val_accuracy: 0.9207 - val_loss: 0.1877 - mcc: 0.8409\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9119 - loss: 0.2083\n","Epoch 24 - MCC: 0.8416\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9121 - loss: 0.2079 - val_accuracy: 0.9210 - val_loss: 0.1854 - mcc: 0.8416\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9174 - loss: 0.1952\n","Epoch 25 - MCC: 0.8391\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9173 - loss: 0.1953 - val_accuracy: 0.9198 - val_loss: 0.1846 - mcc: 0.8391\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9149 - loss: 0.2003\n","Epoch 26 - MCC: 0.8381\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.9150 - loss: 0.2002 - val_accuracy: 0.9192 - val_loss: 0.1934 - mcc: 0.8381\n","Epoch 27/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9145 - loss: 0.2011\n","Epoch 27 - MCC: 0.8407\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.9146 - loss: 0.2011 - val_accuracy: 0.9207 - val_loss: 0.1849 - mcc: 0.8407\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9135 - loss: 0.2030\n","Epoch 28 - MCC: 0.8441\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9136 - loss: 0.2027 - val_accuracy: 0.9222 - val_loss: 0.1853 - mcc: 0.8441\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9131 - loss: 0.2039\n","Epoch 29 - MCC: 0.8464\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9133 - loss: 0.2035 - val_accuracy: 0.9235 - val_loss: 0.1774 - mcc: 0.8464\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9175 - loss: 0.1939\n","Epoch 30 - MCC: 0.8474\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9175 - loss: 0.1938 - val_accuracy: 0.9240 - val_loss: 0.1771 - mcc: 0.8474\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 2\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5459 - loss: 0.6638\n","Epoch 1 - MCC: 0.4227\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.5480 - loss: 0.6621 - val_accuracy: 0.6624 - val_loss: 0.5978 - mcc: 0.4227\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7472 - loss: 0.5480\n","Epoch 2 - MCC: 0.6751\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.7490 - loss: 0.5462 - val_accuracy: 0.8377 - val_loss: 0.4004 - mcc: 0.6751\n","Epoch 3/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8479 - loss: 0.3730\n","Epoch 3 - MCC: 0.7173\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.8486 - loss: 0.3711 - val_accuracy: 0.8580 - val_loss: 0.3375 - mcc: 0.7173\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8766 - loss: 0.3015\n","Epoch 4 - MCC: 0.7725\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8768 - loss: 0.3011 - val_accuracy: 0.8865 - val_loss: 0.2823 - mcc: 0.7725\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8970 - loss: 0.2527\n","Epoch 5 - MCC: 0.7845\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8970 - loss: 0.2527 - val_accuracy: 0.8925 - val_loss: 0.2572 - mcc: 0.7845\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9011 - loss: 0.2391\n","Epoch 6 - MCC: 0.7936\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9011 - loss: 0.2390 - val_accuracy: 0.8971 - val_loss: 0.2497 - mcc: 0.7936\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8996 - loss: 0.2407\n","Epoch 7 - MCC: 0.7938\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8999 - loss: 0.2403 - val_accuracy: 0.8970 - val_loss: 0.2454 - mcc: 0.7938\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9083 - loss: 0.2224\n","Epoch 8 - MCC: 0.7901\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9083 - loss: 0.2224 - val_accuracy: 0.8948 - val_loss: 0.2489 - mcc: 0.7901\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9087 - loss: 0.2182\n","Epoch 9 - MCC: 0.7935\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9086 - loss: 0.2186 - val_accuracy: 0.8968 - val_loss: 0.2433 - mcc: 0.7935\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9031 - loss: 0.2292\n","Epoch 10 - MCC: 0.8025\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9032 - loss: 0.2290 - val_accuracy: 0.9015 - val_loss: 0.2355 - mcc: 0.8025\n","Epoch 11/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9115 - loss: 0.2139\n","Epoch 11 - MCC: 0.8025\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9114 - loss: 0.2140 - val_accuracy: 0.9013 - val_loss: 0.2352 - mcc: 0.8025\n","Epoch 12/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9083 - loss: 0.2199\n","Epoch 12 - MCC: 0.8048\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9084 - loss: 0.2195 - val_accuracy: 0.9026 - val_loss: 0.2327 - mcc: 0.8048\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9091 - loss: 0.2146\n","Epoch 13 - MCC: 0.8010\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9092 - loss: 0.2145 - val_accuracy: 0.9004 - val_loss: 0.2363 - mcc: 0.8010\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9156 - loss: 0.2032\n","Epoch 14 - MCC: 0.8007\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9155 - loss: 0.2035 - val_accuracy: 0.9006 - val_loss: 0.2327 - mcc: 0.8007\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9120 - loss: 0.2106\n","Epoch 15 - MCC: 0.8053\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9120 - loss: 0.2106 - val_accuracy: 0.9027 - val_loss: 0.2303 - mcc: 0.8053\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9156 - loss: 0.2015\n","Epoch 16 - MCC: 0.7965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9155 - loss: 0.2017 - val_accuracy: 0.8984 - val_loss: 0.2349 - mcc: 0.7965\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9156 - loss: 0.2009\n","Epoch 17 - MCC: 0.8089\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9156 - loss: 0.2011 - val_accuracy: 0.9047 - val_loss: 0.2247 - mcc: 0.8089\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9125 - loss: 0.2072\n","Epoch 18 - MCC: 0.8089\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9125 - loss: 0.2071 - val_accuracy: 0.9047 - val_loss: 0.2246 - mcc: 0.8089\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9132 - loss: 0.2068\n","Epoch 19 - MCC: 0.8081\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9132 - loss: 0.2066 - val_accuracy: 0.9042 - val_loss: 0.2254 - mcc: 0.8081\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9198 - loss: 0.1911\n","Epoch 20 - MCC: 0.8082\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9196 - loss: 0.1915 - val_accuracy: 0.9039 - val_loss: 0.2262 - mcc: 0.8082\n","Epoch 21/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9158 - loss: 0.2007\n","Epoch 21 - MCC: 0.8072\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.9158 - loss: 0.2007 - val_accuracy: 0.9037 - val_loss: 0.2244 - mcc: 0.8072\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9162 - loss: 0.1998\n","Epoch 22 - MCC: 0.8104\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.9162 - loss: 0.1998 - val_accuracy: 0.9054 - val_loss: 0.2213 - mcc: 0.8104\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9187 - loss: 0.1938\n","Epoch 23 - MCC: 0.8141\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9186 - loss: 0.1939 - val_accuracy: 0.9072 - val_loss: 0.2177 - mcc: 0.8141\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9167 - loss: 0.1981\n","Epoch 24 - MCC: 0.8152\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9167 - loss: 0.1980 - val_accuracy: 0.9078 - val_loss: 0.2164 - mcc: 0.8152\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9220 - loss: 0.1859\n","Epoch 25 - MCC: 0.8149\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9219 - loss: 0.1863 - val_accuracy: 0.9075 - val_loss: 0.2172 - mcc: 0.8149\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9180 - loss: 0.1939\n","Epoch 26 - MCC: 0.8128\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9181 - loss: 0.1939 - val_accuracy: 0.9067 - val_loss: 0.2163 - mcc: 0.8128\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9187 - loss: 0.1928\n","Epoch 27 - MCC: 0.8129\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9187 - loss: 0.1928 - val_accuracy: 0.9064 - val_loss: 0.2176 - mcc: 0.8129\n","Epoch 28/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9184 - loss: 0.1923\n","Epoch 28 - MCC: 0.8122\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9184 - loss: 0.1923 - val_accuracy: 0.9062 - val_loss: 0.2184 - mcc: 0.8122\n","Epoch 29/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9199 - loss: 0.1882\n","Epoch 29 - MCC: 0.8126\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.9199 - loss: 0.1883 - val_accuracy: 0.9065 - val_loss: 0.2160 - mcc: 0.8126\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9171 - loss: 0.1951\n","Epoch 30 - MCC: 0.8122\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9171 - loss: 0.1951 - val_accuracy: 0.9063 - val_loss: 0.2179 - mcc: 0.8122\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 3\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5002 - loss: 0.6609\n","Epoch 1 - MCC: 0.4481\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.5023 - loss: 0.6595 - val_accuracy: 0.6651 - val_loss: 0.5907 - mcc: 0.4481\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7571 - loss: 0.5562\n","Epoch 2 - MCC: 0.7035\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.7587 - loss: 0.5547 - val_accuracy: 0.8511 - val_loss: 0.4010 - mcc: 0.7035\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8487 - loss: 0.3781\n","Epoch 3 - MCC: 0.7247\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8490 - loss: 0.3771 - val_accuracy: 0.8612 - val_loss: 0.3291 - mcc: 0.7247\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8693 - loss: 0.3142\n","Epoch 4 - MCC: 0.7659\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8694 - loss: 0.3140 - val_accuracy: 0.8810 - val_loss: 0.2910 - mcc: 0.7659\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8843 - loss: 0.2834\n","Epoch 5 - MCC: 0.7940\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.8845 - loss: 0.2830 - val_accuracy: 0.8974 - val_loss: 0.2500 - mcc: 0.7940\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8944 - loss: 0.2568\n","Epoch 6 - MCC: 0.8109\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8945 - loss: 0.2564 - val_accuracy: 0.9052 - val_loss: 0.2320 - mcc: 0.8109\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9018 - loss: 0.2379\n","Epoch 7 - MCC: 0.8190\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.9018 - loss: 0.2379 - val_accuracy: 0.9099 - val_loss: 0.2226 - mcc: 0.8190\n","Epoch 8/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9104 - loss: 0.2202\n","Epoch 8 - MCC: 0.8201\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.9099 - loss: 0.2211 - val_accuracy: 0.9105 - val_loss: 0.2214 - mcc: 0.8201\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9003 - loss: 0.2379\n","Epoch 9 - MCC: 0.8210\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9004 - loss: 0.2377 - val_accuracy: 0.9108 - val_loss: 0.2174 - mcc: 0.8210\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9117 - loss: 0.2130\n","Epoch 10 - MCC: 0.8292\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9116 - loss: 0.2134 - val_accuracy: 0.9149 - val_loss: 0.2074 - mcc: 0.8292\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9074 - loss: 0.2217\n","Epoch 11 - MCC: 0.8329\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9075 - loss: 0.2216 - val_accuracy: 0.9168 - val_loss: 0.2033 - mcc: 0.8329\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9076 - loss: 0.2212\n","Epoch 12 - MCC: 0.8286\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9077 - loss: 0.2209 - val_accuracy: 0.9143 - val_loss: 0.2057 - mcc: 0.8286\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9072 - loss: 0.2206\n","Epoch 13 - MCC: 0.8330\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9072 - loss: 0.2204 - val_accuracy: 0.9168 - val_loss: 0.2034 - mcc: 0.8330\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9118 - loss: 0.2110\n","Epoch 14 - MCC: 0.8368\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9118 - loss: 0.2110 - val_accuracy: 0.9187 - val_loss: 0.1986 - mcc: 0.8368\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9153 - loss: 0.2033\n","Epoch 15 - MCC: 0.8326\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9152 - loss: 0.2035 - val_accuracy: 0.9163 - val_loss: 0.1989 - mcc: 0.8326\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9116 - loss: 0.2099\n","Epoch 16 - MCC: 0.8388\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.9116 - loss: 0.2098 - val_accuracy: 0.9197 - val_loss: 0.1934 - mcc: 0.8388\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9129 - loss: 0.2060\n","Epoch 17 - MCC: 0.8389\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9130 - loss: 0.2060 - val_accuracy: 0.9196 - val_loss: 0.1941 - mcc: 0.8389\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9121 - loss: 0.2087\n","Epoch 18 - MCC: 0.8347\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9122 - loss: 0.2084 - val_accuracy: 0.9173 - val_loss: 0.1959 - mcc: 0.8347\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9185 - loss: 0.1939\n","Epoch 19 - MCC: 0.8371\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9183 - loss: 0.1942 - val_accuracy: 0.9189 - val_loss: 0.1960 - mcc: 0.8371\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9115 - loss: 0.2101\n","Epoch 20 - MCC: 0.8386\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9116 - loss: 0.2098 - val_accuracy: 0.9194 - val_loss: 0.1957 - mcc: 0.8386\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9210 - loss: 0.1902\n","Epoch 21 - MCC: 0.8247\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9208 - loss: 0.1906 - val_accuracy: 0.9123 - val_loss: 0.2038 - mcc: 0.8247\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9123 - loss: 0.2063\n","Epoch 22 - MCC: 0.8402\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9124 - loss: 0.2061 - val_accuracy: 0.9204 - val_loss: 0.1897 - mcc: 0.8402\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9151 - loss: 0.2010\n","Epoch 23 - MCC: 0.8461\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9152 - loss: 0.2007 - val_accuracy: 0.9233 - val_loss: 0.1847 - mcc: 0.8461\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9202 - loss: 0.1909\n","Epoch 24 - MCC: 0.8399\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.9201 - loss: 0.1911 - val_accuracy: 0.9202 - val_loss: 0.1920 - mcc: 0.8399\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9138 - loss: 0.2047\n","Epoch 25 - MCC: 0.8389\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9139 - loss: 0.2044 - val_accuracy: 0.9193 - val_loss: 0.1914 - mcc: 0.8389\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9174 - loss: 0.1951\n","Epoch 26 - MCC: 0.8465\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9175 - loss: 0.1950 - val_accuracy: 0.9236 - val_loss: 0.1827 - mcc: 0.8465\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9203 - loss: 0.1892\n","Epoch 27 - MCC: 0.8463\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9203 - loss: 0.1893 - val_accuracy: 0.9235 - val_loss: 0.1844 - mcc: 0.8463\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9196 - loss: 0.1915\n","Epoch 28 - MCC: 0.8485\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9196 - loss: 0.1916 - val_accuracy: 0.9246 - val_loss: 0.1807 - mcc: 0.8485\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9157 - loss: 0.1989\n","Epoch 29 - MCC: 0.8479\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9158 - loss: 0.1987 - val_accuracy: 0.9243 - val_loss: 0.1817 - mcc: 0.8479\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9154 - loss: 0.2003\n","Epoch 30 - MCC: 0.8502\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9156 - loss: 0.1999 - val_accuracy: 0.9254 - val_loss: 0.1783 - mcc: 0.8502\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 4\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5036 - loss: 0.6743\n","Epoch 1 - MCC: 0.4128\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 0.5056 - loss: 0.6728 - val_accuracy: 0.6456 - val_loss: 0.6097 - mcc: 0.4128\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7193 - loss: 0.5796\n","Epoch 2 - MCC: 0.7059\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.7210 - loss: 0.5780 - val_accuracy: 0.8533 - val_loss: 0.4218 - mcc: 0.7059\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8481 - loss: 0.3920\n","Epoch 3 - MCC: 0.7420\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8481 - loss: 0.3913 - val_accuracy: 0.8713 - val_loss: 0.3290 - mcc: 0.7420\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8650 - loss: 0.3263\n","Epoch 4 - MCC: 0.7563\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8650 - loss: 0.3264 - val_accuracy: 0.8775 - val_loss: 0.2995 - mcc: 0.7563\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8733 - loss: 0.3038\n","Epoch 5 - MCC: 0.7861\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8734 - loss: 0.3036 - val_accuracy: 0.8933 - val_loss: 0.2712 - mcc: 0.7861\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8823 - loss: 0.2863\n","Epoch 6 - MCC: 0.7982\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8824 - loss: 0.2860 - val_accuracy: 0.8993 - val_loss: 0.2536 - mcc: 0.7982\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8956 - loss: 0.2555\n","Epoch 7 - MCC: 0.8062\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8954 - loss: 0.2558 - val_accuracy: 0.9028 - val_loss: 0.2459 - mcc: 0.8062\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8968 - loss: 0.2533\n","Epoch 8 - MCC: 0.8094\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.8968 - loss: 0.2531 - val_accuracy: 0.9048 - val_loss: 0.2352 - mcc: 0.8094\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8996 - loss: 0.2423\n","Epoch 9 - MCC: 0.8165\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.8997 - loss: 0.2422 - val_accuracy: 0.9081 - val_loss: 0.2275 - mcc: 0.8165\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9067 - loss: 0.2291\n","Epoch 10 - MCC: 0.8186\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9066 - loss: 0.2293 - val_accuracy: 0.9095 - val_loss: 0.2227 - mcc: 0.8186\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9090 - loss: 0.2209\n","Epoch 11 - MCC: 0.8203\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9087 - loss: 0.2213 - val_accuracy: 0.9101 - val_loss: 0.2256 - mcc: 0.8203\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9041 - loss: 0.2309\n","Epoch 12 - MCC: 0.8212\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9041 - loss: 0.2308 - val_accuracy: 0.9108 - val_loss: 0.2167 - mcc: 0.8212\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9071 - loss: 0.2215\n","Epoch 13 - MCC: 0.8234\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9071 - loss: 0.2215 - val_accuracy: 0.9119 - val_loss: 0.2132 - mcc: 0.8234\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9083 - loss: 0.2198\n","Epoch 14 - MCC: 0.8194\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9083 - loss: 0.2198 - val_accuracy: 0.9096 - val_loss: 0.2207 - mcc: 0.8194\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9052 - loss: 0.2269\n","Epoch 15 - MCC: 0.8232\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9053 - loss: 0.2267 - val_accuracy: 0.9118 - val_loss: 0.2118 - mcc: 0.8232\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9092 - loss: 0.2169\n","Epoch 16 - MCC: 0.8290\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9092 - loss: 0.2168 - val_accuracy: 0.9147 - val_loss: 0.2075 - mcc: 0.8290\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9087 - loss: 0.2161\n","Epoch 17 - MCC: 0.8290\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9088 - loss: 0.2160 - val_accuracy: 0.9144 - val_loss: 0.2119 - mcc: 0.8290\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9055 - loss: 0.2251\n","Epoch 18 - MCC: 0.8290\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9056 - loss: 0.2248 - val_accuracy: 0.9145 - val_loss: 0.2063 - mcc: 0.8290\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9104 - loss: 0.2118\n","Epoch 19 - MCC: 0.8264\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.9104 - loss: 0.2119 - val_accuracy: 0.9132 - val_loss: 0.2072 - mcc: 0.8264\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9150 - loss: 0.2009\n","Epoch 20 - MCC: 0.8341\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9149 - loss: 0.2012 - val_accuracy: 0.9172 - val_loss: 0.2006 - mcc: 0.8341\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9160 - loss: 0.2007\n","Epoch 21 - MCC: 0.8389\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9159 - loss: 0.2009 - val_accuracy: 0.9196 - val_loss: 0.1954 - mcc: 0.8389\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9149 - loss: 0.2035\n","Epoch 22 - MCC: 0.8399\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9148 - loss: 0.2037 - val_accuracy: 0.9201 - val_loss: 0.1940 - mcc: 0.8399\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9181 - loss: 0.1955\n","Epoch 23 - MCC: 0.8308\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9179 - loss: 0.1959 - val_accuracy: 0.9156 - val_loss: 0.2050 - mcc: 0.8308\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9119 - loss: 0.2085\n","Epoch 24 - MCC: 0.8385\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9120 - loss: 0.2083 - val_accuracy: 0.9192 - val_loss: 0.1945 - mcc: 0.8385\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9165 - loss: 0.1970\n","Epoch 25 - MCC: 0.8418\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9165 - loss: 0.1971 - val_accuracy: 0.9211 - val_loss: 0.1899 - mcc: 0.8418\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9153 - loss: 0.1999\n","Epoch 26 - MCC: 0.8369\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9153 - loss: 0.1999 - val_accuracy: 0.9184 - val_loss: 0.1951 - mcc: 0.8369\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9159 - loss: 0.1971\n","Epoch 27 - MCC: 0.8433\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9159 - loss: 0.1971 - val_accuracy: 0.9218 - val_loss: 0.1881 - mcc: 0.8433\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9194 - loss: 0.1919\n","Epoch 28 - MCC: 0.8399\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9193 - loss: 0.1920 - val_accuracy: 0.9201 - val_loss: 0.1953 - mcc: 0.8399\n","Epoch 29/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9188 - loss: 0.1929\n","Epoch 29 - MCC: 0.8474\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9186 - loss: 0.1933 - val_accuracy: 0.9239 - val_loss: 0.1854 - mcc: 0.8474\n","Epoch 30/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9178 - loss: 0.1931\n","Epoch 30 - MCC: 0.8486\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9178 - loss: 0.1932 - val_accuracy: 0.9245 - val_loss: 0.1840 - mcc: 0.8486\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 5\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5403 - loss: 0.6904\n","Epoch 1 - MCC: 0.4421\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.5429 - loss: 0.6884 - val_accuracy: 0.6785 - val_loss: 0.6121 - mcc: 0.4421\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7367 - loss: 0.5749\n","Epoch 2 - MCC: 0.6754\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.7381 - loss: 0.5732 - val_accuracy: 0.8380 - val_loss: 0.4252 - mcc: 0.6754\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8432 - loss: 0.3873\n","Epoch 3 - MCC: 0.7101\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8435 - loss: 0.3864 - val_accuracy: 0.8547 - val_loss: 0.3446 - mcc: 0.7101\n","Epoch 4/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8636 - loss: 0.3310\n","Epoch 4 - MCC: 0.7431\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.8640 - loss: 0.3302 - val_accuracy: 0.8717 - val_loss: 0.3122 - mcc: 0.7431\n","Epoch 5/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8742 - loss: 0.3042\n","Epoch 5 - MCC: 0.7681\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.8747 - loss: 0.3031 - val_accuracy: 0.8837 - val_loss: 0.2869 - mcc: 0.7681\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8920 - loss: 0.2680\n","Epoch 6 - MCC: 0.7737\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8920 - loss: 0.2679 - val_accuracy: 0.8869 - val_loss: 0.2676 - mcc: 0.7737\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8976 - loss: 0.2500\n","Epoch 7 - MCC: 0.7834\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.8975 - loss: 0.2501 - val_accuracy: 0.8916 - val_loss: 0.2559 - mcc: 0.7834\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9026 - loss: 0.2397\n","Epoch 8 - MCC: 0.7854\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9026 - loss: 0.2398 - val_accuracy: 0.8929 - val_loss: 0.2498 - mcc: 0.7854\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9042 - loss: 0.2329\n","Epoch 9 - MCC: 0.8008\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9042 - loss: 0.2329 - val_accuracy: 0.9005 - val_loss: 0.2338 - mcc: 0.8008\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9064 - loss: 0.2253\n","Epoch 10 - MCC: 0.8022\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9064 - loss: 0.2253 - val_accuracy: 0.9013 - val_loss: 0.2325 - mcc: 0.8022\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9089 - loss: 0.2194\n","Epoch 11 - MCC: 0.8077\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9089 - loss: 0.2194 - val_accuracy: 0.9040 - val_loss: 0.2257 - mcc: 0.8077\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9090 - loss: 0.2188\n","Epoch 12 - MCC: 0.7996\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9090 - loss: 0.2189 - val_accuracy: 0.8999 - val_loss: 0.2330 - mcc: 0.7996\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9098 - loss: 0.2167\n","Epoch 13 - MCC: 0.8001\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.9098 - loss: 0.2167 - val_accuracy: 0.9000 - val_loss: 0.2298 - mcc: 0.8001\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9130 - loss: 0.2075\n","Epoch 14 - MCC: 0.8114\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9129 - loss: 0.2078 - val_accuracy: 0.9056 - val_loss: 0.2230 - mcc: 0.8114\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9116 - loss: 0.2131\n","Epoch 15 - MCC: 0.8123\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9116 - loss: 0.2131 - val_accuracy: 0.9063 - val_loss: 0.2210 - mcc: 0.8123\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9129 - loss: 0.2092\n","Epoch 16 - MCC: 0.8139\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9128 - loss: 0.2094 - val_accuracy: 0.9066 - val_loss: 0.2204 - mcc: 0.8139\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9102 - loss: 0.2153\n","Epoch 17 - MCC: 0.8140\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9102 - loss: 0.2153 - val_accuracy: 0.9072 - val_loss: 0.2179 - mcc: 0.8140\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9136 - loss: 0.2067\n","Epoch 18 - MCC: 0.8167\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9136 - loss: 0.2067 - val_accuracy: 0.9085 - val_loss: 0.2146 - mcc: 0.8167\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9158 - loss: 0.2015\n","Epoch 19 - MCC: 0.8217\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9157 - loss: 0.2018 - val_accuracy: 0.9110 - val_loss: 0.2096 - mcc: 0.8217\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9133 - loss: 0.2077\n","Epoch 20 - MCC: 0.8189\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9134 - loss: 0.2075 - val_accuracy: 0.9095 - val_loss: 0.2155 - mcc: 0.8189\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9113 - loss: 0.2110\n","Epoch 21 - MCC: 0.8211\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9115 - loss: 0.2107 - val_accuracy: 0.9107 - val_loss: 0.2090 - mcc: 0.8211\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9182 - loss: 0.1941\n","Epoch 22 - MCC: 0.8151\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9180 - loss: 0.1945 - val_accuracy: 0.9072 - val_loss: 0.2193 - mcc: 0.8151\n","Epoch 23/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9100 - loss: 0.2131\n","Epoch 23 - MCC: 0.8194\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.9104 - loss: 0.2122 - val_accuracy: 0.9093 - val_loss: 0.2151 - mcc: 0.8194\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9187 - loss: 0.1975\n","Epoch 24 - MCC: 0.8273\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9186 - loss: 0.1975 - val_accuracy: 0.9137 - val_loss: 0.2043 - mcc: 0.8273\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9215 - loss: 0.1873\n","Epoch 25 - MCC: 0.8177\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9213 - loss: 0.1877 - val_accuracy: 0.9090 - val_loss: 0.2094 - mcc: 0.8177\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9136 - loss: 0.2028\n","Epoch 26 - MCC: 0.8256\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9137 - loss: 0.2026 - val_accuracy: 0.9129 - val_loss: 0.2053 - mcc: 0.8256\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9214 - loss: 0.1888\n","Epoch 27 - MCC: 0.8283\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9213 - loss: 0.1889 - val_accuracy: 0.9140 - val_loss: 0.2056 - mcc: 0.8283\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9216 - loss: 0.1877\n","Epoch 28 - MCC: 0.8242\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9215 - loss: 0.1879 - val_accuracy: 0.9116 - val_loss: 0.2121 - mcc: 0.8242\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9196 - loss: 0.1936\n","Epoch 29 - MCC: 0.8303\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9196 - loss: 0.1937 - val_accuracy: 0.9153 - val_loss: 0.1990 - mcc: 0.8303\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9208 - loss: 0.1886\n","Epoch 30 - MCC: 0.8315\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9208 - loss: 0.1887 - val_accuracy: 0.9159 - val_loss: 0.2020 - mcc: 0.8315\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.92542,\n","              'mean': 0.9192133333333332,\n","              'min': 0.9062933333333333,\n","              'std': 0.007313982955043506},\n"," 'Inference Time (s/sample)': {'max': 0.0022634172439575193,\n","                               'mean': 0.0018718118667602538,\n","                               'min': 0.0013329648971557616,\n","                               'std': 0.00030304235545192576},\n"," 'MCC': {'max': 0.8502276288211214,\n","         'mean': 0.8379905489849658,\n","         'min': 0.8121653215936274,\n","         'std': 0.014566018515427237},\n"," 'Parameters': 4513,\n"," 'Train Time (s)': {'max': 46.50385141372681,\n","                    'mean': 44.486596727371214,\n","                    'min': 42.595385789871216,\n","                    'std': 1.4723381442300323},\n"," 'Training Accuracy': [[0.5564084053039551,\n","                        0.7363433837890625,\n","                        0.8390917181968689,\n","                        0.8597398996353149,\n","                        0.8800250291824341,\n","                        0.8929983377456665,\n","                        0.897348165512085,\n","                        0.9012200236320496,\n","                        0.9027633666992188,\n","                        0.9055432677268982,\n","                        0.9055965542793274,\n","                        0.9051966667175293,\n","                        0.9075149297714233,\n","                        0.9082999229431152,\n","                        0.9098532795906067,\n","                        0.9106184244155884,\n","                        0.912403404712677,\n","                        0.913421630859375,\n","                        0.9132832288742065,\n","                        0.9141533374786377,\n","                        0.9139801263809204,\n","                        0.9144865274429321,\n","                        0.9150433540344238,\n","                        0.9168583750724792,\n","                        0.9164049625396729,\n","                        0.91663658618927,\n","                        0.9151633381843567,\n","                        0.9172666668891907,\n","                        0.9183750748634338,\n","                        0.9194067120552063],\n","                       [0.5741115808486938,\n","                        0.7961400747299194,\n","                        0.8567298650741577,\n","                        0.8805183172225952,\n","                        0.8961033821105957,\n","                        0.901563286781311,\n","                        0.9054933190345764,\n","                        0.9071366190910339,\n","                        0.9055549502372742,\n","                        0.906684935092926,\n","                        0.9098883271217346,\n","                        0.9104116559028625,\n","                        0.9106000661849976,\n","                        0.9118400812149048,\n","                        0.9124099612236023,\n","                        0.9131782054901123,\n","                        0.913806676864624,\n","                        0.9137783646583557,\n","                        0.9152466654777527,\n","                        0.9156001210212708,\n","                        0.9160215854644775,\n","                        0.9164400696754456,\n","                        0.91778165102005,\n","                        0.9188433885574341,\n","                        0.917898416519165,\n","                        0.9194032549858093,\n","                        0.9192583560943604,\n","                        0.9183881878852844,\n","                        0.9199367761611938,\n","                        0.91728675365448],\n","                       [0.5267999768257141,\n","                        0.7986384034156799,\n","                        0.8559901714324951,\n","                        0.8717601299285889,\n","                        0.888396680355072,\n","                        0.8986483216285706,\n","                        0.9023750424385071,\n","                        0.904491662979126,\n","                        0.9038481712341309,\n","                        0.9072766900062561,\n","                        0.9092148542404175,\n","                        0.9103900790214539,\n","                        0.9091783165931702,\n","                        0.9118300080299377,\n","                        0.9126966595649719,\n","                        0.9128333926200867,\n","                        0.9141666889190674,\n","                        0.915056586265564,\n","                        0.9152334332466125,\n","                        0.915203332901001,\n","                        0.9164050221443176,\n","                        0.915255069732666,\n","                        0.9180017113685608,\n","                        0.917808473110199,\n","                        0.9171782732009888,\n","                        0.9183700084686279,\n","                        0.9191882610321045,\n","                        0.9186034202575684,\n","                        0.9183132648468018,\n","                        0.920116662979126],\n","                       [0.5302199721336365,\n","                        0.7650666236877441,\n","                        0.8468298316001892,\n","                        0.863758385181427,\n","                        0.8759832978248596,\n","                        0.8864531517028809,\n","                        0.8921282291412354,\n","                        0.8980649709701538,\n","                        0.9005966782569885,\n","                        0.903583288192749,\n","                        0.9034817218780518,\n","                        0.9046417474746704,\n","                        0.9074316620826721,\n","                        0.908446729183197,\n","                        0.9068633913993835,\n","                        0.9102883338928223,\n","                        0.910163164138794,\n","                        0.9087899923324585,\n","                        0.9100000262260437,\n","                        0.9120567440986633,\n","                        0.9127050638198853,\n","                        0.911953330039978,\n","                        0.9130783677101135,\n","                        0.913398265838623,\n","                        0.9154083132743835,\n","                        0.9153583645820618,\n","                        0.9163200855255127,\n","                        0.9170249700546265,\n","                        0.916046679019928,\n","                        0.9175967574119568],\n","                       [0.5751266479492188,\n","                        0.7730982899665833,\n","                        0.8511716723442078,\n","                        0.8689532279968262,\n","                        0.8808000087738037,\n","                        0.8920533657073975,\n","                        0.8953450918197632,\n","                        0.9009400010108948,\n","                        0.903988242149353,\n","                        0.9068466424942017,\n","                        0.9084649085998535,\n","                        0.908568263053894,\n","                        0.910026490688324,\n","                        0.9105534553527832,\n","                        0.91163170337677,\n","                        0.9115216732025146,\n","                        0.9104066491127014,\n","                        0.913481593132019,\n","                        0.9133865237236023,\n","                        0.9152700304985046,\n","                        0.914318323135376,\n","                        0.9139766693115234,\n","                        0.9158115983009338,\n","                        0.9172532558441162,\n","                        0.9165148735046387,\n","                        0.9176667928695679,\n","                        0.9192899465560913,\n","                        0.9188400506973267,\n","                        0.9188166856765747,\n","                        0.920115053653717]],\n"," 'Training Loss': [[0.6729682683944702,\n","                    0.5655213594436646,\n","                    0.3899884521961212,\n","                    0.3390873670578003,\n","                    0.29424455761909485,\n","                    0.259872168302536,\n","                    0.2494836449623108,\n","                    0.23784591257572174,\n","                    0.23374062776565552,\n","                    0.2254258096218109,\n","                    0.22459739446640015,\n","                    0.22415843605995178,\n","                    0.21983124315738678,\n","                    0.21671199798583984,\n","                    0.21321028470993042,\n","                    0.21126322448253632,\n","                    0.20754770934581757,\n","                    0.20587565004825592,\n","                    0.20453092455863953,\n","                    0.20410828292369843,\n","                    0.20379576086997986,\n","                    0.20247656106948853,\n","                    0.20121526718139648,\n","                    0.1965978741645813,\n","                    0.1976882815361023,\n","                    0.19714860618114471,\n","                    0.20058052241802216,\n","                    0.19499006867408752,\n","                    0.19329456984996796,\n","                    0.19052016735076904],\n","                   [0.6416512131690979,\n","                    0.5012758374214172,\n","                    0.349370539188385,\n","                    0.2916273772716522,\n","                    0.25214260816574097,\n","                    0.23743993043899536,\n","                    0.22873374819755554,\n","                    0.22369520366191864,\n","                    0.22778980433940887,\n","                    0.22371834516525269,\n","                    0.216179758310318,\n","                    0.21482160687446594,\n","                    0.21306857466697693,\n","                    0.2110007107257843,\n","                    0.2094622254371643,\n","                    0.2068387269973755,\n","                    0.20567558705806732,\n","                    0.2046755850315094,\n","                    0.20236551761627197,\n","                    0.20082446932792664,\n","                    0.19985303282737732,\n","                    0.19957326352596283,\n","                    0.19594621658325195,\n","                    0.1932583451271057,\n","                    0.19611258804798126,\n","                    0.1917143613100052,\n","                    0.19168034195899963,\n","                    0.19241675734519958,\n","                    0.18990685045719147,\n","                    0.1958886682987213],\n","                   [0.6425549387931824,\n","                    0.5151488780975342,\n","                    0.35359856486320496,\n","                    0.30912578105926514,\n","                    0.271971732378006,\n","                    0.24630063772201538,\n","                    0.23716135323047638,\n","                    0.23132172226905823,\n","                    0.23122037947177887,\n","                    0.22331909835338593,\n","                    0.21780341863632202,\n","                    0.2152281254529953,\n","                    0.21597133576869965,\n","                    0.2106819897890091,\n","                    0.20864444971084595,\n","                    0.20789813995361328,\n","                    0.2046927660703659,\n","                    0.2022971361875534,\n","                    0.2015799582004547,\n","                    0.20272134244441986,\n","                    0.20011171698570251,\n","                    0.20116347074508667,\n","                    0.19548137485980988,\n","                    0.1962851732969284,\n","                    0.19729654490947723,\n","                    0.19459770619869232,\n","                    0.19188106060028076,\n","                    0.1940828114748001,\n","                    0.19427403807640076,\n","                    0.189991757273674],\n","                   [0.6544680595397949,\n","                    0.5378888845443726,\n","                    0.37421104311943054,\n","                    0.32789716124534607,\n","                    0.2998610734939575,\n","                    0.2769254148006439,\n","                    0.2633398175239563,\n","                    0.24970796704292297,\n","                    0.24039804935455322,\n","                    0.23365606367588043,\n","                    0.2324695587158203,\n","                    0.22910647094249725,\n","                    0.22198350727558136,\n","                    0.21914592385292053,\n","                    0.22162732481956482,\n","                    0.2141398936510086,\n","                    0.21298642456531525,\n","                    0.21653497219085693,\n","                    0.21353062987327576,\n","                    0.20863647758960724,\n","                    0.2070859968662262,\n","                    0.2081039845943451,\n","                    0.2055930346250534,\n","                    0.2040501832962036,\n","                    0.19960330426692963,\n","                    0.19928078353405,\n","                    0.19697439670562744,\n","                    0.19560271501541138,\n","                    0.19799138605594635,\n","                    0.19399313628673553],\n","                   [0.6642889380455017,\n","                    0.5331080555915833,\n","                    0.36482593417167664,\n","                    0.3198464512825012,\n","                    0.29080748558044434,\n","                    0.2653149366378784,\n","                    0.25380587577819824,\n","                    0.24248728156089783,\n","                    0.23208603262901306,\n","                    0.22484426200389862,\n","                    0.22090081870555878,\n","                    0.2205323576927185,\n","                    0.21683870255947113,\n","                    0.21482819318771362,\n","                    0.2119164913892746,\n","                    0.21231813728809357,\n","                    0.21397238969802856,\n","                    0.20663411915302277,\n","                    0.2075904905796051,\n","                    0.20256587862968445,\n","                    0.20413349568843842,\n","                    0.20437416434288025,\n","                    0.2010459005832672,\n","                    0.19868680834770203,\n","                    0.1983814239501953,\n","                    0.19639353454113007,\n","                    0.19307327270507812,\n","                    0.19362878799438477,\n","                    0.19386756420135498,\n","                    0.19089162349700928]],\n"," 'Validation Accuracy': [[0.6268199682235718,\n","                          0.8338999152183533,\n","                          0.8679667115211487,\n","                          0.8757334351539612,\n","                          0.8940799832344055,\n","                          0.8998000621795654,\n","                          0.9043532609939575,\n","                          0.9057733416557312,\n","                          0.9083999395370483,\n","                          0.9081466794013977,\n","                          0.9066600203514099,\n","                          0.9113266468048096,\n","                          0.9127199649810791,\n","                          0.914473295211792,\n","                          0.9166666269302368,\n","                          0.9128466248512268,\n","                          0.9163333177566528,\n","                          0.9193199872970581,\n","                          0.918013334274292,\n","                          0.9136266112327576,\n","                          0.9185799956321716,\n","                          0.9199000000953674,\n","                          0.9207400679588318,\n","                          0.9210333824157715,\n","                          0.9198199510574341,\n","                          0.9191800951957703,\n","                          0.9206666350364685,\n","                          0.9222066402435303,\n","                          0.9234799742698669,\n","                          0.9240133762359619],\n","                         [0.662433385848999,\n","                          0.8377199769020081,\n","                          0.8579666614532471,\n","                          0.8865267038345337,\n","                          0.8924600481987,\n","                          0.8970666527748108,\n","                          0.8969999551773071,\n","                          0.8948466777801514,\n","                          0.8968066573143005,\n","                          0.9014732837677002,\n","                          0.9012600183486938,\n","                          0.9025800228118896,\n","                          0.9003666639328003,\n","                          0.9005666375160217,\n","                          0.9027199745178223,\n","                          0.8983533382415771,\n","                          0.9046800136566162,\n","                          0.9046999216079712,\n","                          0.9042466878890991,\n","                          0.9039400219917297,\n","                          0.9037200212478638,\n","                          0.9054266214370728,\n","                          0.9071799516677856,\n","                          0.9077799916267395,\n","                          0.9074933528900146,\n","                          0.9066532850265503,\n","                          0.906440019607544,\n","                          0.9061800241470337,\n","                          0.9065266251564026,\n","                          0.9062933325767517],\n","                         [0.6651066541671753,\n","                          0.8511067032814026,\n","                          0.8611667156219482,\n","                          0.8809932470321655,\n","                          0.8973667025566101,\n","                          0.9052333235740662,\n","                          0.9098598957061768,\n","                          0.9104599952697754,\n","                          0.9108333587646484,\n","                          0.9149466156959534,\n","                          0.9168066382408142,\n","                          0.9143199324607849,\n","                          0.9167865514755249,\n","                          0.9186933040618896,\n","                          0.9163267612457275,\n","                          0.9197400808334351,\n","                          0.919640064239502,\n","                          0.9173333048820496,\n","                          0.9188733696937561,\n","                          0.9194400906562805,\n","                          0.9123399257659912,\n","                          0.9203733801841736,\n","                          0.9233466982841492,\n","                          0.9202133417129517,\n","                          0.9193333387374878,\n","                          0.9235733151435852,\n","                          0.9234667420387268,\n","                          0.9245733618736267,\n","                          0.9242799878120422,\n","                          0.9254199266433716],\n","                         [0.6455999612808228,\n","                          0.8533467054367065,\n","                          0.8712533712387085,\n","                          0.8774666786193848,\n","                          0.8933266401290894,\n","                          0.8993266224861145,\n","                          0.9027933478355408,\n","                          0.9048399925231934,\n","                          0.9081466197967529,\n","                          0.9094666838645935,\n","                          0.9100666642189026,\n","                          0.9107600450515747,\n","                          0.9119000434875488,\n","                          0.9096466302871704,\n","                          0.9117799401283264,\n","                          0.9147199988365173,\n","                          0.9143933057785034,\n","                          0.914513349533081,\n","                          0.9131932854652405,\n","                          0.9172333478927612,\n","                          0.9196399450302124,\n","                          0.920146644115448,\n","                          0.9155533313751221,\n","                          0.9192066192626953,\n","                          0.9210666418075562,\n","                          0.9183599948883057,\n","                          0.9218133687973022,\n","                          0.9200600385665894,\n","                          0.9238667488098145,\n","                          0.9244800806045532],\n","                         [0.6784533858299255,\n","                          0.8380266427993774,\n","                          0.8547199964523315,\n","                          0.8717000484466553,\n","                          0.8836734294891357,\n","                          0.8869333863258362,\n","                          0.8916199207305908,\n","                          0.8929200172424316,\n","                          0.9005266427993774,\n","                          0.9013134241104126,\n","                          0.9040333032608032,\n","                          0.8999466896057129,\n","                          0.9000066518783569,\n","                          0.9055867195129395,\n","                          0.9063066840171814,\n","                          0.9065799713134766,\n","                          0.9071733355522156,\n","                          0.908519983291626,\n","                          0.9109932780265808,\n","                          0.9094600081443787,\n","                          0.9106665849685669,\n","                          0.9071932435035706,\n","                          0.9093332886695862,\n","                          0.91374671459198,\n","                          0.909000039100647,\n","                          0.9128933548927307,\n","                          0.9139999151229858,\n","                          0.9115732312202454,\n","                          0.9153265953063965,\n","                          0.9158599376678467]],\n"," 'Validation Loss': [[0.6248022317886353,\n","                      0.46231526136398315,\n","                      0.3483988046646118,\n","                      0.3071199655532837,\n","                      0.25768765807151794,\n","                      0.24322013556957245,\n","                      0.23201929032802582,\n","                      0.22609519958496094,\n","                      0.21759913861751556,\n","                      0.21516677737236023,\n","                      0.21568623185157776,\n","                      0.20760555565357208,\n","                      0.20495374500751495,\n","                      0.20217512547969818,\n","                      0.19671986997127533,\n","                      0.20030450820922852,\n","                      0.19653388857841492,\n","                      0.19214753806591034,\n","                      0.19499614834785461,\n","                      0.19757476449012756,\n","                      0.189244344830513,\n","                      0.18742378056049347,\n","                      0.18771082162857056,\n","                      0.18544013798236847,\n","                      0.18458084762096405,\n","                      0.19343484938144684,\n","                      0.18491388857364655,\n","                      0.1852908730506897,\n","                      0.17742933332920074,\n","                      0.17711789906024933],\n","                     [0.5978071689605713,\n","                      0.40041524171829224,\n","                      0.3375084698200226,\n","                      0.28226467967033386,\n","                      0.257211297750473,\n","                      0.24973230063915253,\n","                      0.2454419583082199,\n","                      0.24886101484298706,\n","                      0.2433173507452011,\n","                      0.235510915517807,\n","                      0.2351933866739273,\n","                      0.23272430896759033,\n","                      0.23628577589988708,\n","                      0.23272554576396942,\n","                      0.23031505942344666,\n","                      0.23492930829524994,\n","                      0.2246822863817215,\n","                      0.224589005112648,\n","                      0.22537681460380554,\n","                      0.22615627944469452,\n","                      0.22444218397140503,\n","                      0.22130195796489716,\n","                      0.21766728162765503,\n","                      0.21642394363880157,\n","                      0.21716643869876862,\n","                      0.21625152230262756,\n","                      0.21764706075191498,\n","                      0.21839484572410583,\n","                      0.21595686674118042,\n","                      0.21786285936832428],\n","                     [0.5907355546951294,\n","                      0.40101224184036255,\n","                      0.3291057348251343,\n","                      0.2910347282886505,\n","                      0.25000113248825073,\n","                      0.2319563329219818,\n","                      0.22258582711219788,\n","                      0.22139570116996765,\n","                      0.21737989783287048,\n","                      0.20739111304283142,\n","                      0.2032841295003891,\n","                      0.20566999912261963,\n","                      0.20338882505893707,\n","                      0.1985825002193451,\n","                      0.19893161952495575,\n","                      0.1934417486190796,\n","                      0.1940886676311493,\n","                      0.19586004316806793,\n","                      0.19596721231937408,\n","                      0.19571073353290558,\n","                      0.2037714570760727,\n","                      0.18966901302337646,\n","                      0.18466109037399292,\n","                      0.192001074552536,\n","                      0.19144152104854584,\n","                      0.18270137906074524,\n","                      0.18444910645484924,\n","                      0.18070289492607117,\n","                      0.18174241483211517,\n","                      0.17830635607242584],\n","                     [0.6096868515014648,\n","                      0.4217715859413147,\n","                      0.32896098494529724,\n","                      0.2994650602340698,\n","                      0.2711865305900574,\n","                      0.25362586975097656,\n","                      0.24588482081890106,\n","                      0.2351762056350708,\n","                      0.22745835781097412,\n","                      0.22271761298179626,\n","                      0.2256198674440384,\n","                      0.21667622029781342,\n","                      0.21317607164382935,\n","                      0.22072377800941467,\n","                      0.21179531514644623,\n","                      0.20748507976531982,\n","                      0.21190793812274933,\n","                      0.20633287727832794,\n","                      0.20717868208885193,\n","                      0.20062941312789917,\n","                      0.19542378187179565,\n","                      0.19403865933418274,\n","                      0.20497404038906097,\n","                      0.19450847804546356,\n","                      0.18993622064590454,\n","                      0.19512130320072174,\n","                      0.18813732266426086,\n","                      0.19533373415470123,\n","                      0.18541105091571808,\n","                      0.18395735323429108],\n","                     [0.6120880246162415,\n","                      0.42520323395729065,\n","                      0.3446037769317627,\n","                      0.31223711371421814,\n","                      0.2869107127189636,\n","                      0.2676388919353485,\n","                      0.2559008002281189,\n","                      0.24981847405433655,\n","                      0.23380440473556519,\n","                      0.23248565196990967,\n","                      0.22566713392734528,\n","                      0.23299767076969147,\n","                      0.2297780066728592,\n","                      0.22299757599830627,\n","                      0.22098293900489807,\n","                      0.22042663395404816,\n","                      0.21791605651378632,\n","                      0.21464277803897858,\n","                      0.20964986085891724,\n","                      0.215481698513031,\n","                      0.2090473175048828,\n","                      0.21930471062660217,\n","                      0.21513845026493073,\n","                      0.20428501069545746,\n","                      0.20944266021251678,\n","                      0.2052702158689499,\n","                      0.20562796294689178,\n","                      0.21213895082473755,\n","                      0.19896166026592255,\n","                      0.20196494460105896]],\n"," 'Validation MCC': [[0.36848713450228504,\n","                     0.6671893860708213,\n","                     0.7354521328376706,\n","                     0.7504707964689442,\n","                     0.7874449518303598,\n","                     0.8001909032229727,\n","                     0.808015580526836,\n","                     0.8113679763680164,\n","                     0.8163650891272596,\n","                     0.8161222866877162,\n","                     0.8134454328568446,\n","                     0.822859410476393,\n","                     0.8254092809792183,\n","                     0.8289114146290831,\n","                     0.8327331282094939,\n","                     0.825680621622077,\n","                     0.8324194319261591,\n","                     0.8380087131068437,\n","                     0.8353924634564964,\n","                     0.8277825436350195,\n","                     0.8365258818143243,\n","                     0.8392765562776539,\n","                     0.8408856871688054,\n","                     0.8415899001624367,\n","                     0.8390774363598426,\n","                     0.8381091598161807,\n","                     0.8407095073954952,\n","                     0.8441473806981163,\n","                     0.8463930269957102,\n","                     0.8474394406167394],\n","                    [0.42266709762367993,\n","                     0.675090292939266,\n","                     0.717290989820331,\n","                     0.7724789340888205,\n","                     0.7845154282982744,\n","                     0.7936255096015078,\n","                     0.7937914531807677,\n","                     0.7900574219658008,\n","                     0.793497875667193,\n","                     0.8025466118978225,\n","                     0.8024820341908686,\n","                     0.8047739607544405,\n","                     0.8009876781294851,\n","                     0.800689745633597,\n","                     0.8053154217467917,\n","                     0.7965056873407979,\n","                     0.8088963460032187,\n","                     0.8089322830925781,\n","                     0.8081038184175761,\n","                     0.8082368265466946,\n","                     0.8072027927341622,\n","                     0.8103858741069997,\n","                     0.8140532129179943,\n","                     0.815188980202539,\n","                     0.8148601837031485,\n","                     0.8128499188758213,\n","                     0.8128750007575638,\n","                     0.812226728573692,\n","                     0.8126129774010504,\n","                     0.8121653215936274],\n","                    [0.4481012553103266,\n","                     0.7034646707966936,\n","                     0.7246752178775083,\n","                     0.7658531949963425,\n","                     0.7939857434267971,\n","                     0.8108867884842307,\n","                     0.8189554935840989,\n","                     0.8201475798046598,\n","                     0.8209820717371205,\n","                     0.8291693383254155,\n","                     0.8329006041852429,\n","                     0.8286050880581622,\n","                     0.8330065098187412,\n","                     0.8367570551483351,\n","                     0.8325600911533365,\n","                     0.8388287266151166,\n","                     0.8389145301983353,\n","                     0.834691103771773,\n","                     0.8370540679450761,\n","                     0.8385956873544682,\n","                     0.8247289000084316,\n","                     0.8401969547023871,\n","                     0.8461088162546965,\n","                     0.839898067528488,\n","                     0.8388687322144661,\n","                     0.8464992092705139,\n","                     0.8462845256338433,\n","                     0.8485271755417831,\n","                     0.8479349540071854,\n","                     0.8502276288211214],\n","                    [0.41275937672924123,\n","                     0.7059200388874571,\n","                     0.7419988735339602,\n","                     0.7563062436833784,\n","                     0.7861033782476424,\n","                     0.7981714140397252,\n","                     0.8062459720315596,\n","                     0.8093670040148903,\n","                     0.8164839680333503,\n","                     0.8185711906445783,\n","                     0.8202747308899502,\n","                     0.8212484382959662,\n","                     0.8233754331435504,\n","                     0.8194387142367955,\n","                     0.8231837844552521,\n","                     0.8290213947000914,\n","                     0.8290452462165186,\n","                     0.8289761031204587,\n","                     0.8263870022526895,\n","                     0.8341059111906954,\n","                     0.8389370386821691,\n","                     0.8398981927287675,\n","                     0.8307934970715631,\n","                     0.8384876564374514,\n","                     0.8417769619126066,\n","                     0.8369408490202168,\n","                     0.8433211340213067,\n","                     0.8398719197106456,\n","                     0.847395296151082,\n","                     0.8486131796755175],\n","                    [0.44214267311168626,\n","                     0.6754387458643017,\n","                     0.710099924178648,\n","                     0.7430797062577993,\n","                     0.7681115120974508,\n","                     0.7736737996456017,\n","                     0.7834261095992388,\n","                     0.7854394793089475,\n","                     0.8007713424890937,\n","                     0.8022474547388616,\n","                     0.8077001973053214,\n","                     0.7996421106110829,\n","                     0.800111558571277,\n","                     0.8113618479271959,\n","                     0.8123026031402887,\n","                     0.8139393964012841,\n","                     0.8140225943461357,\n","                     0.816691571445879,\n","                     0.8216558500649203,\n","                     0.8188834524596615,\n","                     0.8211154332100822,\n","                     0.8151016792167997,\n","                     0.819403733048436,\n","                     0.827280431729073,\n","                     0.8177161084472192,\n","                     0.8256122265805118,\n","                     0.8282546481376961,\n","                     0.8241664276599546,\n","                     0.8303341532329565,\n","                     0.8315071742178237]]}\n","Training Model: LSTM_Dense, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4817 - loss: 0.6685\n","Epoch 1 - MCC: 0.3490\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.4824 - loss: 0.6679 - val_accuracy: 0.5915 - val_loss: 0.6112 - mcc: 0.3490\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6809 - loss: 0.5880\n","Epoch 2 - MCC: 0.6884\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.6838 - loss: 0.5871 - val_accuracy: 0.8347 - val_loss: 0.5123 - mcc: 0.6884\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8463 - loss: 0.4831\n","Epoch 3 - MCC: 0.7583\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8467 - loss: 0.4816 - val_accuracy: 0.8793 - val_loss: 0.3700 - mcc: 0.7583\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8746 - loss: 0.3518\n","Epoch 4 - MCC: 0.7902\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8748 - loss: 0.3508 - val_accuracy: 0.8948 - val_loss: 0.2740 - mcc: 0.7902\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8941 - loss: 0.2700\n","Epoch 5 - MCC: 0.8056\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8941 - loss: 0.2698 - val_accuracy: 0.9024 - val_loss: 0.2432 - mcc: 0.8056\n","Epoch 6/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8956 - loss: 0.2553\n","Epoch 6 - MCC: 0.8177\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8961 - loss: 0.2542 - val_accuracy: 0.9092 - val_loss: 0.2208 - mcc: 0.8177\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9052 - loss: 0.2307\n","Epoch 7 - MCC: 0.8241\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9052 - loss: 0.2306 - val_accuracy: 0.9123 - val_loss: 0.2134 - mcc: 0.8241\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9073 - loss: 0.2232\n","Epoch 8 - MCC: 0.8259\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9073 - loss: 0.2231 - val_accuracy: 0.9133 - val_loss: 0.2068 - mcc: 0.8259\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9096 - loss: 0.2160\n","Epoch 9 - MCC: 0.8307\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9096 - loss: 0.2160 - val_accuracy: 0.9155 - val_loss: 0.1996 - mcc: 0.8307\n","Epoch 10/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9078 - loss: 0.2173\n","Epoch 10 - MCC: 0.8319\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9080 - loss: 0.2169 - val_accuracy: 0.9163 - val_loss: 0.1979 - mcc: 0.8319\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9087 - loss: 0.2159\n","Epoch 11 - MCC: 0.8319\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.9088 - loss: 0.2158 - val_accuracy: 0.9162 - val_loss: 0.1983 - mcc: 0.8319\n","Epoch 12/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9093 - loss: 0.2126\n","Epoch 12 - MCC: 0.8337\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9095 - loss: 0.2123 - val_accuracy: 0.9172 - val_loss: 0.1968 - mcc: 0.8337\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9130 - loss: 0.2053\n","Epoch 13 - MCC: 0.8390\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - accuracy: 0.9130 - loss: 0.2053 - val_accuracy: 0.9198 - val_loss: 0.1890 - mcc: 0.8390\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9172 - loss: 0.1976\n","Epoch 14 - MCC: 0.8345\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9171 - loss: 0.1979 - val_accuracy: 0.9174 - val_loss: 0.1958 - mcc: 0.8345\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9101 - loss: 0.2110\n","Epoch 15 - MCC: 0.8388\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9103 - loss: 0.2106 - val_accuracy: 0.9197 - val_loss: 0.1883 - mcc: 0.8388\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9152 - loss: 0.2011\n","Epoch 16 - MCC: 0.8396\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9152 - loss: 0.2011 - val_accuracy: 0.9201 - val_loss: 0.1863 - mcc: 0.8396\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9167 - loss: 0.1983\n","Epoch 17 - MCC: 0.8364\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9166 - loss: 0.1984 - val_accuracy: 0.9184 - val_loss: 0.1917 - mcc: 0.8364\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9159 - loss: 0.1989\n","Epoch 18 - MCC: 0.8396\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.9159 - loss: 0.1989 - val_accuracy: 0.9201 - val_loss: 0.1855 - mcc: 0.8396\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9185 - loss: 0.1944\n","Epoch 19 - MCC: 0.8344\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9184 - loss: 0.1945 - val_accuracy: 0.9171 - val_loss: 0.1912 - mcc: 0.8344\n","Epoch 20/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9141 - loss: 0.2003\n","Epoch 20 - MCC: 0.8404\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9142 - loss: 0.2001 - val_accuracy: 0.9205 - val_loss: 0.1886 - mcc: 0.8404\n","Epoch 21/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9128 - loss: 0.2050\n","Epoch 21 - MCC: 0.8408\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9131 - loss: 0.2044 - val_accuracy: 0.9204 - val_loss: 0.1866 - mcc: 0.8408\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9198 - loss: 0.1926\n","Epoch 22 - MCC: 0.8369\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9197 - loss: 0.1927 - val_accuracy: 0.9182 - val_loss: 0.1917 - mcc: 0.8369\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9128 - loss: 0.2037\n","Epoch 23 - MCC: 0.8425\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9129 - loss: 0.2035 - val_accuracy: 0.9212 - val_loss: 0.1861 - mcc: 0.8425\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9158 - loss: 0.1977\n","Epoch 24 - MCC: 0.8474\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9159 - loss: 0.1975 - val_accuracy: 0.9238 - val_loss: 0.1791 - mcc: 0.8474\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9218 - loss: 0.1844\n","Epoch 25 - MCC: 0.8499\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9217 - loss: 0.1846 - val_accuracy: 0.9252 - val_loss: 0.1763 - mcc: 0.8499\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9207 - loss: 0.1891\n","Epoch 26 - MCC: 0.8524\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9207 - loss: 0.1891 - val_accuracy: 0.9265 - val_loss: 0.1734 - mcc: 0.8524\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9212 - loss: 0.1868\n","Epoch 27 - MCC: 0.8501\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9212 - loss: 0.1868 - val_accuracy: 0.9250 - val_loss: 0.1772 - mcc: 0.8501\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9234 - loss: 0.1842\n","Epoch 28 - MCC: 0.8518\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9232 - loss: 0.1844 - val_accuracy: 0.9262 - val_loss: 0.1738 - mcc: 0.8518\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9192 - loss: 0.1910\n","Epoch 29 - MCC: 0.8543\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9193 - loss: 0.1909 - val_accuracy: 0.9274 - val_loss: 0.1712 - mcc: 0.8543\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9191 - loss: 0.1925\n","Epoch 30 - MCC: 0.8536\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9192 - loss: 0.1922 - val_accuracy: 0.9271 - val_loss: 0.1719 - mcc: 0.8536\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5276 - loss: 0.6883\n","Epoch 1 - MCC: 0.4391\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.5296 - loss: 0.6876 - val_accuracy: 0.6854 - val_loss: 0.6322 - mcc: 0.4391\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7317 - loss: 0.5970\n","Epoch 2 - MCC: 0.6379\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.7333 - loss: 0.5954 - val_accuracy: 0.8183 - val_loss: 0.4698 - mcc: 0.6379\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8431 - loss: 0.4035\n","Epoch 3 - MCC: 0.7046\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8434 - loss: 0.4022 - val_accuracy: 0.8500 - val_loss: 0.3520 - mcc: 0.7046\n","Epoch 4/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8734 - loss: 0.3025\n","Epoch 4 - MCC: 0.7691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8738 - loss: 0.3015 - val_accuracy: 0.8843 - val_loss: 0.2784 - mcc: 0.7691\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8964 - loss: 0.2462\n","Epoch 5 - MCC: 0.7764\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 60ms/step - accuracy: 0.8963 - loss: 0.2462 - val_accuracy: 0.8885 - val_loss: 0.2609 - mcc: 0.7764\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9049 - loss: 0.2274\n","Epoch 6 - MCC: 0.7855\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9048 - loss: 0.2276 - val_accuracy: 0.8930 - val_loss: 0.2497 - mcc: 0.7855\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9006 - loss: 0.2324\n","Epoch 7 - MCC: 0.7921\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9007 - loss: 0.2323 - val_accuracy: 0.8956 - val_loss: 0.2443 - mcc: 0.7921\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9059 - loss: 0.2204\n","Epoch 8 - MCC: 0.7939\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9059 - loss: 0.2203 - val_accuracy: 0.8970 - val_loss: 0.2381 - mcc: 0.7939\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9088 - loss: 0.2133\n","Epoch 9 - MCC: 0.7970\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9088 - loss: 0.2134 - val_accuracy: 0.8981 - val_loss: 0.2400 - mcc: 0.7970\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9074 - loss: 0.2173\n","Epoch 10 - MCC: 0.8025\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9075 - loss: 0.2170 - val_accuracy: 0.9014 - val_loss: 0.2300 - mcc: 0.8025\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9142 - loss: 0.2012\n","Epoch 11 - MCC: 0.7955\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9141 - loss: 0.2014 - val_accuracy: 0.8979 - val_loss: 0.2339 - mcc: 0.7955\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9075 - loss: 0.2146\n","Epoch 12 - MCC: 0.8041\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9076 - loss: 0.2145 - val_accuracy: 0.9023 - val_loss: 0.2267 - mcc: 0.8041\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9104 - loss: 0.2086\n","Epoch 13 - MCC: 0.8038\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.9105 - loss: 0.2084 - val_accuracy: 0.9021 - val_loss: 0.2263 - mcc: 0.8038\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9145 - loss: 0.1999\n","Epoch 14 - MCC: 0.8083\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9145 - loss: 0.2000 - val_accuracy: 0.9041 - val_loss: 0.2248 - mcc: 0.8083\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9168 - loss: 0.1940\n","Epoch 15 - MCC: 0.8071\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9167 - loss: 0.1942 - val_accuracy: 0.9038 - val_loss: 0.2234 - mcc: 0.8071\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9096 - loss: 0.2113\n","Epoch 16 - MCC: 0.8042\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9098 - loss: 0.2109 - val_accuracy: 0.9023 - val_loss: 0.2251 - mcc: 0.8042\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9126 - loss: 0.2043\n","Epoch 17 - MCC: 0.8129\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9126 - loss: 0.2042 - val_accuracy: 0.9066 - val_loss: 0.2184 - mcc: 0.8129\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9138 - loss: 0.2023\n","Epoch 18 - MCC: 0.8138\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9139 - loss: 0.2021 - val_accuracy: 0.9069 - val_loss: 0.2174 - mcc: 0.8138\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9138 - loss: 0.2023\n","Epoch 19 - MCC: 0.8091\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9139 - loss: 0.2019 - val_accuracy: 0.9039 - val_loss: 0.2254 - mcc: 0.8091\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9168 - loss: 0.1950\n","Epoch 20 - MCC: 0.8091\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9168 - loss: 0.1950 - val_accuracy: 0.9048 - val_loss: 0.2203 - mcc: 0.8091\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9116 - loss: 0.2054\n","Epoch 21 - MCC: 0.8145\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.9118 - loss: 0.2050 - val_accuracy: 0.9074 - val_loss: 0.2160 - mcc: 0.8145\n","Epoch 22/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9196 - loss: 0.1886\n","Epoch 22 - MCC: 0.8077\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9194 - loss: 0.1887 - val_accuracy: 0.9040 - val_loss: 0.2203 - mcc: 0.8077\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9143 - loss: 0.1979\n","Epoch 23 - MCC: 0.8149\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9144 - loss: 0.1978 - val_accuracy: 0.9069 - val_loss: 0.2201 - mcc: 0.8149\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9187 - loss: 0.1905\n","Epoch 24 - MCC: 0.8157\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9187 - loss: 0.1906 - val_accuracy: 0.9080 - val_loss: 0.2147 - mcc: 0.8157\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9190 - loss: 0.1892\n","Epoch 25 - MCC: 0.8189\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9190 - loss: 0.1891 - val_accuracy: 0.9095 - val_loss: 0.2116 - mcc: 0.8189\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9185 - loss: 0.1902\n","Epoch 26 - MCC: 0.8151\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9185 - loss: 0.1901 - val_accuracy: 0.9074 - val_loss: 0.2152 - mcc: 0.8151\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9160 - loss: 0.1956\n","Epoch 27 - MCC: 0.8169\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9160 - loss: 0.1955 - val_accuracy: 0.9080 - val_loss: 0.2152 - mcc: 0.8169\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9180 - loss: 0.1920\n","Epoch 28 - MCC: 0.8176\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9180 - loss: 0.1919 - val_accuracy: 0.9089 - val_loss: 0.2115 - mcc: 0.8176\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9216 - loss: 0.1813\n","Epoch 29 - MCC: 0.8087\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.9215 - loss: 0.1816 - val_accuracy: 0.9046 - val_loss: 0.2218 - mcc: 0.8087\n","Epoch 30/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9180 - loss: 0.1939\n","Epoch 30 - MCC: 0.8126\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.9182 - loss: 0.1935 - val_accuracy: 0.9065 - val_loss: 0.2149 - mcc: 0.8126\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 3\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5537 - loss: 0.6973\n","Epoch 1 - MCC: 0.5465\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.5571 - loss: 0.6959 - val_accuracy: 0.7523 - val_loss: 0.6396 - mcc: 0.5465\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7712 - loss: 0.6046\n","Epoch 2 - MCC: 0.6211\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.7714 - loss: 0.6030 - val_accuracy: 0.8114 - val_loss: 0.4583 - mcc: 0.6211\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8178 - loss: 0.4318\n","Epoch 3 - MCC: 0.7111\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8185 - loss: 0.4302 - val_accuracy: 0.8562 - val_loss: 0.3371 - mcc: 0.7111\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8628 - loss: 0.3263\n","Epoch 4 - MCC: 0.7662\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8631 - loss: 0.3258 - val_accuracy: 0.8832 - val_loss: 0.2816 - mcc: 0.7662\n","Epoch 5/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8849 - loss: 0.2794\n","Epoch 5 - MCC: 0.7895\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8850 - loss: 0.2790 - val_accuracy: 0.8951 - val_loss: 0.2500 - mcc: 0.7895\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8955 - loss: 0.2514\n","Epoch 6 - MCC: 0.8027\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.8955 - loss: 0.2514 - val_accuracy: 0.9018 - val_loss: 0.2334 - mcc: 0.8027\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8926 - loss: 0.2519\n","Epoch 7 - MCC: 0.8109\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8929 - loss: 0.2514 - val_accuracy: 0.9058 - val_loss: 0.2237 - mcc: 0.8109\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9012 - loss: 0.2337\n","Epoch 8 - MCC: 0.8103\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9012 - loss: 0.2337 - val_accuracy: 0.9051 - val_loss: 0.2202 - mcc: 0.8103\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9056 - loss: 0.2226\n","Epoch 9 - MCC: 0.8192\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9055 - loss: 0.2228 - val_accuracy: 0.9098 - val_loss: 0.2138 - mcc: 0.8192\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9089 - loss: 0.2168\n","Epoch 10 - MCC: 0.8229\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9088 - loss: 0.2169 - val_accuracy: 0.9118 - val_loss: 0.2082 - mcc: 0.8229\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9034 - loss: 0.2283\n","Epoch 11 - MCC: 0.8250\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9036 - loss: 0.2279 - val_accuracy: 0.9127 - val_loss: 0.2069 - mcc: 0.8250\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9069 - loss: 0.2174\n","Epoch 12 - MCC: 0.8137\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9069 - loss: 0.2174 - val_accuracy: 0.9066 - val_loss: 0.2145 - mcc: 0.8137\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9036 - loss: 0.2251\n","Epoch 13 - MCC: 0.8306\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9038 - loss: 0.2247 - val_accuracy: 0.9156 - val_loss: 0.1995 - mcc: 0.8306\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9091 - loss: 0.2136\n","Epoch 14 - MCC: 0.8305\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9092 - loss: 0.2135 - val_accuracy: 0.9154 - val_loss: 0.1976 - mcc: 0.8305\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9125 - loss: 0.2059\n","Epoch 15 - MCC: 0.8342\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 53ms/step - accuracy: 0.9124 - loss: 0.2061 - val_accuracy: 0.9175 - val_loss: 0.1949 - mcc: 0.8342\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9140 - loss: 0.2039\n","Epoch 16 - MCC: 0.8352\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9140 - loss: 0.2040 - val_accuracy: 0.9179 - val_loss: 0.1939 - mcc: 0.8352\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9135 - loss: 0.2032\n","Epoch 17 - MCC: 0.8357\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9135 - loss: 0.2032 - val_accuracy: 0.9179 - val_loss: 0.1930 - mcc: 0.8357\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9100 - loss: 0.2125\n","Epoch 18 - MCC: 0.8353\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9100 - loss: 0.2124 - val_accuracy: 0.9180 - val_loss: 0.1938 - mcc: 0.8353\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9171 - loss: 0.1937\n","Epoch 19 - MCC: 0.8354\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9169 - loss: 0.1941 - val_accuracy: 0.9176 - val_loss: 0.1947 - mcc: 0.8354\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9158 - loss: 0.1974\n","Epoch 20 - MCC: 0.8395\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9158 - loss: 0.1975 - val_accuracy: 0.9199 - val_loss: 0.1908 - mcc: 0.8395\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9037 - loss: 0.2243\n","Epoch 21 - MCC: 0.8343\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9040 - loss: 0.2238 - val_accuracy: 0.9174 - val_loss: 0.1961 - mcc: 0.8343\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9104 - loss: 0.2105\n","Epoch 22 - MCC: 0.8389\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9105 - loss: 0.2102 - val_accuracy: 0.9198 - val_loss: 0.1900 - mcc: 0.8389\n","Epoch 23/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9135 - loss: 0.2047\n","Epoch 23 - MCC: 0.8453\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9137 - loss: 0.2044 - val_accuracy: 0.9230 - val_loss: 0.1838 - mcc: 0.8453\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9186 - loss: 0.1920\n","Epoch 24 - MCC: 0.8416\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.9185 - loss: 0.1922 - val_accuracy: 0.9209 - val_loss: 0.1872 - mcc: 0.8416\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9194 - loss: 0.1910\n","Epoch 25 - MCC: 0.8407\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9194 - loss: 0.1912 - val_accuracy: 0.9205 - val_loss: 0.1878 - mcc: 0.8407\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9149 - loss: 0.2004\n","Epoch 26 - MCC: 0.8399\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9150 - loss: 0.2003 - val_accuracy: 0.9195 - val_loss: 0.1914 - mcc: 0.8399\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9183 - loss: 0.1920\n","Epoch 27 - MCC: 0.8490\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9183 - loss: 0.1923 - val_accuracy: 0.9248 - val_loss: 0.1792 - mcc: 0.8490\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9170 - loss: 0.1967\n","Epoch 28 - MCC: 0.8505\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9171 - loss: 0.1965 - val_accuracy: 0.9256 - val_loss: 0.1775 - mcc: 0.8505\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9205 - loss: 0.1887\n","Epoch 29 - MCC: 0.8474\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9204 - loss: 0.1888 - val_accuracy: 0.9238 - val_loss: 0.1818 - mcc: 0.8474\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9176 - loss: 0.1946\n","Epoch 30 - MCC: 0.8507\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9177 - loss: 0.1944 - val_accuracy: 0.9257 - val_loss: 0.1778 - mcc: 0.8507\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 4\n","Epoch 1/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5187 - loss: 0.6581\n","Epoch 1 - MCC: 0.5365\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.5244 - loss: 0.6560 - val_accuracy: 0.7558 - val_loss: 0.5524 - mcc: 0.5365\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7795 - loss: 0.5123\n","Epoch 2 - MCC: 0.7118\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.7803 - loss: 0.5108 - val_accuracy: 0.8559 - val_loss: 0.3641 - mcc: 0.7118\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8599 - loss: 0.3409\n","Epoch 3 - MCC: 0.7905\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8603 - loss: 0.3398 - val_accuracy: 0.8955 - val_loss: 0.2588 - mcc: 0.7905\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8858 - loss: 0.2715\n","Epoch 4 - MCC: 0.7964\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8860 - loss: 0.2711 - val_accuracy: 0.8985 - val_loss: 0.2441 - mcc: 0.7964\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8961 - loss: 0.2447\n","Epoch 5 - MCC: 0.8127\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8962 - loss: 0.2446 - val_accuracy: 0.9065 - val_loss: 0.2244 - mcc: 0.8127\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9001 - loss: 0.2361\n","Epoch 6 - MCC: 0.8139\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9003 - loss: 0.2358 - val_accuracy: 0.9068 - val_loss: 0.2263 - mcc: 0.8139\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9054 - loss: 0.2220\n","Epoch 7 - MCC: 0.8219\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9053 - loss: 0.2221 - val_accuracy: 0.9111 - val_loss: 0.2113 - mcc: 0.8219\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9058 - loss: 0.2194\n","Epoch 8 - MCC: 0.8132\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9057 - loss: 0.2197 - val_accuracy: 0.9067 - val_loss: 0.2233 - mcc: 0.8132\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9095 - loss: 0.2129\n","Epoch 9 - MCC: 0.8289\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9094 - loss: 0.2129 - val_accuracy: 0.9146 - val_loss: 0.2028 - mcc: 0.8289\n","Epoch 10/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9127 - loss: 0.2050\n","Epoch 10 - MCC: 0.8306\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9126 - loss: 0.2053 - val_accuracy: 0.9155 - val_loss: 0.2008 - mcc: 0.8306\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9108 - loss: 0.2098\n","Epoch 11 - MCC: 0.8336\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.9109 - loss: 0.2097 - val_accuracy: 0.9170 - val_loss: 0.1974 - mcc: 0.8336\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9113 - loss: 0.2076\n","Epoch 12 - MCC: 0.8344\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.9113 - loss: 0.2075 - val_accuracy: 0.9174 - val_loss: 0.1968 - mcc: 0.8344\n","Epoch 13/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9085 - loss: 0.2131\n","Epoch 13 - MCC: 0.8356\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9088 - loss: 0.2124 - val_accuracy: 0.9180 - val_loss: 0.1960 - mcc: 0.8356\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9154 - loss: 0.1987\n","Epoch 14 - MCC: 0.8363\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9153 - loss: 0.1989 - val_accuracy: 0.9183 - val_loss: 0.1953 - mcc: 0.8363\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9114 - loss: 0.2074\n","Epoch 15 - MCC: 0.8319\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9115 - loss: 0.2071 - val_accuracy: 0.9156 - val_loss: 0.2000 - mcc: 0.8319\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9146 - loss: 0.1984\n","Epoch 16 - MCC: 0.8370\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9146 - loss: 0.1984 - val_accuracy: 0.9186 - val_loss: 0.1939 - mcc: 0.8370\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9144 - loss: 0.2012\n","Epoch 17 - MCC: 0.8394\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9144 - loss: 0.2012 - val_accuracy: 0.9197 - val_loss: 0.1932 - mcc: 0.8394\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9131 - loss: 0.2053\n","Epoch 18 - MCC: 0.8309\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9132 - loss: 0.2051 - val_accuracy: 0.9148 - val_loss: 0.1997 - mcc: 0.8309\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9139 - loss: 0.2032\n","Epoch 19 - MCC: 0.8394\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9140 - loss: 0.2030 - val_accuracy: 0.9198 - val_loss: 0.1911 - mcc: 0.8394\n","Epoch 20/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9160 - loss: 0.1951\n","Epoch 20 - MCC: 0.8451\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9160 - loss: 0.1953 - val_accuracy: 0.9227 - val_loss: 0.1870 - mcc: 0.8451\n","Epoch 21/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9183 - loss: 0.1934\n","Epoch 21 - MCC: 0.8368\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9182 - loss: 0.1935 - val_accuracy: 0.9181 - val_loss: 0.1940 - mcc: 0.8368\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9161 - loss: 0.1963\n","Epoch 22 - MCC: 0.8460\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9161 - loss: 0.1963 - val_accuracy: 0.9232 - val_loss: 0.1854 - mcc: 0.8460\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9198 - loss: 0.1891\n","Epoch 23 - MCC: 0.8433\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9197 - loss: 0.1894 - val_accuracy: 0.9217 - val_loss: 0.1878 - mcc: 0.8433\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9172 - loss: 0.1957\n","Epoch 24 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9172 - loss: 0.1956 - val_accuracy: 0.9231 - val_loss: 0.1861 - mcc: 0.8458\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9160 - loss: 0.1952\n","Epoch 25 - MCC: 0.8428\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9161 - loss: 0.1951 - val_accuracy: 0.9215 - val_loss: 0.1897 - mcc: 0.8428\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9172 - loss: 0.1938\n","Epoch 26 - MCC: 0.8414\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9172 - loss: 0.1938 - val_accuracy: 0.9202 - val_loss: 0.1935 - mcc: 0.8414\n","Epoch 27/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9146 - loss: 0.1996\n","Epoch 27 - MCC: 0.8479\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.9149 - loss: 0.1990 - val_accuracy: 0.9242 - val_loss: 0.1832 - mcc: 0.8479\n","Epoch 28/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9192 - loss: 0.1927\n","Epoch 28 - MCC: 0.8449\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9191 - loss: 0.1927 - val_accuracy: 0.9225 - val_loss: 0.1850 - mcc: 0.8449\n","Epoch 29/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9204 - loss: 0.1869\n","Epoch 29 - MCC: 0.8492\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9203 - loss: 0.1872 - val_accuracy: 0.9247 - val_loss: 0.1833 - mcc: 0.8492\n","Epoch 30/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9192 - loss: 0.1879\n","Epoch 30 - MCC: 0.8461\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9192 - loss: 0.1879 - val_accuracy: 0.9232 - val_loss: 0.1839 - mcc: 0.8461\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5764 - loss: 0.6840\n","Epoch 1 - MCC: 0.5355\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.5786 - loss: 0.6832 - val_accuracy: 0.7353 - val_loss: 0.6104 - mcc: 0.5355\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7809 - loss: 0.5742\n","Epoch 2 - MCC: 0.6856\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.7818 - loss: 0.5726 - val_accuracy: 0.8431 - val_loss: 0.4363 - mcc: 0.6856\n","Epoch 3/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8498 - loss: 0.3889\n","Epoch 3 - MCC: 0.7366\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.8505 - loss: 0.3861 - val_accuracy: 0.8686 - val_loss: 0.3103 - mcc: 0.7366\n","Epoch 4/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8683 - loss: 0.3081\n","Epoch 4 - MCC: 0.7655\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.8691 - loss: 0.3064 - val_accuracy: 0.8824 - val_loss: 0.2780 - mcc: 0.7655\n","Epoch 5/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8897 - loss: 0.2607\n","Epoch 5 - MCC: 0.7812\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8897 - loss: 0.2606 - val_accuracy: 0.8908 - val_loss: 0.2562 - mcc: 0.7812\n","Epoch 6/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8967 - loss: 0.2451\n","Epoch 6 - MCC: 0.7845\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8968 - loss: 0.2449 - val_accuracy: 0.8915 - val_loss: 0.2493 - mcc: 0.7845\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8980 - loss: 0.2395\n","Epoch 7 - MCC: 0.7958\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8980 - loss: 0.2393 - val_accuracy: 0.8981 - val_loss: 0.2398 - mcc: 0.7958\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9012 - loss: 0.2315\n","Epoch 8 - MCC: 0.8036\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9013 - loss: 0.2313 - val_accuracy: 0.9019 - val_loss: 0.2292 - mcc: 0.8036\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9048 - loss: 0.2236\n","Epoch 9 - MCC: 0.8047\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9049 - loss: 0.2234 - val_accuracy: 0.9025 - val_loss: 0.2261 - mcc: 0.8047\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9108 - loss: 0.2130\n","Epoch 10 - MCC: 0.8079\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9107 - loss: 0.2131 - val_accuracy: 0.9040 - val_loss: 0.2246 - mcc: 0.8079\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9073 - loss: 0.2181\n","Epoch 11 - MCC: 0.8113\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9074 - loss: 0.2179 - val_accuracy: 0.9058 - val_loss: 0.2150 - mcc: 0.8113\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9101 - loss: 0.2127\n","Epoch 12 - MCC: 0.8133\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.9101 - loss: 0.2126 - val_accuracy: 0.9067 - val_loss: 0.2158 - mcc: 0.8133\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9092 - loss: 0.2131\n","Epoch 13 - MCC: 0.8083\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9092 - loss: 0.2131 - val_accuracy: 0.9038 - val_loss: 0.2242 - mcc: 0.8083\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9179 - loss: 0.1961\n","Epoch 14 - MCC: 0.8186\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9178 - loss: 0.1964 - val_accuracy: 0.9094 - val_loss: 0.2092 - mcc: 0.8186\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9143 - loss: 0.2019\n","Epoch 15 - MCC: 0.8217\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9144 - loss: 0.2019 - val_accuracy: 0.9110 - val_loss: 0.2067 - mcc: 0.8217\n","Epoch 16/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9209 - loss: 0.1894\n","Epoch 16 - MCC: 0.8223\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9206 - loss: 0.1900 - val_accuracy: 0.9113 - val_loss: 0.2032 - mcc: 0.8223\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9150 - loss: 0.2023\n","Epoch 17 - MCC: 0.8246\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9150 - loss: 0.2021 - val_accuracy: 0.9125 - val_loss: 0.2021 - mcc: 0.8246\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9190 - loss: 0.1915\n","Epoch 18 - MCC: 0.8286\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9189 - loss: 0.1918 - val_accuracy: 0.9143 - val_loss: 0.2006 - mcc: 0.8286\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9206 - loss: 0.1886\n","Epoch 19 - MCC: 0.8208\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9205 - loss: 0.1888 - val_accuracy: 0.9105 - val_loss: 0.2078 - mcc: 0.8208\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9175 - loss: 0.1939\n","Epoch 20 - MCC: 0.8272\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9176 - loss: 0.1939 - val_accuracy: 0.9137 - val_loss: 0.2018 - mcc: 0.8272\n","Epoch 21/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9195 - loss: 0.1897\n","Epoch 21 - MCC: 0.8320\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.9196 - loss: 0.1896 - val_accuracy: 0.9161 - val_loss: 0.1967 - mcc: 0.8320\n","Epoch 22/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9222 - loss: 0.1847\n","Epoch 22 - MCC: 0.8313\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.9221 - loss: 0.1848 - val_accuracy: 0.9157 - val_loss: 0.1976 - mcc: 0.8313\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9192 - loss: 0.1923\n","Epoch 23 - MCC: 0.8345\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9193 - loss: 0.1921 - val_accuracy: 0.9174 - val_loss: 0.1952 - mcc: 0.8345\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9241 - loss: 0.1823\n","Epoch 24 - MCC: 0.8233\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9240 - loss: 0.1826 - val_accuracy: 0.9105 - val_loss: 0.2089 - mcc: 0.8233\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9248 - loss: 0.1806\n","Epoch 25 - MCC: 0.8312\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.9248 - loss: 0.1807 - val_accuracy: 0.9156 - val_loss: 0.1925 - mcc: 0.8312\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9252 - loss: 0.1771\n","Epoch 26 - MCC: 0.8307\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9252 - loss: 0.1772 - val_accuracy: 0.9153 - val_loss: 0.1929 - mcc: 0.8307\n","Epoch 27/30\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9253 - loss: 0.1782\n","Epoch 27 - MCC: 0.8366\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.9252 - loss: 0.1784 - val_accuracy: 0.9184 - val_loss: 0.1888 - mcc: 0.8366\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9222 - loss: 0.1868\n","Epoch 28 - MCC: 0.8395\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9223 - loss: 0.1865 - val_accuracy: 0.9199 - val_loss: 0.1878 - mcc: 0.8395\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9311 - loss: 0.1654\n","Epoch 29 - MCC: 0.8374\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.9309 - loss: 0.1659 - val_accuracy: 0.9188 - val_loss: 0.1882 - mcc: 0.8374\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9261 - loss: 0.1772\n","Epoch 30 - MCC: 0.8414\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9262 - loss: 0.1771 - val_accuracy: 0.9206 - val_loss: 0.1868 - mcc: 0.8414\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9270866666666666,\n","              'mean': 0.9206026666666667,\n","              'min': 0.9064866666666667,\n","              'std': 0.0073986108005339585},\n"," 'Inference Time (s/sample)': {'max': 0.0019974756240844728,\n","                               'mean': 0.0017104415893554686,\n","                               'min': 0.0013200068473815918,\n","                               'std': 0.0002787153781394591},\n"," 'MCC': {'max': 0.8535991011080161,\n","         'mean': 0.8408753748756714,\n","         'min': 0.8125662043137666,\n","         'std': 0.014746610125389272},\n"," 'Parameters': 5025,\n"," 'Train Time (s)': {'max': 51.99972081184387,\n","                    'mean': 46.974711894989014,\n","                    'min': 44.03724408149719,\n","                    'std': 2.8001930743273338},\n"," 'Training Accuracy': [[0.49994155764579773,\n","                        0.7561166882514954,\n","                        0.8562532663345337,\n","                        0.8809700012207031,\n","                        0.8947800397872925,\n","                        0.901681661605835,\n","                        0.9051349759101868,\n","                        0.9083832502365112,\n","                        0.9095733165740967,\n","                        0.9107300043106079,\n","                        0.9102316498756409,\n","                        0.9112350940704346,\n","                        0.9135016798973083,\n","                        0.9145200252532959,\n","                        0.9145849347114563,\n","                        0.9161316156387329,\n","                        0.9154516458511353,\n","                        0.9161033630371094,\n","                        0.9173900485038757,\n","                        0.9159517884254456,\n","                        0.9167799949645996,\n","                        0.917460024356842,\n","                        0.9161232113838196,\n","                        0.9197666645050049,\n","                        0.9201633930206299,\n","                        0.920708417892456,\n","                        0.9212867021560669,\n","                        0.9203866720199585,\n","                        0.921779990196228,\n","                        0.9226632118225098],\n","                       [0.5778433084487915,\n","                        0.7731033563613892,\n","                        0.8513899445533752,\n","                        0.878978431224823,\n","                        0.8960584998130798,\n","                        0.9020750522613525,\n","                        0.9017366170883179,\n","                        0.9074516892433167,\n","                        0.9073382616043091,\n","                        0.9093883633613586,\n","                        0.9102767109870911,\n","                        0.9090900421142578,\n","                        0.9125434160232544,\n","                        0.9138984680175781,\n","                        0.9140934944152832,\n","                        0.9143799543380737,\n","                        0.9145333170890808,\n","                        0.9156549572944641,\n","                        0.9172133803367615,\n","                        0.9161882996559143,\n","                        0.9158249497413635,\n","                        0.9179698824882507,\n","                        0.9155450463294983,\n","                        0.9172266125679016,\n","                        0.9192265868186951,\n","                        0.9189916849136353,\n","                        0.9168350100517273,\n","                        0.9190483689308167,\n","                        0.9195417165756226,\n","                        0.9195383191108704],\n","                       [0.5972200036048889,\n","                        0.7775783538818359,\n","                        0.8362634181976318,\n","                        0.8692400455474854,\n","                        0.8854716420173645,\n","                        0.894748330116272,\n","                        0.8989734053611755,\n","                        0.9018316864967346,\n","                        0.9035298824310303,\n","                        0.9065250158309937,\n","                        0.9079582691192627,\n","                        0.9073699712753296,\n","                        0.9090316891670227,\n","                        0.9102500081062317,\n","                        0.909945011138916,\n","                        0.9127616882324219,\n","                        0.913496732711792,\n","                        0.9113966226577759,\n","                        0.9127365946769714,\n","                        0.9152516722679138,\n","                        0.9101801514625549,\n","                        0.9131300449371338,\n","                        0.9154584407806396,\n","                        0.916170060634613,\n","                        0.9179049730300903,\n","                        0.9161882996559143,\n","                        0.9165583848953247,\n","                        0.9186882972717285,\n","                        0.9191483855247498,\n","                        0.9195700883865356],\n","                       [0.5931382775306702,\n","                        0.8017449378967285,\n","                        0.8707798719406128,\n","                        0.8906983137130737,\n","                        0.897629976272583,\n","                        0.9032084941864014,\n","                        0.9034968018531799,\n","                        0.90326988697052,\n","                        0.908603310585022,\n","                        0.9109966158866882,\n","                        0.9118881821632385,\n","                        0.9118533134460449,\n","                        0.9125417470932007,\n","                        0.912548303604126,\n","                        0.9141700267791748,\n","                        0.914741575717926,\n","                        0.9138649702072144,\n","                        0.9153284430503845,\n","                        0.9151449799537659,\n","                        0.9156249761581421,\n","                        0.9169933795928955,\n","                        0.9166983962059021,\n","                        0.9166483879089355,\n","                        0.9172783493995667,\n","                        0.917639970779419,\n","                        0.917279839515686,\n","                        0.9184967279434204,\n","                        0.918229877948761,\n","                        0.9189632534980774,\n","                        0.9199565649032593],\n","                       [0.6323433518409729,\n","                        0.8037334680557251,\n","                        0.8578248023986816,\n","                        0.8788465857505798,\n","                        0.8897799849510193,\n","                        0.898348331451416,\n","                        0.900418221950531,\n","                        0.904059886932373,\n","                        0.9075266122817993,\n","                        0.9093633890151978,\n","                        0.910111665725708,\n","                        0.9119333028793335,\n","                        0.909258246421814,\n","                        0.9138666391372681,\n","                        0.914863109588623,\n","                        0.9166783094406128,\n","                        0.9164501428604126,\n","                        0.9164115786552429,\n","                        0.9175817370414734,\n","                        0.9186665415763855,\n","                        0.9202783107757568,\n","                        0.9214949011802673,\n","                        0.9208000302314758,\n","                        0.9212365746498108,\n","                        0.923194944858551,\n","                        0.923861563205719,\n","                        0.9244849681854248,\n","                        0.9255799651145935,\n","                        0.9257232546806335,\n","                        0.9269882440567017]],\n"," 'Training Loss': [[0.6527655720710754,\n","                    0.5636637806892395,\n","                    0.44578060507774353,\n","                    0.3255830407142639,\n","                    0.26446235179901123,\n","                    0.24100567400455475,\n","                    0.22913846373558044,\n","                    0.21983571350574493,\n","                    0.215712770819664,\n","                    0.21207010746002197,\n","                    0.21339577436447144,\n","                    0.20877094566822052,\n","                    0.20489664375782013,\n","                    0.20394958555698395,\n","                    0.20199914276599884,\n","                    0.19882407784461975,\n","                    0.19982494413852692,\n","                    0.19894784688949585,\n","                    0.19608813524246216,\n","                    0.19806575775146484,\n","                    0.19735194742679596,\n","                    0.19607773423194885,\n","                    0.19775456190109253,\n","                    0.1909458190202713,\n","                    0.1890973299741745,\n","                    0.1888009011745453,\n","                    0.18722572922706604,\n","                    0.18951945006847382,\n","                    0.18607588112354279,\n","                    0.18423473834991455],\n","                   [0.67097008228302,\n","                    0.5560081005096436,\n","                    0.3682085871696472,\n","                    0.2895902097225189,\n","                    0.24749864637851715,\n","                    0.23268431425094604,\n","                    0.22944292426109314,\n","                    0.21725080907344818,\n","                    0.21629886329174042,\n","                    0.21192815899848938,\n","                    0.20844049751758575,\n","                    0.211069718003273,\n","                    0.20445413887500763,\n","                    0.20176783204078674,\n","                    0.1993415802717209,\n","                    0.2009015828371048,\n","                    0.19992493093013763,\n","                    0.19656193256378174,\n","                    0.19406813383102417,\n","                    0.1949797421693802,\n","                    0.1957879513502121,\n","                    0.19041728973388672,\n","                    0.1960315853357315,\n","                    0.19284145534038544,\n","                    0.188598170876503,\n","                    0.18888984620571136,\n","                    0.19405673444271088,\n","                    0.18972593545913696,\n","                    0.1875198483467102,\n","                    0.18880583345890045],\n","                   [0.679415762424469,\n","                    0.5607799887657166,\n","                    0.3916827142238617,\n","                    0.31336861848831177,\n","                    0.2737080454826355,\n","                    0.25069424510002136,\n","                    0.23866944015026093,\n","                    0.2319491058588028,\n","                    0.2268000841140747,\n","                    0.21951894462108612,\n","                    0.21729804575443268,\n","                    0.21679618954658508,\n","                    0.21394287049770355,\n","                    0.21106493473052979,\n","                    0.21056494116783142,\n","                    0.20501689612865448,\n","                    0.2030632644891739,\n","                    0.20833361148834229,\n","                    0.20492801070213318,\n","                    0.2001558095216751,\n","                    0.21039992570877075,\n","                    0.20416587591171265,\n","                    0.200937882065773,\n","                    0.1976548135280609,\n","                    0.19442659616470337,\n","                    0.19788719713687897,\n","                    0.19743004441261292,\n","                    0.1921190470457077,\n","                    0.1916467547416687,\n","                    0.19048263132572174],\n","                   [0.6305559277534485,\n","                    0.47475987672805786,\n","                    0.31294915080070496,\n","                    0.2604626715183258,\n","                    0.24159276485443115,\n","                    0.2280389815568924,\n","                    0.22491726279258728,\n","                    0.22522340714931488,\n","                    0.21443882584571838,\n","                    0.20897197723388672,\n","                    0.20599742233753204,\n","                    0.20574802160263062,\n","                    0.20406533777713776,\n","                    0.20433162152767181,\n","                    0.20125523209571838,\n","                    0.1994221806526184,\n","                    0.20095060765743256,\n","                    0.19973918795585632,\n","                    0.1992962509393692,\n","                    0.19752532243728638,\n","                    0.19495117664337158,\n","                    0.1952635794878006,\n","                    0.19520074129104614,\n","                    0.19411100447177887,\n","                    0.19285115599632263,\n","                    0.19391639530658722,\n","                    0.1915311962366104,\n","                    0.1923060268163681,\n","                    0.19021549820899963,\n","                    0.18735477328300476],\n","                   [0.66169273853302,\n","                    0.5336806774139404,\n","                    0.35265105962753296,\n","                    0.2859344780445099,\n","                    0.26010552048683167,\n","                    0.2416096180677414,\n","                    0.23471060395240784,\n","                    0.22615373134613037,\n","                    0.21901550889015198,\n","                    0.21503189206123352,\n","                    0.21222160756587982,\n","                    0.20831559598445892,\n","                    0.21218879520893097,\n","                    0.20436683297157288,\n","                    0.2010853886604309,\n","                    0.19777826964855194,\n","                    0.19743612408638,\n","                    0.19717155396938324,\n","                    0.19574280083179474,\n","                    0.19299818575382233,\n","                    0.18833665549755096,\n","                    0.18671521544456482,\n","                    0.18807384371757507,\n","                    0.18901187181472778,\n","                    0.18290391564369202,\n","                    0.1814589947462082,\n","                    0.18015269935131073,\n","                    0.17826257646083832,\n","                    0.17711521685123444,\n","                    0.17518138885498047]],\n"," 'Validation Accuracy': [[0.5915066599845886,\n","                          0.8347132802009583,\n","                          0.8793266415596008,\n","                          0.8947999477386475,\n","                          0.9024398922920227,\n","                          0.9092133045196533,\n","                          0.9123000502586365,\n","                          0.9132665991783142,\n","                          0.9155199527740479,\n","                          0.9162999987602234,\n","                          0.9162467122077942,\n","                          0.9171599745750427,\n","                          0.9198200702667236,\n","                          0.9173732995986938,\n","                          0.9197332859039307,\n","                          0.9200533032417297,\n","                          0.9184466600418091,\n","                          0.9201266765594482,\n","                          0.9170801043510437,\n","                          0.9204999804496765,\n","                          0.9203534126281738,\n","                          0.9182199835777283,\n","                          0.9212465882301331,\n","                          0.9238466620445251,\n","                          0.9251866936683655,\n","                          0.926466703414917,\n","                          0.9250332713127136,\n","                          0.9261600375175476,\n","                          0.9274133443832397,\n","                          0.9270866513252258],\n","                         [0.6854400038719177,\n","                          0.8182799816131592,\n","                          0.8500266075134277,\n","                          0.8843467235565186,\n","                          0.8885066509246826,\n","                          0.8929999470710754,\n","                          0.8955533504486084,\n","                          0.897046685218811,\n","                          0.8980866074562073,\n","                          0.9014266133308411,\n","                          0.8979200124740601,\n","                          0.9023066759109497,\n","                          0.9020599126815796,\n","                          0.904093325138092,\n","                          0.9037666320800781,\n","                          0.9023200273513794,\n","                          0.9065867066383362,\n","                          0.9069466590881348,\n","                          0.9039466381072998,\n","                          0.9047600626945496,\n","                          0.9074265956878662,\n","                          0.904033362865448,\n","                          0.906873345375061,\n","                          0.9080066680908203,\n","                          0.9095132946968079,\n","                          0.9074400067329407,\n","                          0.9080266356468201,\n","                          0.9089000225067139,\n","                          0.9045732617378235,\n","                          0.9064866900444031],\n","                         [0.7522599697113037,\n","                          0.8114266395568848,\n","                          0.8561867475509644,\n","                          0.8831800222396851,\n","                          0.8950799703598022,\n","                          0.9017666578292847,\n","                          0.9058066606521606,\n","                          0.9051333665847778,\n","                          0.909779965877533,\n","                          0.9118266105651855,\n","                          0.9127066731452942,\n","                          0.9066199660301208,\n","                          0.915600061416626,\n","                          0.9153532385826111,\n","                          0.9174666404724121,\n","                          0.9178532958030701,\n","                          0.9178933501243591,\n","                          0.9179600477218628,\n","                          0.9176132678985596,\n","                          0.9199333786964417,\n","                          0.9174066185951233,\n","                          0.9197867512702942,\n","                          0.9229799509048462,\n","                          0.9208866953849792,\n","                          0.9205467104911804,\n","                          0.9195066690444946,\n","                          0.9248066544532776,\n","                          0.9255667328834534,\n","                          0.9237933158874512,\n","                          0.9256600141525269],\n","                         [0.755766749382019,\n","                          0.855886697769165,\n","                          0.895526647567749,\n","                          0.8984733819961548,\n","                          0.9065267443656921,\n","                          0.906760036945343,\n","                          0.9111066460609436,\n","                          0.9067067503929138,\n","                          0.914639949798584,\n","                          0.915513277053833,\n","                          0.9170066714286804,\n","                          0.9173799753189087,\n","                          0.9179867506027222,\n","                          0.918326735496521,\n","                          0.915626585483551,\n","                          0.9185665845870972,\n","                          0.9196600317955017,\n","                          0.9148333072662354,\n","                          0.9198400378227234,\n","                          0.9227399230003357,\n","                          0.9180934429168701,\n","                          0.9231666326522827,\n","                          0.9217333793640137,\n","                          0.9230934381484985,\n","                          0.921466588973999,\n","                          0.9201733469963074,\n","                          0.9241532683372498,\n","                          0.9225466847419739,\n","                          0.9246599674224854,\n","                          0.9232133626937866],\n","                         [0.735333263874054,\n","                          0.8430533409118652,\n","                          0.8685533404350281,\n","                          0.8824466466903687,\n","                          0.8907533884048462,\n","                          0.8914732933044434,\n","                          0.898099958896637,\n","                          0.901926577091217,\n","                          0.902546763420105,\n","                          0.9039599895477295,\n","                          0.9058266282081604,\n","                          0.9066799879074097,\n","                          0.9037932753562927,\n","                          0.9094467163085938,\n","                          0.9109932780265808,\n","                          0.9112933278083801,\n","                          0.912453293800354,\n","                          0.9143467545509338,\n","                          0.9105266332626343,\n","                          0.9137066602706909,\n","                          0.916053295135498,\n","                          0.915713369846344,\n","                          0.9173867106437683,\n","                          0.9105401039123535,\n","                          0.915619969367981,\n","                          0.9152534604072571,\n","                          0.9184467196464539,\n","                          0.9198733568191528,\n","                          0.918826699256897,\n","                          0.920566737651825]],\n"," 'Validation Loss': [[0.6111693382263184,\n","                      0.5122992992401123,\n","                      0.36999931931495667,\n","                      0.2740441560745239,\n","                      0.2431676983833313,\n","                      0.22083348035812378,\n","                      0.21341964602470398,\n","                      0.20683687925338745,\n","                      0.19961746037006378,\n","                      0.19788417220115662,\n","                      0.19825513660907745,\n","                      0.19682058691978455,\n","                      0.18901966512203217,\n","                      0.19584770500659943,\n","                      0.18830031156539917,\n","                      0.18627385795116425,\n","                      0.19166646897792816,\n","                      0.18548136949539185,\n","                      0.19122761487960815,\n","                      0.188590407371521,\n","                      0.18661130964756012,\n","                      0.19173504412174225,\n","                      0.18609735369682312,\n","                      0.1790737509727478,\n","                      0.17625561356544495,\n","                      0.17342735826969147,\n","                      0.17719079554080963,\n","                      0.1738356202840805,\n","                      0.17120082676410675,\n","                      0.1718875914812088],\n","                     [0.6322025060653687,\n","                      0.469819575548172,\n","                      0.352018803358078,\n","                      0.27840808033943176,\n","                      0.2609472870826721,\n","                      0.2496567964553833,\n","                      0.244257852435112,\n","                      0.23809795081615448,\n","                      0.23995395004749298,\n","                      0.22999213635921478,\n","                      0.2339331954717636,\n","                      0.2267216145992279,\n","                      0.226313054561615,\n","                      0.22482386231422424,\n","                      0.22338572144508362,\n","                      0.2250916063785553,\n","                      0.2184005230665207,\n","                      0.21737617254257202,\n","                      0.22543254494667053,\n","                      0.22033028304576874,\n","                      0.2160068303346634,\n","                      0.22032415866851807,\n","                      0.22012576460838318,\n","                      0.21474908292293549,\n","                      0.21161024272441864,\n","                      0.21519917249679565,\n","                      0.21524009108543396,\n","                      0.21151421964168549,\n","                      0.22176812589168549,\n","                      0.21488183736801147],\n","                     [0.6395609974861145,\n","                      0.4582805633544922,\n","                      0.33714017271995544,\n","                      0.28162315487861633,\n","                      0.24999116361141205,\n","                      0.2334367036819458,\n","                      0.2236676812171936,\n","                      0.2201528549194336,\n","                      0.21382023394107819,\n","                      0.20816878974437714,\n","                      0.20693545043468475,\n","                      0.21451808512210846,\n","                      0.19947507977485657,\n","                      0.19755767285823822,\n","                      0.19490700960159302,\n","                      0.19388295710086823,\n","                      0.1929696649312973,\n","                      0.19378551840782166,\n","                      0.19474847614765167,\n","                      0.19077874720096588,\n","                      0.19609716534614563,\n","                      0.1899884045124054,\n","                      0.18375180661678314,\n","                      0.18716439604759216,\n","                      0.18776749074459076,\n","                      0.1913738250732422,\n","                      0.17923632264137268,\n","                      0.17749078571796417,\n","                      0.18178331851959229,\n","                      0.1778169572353363],\n","                     [0.5523668527603149,\n","                      0.3641180396080017,\n","                      0.25884491205215454,\n","                      0.24413342773914337,\n","                      0.22444196045398712,\n","                      0.2262832224369049,\n","                      0.211341992020607,\n","                      0.22330345213413239,\n","                      0.20279590785503387,\n","                      0.20080214738845825,\n","                      0.1974397897720337,\n","                      0.19682304561138153,\n","                      0.1959540992975235,\n","                      0.19532586634159088,\n","                      0.19996002316474915,\n","                      0.19394242763519287,\n","                      0.1932249814271927,\n","                      0.1996656060218811,\n","                      0.1910897046327591,\n","                      0.18696129322052002,\n","                      0.19400985538959503,\n","                      0.18535178899765015,\n","                      0.18780776858329773,\n","                      0.18610946834087372,\n","                      0.18966640532016754,\n","                      0.19354860484600067,\n","                      0.1831798106431961,\n","                      0.18497945368289948,\n","                      0.18326479196548462,\n","                      0.18385976552963257],\n","                     [0.6103842854499817,\n","                      0.43625250458717346,\n","                      0.3103056252002716,\n","                      0.2780146896839142,\n","                      0.2562280595302582,\n","                      0.2492794543504715,\n","                      0.2398097813129425,\n","                      0.22918975353240967,\n","                      0.22606658935546875,\n","                      0.2246362715959549,\n","                      0.21499185264110565,\n","                      0.21576978266239166,\n","                      0.2241838425397873,\n","                      0.20922696590423584,\n","                      0.20665259659290314,\n","                      0.2031598538160324,\n","                      0.2021142542362213,\n","                      0.20063026249408722,\n","                      0.20783333480358124,\n","                      0.20180892944335938,\n","                      0.19668076932430267,\n","                      0.19759845733642578,\n","                      0.19515617191791534,\n","                      0.20893871784210205,\n","                      0.1925310343503952,\n","                      0.19290949404239655,\n","                      0.18877944350242615,\n","                      0.18778370320796967,\n","                      0.18815726041793823,\n","                      0.18681439757347107]],\n"," 'Validation MCC': [[0.34902924245017364,\n","                     0.6883913819699982,\n","                     0.758298022528565,\n","                     0.7902214859099187,\n","                     0.8056171177617237,\n","                     0.8177068330167554,\n","                     0.8240583716827464,\n","                     0.8258837924386108,\n","                     0.8306686627361379,\n","                     0.8319418108774965,\n","                     0.8318590339332229,\n","                     0.8336770953343577,\n","                     0.8390044242366358,\n","                     0.8345408952293554,\n","                     0.8388379595799249,\n","                     0.8395902936683364,\n","                     0.8363822904052592,\n","                     0.8396253165455395,\n","                     0.8344099505943324,\n","                     0.8403685809910202,\n","                     0.8408366579419547,\n","                     0.8368943253887009,\n","                     0.8425074404220338,\n","                     0.8474327565211709,\n","                     0.8499068860500787,\n","                     0.8523598874885774,\n","                     0.8501372838959153,\n","                     0.8517729521654082,\n","                     0.8542739696292506,\n","                     0.8535991011080161],\n","                    [0.4391159352708608,\n","                     0.6378659718494245,\n","                     0.7045731430408346,\n","                     0.7691140623695789,\n","                     0.776448948124122,\n","                     0.7855099621905767,\n","                     0.7921337836438235,\n","                     0.7938621215465741,\n","                     0.797044383991036,\n","                     0.8024931749665324,\n","                     0.7954758660793505,\n","                     0.8041300030234934,\n","                     0.8038204785243702,\n","                     0.8083078147494027,\n","                     0.8070618958230446,\n","                     0.804158491112717,\n","                     0.8128635958985518,\n","                     0.8138123929980473,\n","                     0.80914712561943,\n","                     0.8090724183232391,\n","                     0.8144814187507403,\n","                     0.8077434939864436,\n","                     0.8148799312538911,\n","                     0.8156823915095995,\n","                     0.818883128684535,\n","                     0.8151235558489673,\n","                     0.8169014532940594,\n","                     0.8176083988571367,\n","                     0.8086834506634937,\n","                     0.8125662043137666],\n","                    [0.5465198994138013,\n","                     0.6210979968879772,\n","                     0.7110552463775935,\n","                     0.7661768069034818,\n","                     0.7894741733016984,\n","                     0.8026843779214945,\n","                     0.8108612001517034,\n","                     0.8102917341254863,\n","                     0.8192260391735028,\n","                     0.8229449283614981,\n","                     0.8249839628516791,\n","                     0.8137230437213658,\n","                     0.830648250547574,\n","                     0.8304717862316817,\n","                     0.8342432221984792,\n","                     0.8351556064970607,\n","                     0.8357385624043148,\n","                     0.8352623413844638,\n","                     0.8353606788444351,\n","                     0.839482035098351,\n","                     0.8342602223988692,\n","                     0.8389329227368684,\n","                     0.8453248678121669,\n","                     0.8415691583131008,\n","                     0.8407283746256025,\n","                     0.8399254262044596,\n","                     0.8489847204721488,\n","                     0.8505412019888556,\n","                     0.84741073235371,\n","                     0.8507117774771287],\n","                    [0.5364639284576156,\n","                     0.7117745830469959,\n","                     0.7905269822201855,\n","                     0.7964392735814412,\n","                     0.8126587296257,\n","                     0.8138576592862787,\n","                     0.8219051952661582,\n","                     0.8131723601914232,\n","                     0.8289325246146125,\n","                     0.8306424191966891,\n","                     0.8335969813907578,\n","                     0.834411225586211,\n","                     0.8356301124091728,\n","                     0.8362565715272242,\n","                     0.8318717735743928,\n","                     0.8369774632680017,\n","                     0.839411463392177,\n","                     0.830906112967129,\n","                     0.8393756948291522,\n","                     0.845100641750391,\n","                     0.8367626266198966,\n","                     0.8459527337758668,\n","                     0.8432584379771421,\n","                     0.845803869758636,\n","                     0.8428370331732468,\n","                     0.8414053101714211,\n","                     0.8479387425555767,\n","                     0.844880161165192,\n","                     0.8491945349121243,\n","                     0.8460661046334546],\n","                    [0.5354843495882039,\n","                     0.685563953796833,\n","                     0.7365766092477976,\n","                     0.765477002849265,\n","                     0.7812164165655694,\n","                     0.7845197580747069,\n","                     0.7958256517197663,\n","                     0.8036145837529455,\n","                     0.8047252292891758,\n","                     0.8079363945256369,\n","                     0.8113476916231388,\n","                     0.8133127732711084,\n","                     0.8082814496104059,\n","                     0.8186431160247757,\n","                     0.8216866912559578,\n","                     0.8222946432757562,\n","                     0.82462781671705,\n","                     0.8286192337214149,\n","                     0.8208125760597994,\n","                     0.8271901223440855,\n","                     0.8319660439923432,\n","                     0.8313401470677211,\n","                     0.834494582287362,\n","                     0.8232636102261287,\n","                     0.8311602620162226,\n","                     0.8306775506036921,\n","                     0.8366206687732451,\n","                     0.8394935350310915,\n","                     0.8373551769870137,\n","                     0.841433686845991]]}\n","Training Model: LSTM_Deep, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6028 - loss: 0.6617\n","Epoch 1 - MCC: 0.5998\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 170ms/step - accuracy: 0.6051 - loss: 0.6601 - val_accuracy: 0.8008 - val_loss: 0.5042 - mcc: 0.5998\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8117 - loss: 0.4562\n","Epoch 2 - MCC: 0.7625\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8124 - loss: 0.4543 - val_accuracy: 0.8817 - val_loss: 0.2985 - mcc: 0.7625\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8791 - loss: 0.2977\n","Epoch 3 - MCC: 0.7916\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8791 - loss: 0.2975 - val_accuracy: 0.8955 - val_loss: 0.2555 - mcc: 0.7916\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8907 - loss: 0.2639\n","Epoch 4 - MCC: 0.8043\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8908 - loss: 0.2636 - val_accuracy: 0.9022 - val_loss: 0.2353 - mcc: 0.8043\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8993 - loss: 0.2410\n","Epoch 5 - MCC: 0.8151\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.8993 - loss: 0.2410 - val_accuracy: 0.9079 - val_loss: 0.2228 - mcc: 0.8151\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9061 - loss: 0.2245\n","Epoch 6 - MCC: 0.8188\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9060 - loss: 0.2247 - val_accuracy: 0.9097 - val_loss: 0.2141 - mcc: 0.8188\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8980 - loss: 0.2364\n","Epoch 7 - MCC: 0.8236\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.8982 - loss: 0.2361 - val_accuracy: 0.9119 - val_loss: 0.2080 - mcc: 0.8236\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9068 - loss: 0.2189\n","Epoch 8 - MCC: 0.8240\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9067 - loss: 0.2190 - val_accuracy: 0.9122 - val_loss: 0.2066 - mcc: 0.8240\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9033 - loss: 0.2248\n","Epoch 9 - MCC: 0.8241\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9034 - loss: 0.2246 - val_accuracy: 0.9124 - val_loss: 0.2031 - mcc: 0.8241\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9063 - loss: 0.2190\n","Epoch 10 - MCC: 0.8298\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9064 - loss: 0.2187 - val_accuracy: 0.9150 - val_loss: 0.1997 - mcc: 0.8298\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9088 - loss: 0.2136\n","Epoch 11 - MCC: 0.8339\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9088 - loss: 0.2135 - val_accuracy: 0.9171 - val_loss: 0.1937 - mcc: 0.8339\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9051 - loss: 0.2237\n","Epoch 12 - MCC: 0.8354\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9052 - loss: 0.2234 - val_accuracy: 0.9179 - val_loss: 0.1914 - mcc: 0.8354\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9137 - loss: 0.2026\n","Epoch 13 - MCC: 0.8346\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9137 - loss: 0.2027 - val_accuracy: 0.9173 - val_loss: 0.1965 - mcc: 0.8346\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9165 - loss: 0.1969\n","Epoch 14 - MCC: 0.8384\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9163 - loss: 0.1972 - val_accuracy: 0.9194 - val_loss: 0.1897 - mcc: 0.8384\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9090 - loss: 0.2120\n","Epoch 15 - MCC: 0.8420\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9091 - loss: 0.2117 - val_accuracy: 0.9213 - val_loss: 0.1835 - mcc: 0.8420\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9102 - loss: 0.2113\n","Epoch 16 - MCC: 0.8414\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9103 - loss: 0.2111 - val_accuracy: 0.9209 - val_loss: 0.1854 - mcc: 0.8414\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9116 - loss: 0.2067\n","Epoch 17 - MCC: 0.8321\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9117 - loss: 0.2065 - val_accuracy: 0.9156 - val_loss: 0.1966 - mcc: 0.8321\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9203 - loss: 0.1889\n","Epoch 18 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9201 - loss: 0.1892 - val_accuracy: 0.9231 - val_loss: 0.1797 - mcc: 0.8458\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9140 - loss: 0.2026\n","Epoch 19 - MCC: 0.8356\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9142 - loss: 0.2022 - val_accuracy: 0.9166 - val_loss: 0.1982 - mcc: 0.8356\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9146 - loss: 0.2036\n","Epoch 20 - MCC: 0.8517\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9146 - loss: 0.2034 - val_accuracy: 0.9261 - val_loss: 0.1750 - mcc: 0.8517\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9211 - loss: 0.1862\n","Epoch 21 - MCC: 0.8570\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9211 - loss: 0.1862 - val_accuracy: 0.9286 - val_loss: 0.1693 - mcc: 0.8570\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9246 - loss: 0.1801\n","Epoch 22 - MCC: 0.8549\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9245 - loss: 0.1804 - val_accuracy: 0.9277 - val_loss: 0.1715 - mcc: 0.8549\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9234 - loss: 0.1809\n","Epoch 23 - MCC: 0.8539\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9233 - loss: 0.1811 - val_accuracy: 0.9270 - val_loss: 0.1720 - mcc: 0.8539\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9157 - loss: 0.1976\n","Epoch 24 - MCC: 0.8569\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9159 - loss: 0.1971 - val_accuracy: 0.9287 - val_loss: 0.1692 - mcc: 0.8569\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9226 - loss: 0.1824\n","Epoch 25 - MCC: 0.8591\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9226 - loss: 0.1824 - val_accuracy: 0.9298 - val_loss: 0.1639 - mcc: 0.8591\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9206 - loss: 0.1856\n","Epoch 26 - MCC: 0.8513\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9206 - loss: 0.1856 - val_accuracy: 0.9256 - val_loss: 0.1726 - mcc: 0.8513\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9224 - loss: 0.1840\n","Epoch 27 - MCC: 0.8613\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9224 - loss: 0.1840 - val_accuracy: 0.9309 - val_loss: 0.1639 - mcc: 0.8613\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9219 - loss: 0.1843\n","Epoch 28 - MCC: 0.8553\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9220 - loss: 0.1839 - val_accuracy: 0.9275 - val_loss: 0.1677 - mcc: 0.8553\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9224 - loss: 0.1826\n","Epoch 29 - MCC: 0.8633\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9225 - loss: 0.1825 - val_accuracy: 0.9319 - val_loss: 0.1606 - mcc: 0.8633\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9252 - loss: 0.1771\n","Epoch 30 - MCC: 0.8594\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9252 - loss: 0.1772 - val_accuracy: 0.9294 - val_loss: 0.1645 - mcc: 0.8594\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6610 - loss: 0.6569\n","Epoch 1 - MCC: 0.6100\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 140ms/step - accuracy: 0.6633 - loss: 0.6548 - val_accuracy: 0.8056 - val_loss: 0.4663 - mcc: 0.6100\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8372 - loss: 0.3987\n","Epoch 2 - MCC: 0.7465\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.8380 - loss: 0.3969 - val_accuracy: 0.8719 - val_loss: 0.3195 - mcc: 0.7465\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8855 - loss: 0.2793\n","Epoch 3 - MCC: 0.7697\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8856 - loss: 0.2790 - val_accuracy: 0.8851 - val_loss: 0.2757 - mcc: 0.7697\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9012 - loss: 0.2396\n","Epoch 4 - MCC: 0.7920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9011 - loss: 0.2397 - val_accuracy: 0.8962 - val_loss: 0.2468 - mcc: 0.7920\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9031 - loss: 0.2318\n","Epoch 5 - MCC: 0.7944\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9031 - loss: 0.2318 - val_accuracy: 0.8971 - val_loss: 0.2443 - mcc: 0.7944\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9108 - loss: 0.2146\n","Epoch 6 - MCC: 0.8017\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9107 - loss: 0.2148 - val_accuracy: 0.9011 - val_loss: 0.2327 - mcc: 0.8017\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9062 - loss: 0.2247\n","Epoch 7 - MCC: 0.8032\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9064 - loss: 0.2244 - val_accuracy: 0.9018 - val_loss: 0.2326 - mcc: 0.8032\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9105 - loss: 0.2116\n","Epoch 8 - MCC: 0.8052\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9105 - loss: 0.2114 - val_accuracy: 0.9023 - val_loss: 0.2295 - mcc: 0.8052\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9121 - loss: 0.2084\n","Epoch 9 - MCC: 0.8086\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9121 - loss: 0.2083 - val_accuracy: 0.9045 - val_loss: 0.2244 - mcc: 0.8086\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9140 - loss: 0.2029\n","Epoch 10 - MCC: 0.8029\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.9140 - loss: 0.2028 - val_accuracy: 0.9016 - val_loss: 0.2273 - mcc: 0.8029\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9177 - loss: 0.1949\n","Epoch 11 - MCC: 0.8095\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9176 - loss: 0.1950 - val_accuracy: 0.9049 - val_loss: 0.2245 - mcc: 0.8095\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9153 - loss: 0.2002\n","Epoch 12 - MCC: 0.7939\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9154 - loss: 0.2002 - val_accuracy: 0.8955 - val_loss: 0.2490 - mcc: 0.7939\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9119 - loss: 0.2088\n","Epoch 13 - MCC: 0.8072\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9119 - loss: 0.2087 - val_accuracy: 0.9037 - val_loss: 0.2247 - mcc: 0.8072\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9165 - loss: 0.1987\n","Epoch 14 - MCC: 0.8065\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9165 - loss: 0.1987 - val_accuracy: 0.9033 - val_loss: 0.2234 - mcc: 0.8065\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9212 - loss: 0.1877\n","Epoch 15 - MCC: 0.8125\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.9212 - loss: 0.1879 - val_accuracy: 0.9065 - val_loss: 0.2189 - mcc: 0.8125\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9187 - loss: 0.1933\n","Epoch 16 - MCC: 0.8160\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9187 - loss: 0.1933 - val_accuracy: 0.9081 - val_loss: 0.2158 - mcc: 0.8160\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9231 - loss: 0.1838\n","Epoch 17 - MCC: 0.8229\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9230 - loss: 0.1840 - val_accuracy: 0.9117 - val_loss: 0.2080 - mcc: 0.8229\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9240 - loss: 0.1828\n","Epoch 18 - MCC: 0.8235\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9240 - loss: 0.1829 - val_accuracy: 0.9120 - val_loss: 0.2075 - mcc: 0.8235\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9210 - loss: 0.1871\n","Epoch 19 - MCC: 0.8130\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9210 - loss: 0.1872 - val_accuracy: 0.9067 - val_loss: 0.2173 - mcc: 0.8130\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9200 - loss: 0.1909\n","Epoch 20 - MCC: 0.8215\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9201 - loss: 0.1907 - val_accuracy: 0.9106 - val_loss: 0.2085 - mcc: 0.8215\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9223 - loss: 0.1858\n","Epoch 21 - MCC: 0.8281\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9224 - loss: 0.1856 - val_accuracy: 0.9141 - val_loss: 0.2029 - mcc: 0.8281\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9296 - loss: 0.1688\n","Epoch 22 - MCC: 0.8282\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9295 - loss: 0.1691 - val_accuracy: 0.9143 - val_loss: 0.2024 - mcc: 0.8282\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9240 - loss: 0.1801\n","Epoch 23 - MCC: 0.8255\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9240 - loss: 0.1800 - val_accuracy: 0.9124 - val_loss: 0.2066 - mcc: 0.8255\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9234 - loss: 0.1818\n","Epoch 24 - MCC: 0.8241\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9234 - loss: 0.1819 - val_accuracy: 0.9121 - val_loss: 0.2060 - mcc: 0.8241\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9240 - loss: 0.1819\n","Epoch 25 - MCC: 0.8330\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9241 - loss: 0.1816 - val_accuracy: 0.9163 - val_loss: 0.1976 - mcc: 0.8330\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9274 - loss: 0.1731\n","Epoch 26 - MCC: 0.8329\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9274 - loss: 0.1731 - val_accuracy: 0.9167 - val_loss: 0.1969 - mcc: 0.8329\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9293 - loss: 0.1688\n","Epoch 27 - MCC: 0.8322\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9292 - loss: 0.1690 - val_accuracy: 0.9162 - val_loss: 0.1962 - mcc: 0.8322\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9300 - loss: 0.1684\n","Epoch 28 - MCC: 0.8347\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9299 - loss: 0.1684 - val_accuracy: 0.9175 - val_loss: 0.1938 - mcc: 0.8347\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9299 - loss: 0.1701\n","Epoch 29 - MCC: 0.8335\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9299 - loss: 0.1700 - val_accuracy: 0.9169 - val_loss: 0.1922 - mcc: 0.8335\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9315 - loss: 0.1627\n","Epoch 30 - MCC: 0.8307\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9314 - loss: 0.1629 - val_accuracy: 0.9155 - val_loss: 0.1962 - mcc: 0.8307\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5523 - loss: 0.6632\n","Epoch 1 - MCC: 0.6273\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.5554 - loss: 0.6616 - val_accuracy: 0.8141 - val_loss: 0.5093 - mcc: 0.6273\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8437 - loss: 0.4350\n","Epoch 2 - MCC: 0.7825\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.8443 - loss: 0.4327 - val_accuracy: 0.8917 - val_loss: 0.2742 - mcc: 0.7825\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8938 - loss: 0.2633\n","Epoch 3 - MCC: 0.7993\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.8938 - loss: 0.2631 - val_accuracy: 0.8988 - val_loss: 0.2408 - mcc: 0.7993\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9024 - loss: 0.2340\n","Epoch 4 - MCC: 0.8118\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9024 - loss: 0.2339 - val_accuracy: 0.9055 - val_loss: 0.2262 - mcc: 0.8118\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9044 - loss: 0.2250\n","Epoch 5 - MCC: 0.8174\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9044 - loss: 0.2249 - val_accuracy: 0.9089 - val_loss: 0.2120 - mcc: 0.8174\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9070 - loss: 0.2202\n","Epoch 6 - MCC: 0.8182\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9071 - loss: 0.2200 - val_accuracy: 0.9086 - val_loss: 0.2145 - mcc: 0.8182\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9042 - loss: 0.2220\n","Epoch 7 - MCC: 0.8267\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9042 - loss: 0.2219 - val_accuracy: 0.9135 - val_loss: 0.2056 - mcc: 0.8267\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9125 - loss: 0.2049\n","Epoch 8 - MCC: 0.8301\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9124 - loss: 0.2053 - val_accuracy: 0.9153 - val_loss: 0.2006 - mcc: 0.8301\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9111 - loss: 0.2089\n","Epoch 9 - MCC: 0.8281\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9111 - loss: 0.2089 - val_accuracy: 0.9139 - val_loss: 0.2016 - mcc: 0.8281\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9132 - loss: 0.2041\n","Epoch 10 - MCC: 0.8367\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9132 - loss: 0.2042 - val_accuracy: 0.9186 - val_loss: 0.1919 - mcc: 0.8367\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9159 - loss: 0.1980\n","Epoch 11 - MCC: 0.8364\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9158 - loss: 0.1981 - val_accuracy: 0.9185 - val_loss: 0.1911 - mcc: 0.8364\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9120 - loss: 0.2057\n","Epoch 12 - MCC: 0.8401\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9121 - loss: 0.2056 - val_accuracy: 0.9204 - val_loss: 0.1876 - mcc: 0.8401\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9137 - loss: 0.2008\n","Epoch 13 - MCC: 0.8364\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9137 - loss: 0.2008 - val_accuracy: 0.9179 - val_loss: 0.1945 - mcc: 0.8364\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9154 - loss: 0.1982\n","Epoch 14 - MCC: 0.8389\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9154 - loss: 0.1981 - val_accuracy: 0.9196 - val_loss: 0.1860 - mcc: 0.8389\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9114 - loss: 0.2039\n","Epoch 15 - MCC: 0.8338\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9116 - loss: 0.2037 - val_accuracy: 0.9167 - val_loss: 0.1959 - mcc: 0.8338\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9164 - loss: 0.1964\n","Epoch 16 - MCC: 0.8440\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - accuracy: 0.9165 - loss: 0.1962 - val_accuracy: 0.9222 - val_loss: 0.1810 - mcc: 0.8440\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9204 - loss: 0.1872\n","Epoch 17 - MCC: 0.8432\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.9204 - loss: 0.1873 - val_accuracy: 0.9216 - val_loss: 0.1814 - mcc: 0.8432\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9222 - loss: 0.1830\n","Epoch 18 - MCC: 0.8409\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9221 - loss: 0.1832 - val_accuracy: 0.9205 - val_loss: 0.1872 - mcc: 0.8409\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9200 - loss: 0.1896\n","Epoch 19 - MCC: 0.8490\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9201 - loss: 0.1895 - val_accuracy: 0.9248 - val_loss: 0.1760 - mcc: 0.8490\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9227 - loss: 0.1818\n","Epoch 20 - MCC: 0.8469\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9226 - loss: 0.1821 - val_accuracy: 0.9238 - val_loss: 0.1773 - mcc: 0.8469\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9235 - loss: 0.1807\n","Epoch 21 - MCC: 0.8523\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9235 - loss: 0.1808 - val_accuracy: 0.9263 - val_loss: 0.1734 - mcc: 0.8523\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9196 - loss: 0.1884\n","Epoch 22 - MCC: 0.8472\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9197 - loss: 0.1881 - val_accuracy: 0.9237 - val_loss: 0.1790 - mcc: 0.8472\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9248 - loss: 0.1772\n","Epoch 23 - MCC: 0.8575\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9248 - loss: 0.1772 - val_accuracy: 0.9291 - val_loss: 0.1651 - mcc: 0.8575\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9253 - loss: 0.1753\n","Epoch 24 - MCC: 0.8533\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9252 - loss: 0.1756 - val_accuracy: 0.9265 - val_loss: 0.1724 - mcc: 0.8533\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9283 - loss: 0.1703\n","Epoch 25 - MCC: 0.8596\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9281 - loss: 0.1706 - val_accuracy: 0.9299 - val_loss: 0.1661 - mcc: 0.8596\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9270 - loss: 0.1723\n","Epoch 26 - MCC: 0.8573\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9268 - loss: 0.1725 - val_accuracy: 0.9284 - val_loss: 0.1663 - mcc: 0.8573\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9215 - loss: 0.1830\n","Epoch 27 - MCC: 0.8598\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9216 - loss: 0.1827 - val_accuracy: 0.9301 - val_loss: 0.1638 - mcc: 0.8598\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9261 - loss: 0.1749\n","Epoch 28 - MCC: 0.8568\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9260 - loss: 0.1750 - val_accuracy: 0.9287 - val_loss: 0.1687 - mcc: 0.8568\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9214 - loss: 0.1859\n","Epoch 29 - MCC: 0.8602\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9216 - loss: 0.1854 - val_accuracy: 0.9304 - val_loss: 0.1624 - mcc: 0.8602\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9273 - loss: 0.1717\n","Epoch 30 - MCC: 0.8634\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9273 - loss: 0.1716 - val_accuracy: 0.9320 - val_loss: 0.1591 - mcc: 0.8634\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5989 - loss: 0.6842\n","Epoch 1 - MCC: 0.6005\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.6017 - loss: 0.6826 - val_accuracy: 0.8007 - val_loss: 0.5215 - mcc: 0.6005\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8104 - loss: 0.4631\n","Epoch 2 - MCC: 0.7774\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8113 - loss: 0.4609 - val_accuracy: 0.8889 - val_loss: 0.2813 - mcc: 0.7774\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8773 - loss: 0.2977\n","Epoch 3 - MCC: 0.7671\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.8775 - loss: 0.2972 - val_accuracy: 0.8795 - val_loss: 0.2801 - mcc: 0.7671\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8874 - loss: 0.2682\n","Epoch 4 - MCC: 0.8042\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8875 - loss: 0.2679 - val_accuracy: 0.9018 - val_loss: 0.2365 - mcc: 0.8042\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8969 - loss: 0.2445\n","Epoch 5 - MCC: 0.8105\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - accuracy: 0.8970 - loss: 0.2443 - val_accuracy: 0.9050 - val_loss: 0.2249 - mcc: 0.8105\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9044 - loss: 0.2256\n","Epoch 6 - MCC: 0.8206\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9043 - loss: 0.2258 - val_accuracy: 0.9103 - val_loss: 0.2160 - mcc: 0.8206\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9054 - loss: 0.2226\n","Epoch 7 - MCC: 0.8235\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9054 - loss: 0.2225 - val_accuracy: 0.9119 - val_loss: 0.2067 - mcc: 0.8235\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9037 - loss: 0.2239\n","Epoch 8 - MCC: 0.8276\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9037 - loss: 0.2239 - val_accuracy: 0.9136 - val_loss: 0.2085 - mcc: 0.8276\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9085 - loss: 0.2126\n","Epoch 9 - MCC: 0.8337\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9085 - loss: 0.2126 - val_accuracy: 0.9170 - val_loss: 0.2005 - mcc: 0.8337\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9058 - loss: 0.2193\n","Epoch 10 - MCC: 0.8304\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.9060 - loss: 0.2189 - val_accuracy: 0.9154 - val_loss: 0.2009 - mcc: 0.8304\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9096 - loss: 0.2103\n","Epoch 11 - MCC: 0.8379\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9098 - loss: 0.2100 - val_accuracy: 0.9191 - val_loss: 0.1942 - mcc: 0.8379\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9134 - loss: 0.2036\n","Epoch 12 - MCC: 0.8401\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9135 - loss: 0.2035 - val_accuracy: 0.9202 - val_loss: 0.1924 - mcc: 0.8401\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9116 - loss: 0.2088\n","Epoch 13 - MCC: 0.8391\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9118 - loss: 0.2085 - val_accuracy: 0.9197 - val_loss: 0.1928 - mcc: 0.8391\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9121 - loss: 0.2052\n","Epoch 14 - MCC: 0.8405\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9122 - loss: 0.2050 - val_accuracy: 0.9201 - val_loss: 0.1927 - mcc: 0.8405\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9161 - loss: 0.1971\n","Epoch 15 - MCC: 0.8463\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.9161 - loss: 0.1971 - val_accuracy: 0.9233 - val_loss: 0.1845 - mcc: 0.8463\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9206 - loss: 0.1868\n","Epoch 16 - MCC: 0.8440\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9205 - loss: 0.1871 - val_accuracy: 0.9221 - val_loss: 0.1866 - mcc: 0.8440\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9124 - loss: 0.2073\n","Epoch 17 - MCC: 0.8467\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9125 - loss: 0.2069 - val_accuracy: 0.9235 - val_loss: 0.1836 - mcc: 0.8467\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9166 - loss: 0.1976\n","Epoch 18 - MCC: 0.8444\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9166 - loss: 0.1977 - val_accuracy: 0.9224 - val_loss: 0.1887 - mcc: 0.8444\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9240 - loss: 0.1816\n","Epoch 19 - MCC: 0.8471\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9238 - loss: 0.1819 - val_accuracy: 0.9238 - val_loss: 0.1830 - mcc: 0.8471\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9152 - loss: 0.1992\n","Epoch 20 - MCC: 0.8509\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9154 - loss: 0.1988 - val_accuracy: 0.9254 - val_loss: 0.1792 - mcc: 0.8509\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9242 - loss: 0.1813\n","Epoch 21 - MCC: 0.8430\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9241 - loss: 0.1815 - val_accuracy: 0.9217 - val_loss: 0.1863 - mcc: 0.8430\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9173 - loss: 0.1934\n","Epoch 22 - MCC: 0.8542\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9174 - loss: 0.1932 - val_accuracy: 0.9273 - val_loss: 0.1759 - mcc: 0.8542\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9225 - loss: 0.1827\n","Epoch 23 - MCC: 0.8511\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9225 - loss: 0.1828 - val_accuracy: 0.9257 - val_loss: 0.1784 - mcc: 0.8511\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9195 - loss: 0.1883\n","Epoch 24 - MCC: 0.8407\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9196 - loss: 0.1881 - val_accuracy: 0.9199 - val_loss: 0.1923 - mcc: 0.8407\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9242 - loss: 0.1793\n","Epoch 25 - MCC: 0.8560\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9241 - loss: 0.1795 - val_accuracy: 0.9280 - val_loss: 0.1720 - mcc: 0.8560\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9207 - loss: 0.1869\n","Epoch 26 - MCC: 0.8559\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9208 - loss: 0.1867 - val_accuracy: 0.9278 - val_loss: 0.1714 - mcc: 0.8559\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9216 - loss: 0.1827\n","Epoch 27 - MCC: 0.8576\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.9217 - loss: 0.1826 - val_accuracy: 0.9289 - val_loss: 0.1685 - mcc: 0.8576\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9218 - loss: 0.1829\n","Epoch 28 - MCC: 0.8584\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9219 - loss: 0.1826 - val_accuracy: 0.9293 - val_loss: 0.1683 - mcc: 0.8584\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9235 - loss: 0.1779\n","Epoch 29 - MCC: 0.8572\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9236 - loss: 0.1778 - val_accuracy: 0.9288 - val_loss: 0.1682 - mcc: 0.8572\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9280 - loss: 0.1700\n","Epoch 30 - MCC: 0.8581\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9279 - loss: 0.1701 - val_accuracy: 0.9292 - val_loss: 0.1671 - mcc: 0.8581\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5886 - loss: 0.6449\n","Epoch 1 - MCC: 0.6007\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.5917 - loss: 0.6430 - val_accuracy: 0.7924 - val_loss: 0.4769 - mcc: 0.6007\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8245 - loss: 0.4263\n","Epoch 2 - MCC: 0.7063\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - accuracy: 0.8251 - loss: 0.4247 - val_accuracy: 0.8473 - val_loss: 0.3533 - mcc: 0.7063\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8853 - loss: 0.2811\n","Epoch 3 - MCC: 0.7726\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.8853 - loss: 0.2810 - val_accuracy: 0.8865 - val_loss: 0.2664 - mcc: 0.7726\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8956 - loss: 0.2482\n","Epoch 4 - MCC: 0.7856\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.8957 - loss: 0.2481 - val_accuracy: 0.8926 - val_loss: 0.2481 - mcc: 0.7856\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9018 - loss: 0.2330\n","Epoch 5 - MCC: 0.8024\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9019 - loss: 0.2329 - val_accuracy: 0.9013 - val_loss: 0.2307 - mcc: 0.8024\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9067 - loss: 0.2203\n","Epoch 6 - MCC: 0.7992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9068 - loss: 0.2201 - val_accuracy: 0.8983 - val_loss: 0.2420 - mcc: 0.7992\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9057 - loss: 0.2223\n","Epoch 7 - MCC: 0.8084\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9059 - loss: 0.2220 - val_accuracy: 0.9032 - val_loss: 0.2232 - mcc: 0.8084\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9127 - loss: 0.2051\n","Epoch 8 - MCC: 0.8138\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9128 - loss: 0.2051 - val_accuracy: 0.9069 - val_loss: 0.2190 - mcc: 0.8138\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9099 - loss: 0.2127\n","Epoch 9 - MCC: 0.8166\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9101 - loss: 0.2124 - val_accuracy: 0.9084 - val_loss: 0.2163 - mcc: 0.8166\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9128 - loss: 0.2049\n","Epoch 10 - MCC: 0.8225\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9128 - loss: 0.2049 - val_accuracy: 0.9113 - val_loss: 0.2079 - mcc: 0.8225\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9179 - loss: 0.1956\n","Epoch 11 - MCC: 0.8151\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9178 - loss: 0.1958 - val_accuracy: 0.9071 - val_loss: 0.2170 - mcc: 0.8151\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9148 - loss: 0.2018\n","Epoch 12 - MCC: 0.8225\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9148 - loss: 0.2018 - val_accuracy: 0.9114 - val_loss: 0.2081 - mcc: 0.8225\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9151 - loss: 0.2020\n","Epoch 13 - MCC: 0.8260\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9151 - loss: 0.2019 - val_accuracy: 0.9130 - val_loss: 0.2051 - mcc: 0.8260\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9229 - loss: 0.1840\n","Epoch 14 - MCC: 0.8228\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9227 - loss: 0.1845 - val_accuracy: 0.9115 - val_loss: 0.2069 - mcc: 0.8228\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9218 - loss: 0.1865\n","Epoch 15 - MCC: 0.8034\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9216 - loss: 0.1868 - val_accuracy: 0.9016 - val_loss: 0.2261 - mcc: 0.8034\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9142 - loss: 0.2024\n","Epoch 16 - MCC: 0.8200\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9143 - loss: 0.2022 - val_accuracy: 0.9091 - val_loss: 0.2114 - mcc: 0.8200\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9204 - loss: 0.1873\n","Epoch 17 - MCC: 0.8325\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9204 - loss: 0.1873 - val_accuracy: 0.9164 - val_loss: 0.1978 - mcc: 0.8325\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9248 - loss: 0.1800\n","Epoch 18 - MCC: 0.8139\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9246 - loss: 0.1803 - val_accuracy: 0.9063 - val_loss: 0.2166 - mcc: 0.8139\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9225 - loss: 0.1834\n","Epoch 19 - MCC: 0.8274\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9225 - loss: 0.1836 - val_accuracy: 0.9125 - val_loss: 0.2055 - mcc: 0.8274\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9175 - loss: 0.1921\n","Epoch 20 - MCC: 0.8343\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9177 - loss: 0.1918 - val_accuracy: 0.9166 - val_loss: 0.1982 - mcc: 0.8343\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9277 - loss: 0.1731\n","Epoch 21 - MCC: 0.8326\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9276 - loss: 0.1734 - val_accuracy: 0.9165 - val_loss: 0.1952 - mcc: 0.8326\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9240 - loss: 0.1805\n","Epoch 22 - MCC: 0.8402\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9241 - loss: 0.1804 - val_accuracy: 0.9201 - val_loss: 0.1915 - mcc: 0.8402\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9237 - loss: 0.1818\n","Epoch 23 - MCC: 0.8398\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9238 - loss: 0.1816 - val_accuracy: 0.9193 - val_loss: 0.1944 - mcc: 0.8398\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9273 - loss: 0.1728\n","Epoch 24 - MCC: 0.8381\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9273 - loss: 0.1729 - val_accuracy: 0.9187 - val_loss: 0.1952 - mcc: 0.8381\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9287 - loss: 0.1691\n","Epoch 25 - MCC: 0.8404\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9286 - loss: 0.1694 - val_accuracy: 0.9198 - val_loss: 0.1904 - mcc: 0.8404\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9243 - loss: 0.1777\n","Epoch 26 - MCC: 0.8368\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9244 - loss: 0.1776 - val_accuracy: 0.9185 - val_loss: 0.1948 - mcc: 0.8368\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9294 - loss: 0.1686\n","Epoch 27 - MCC: 0.8438\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9293 - loss: 0.1688 - val_accuracy: 0.9219 - val_loss: 0.1861 - mcc: 0.8438\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9290 - loss: 0.1672\n","Epoch 28 - MCC: 0.8441\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.9290 - loss: 0.1674 - val_accuracy: 0.9216 - val_loss: 0.1862 - mcc: 0.8441\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9289 - loss: 0.1677\n","Epoch 29 - MCC: 0.8434\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9289 - loss: 0.1678 - val_accuracy: 0.9217 - val_loss: 0.1875 - mcc: 0.8434\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9256 - loss: 0.1761\n","Epoch 30 - MCC: 0.8480\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9257 - loss: 0.1759 - val_accuracy: 0.9241 - val_loss: 0.1811 - mcc: 0.8480\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.93196,\n","              'mean': 0.9260200000000001,\n","              'min': 0.9154866666666667,\n","              'std': 0.0058556032045288834},\n"," 'Inference Time (s/sample)': {'max': 0.003835489749908447,\n","                               'mean': 0.0032799577713012694,\n","                               'min': 0.0019155383110046388,\n","                               'std': 0.0006915553645100655},\n"," 'MCC': {'max': 0.8633784532482994,\n","         'mean': 0.85192920121099,\n","         'min': 0.8306995550666223,\n","         'std': 0.01175799545685088},\n"," 'Parameters': 30113,\n"," 'Train Time (s)': {'max': 84.46629810333252,\n","                    'mean': 82.269331741333,\n","                    'min': 80.53150272369385,\n","                    'std': 1.4426156368305298},\n"," 'Training Accuracy': [[0.6621266007423401,\n","                        0.8300685286521912,\n","                        0.8801383376121521,\n","                        0.8924500942230225,\n","                        0.8998600840568542,\n","                        0.9030516147613525,\n","                        0.9027567505836487,\n","                        0.9050349593162537,\n","                        0.9057884216308594,\n","                        0.9090481400489807,\n","                        0.910165011882782,\n","                        0.908486545085907,\n","                        0.9124466776847839,\n","                        0.9125616550445557,\n","                        0.9125850796699524,\n","                        0.9131600260734558,\n","                        0.9137783646583557,\n","                        0.9161883592605591,\n","                        0.9186766743659973,\n","                        0.9164117574691772,\n","                        0.9203298687934875,\n","                        0.9211350083351135,\n","                        0.9210600256919861,\n","                        0.921333372592926,\n","                        0.9223232269287109,\n","                        0.9212068319320679,\n","                        0.9230365753173828,\n","                        0.9257633090019226,\n","                        0.9241365790367126,\n","                        0.9246600270271301],\n","                       [0.720978319644928,\n","                        0.856336772441864,\n","                        0.8890315294265747,\n","                        0.8994783759117126,\n","                        0.902116596698761,\n","                        0.9079632759094238,\n","                        0.9089968204498291,\n","                        0.9122000932693481,\n","                        0.9128949642181396,\n","                        0.9144933223724365,\n","                        0.9160616993904114,\n","                        0.9161683917045593,\n","                        0.9134150743484497,\n","                        0.9174333214759827,\n","                        0.9195816516876221,\n","                        0.9185249209403992,\n","                        0.9210599660873413,\n","                        0.9219934344291687,\n","                        0.9206317067146301,\n","                        0.9218082427978516,\n","                        0.924828290939331,\n","                        0.9264982342720032,\n","                        0.9249651432037354,\n","                        0.9235183596611023,\n","                        0.9265033006668091,\n","                        0.9278749823570251,\n","                        0.9260150194168091,\n","                        0.9291349649429321,\n","                        0.9294082522392273,\n","                        0.9291934967041016],\n","                       [0.6348332762718201,\n","                        0.8591250777244568,\n","                        0.8930667042732239,\n","                        0.9024118185043335,\n","                        0.9052984118461609,\n","                        0.9085884094238281,\n","                        0.9052266478538513,\n","                        0.9082949757575989,\n","                        0.9103518724441528,\n","                        0.9123333692550659,\n","                        0.9139350056648254,\n","                        0.9147334098815918,\n","                        0.9151417016983032,\n","                        0.9158632755279541,\n","                        0.9150050282478333,\n","                        0.9180632829666138,\n","                        0.9194700121879578,\n","                        0.920021653175354,\n","                        0.9207850098609924,\n","                        0.9199983477592468,\n","                        0.9222248792648315,\n","                        0.922760009765625,\n","                        0.9243066310882568,\n","                        0.9219899773597717,\n","                        0.9244632720947266,\n","                        0.9236350059509277,\n","                        0.925313413143158,\n","                        0.9251815676689148,\n","                        0.9261283278465271,\n","                        0.9286665320396423],\n","                       [0.671845018863678,\n","                        0.8340317606925964,\n","                        0.882148265838623,\n","                        0.8903383612632751,\n","                        0.8993349671363831,\n","                        0.9022466540336609,\n","                        0.9059799909591675,\n","                        0.9038599133491516,\n","                        0.9086349606513977,\n","                        0.9108064770698547,\n","                        0.9133332967758179,\n","                        0.9144049286842346,\n","                        0.915191650390625,\n","                        0.9155850410461426,\n","                        0.9160701036453247,\n","                        0.91743004322052,\n","                        0.9159616231918335,\n","                        0.9148499965667725,\n","                        0.9191001057624817,\n","                        0.9204733967781067,\n","                        0.9206581711769104,\n","                        0.9199884533882141,\n","                        0.9216367602348328,\n","                        0.9220050573348999,\n","                        0.9220550060272217,\n","                        0.9233567118644714,\n","                        0.9239216446876526,\n","                        0.9255115985870361,\n","                        0.9250651597976685,\n","                        0.926746666431427],\n","                       [0.6694899201393127,\n","                        0.8414616584777832,\n","                        0.8850432634353638,\n","                        0.897193193435669,\n","                        0.9039350748062134,\n","                        0.9096066355705261,\n","                        0.909111738204956,\n","                        0.9136099815368652,\n","                        0.9143916964530945,\n","                        0.9127750396728516,\n","                        0.9151516556739807,\n","                        0.9150516986846924,\n","                        0.9155867099761963,\n","                        0.9168417453765869,\n","                        0.9185400605201721,\n","                        0.9166433811187744,\n","                        0.9204283952713013,\n","                        0.9210966229438782,\n","                        0.9208500385284424,\n","                        0.9215883016586304,\n","                        0.9244517683982849,\n","                        0.9252066016197205,\n","                        0.9258833527565002,\n","                        0.9268499612808228,\n","                        0.925978422164917,\n","                        0.9258183836936951,\n","                        0.9266366362571716,\n","                        0.9269933104515076,\n","                        0.9282499551773071,\n","                        0.9279817342758179]],\n"," 'Training Loss': [[0.6218934059143066,\n","                    0.40691542625427246,\n","                    0.29064059257507324,\n","                    0.25795501470565796,\n","                    0.23943328857421875,\n","                    0.22944368422031403,\n","                    0.22870971262454987,\n","                    0.22252070903778076,\n","                    0.2206674963235855,\n","                    0.21307823061943054,\n","                    0.21038620173931122,\n","                    0.2150212824344635,\n","                    0.20625509321689606,\n","                    0.20517784357070923,\n","                    0.2043878734111786,\n","                    0.20438431203365326,\n","                    0.20188023149967194,\n","                    0.19792765378952026,\n","                    0.19239889085292816,\n","                    0.19830673933029175,\n","                    0.1877189576625824,\n","                    0.1862272322177887,\n","                    0.1864413619041443,\n","                    0.1857793629169464,\n","                    0.18359550833702087,\n","                    0.18537798523902893,\n","                    0.18209803104400635,\n","                    0.1759657859802246,\n","                    0.17918018996715546,\n","                    0.17836469411849976],\n","                   [0.6039815545082092,\n","                    0.3521396517753601,\n","                    0.270364910364151,\n","                    0.24238094687461853,\n","                    0.23225121200084686,\n","                    0.21929030120372772,\n","                    0.21656982600688934,\n","                    0.20803029835224152,\n","                    0.20561951398849487,\n","                    0.20214100182056427,\n","                    0.19952374696731567,\n","                    0.19888761639595032,\n","                    0.20603859424591064,\n","                    0.19706600904464722,\n","                    0.19126635789871216,\n","                    0.193928062915802,\n","                    0.18887785077095032,\n","                    0.18678799271583557,\n","                    0.18898075819015503,\n","                    0.18673047423362732,\n","                    0.1808440238237381,\n","                    0.17669469118118286,\n","                    0.17886997759342194,\n","                    0.1828179806470871,\n","                    0.17555664479732513,\n","                    0.17231261730194092,\n","                    0.17530694603919983,\n","                    0.16993439197540283,\n","                    0.16787564754486084,\n","                    0.16819027066230774],\n","                   [0.6225809454917908,\n","                    0.3769427537918091,\n","                    0.2601071000099182,\n","                    0.2326197773218155,\n","                    0.22250275313854218,\n","                    0.2155865877866745,\n","                    0.22166544198989868,\n","                    0.21493381261825562,\n","                    0.2096593677997589,\n","                    0.20553632080554962,\n","                    0.20173503458499908,\n","                    0.20145674049854279,\n","                    0.19917292892932892,\n","                    0.19741475582122803,\n","                    0.19893454015254974,\n","                    0.1931091696023941,\n","                    0.18943631649017334,\n","                    0.18708370625972748,\n","                    0.18678127229213715,\n","                    0.18841080367565155,\n","                    0.1836426854133606,\n","                    0.18180972337722778,\n","                    0.1777227520942688,\n","                    0.1826307624578476,\n","                    0.1781504601240158,\n","                    0.17947925627231598,\n","                    0.17560316622257233,\n","                    0.17713873088359833,\n","                    0.1742323935031891,\n","                    0.16871565580368042],\n","                   [0.6429190039634705,\n","                    0.4048275649547577,\n","                    0.2845255136489868,\n","                    0.26029375195503235,\n","                    0.2372671365737915,\n","                    0.23052316904067993,\n","                    0.22160713374614716,\n","                    0.22393478453159332,\n","                    0.2134501039981842,\n","                    0.20856164395809174,\n","                    0.2033666968345642,\n","                    0.20125886797904968,\n","                    0.1996665745973587,\n","                    0.19972650706768036,\n","                    0.19651643931865692,\n","                    0.19479034841060638,\n","                    0.19841866195201874,\n","                    0.20188909769058228,\n","                    0.19140945374965668,\n","                    0.1885000765323639,\n","                    0.18844544887542725,\n","                    0.18836496770381927,\n","                    0.18494290113449097,\n","                    0.1839759200811386,\n","                    0.18517567217350006,\n","                    0.18162944912910461,\n","                    0.17880745232105255,\n","                    0.17555640637874603,\n","                    0.17571264505386353,\n","                    0.17277522385120392],\n","                   [0.5965057611465454,\n","                    0.38320842385292053,\n","                    0.2781144976615906,\n","                    0.24519367516040802,\n","                    0.22946523129940033,\n","                    0.21478626132011414,\n","                    0.21404103934764862,\n","                    0.2048271745443344,\n","                    0.20269900560379028,\n","                    0.20557351410388947,\n","                    0.2001536786556244,\n","                    0.20088565349578857,\n","                    0.20015817880630493,\n","                    0.1970008909702301,\n","                    0.19389179348945618,\n","                    0.19811171293258667,\n","                    0.18808883428573608,\n","                    0.18705515563488007,\n","                    0.18733617663383484,\n","                    0.1858055293560028,\n","                    0.1793746054172516,\n","                    0.1767909973859787,\n","                    0.17625877261161804,\n","                    0.1745525598526001,\n","                    0.17585565149784088,\n","                    0.17445650696754456,\n","                    0.17383436858654022,\n","                    0.1725502759218216,\n","                    0.16969512403011322,\n","                    0.17045307159423828]],\n"," 'Validation Accuracy': [[0.8007999658584595,\n","                          0.8817266225814819,\n","                          0.8954866528511047,\n","                          0.9021867513656616,\n","                          0.9078867435455322,\n","                          0.9097466468811035,\n","                          0.9119333624839783,\n","                          0.9122399687767029,\n","                          0.9124066233634949,\n","                          0.9149866700172424,\n","                          0.9170600771903992,\n","                          0.9179132580757141,\n","                          0.9173199534416199,\n","                          0.9194000363349915,\n","                          0.9212666153907776,\n","                          0.9208999872207642,\n","                          0.9156000018119812,\n","                          0.9231400489807129,\n","                          0.916640043258667,\n","                          0.9260733127593994,\n","                          0.9286266565322876,\n","                          0.927733302116394,\n","                          0.9270066618919373,\n","                          0.9286532402038574,\n","                          0.9297667145729065,\n","                          0.9255866408348083,\n","                          0.9309200048446655,\n","                          0.927506685256958,\n","                          0.9318533539772034,\n","                          0.929413378238678],\n","                         [0.8055533766746521,\n","                          0.871940016746521,\n","                          0.8851266503334045,\n","                          0.8962467312812805,\n","                          0.8970533609390259,\n","                          0.9010865688323975,\n","                          0.9017665982246399,\n","                          0.9023066759109497,\n","                          0.9045132994651794,\n","                          0.9016132354736328,\n","                          0.9048799872398376,\n","                          0.8955467343330383,\n","                          0.9036666750907898,\n","                          0.9033266305923462,\n","                          0.9064933061599731,\n","                          0.9080666899681091,\n","                          0.9116533398628235,\n","                          0.911953330039978,\n","                          0.9066999554634094,\n","                          0.9105666279792786,\n","                          0.9141333699226379,\n","                          0.914306640625,\n","                          0.9124265909194946,\n","                          0.9120867252349854,\n","                          0.9162666201591492,\n","                          0.9166532754898071,\n","                          0.9162333607673645,\n","                          0.917513370513916,\n","                          0.9168932437896729,\n","                          0.9154866933822632],\n","                         [0.8141400218009949,\n","                          0.8916733264923096,\n","                          0.8988199830055237,\n","                          0.9054600596427917,\n","                          0.9089133739471436,\n","                          0.9085800647735596,\n","                          0.9134532809257507,\n","                          0.9153000116348267,\n","                          0.9138733148574829,\n","                          0.9186333417892456,\n","                          0.9184799790382385,\n","                          0.9204132556915283,\n","                          0.9178599715232849,\n","                          0.9196000099182129,\n","                          0.9166600108146667,\n","                          0.9222133159637451,\n","                          0.9215866923332214,\n","                          0.9204933047294617,\n","                          0.9247533679008484,\n","                          0.923780083656311,\n","                          0.9263066053390503,\n","                          0.923720121383667,\n","                          0.9290667176246643,\n","                          0.9265333414077759,\n","                          0.9298932552337646,\n","                          0.9284200072288513,\n","                          0.9301133155822754,\n","                          0.9286665916442871,\n","                          0.9303867220878601,\n","                          0.9319599866867065],\n","                         [0.8007400035858154,\n","                          0.8889399766921997,\n","                          0.8794933557510376,\n","                          0.9017533659934998,\n","                          0.9049599170684814,\n","                          0.9103333950042725,\n","                          0.9118865728378296,\n","                          0.9135532379150391,\n","                          0.9169667959213257,\n","                          0.9153799414634705,\n","                          0.9191133379936218,\n","                          0.9202266931533813,\n","                          0.9196532964706421,\n","                          0.9201266765594482,\n","                          0.9233267307281494,\n","                          0.9220532774925232,\n","                          0.9235199689865112,\n","                          0.9223599433898926,\n","                          0.9237533807754517,\n","                          0.9253532886505127,\n","                          0.9216732978820801,\n","                          0.9272533655166626,\n","                          0.9257267117500305,\n","                          0.9198867082595825,\n","                          0.9280400276184082,\n","                          0.9278400540351868,\n","                          0.9289199709892273,\n","                          0.9293200969696045,\n","                          0.9287733435630798,\n","                          0.9291667342185974],\n","                         [0.7924333214759827,\n","                          0.8472800254821777,\n","                          0.8864799737930298,\n","                          0.8925999402999878,\n","                          0.9012666940689087,\n","                          0.8982533812522888,\n","                          0.9031999707221985,\n","                          0.9069333076477051,\n","                          0.9084067344665527,\n","                          0.9113466739654541,\n","                          0.9070666432380676,\n","                          0.9114000201225281,\n","                          0.9130200147628784,\n","                          0.9114933609962463,\n","                          0.9015733599662781,\n","                          0.9091399908065796,\n","                          0.9163600206375122,\n","                          0.906279981136322,\n","                          0.9124866724014282,\n","                          0.9166066646575928,\n","                          0.9164733290672302,\n","                          0.920053243637085,\n","                          0.9192799925804138,\n","                          0.9186933636665344,\n","                          0.91975998878479,\n","                          0.9185399413108826,\n","                          0.9219200015068054,\n","                          0.921613335609436,\n","                          0.9216734170913696,\n","                          0.9240733981132507]],\n"," 'Validation Loss': [[0.5042439103126526,\n","                      0.29849717020988464,\n","                      0.25548627972602844,\n","                      0.2352559119462967,\n","                      0.22281354665756226,\n","                      0.21408864855766296,\n","                      0.20802707970142365,\n","                      0.20662455260753632,\n","                      0.20308609306812286,\n","                      0.19971813261508942,\n","                      0.1936730146408081,\n","                      0.1914178729057312,\n","                      0.19652357697486877,\n","                      0.1897120475769043,\n","                      0.183543860912323,\n","                      0.18537725508213043,\n","                      0.19660362601280212,\n","                      0.17971745133399963,\n","                      0.19824104011058807,\n","                      0.17501790821552277,\n","                      0.16926340758800507,\n","                      0.17146919667720795,\n","                      0.17200620472431183,\n","                      0.1692037582397461,\n","                      0.16386482119560242,\n","                      0.17262263596057892,\n","                      0.16385343670845032,\n","                      0.16767023503780365,\n","                      0.1605820506811142,\n","                      0.16451066732406616],\n","                     [0.4663460850715637,\n","                      0.3195428252220154,\n","                      0.2757011353969574,\n","                      0.24675756692886353,\n","                      0.24427767097949982,\n","                      0.23271292448043823,\n","                      0.23262880742549896,\n","                      0.22951997816562653,\n","                      0.22441934049129486,\n","                      0.22728972136974335,\n","                      0.22448387742042542,\n","                      0.24898220598697662,\n","                      0.22473180294036865,\n","                      0.223375603556633,\n","                      0.21889953315258026,\n","                      0.21579156816005707,\n","                      0.2079775184392929,\n","                      0.20754891633987427,\n","                      0.2173468917608261,\n","                      0.2084747552871704,\n","                      0.20288796722888947,\n","                      0.20240482687950134,\n","                      0.2066335678100586,\n","                      0.2060128003358841,\n","                      0.19757163524627686,\n","                      0.1969119906425476,\n","                      0.19622617959976196,\n","                      0.1938437819480896,\n","                      0.19219693541526794,\n","                      0.19622518122196198],\n","                     [0.5093175172805786,\n","                      0.27422866225242615,\n","                      0.24077636003494263,\n","                      0.22619248926639557,\n","                      0.2120029330253601,\n","                      0.2144828736782074,\n","                      0.20555277168750763,\n","                      0.20061975717544556,\n","                      0.2016211301088333,\n","                      0.19187122583389282,\n","                      0.19106611609458923,\n","                      0.18758079409599304,\n","                      0.19452886283397675,\n","                      0.1859825700521469,\n","                      0.1959279477596283,\n","                      0.18097271025180817,\n","                      0.18143177032470703,\n","                      0.1871718466281891,\n","                      0.17598426342010498,\n","                      0.1772727072238922,\n","                      0.17340147495269775,\n","                      0.1790091097354889,\n","                      0.16510750353336334,\n","                      0.17237839102745056,\n","                      0.16607175767421722,\n","                      0.16634199023246765,\n","                      0.16383691132068634,\n","                      0.16874468326568604,\n","                      0.162369042634964,\n","                      0.1591164469718933],\n","                     [0.5214850902557373,\n","                      0.28129205107688904,\n","                      0.28006604313850403,\n","                      0.23653998970985413,\n","                      0.2249029129743576,\n","                      0.21603885293006897,\n","                      0.20672239363193512,\n","                      0.2085212767124176,\n","                      0.20048362016677856,\n","                      0.2008615881204605,\n","                      0.19419284164905548,\n","                      0.19243167340755463,\n","                      0.192840114235878,\n","                      0.192658469080925,\n","                      0.1845032274723053,\n","                      0.18663190305233002,\n","                      0.1836341917514801,\n","                      0.1886654645204544,\n","                      0.182956263422966,\n","                      0.17921680212020874,\n","                      0.18631424009799957,\n","                      0.17586395144462585,\n","                      0.17842169106006622,\n","                      0.19231319427490234,\n","                      0.17196165025234222,\n","                      0.17140363156795502,\n","                      0.16847869753837585,\n","                      0.1682543158531189,\n","                      0.16824088990688324,\n","                      0.16714397072792053],\n","                     [0.47685492038726807,\n","                      0.3533450961112976,\n","                      0.26638180017471313,\n","                      0.24812011420726776,\n","                      0.23071086406707764,\n","                      0.24203823506832123,\n","                      0.22315636277198792,\n","                      0.2189655750989914,\n","                      0.2162792831659317,\n","                      0.20788049697875977,\n","                      0.21701662242412567,\n","                      0.20805558562278748,\n","                      0.20513972640037537,\n","                      0.20685508847236633,\n","                      0.2261400669813156,\n","                      0.21144941449165344,\n","                      0.19776155054569244,\n","                      0.21664310991764069,\n","                      0.20551806688308716,\n","                      0.19815921783447266,\n","                      0.1952131688594818,\n","                      0.19154296815395355,\n","                      0.194408118724823,\n","                      0.19518856704235077,\n","                      0.19042156636714935,\n","                      0.19475220143795013,\n","                      0.18605394661426544,\n","                      0.18618535995483398,\n","                      0.18747812509536743,\n","                      0.18106351792812347]],\n"," 'Validation MCC': [[0.5997832864528045,\n","                     0.7624998159072349,\n","                     0.791641052119611,\n","                     0.8043022658401482,\n","                     0.8150845006140929,\n","                     0.8187676047868925,\n","                     0.8236005588635676,\n","                     0.8240249772057694,\n","                     0.824120711739972,\n","                     0.8297649048780589,\n","                     0.8338661224232155,\n","                     0.8354075775859222,\n","                     0.8345500016875187,\n","                     0.8384059003462699,\n","                     0.8419890871221144,\n","                     0.8414192080553899,\n","                     0.8320940597964311,\n","                     0.8458111167445383,\n","                     0.8355682326932169,\n","                     0.8516704271435277,\n","                     0.8569683887619308,\n","                     0.8549384888582335,\n","                     0.8538761493944321,\n","                     0.8568912856552203,\n","                     0.8590598764162735,\n","                     0.8512531663643125,\n","                     0.8613193384598212,\n","                     0.8553315950890241,\n","                     0.8633211507503437,\n","                     0.8594350308598113],\n","                    [0.6099882253027992,\n","                     0.7464972007961859,\n","                     0.7697191721922303,\n","                     0.7919851701019204,\n","                     0.7943501142681608,\n","                     0.801718861792301,\n","                     0.8032385859433822,\n","                     0.8051770888331242,\n","                     0.808599682224835,\n","                     0.8029467448906897,\n","                     0.8095383397730604,\n","                     0.7939460635904356,\n","                     0.8071538183535866,\n","                     0.8064688129172379,\n","                     0.8125252228040037,\n","                     0.8159608086202335,\n","                     0.8228920413188756,\n","                     0.8234767684047718,\n","                     0.8130253615184222,\n","                     0.82152062652164,\n","                     0.828109637157366,\n","                     0.8281954368812435,\n","                     0.8255425085642023,\n","                     0.8241044882006414,\n","                     0.8329974454391719,\n","                     0.8329024549944976,\n","                     0.8321613331685025,\n","                     0.8347144779068113,\n","                     0.8334726245621065,\n","                     0.8306995550666223],\n","                    [0.6272778153515804,\n","                     0.7824988478026768,\n","                     0.799290442093826,\n","                     0.8117887184829773,\n","                     0.8173648977154601,\n","                     0.8182216870433869,\n","                     0.8267339266385173,\n","                     0.8301292880150086,\n","                     0.8281078140558428,\n","                     0.8366579202934299,\n","                     0.8364339123969968,\n","                     0.8401480557317055,\n","                     0.8363527593742068,\n","                     0.83886944846163,\n","                     0.8338293212782406,\n","                     0.8440033928983665,\n","                     0.8432078509218256,\n","                     0.8408813399177699,\n","                     0.8489829955406389,\n","                     0.8469291069188448,\n","                     0.8522706953129928,\n","                     0.8471547979904764,\n","                     0.8575444792333065,\n","                     0.8533279359790544,\n","                     0.8596187318254045,\n","                     0.8572666487299031,\n","                     0.8598099765162761,\n","                     0.8568079872831719,\n","                     0.8602074692240104,\n","                     0.8633784532482994],\n","                    [0.6004596447165358,\n","                     0.7774391400104693,\n","                     0.7670864113865212,\n","                     0.8041520781045634,\n","                     0.8105370625396388,\n","                     0.8205732910766038,\n","                     0.8235490005562309,\n","                     0.8276048291948631,\n","                     0.8336609569861667,\n","                     0.8304234007929926,\n","                     0.8379155457895083,\n","                     0.8401384372261405,\n","                     0.8390775774830685,\n","                     0.8404663335476806,\n","                     0.846276877058251,\n","                     0.844047268506228,\n","                     0.8467355637533939,\n","                     0.8444248375374083,\n","                     0.8471319200916926,\n","                     0.8508666192247374,\n","                     0.8429562390003585,\n","                     0.8541545014901675,\n","                     0.8510991920330017,\n","                     0.8406794038089105,\n","                     0.8560417253383863,\n","                     0.8559215352581252,\n","                     0.8576001428575049,\n","                     0.8584084251977795,\n","                     0.8572096789079953,\n","                     0.8580936465433541],\n","                    [0.6007402298939603,\n","                     0.7062983794115193,\n","                     0.7725989562313177,\n","                     0.785574607831917,\n","                     0.8024001567540446,\n","                     0.7992217689636424,\n","                     0.8083857403698455,\n","                     0.8137629554496978,\n","                     0.8166400380611252,\n","                     0.8224663458630415,\n","                     0.8150603282747478,\n","                     0.8225426023473136,\n","                     0.8260337446782103,\n","                     0.8227968467194986,\n","                     0.803405804466995,\n","                     0.8200387890050191,\n","                     0.8324547191630629,\n","                     0.8138870192479142,\n","                     0.8273900481886607,\n","                     0.8342568951079957,\n","                     0.832638825725885,\n","                     0.840208376725751,\n","                     0.8397560912288894,\n","                     0.8381218461998128,\n","                     0.8404088552786065,\n","                     0.8367807116143575,\n","                     0.8437747812604329,\n","                     0.8440960852901169,\n","                     0.8433612370479161,\n","                     0.848039320336863]]}\n","Training Model: BiLSTM, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5661 - loss: 0.6643\n","Epoch 1 - MCC: 0.5792\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - accuracy: 0.5672 - loss: 0.6632 - val_accuracy: 0.7660 - val_loss: 0.5611 - mcc: 0.5792\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7993 - loss: 0.5151\n","Epoch 2 - MCC: 0.7116\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.7998 - loss: 0.5134 - val_accuracy: 0.8564 - val_loss: 0.3435 - mcc: 0.7116\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8667 - loss: 0.3173\n","Epoch 3 - MCC: 0.7804\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8670 - loss: 0.3166 - val_accuracy: 0.8907 - val_loss: 0.2592 - mcc: 0.7804\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8886 - loss: 0.2687\n","Epoch 4 - MCC: 0.7999\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8888 - loss: 0.2683 - val_accuracy: 0.9003 - val_loss: 0.2368 - mcc: 0.7999\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8989 - loss: 0.2428\n","Epoch 5 - MCC: 0.8132\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.8990 - loss: 0.2427 - val_accuracy: 0.9068 - val_loss: 0.2217 - mcc: 0.8132\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9065 - loss: 0.2253\n","Epoch 6 - MCC: 0.8211\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.9065 - loss: 0.2254 - val_accuracy: 0.9109 - val_loss: 0.2107 - mcc: 0.8211\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9077 - loss: 0.2207\n","Epoch 7 - MCC: 0.8247\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9077 - loss: 0.2207 - val_accuracy: 0.9127 - val_loss: 0.2052 - mcc: 0.8247\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9084 - loss: 0.2163\n","Epoch 8 - MCC: 0.8278\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9085 - loss: 0.2162 - val_accuracy: 0.9141 - val_loss: 0.2006 - mcc: 0.8278\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9137 - loss: 0.2059\n","Epoch 9 - MCC: 0.8321\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9137 - loss: 0.2061 - val_accuracy: 0.9163 - val_loss: 0.1973 - mcc: 0.8321\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9142 - loss: 0.2026\n","Epoch 10 - MCC: 0.8373\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9142 - loss: 0.2027 - val_accuracy: 0.9188 - val_loss: 0.1914 - mcc: 0.8373\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9131 - loss: 0.2054\n","Epoch 11 - MCC: 0.8384\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - accuracy: 0.9132 - loss: 0.2053 - val_accuracy: 0.9192 - val_loss: 0.1905 - mcc: 0.8384\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9170 - loss: 0.1967\n","Epoch 12 - MCC: 0.8320\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9169 - loss: 0.1970 - val_accuracy: 0.9161 - val_loss: 0.1951 - mcc: 0.8320\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9147 - loss: 0.2013\n","Epoch 13 - MCC: 0.8418\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9147 - loss: 0.2013 - val_accuracy: 0.9212 - val_loss: 0.1838 - mcc: 0.8418\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9145 - loss: 0.2019\n","Epoch 14 - MCC: 0.8491\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9146 - loss: 0.2016 - val_accuracy: 0.9248 - val_loss: 0.1785 - mcc: 0.8491\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9169 - loss: 0.1965\n","Epoch 15 - MCC: 0.8483\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9170 - loss: 0.1963 - val_accuracy: 0.9243 - val_loss: 0.1769 - mcc: 0.8483\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9238 - loss: 0.1822\n","Epoch 16 - MCC: 0.8532\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - accuracy: 0.9237 - loss: 0.1824 - val_accuracy: 0.9268 - val_loss: 0.1728 - mcc: 0.8532\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9258 - loss: 0.1771\n","Epoch 17 - MCC: 0.8580\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9257 - loss: 0.1774 - val_accuracy: 0.9293 - val_loss: 0.1677 - mcc: 0.8580\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9253 - loss: 0.1785\n","Epoch 18 - MCC: 0.8505\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9252 - loss: 0.1787 - val_accuracy: 0.9255 - val_loss: 0.1754 - mcc: 0.8505\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9262 - loss: 0.1767\n","Epoch 19 - MCC: 0.8629\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9262 - loss: 0.1768 - val_accuracy: 0.9317 - val_loss: 0.1621 - mcc: 0.8629\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9288 - loss: 0.1698\n","Epoch 20 - MCC: 0.8619\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9287 - loss: 0.1701 - val_accuracy: 0.9311 - val_loss: 0.1638 - mcc: 0.8619\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9224 - loss: 0.1838\n","Epoch 21 - MCC: 0.8679\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9226 - loss: 0.1834 - val_accuracy: 0.9341 - val_loss: 0.1565 - mcc: 0.8679\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9275 - loss: 0.1740\n","Epoch 22 - MCC: 0.8638\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9275 - loss: 0.1740 - val_accuracy: 0.9318 - val_loss: 0.1612 - mcc: 0.8638\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9253 - loss: 0.1786\n","Epoch 23 - MCC: 0.8705\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9254 - loss: 0.1784 - val_accuracy: 0.9354 - val_loss: 0.1545 - mcc: 0.8705\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9290 - loss: 0.1698\n","Epoch 24 - MCC: 0.8744\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9290 - loss: 0.1698 - val_accuracy: 0.9374 - val_loss: 0.1517 - mcc: 0.8744\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9311 - loss: 0.1644\n","Epoch 25 - MCC: 0.8726\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9311 - loss: 0.1645 - val_accuracy: 0.9365 - val_loss: 0.1505 - mcc: 0.8726\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9330 - loss: 0.1616\n","Epoch 26 - MCC: 0.8754\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9330 - loss: 0.1617 - val_accuracy: 0.9379 - val_loss: 0.1486 - mcc: 0.8754\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9365 - loss: 0.1531\n","Epoch 27 - MCC: 0.8777\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9364 - loss: 0.1534 - val_accuracy: 0.9391 - val_loss: 0.1462 - mcc: 0.8777\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9346 - loss: 0.1564\n","Epoch 28 - MCC: 0.8798\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9346 - loss: 0.1565 - val_accuracy: 0.9400 - val_loss: 0.1445 - mcc: 0.8798\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9334 - loss: 0.1583\n","Epoch 29 - MCC: 0.8791\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9334 - loss: 0.1583 - val_accuracy: 0.9398 - val_loss: 0.1435 - mcc: 0.8791\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9364 - loss: 0.1507\n","Epoch 30 - MCC: 0.8837\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9363 - loss: 0.1510 - val_accuracy: 0.9420 - val_loss: 0.1433 - mcc: 0.8837\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5212 - loss: 0.6723\n","Epoch 1 - MCC: 0.4398\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - accuracy: 0.5221 - loss: 0.6713 - val_accuracy: 0.6622 - val_loss: 0.5952 - mcc: 0.4398\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7496 - loss: 0.5427\n","Epoch 2 - MCC: 0.6457\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.7510 - loss: 0.5409 - val_accuracy: 0.8233 - val_loss: 0.4132 - mcc: 0.6457\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8567 - loss: 0.3490\n","Epoch 3 - MCC: 0.7099\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8568 - loss: 0.3483 - val_accuracy: 0.8514 - val_loss: 0.3448 - mcc: 0.7099\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8766 - loss: 0.2926\n","Epoch 4 - MCC: 0.7614\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.8769 - loss: 0.2921 - val_accuracy: 0.8810 - val_loss: 0.2806 - mcc: 0.7614\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8957 - loss: 0.2539\n","Epoch 5 - MCC: 0.7798\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.8957 - loss: 0.2538 - val_accuracy: 0.8902 - val_loss: 0.2596 - mcc: 0.7798\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9013 - loss: 0.2387\n","Epoch 6 - MCC: 0.7869\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9014 - loss: 0.2385 - val_accuracy: 0.8937 - val_loss: 0.2499 - mcc: 0.7869\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9065 - loss: 0.2251\n","Epoch 7 - MCC: 0.7994\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9065 - loss: 0.2250 - val_accuracy: 0.8997 - val_loss: 0.2347 - mcc: 0.7994\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9095 - loss: 0.2159\n","Epoch 8 - MCC: 0.8074\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9096 - loss: 0.2157 - val_accuracy: 0.9039 - val_loss: 0.2264 - mcc: 0.8074\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9117 - loss: 0.2093\n","Epoch 9 - MCC: 0.8018\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9118 - loss: 0.2091 - val_accuracy: 0.9010 - val_loss: 0.2307 - mcc: 0.8018\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9120 - loss: 0.2082\n","Epoch 10 - MCC: 0.8156\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - accuracy: 0.9121 - loss: 0.2079 - val_accuracy: 0.9080 - val_loss: 0.2176 - mcc: 0.8156\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9180 - loss: 0.1943\n","Epoch 11 - MCC: 0.8212\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9180 - loss: 0.1943 - val_accuracy: 0.9108 - val_loss: 0.2126 - mcc: 0.8212\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9241 - loss: 0.1811\n","Epoch 12 - MCC: 0.8248\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9240 - loss: 0.1814 - val_accuracy: 0.9126 - val_loss: 0.2078 - mcc: 0.8248\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9198 - loss: 0.1893\n","Epoch 13 - MCC: 0.8292\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9199 - loss: 0.1891 - val_accuracy: 0.9148 - val_loss: 0.2036 - mcc: 0.8292\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9242 - loss: 0.1811\n","Epoch 14 - MCC: 0.8305\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9242 - loss: 0.1810 - val_accuracy: 0.9155 - val_loss: 0.2019 - mcc: 0.8305\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9252 - loss: 0.1786\n","Epoch 15 - MCC: 0.8323\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - accuracy: 0.9252 - loss: 0.1785 - val_accuracy: 0.9162 - val_loss: 0.2011 - mcc: 0.8323\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9256 - loss: 0.1781\n","Epoch 16 - MCC: 0.8374\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9257 - loss: 0.1779 - val_accuracy: 0.9188 - val_loss: 0.1938 - mcc: 0.8374\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9312 - loss: 0.1655\n","Epoch 17 - MCC: 0.8398\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9312 - loss: 0.1656 - val_accuracy: 0.9201 - val_loss: 0.1904 - mcc: 0.8398\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9346 - loss: 0.1582\n","Epoch 18 - MCC: 0.8432\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9345 - loss: 0.1584 - val_accuracy: 0.9218 - val_loss: 0.1881 - mcc: 0.8432\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9328 - loss: 0.1621\n","Epoch 19 - MCC: 0.8473\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9328 - loss: 0.1620 - val_accuracy: 0.9238 - val_loss: 0.1831 - mcc: 0.8473\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9368 - loss: 0.1541\n","Epoch 20 - MCC: 0.8413\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9367 - loss: 0.1543 - val_accuracy: 0.9204 - val_loss: 0.1910 - mcc: 0.8413\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9322 - loss: 0.1615\n","Epoch 21 - MCC: 0.8461\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9322 - loss: 0.1615 - val_accuracy: 0.9231 - val_loss: 0.1849 - mcc: 0.8461\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9351 - loss: 0.1557\n","Epoch 22 - MCC: 0.8419\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9351 - loss: 0.1558 - val_accuracy: 0.9208 - val_loss: 0.1898 - mcc: 0.8419\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9319 - loss: 0.1639\n","Epoch 23 - MCC: 0.8520\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9320 - loss: 0.1636 - val_accuracy: 0.9261 - val_loss: 0.1757 - mcc: 0.8520\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9338 - loss: 0.1594\n","Epoch 24 - MCC: 0.8514\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9339 - loss: 0.1592 - val_accuracy: 0.9259 - val_loss: 0.1773 - mcc: 0.8514\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9378 - loss: 0.1506\n","Epoch 25 - MCC: 0.8481\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9378 - loss: 0.1506 - val_accuracy: 0.9242 - val_loss: 0.1784 - mcc: 0.8481\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9398 - loss: 0.1476\n","Epoch 26 - MCC: 0.8567\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9398 - loss: 0.1477 - val_accuracy: 0.9285 - val_loss: 0.1709 - mcc: 0.8567\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9420 - loss: 0.1401\n","Epoch 27 - MCC: 0.8554\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9419 - loss: 0.1404 - val_accuracy: 0.9278 - val_loss: 0.1722 - mcc: 0.8554\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9422 - loss: 0.1399\n","Epoch 28 - MCC: 0.8554\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9421 - loss: 0.1402 - val_accuracy: 0.9279 - val_loss: 0.1718 - mcc: 0.8554\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9400 - loss: 0.1442\n","Epoch 29 - MCC: 0.8553\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9399 - loss: 0.1443 - val_accuracy: 0.9277 - val_loss: 0.1721 - mcc: 0.8553\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9407 - loss: 0.1417\n","Epoch 30 - MCC: 0.8565\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 134ms/step - accuracy: 0.9406 - loss: 0.1419 - val_accuracy: 0.9283 - val_loss: 0.1713 - mcc: 0.8565\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.4986 - loss: 0.6516\n","Epoch 1 - MCC: 0.5867\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.5010 - loss: 0.6506 - val_accuracy: 0.7659 - val_loss: 0.5526 - mcc: 0.5867\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8035 - loss: 0.4951\n","Epoch 2 - MCC: 0.7025\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.8039 - loss: 0.4933 - val_accuracy: 0.8517 - val_loss: 0.3440 - mcc: 0.7025\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8682 - loss: 0.3138\n","Epoch 3 - MCC: 0.7847\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.8685 - loss: 0.3130 - val_accuracy: 0.8928 - val_loss: 0.2555 - mcc: 0.7847\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8949 - loss: 0.2536\n","Epoch 4 - MCC: 0.8112\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8950 - loss: 0.2535 - val_accuracy: 0.9059 - val_loss: 0.2301 - mcc: 0.8112\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9012 - loss: 0.2346\n","Epoch 5 - MCC: 0.8155\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9013 - loss: 0.2345 - val_accuracy: 0.9081 - val_loss: 0.2194 - mcc: 0.8155\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9069 - loss: 0.2205\n","Epoch 6 - MCC: 0.8233\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9069 - loss: 0.2205 - val_accuracy: 0.9117 - val_loss: 0.2086 - mcc: 0.8233\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9124 - loss: 0.2072\n","Epoch 7 - MCC: 0.8293\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9123 - loss: 0.2073 - val_accuracy: 0.9148 - val_loss: 0.2017 - mcc: 0.8293\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9121 - loss: 0.2073\n","Epoch 8 - MCC: 0.8188\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9122 - loss: 0.2072 - val_accuracy: 0.9088 - val_loss: 0.2114 - mcc: 0.8188\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9093 - loss: 0.2116\n","Epoch 9 - MCC: 0.8290\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9094 - loss: 0.2113 - val_accuracy: 0.9147 - val_loss: 0.2005 - mcc: 0.8290\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9100 - loss: 0.2111\n","Epoch 10 - MCC: 0.8332\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9102 - loss: 0.2107 - val_accuracy: 0.9163 - val_loss: 0.1954 - mcc: 0.8332\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9207 - loss: 0.1889\n","Epoch 11 - MCC: 0.8407\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9206 - loss: 0.1890 - val_accuracy: 0.9205 - val_loss: 0.1872 - mcc: 0.8407\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9209 - loss: 0.1875\n","Epoch 12 - MCC: 0.8420\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9208 - loss: 0.1876 - val_accuracy: 0.9210 - val_loss: 0.1856 - mcc: 0.8420\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9220 - loss: 0.1859\n","Epoch 13 - MCC: 0.8463\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9219 - loss: 0.1859 - val_accuracy: 0.9234 - val_loss: 0.1807 - mcc: 0.8463\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9238 - loss: 0.1804\n","Epoch 14 - MCC: 0.8436\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9238 - loss: 0.1805 - val_accuracy: 0.9221 - val_loss: 0.1809 - mcc: 0.8436\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9268 - loss: 0.1733\n","Epoch 15 - MCC: 0.8496\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9266 - loss: 0.1736 - val_accuracy: 0.9251 - val_loss: 0.1740 - mcc: 0.8496\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9255 - loss: 0.1756\n","Epoch 16 - MCC: 0.8547\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9254 - loss: 0.1757 - val_accuracy: 0.9276 - val_loss: 0.1717 - mcc: 0.8547\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9259 - loss: 0.1786\n","Epoch 17 - MCC: 0.8551\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9259 - loss: 0.1785 - val_accuracy: 0.9277 - val_loss: 0.1681 - mcc: 0.8551\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9308 - loss: 0.1654\n","Epoch 18 - MCC: 0.8591\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9307 - loss: 0.1656 - val_accuracy: 0.9298 - val_loss: 0.1661 - mcc: 0.8591\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9267 - loss: 0.1733\n","Epoch 19 - MCC: 0.8588\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9267 - loss: 0.1732 - val_accuracy: 0.9296 - val_loss: 0.1652 - mcc: 0.8588\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9244 - loss: 0.1765\n","Epoch 20 - MCC: 0.8643\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9246 - loss: 0.1762 - val_accuracy: 0.9324 - val_loss: 0.1615 - mcc: 0.8643\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9308 - loss: 0.1652\n","Epoch 21 - MCC: 0.8577\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - accuracy: 0.9307 - loss: 0.1653 - val_accuracy: 0.9291 - val_loss: 0.1663 - mcc: 0.8577\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9316 - loss: 0.1625\n","Epoch 22 - MCC: 0.8635\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9316 - loss: 0.1627 - val_accuracy: 0.9320 - val_loss: 0.1603 - mcc: 0.8635\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9298 - loss: 0.1660\n","Epoch 23 - MCC: 0.8646\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9299 - loss: 0.1658 - val_accuracy: 0.9326 - val_loss: 0.1585 - mcc: 0.8646\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9350 - loss: 0.1547\n","Epoch 24 - MCC: 0.8646\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9349 - loss: 0.1549 - val_accuracy: 0.9326 - val_loss: 0.1598 - mcc: 0.8646\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9361 - loss: 0.1519\n","Epoch 25 - MCC: 0.8688\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9360 - loss: 0.1522 - val_accuracy: 0.9346 - val_loss: 0.1549 - mcc: 0.8688\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9361 - loss: 0.1537\n","Epoch 26 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9360 - loss: 0.1538 - val_accuracy: 0.9353 - val_loss: 0.1537 - mcc: 0.8701\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9323 - loss: 0.1607\n","Epoch 27 - MCC: 0.8691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9323 - loss: 0.1605 - val_accuracy: 0.9348 - val_loss: 0.1525 - mcc: 0.8691\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9364 - loss: 0.1515\n","Epoch 28 - MCC: 0.8716\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9363 - loss: 0.1516 - val_accuracy: 0.9360 - val_loss: 0.1503 - mcc: 0.8716\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9379 - loss: 0.1496\n","Epoch 29 - MCC: 0.8724\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9378 - loss: 0.1498 - val_accuracy: 0.9365 - val_loss: 0.1505 - mcc: 0.8724\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9368 - loss: 0.1503\n","Epoch 30 - MCC: 0.8734\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9367 - loss: 0.1505 - val_accuracy: 0.9369 - val_loss: 0.1476 - mcc: 0.8734\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.4973 - loss: 0.6995\n","Epoch 1 - MCC: 0.3475\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.4981 - loss: 0.6982 - val_accuracy: 0.5967 - val_loss: 0.6065 - mcc: 0.3475\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6862 - loss: 0.5719\n","Epoch 2 - MCC: 0.6769\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - accuracy: 0.6887 - loss: 0.5702 - val_accuracy: 0.8388 - val_loss: 0.4065 - mcc: 0.6769\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8425 - loss: 0.3830\n","Epoch 3 - MCC: 0.7759\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.8429 - loss: 0.3819 - val_accuracy: 0.8880 - val_loss: 0.2777 - mcc: 0.7759\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8863 - loss: 0.2771\n","Epoch 4 - MCC: 0.8057\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8864 - loss: 0.2767 - val_accuracy: 0.9031 - val_loss: 0.2399 - mcc: 0.8057\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9003 - loss: 0.2401\n","Epoch 5 - MCC: 0.8081\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9003 - loss: 0.2401 - val_accuracy: 0.9041 - val_loss: 0.2305 - mcc: 0.8081\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9031 - loss: 0.2315\n","Epoch 6 - MCC: 0.8276\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.9031 - loss: 0.2314 - val_accuracy: 0.9140 - val_loss: 0.2096 - mcc: 0.8276\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9118 - loss: 0.2122\n","Epoch 7 - MCC: 0.8284\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.9117 - loss: 0.2124 - val_accuracy: 0.9144 - val_loss: 0.2083 - mcc: 0.8284\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9100 - loss: 0.2138\n","Epoch 8 - MCC: 0.8355\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9100 - loss: 0.2139 - val_accuracy: 0.9177 - val_loss: 0.2016 - mcc: 0.8355\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9139 - loss: 0.2080\n","Epoch 9 - MCC: 0.8397\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9139 - loss: 0.2080 - val_accuracy: 0.9200 - val_loss: 0.1959 - mcc: 0.8397\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9123 - loss: 0.2098\n","Epoch 10 - MCC: 0.8424\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9124 - loss: 0.2095 - val_accuracy: 0.9214 - val_loss: 0.1899 - mcc: 0.8424\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9194 - loss: 0.1915\n","Epoch 11 - MCC: 0.8382\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.9192 - loss: 0.1918 - val_accuracy: 0.9189 - val_loss: 0.1956 - mcc: 0.8382\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9160 - loss: 0.1994\n","Epoch 12 - MCC: 0.8489\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.9160 - loss: 0.1994 - val_accuracy: 0.9246 - val_loss: 0.1841 - mcc: 0.8489\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9152 - loss: 0.2003\n","Epoch 13 - MCC: 0.8455\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9153 - loss: 0.2001 - val_accuracy: 0.9227 - val_loss: 0.1849 - mcc: 0.8455\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9149 - loss: 0.1997\n","Epoch 14 - MCC: 0.8500\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9151 - loss: 0.1993 - val_accuracy: 0.9250 - val_loss: 0.1817 - mcc: 0.8500\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9186 - loss: 0.1930\n","Epoch 15 - MCC: 0.8511\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9187 - loss: 0.1929 - val_accuracy: 0.9257 - val_loss: 0.1789 - mcc: 0.8511\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9243 - loss: 0.1787\n","Epoch 16 - MCC: 0.8570\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9242 - loss: 0.1789 - val_accuracy: 0.9287 - val_loss: 0.1758 - mcc: 0.8570\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9238 - loss: 0.1812\n","Epoch 17 - MCC: 0.8537\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9237 - loss: 0.1814 - val_accuracy: 0.9269 - val_loss: 0.1755 - mcc: 0.8537\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9256 - loss: 0.1773\n","Epoch 18 - MCC: 0.8511\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9254 - loss: 0.1776 - val_accuracy: 0.9256 - val_loss: 0.1789 - mcc: 0.8511\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9268 - loss: 0.1744\n","Epoch 19 - MCC: 0.8570\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9267 - loss: 0.1746 - val_accuracy: 0.9286 - val_loss: 0.1705 - mcc: 0.8570\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9232 - loss: 0.1842\n","Epoch 20 - MCC: 0.8620\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9233 - loss: 0.1839 - val_accuracy: 0.9312 - val_loss: 0.1656 - mcc: 0.8620\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9280 - loss: 0.1710\n","Epoch 21 - MCC: 0.8633\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9280 - loss: 0.1711 - val_accuracy: 0.9318 - val_loss: 0.1654 - mcc: 0.8633\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9272 - loss: 0.1742\n","Epoch 22 - MCC: 0.8648\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9271 - loss: 0.1743 - val_accuracy: 0.9324 - val_loss: 0.1657 - mcc: 0.8648\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9254 - loss: 0.1764\n","Epoch 23 - MCC: 0.8619\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9254 - loss: 0.1764 - val_accuracy: 0.9311 - val_loss: 0.1668 - mcc: 0.8619\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9322 - loss: 0.1626\n","Epoch 24 - MCC: 0.8627\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9320 - loss: 0.1630 - val_accuracy: 0.9315 - val_loss: 0.1638 - mcc: 0.8627\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9288 - loss: 0.1693\n","Epoch 25 - MCC: 0.8665\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9289 - loss: 0.1693 - val_accuracy: 0.9334 - val_loss: 0.1598 - mcc: 0.8665\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9316 - loss: 0.1640\n","Epoch 26 - MCC: 0.8580\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9316 - loss: 0.1641 - val_accuracy: 0.9287 - val_loss: 0.1681 - mcc: 0.8580\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9276 - loss: 0.1709\n","Epoch 27 - MCC: 0.8666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9276 - loss: 0.1708 - val_accuracy: 0.9334 - val_loss: 0.1598 - mcc: 0.8666\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9304 - loss: 0.1652\n","Epoch 28 - MCC: 0.8721\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9304 - loss: 0.1652 - val_accuracy: 0.9362 - val_loss: 0.1542 - mcc: 0.8721\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9325 - loss: 0.1622\n","Epoch 29 - MCC: 0.8668\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9325 - loss: 0.1623 - val_accuracy: 0.9335 - val_loss: 0.1591 - mcc: 0.8668\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9305 - loss: 0.1646\n","Epoch 30 - MCC: 0.8670\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9305 - loss: 0.1647 - val_accuracy: 0.9333 - val_loss: 0.1638 - mcc: 0.8670\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5387 - loss: 0.7020\n","Epoch 1 - MCC: 0.3584\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - accuracy: 0.5393 - loss: 0.7008 - val_accuracy: 0.6131 - val_loss: 0.6120 - mcc: 0.3584\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7140 - loss: 0.5780\n","Epoch 2 - MCC: 0.6164\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.7161 - loss: 0.5764 - val_accuracy: 0.8035 - val_loss: 0.4458 - mcc: 0.6164\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8293 - loss: 0.3978\n","Epoch 3 - MCC: 0.7610\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.8303 - loss: 0.3959 - val_accuracy: 0.8806 - val_loss: 0.2908 - mcc: 0.7610\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8863 - loss: 0.2736\n","Epoch 4 - MCC: 0.7894\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.8865 - loss: 0.2731 - val_accuracy: 0.8949 - val_loss: 0.2544 - mcc: 0.7894\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9051 - loss: 0.2338\n","Epoch 5 - MCC: 0.7937\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9050 - loss: 0.2339 - val_accuracy: 0.8969 - val_loss: 0.2429 - mcc: 0.7937\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9055 - loss: 0.2247\n","Epoch 6 - MCC: 0.8036\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9055 - loss: 0.2247 - val_accuracy: 0.9017 - val_loss: 0.2274 - mcc: 0.8036\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9074 - loss: 0.2190\n","Epoch 7 - MCC: 0.8105\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.9074 - loss: 0.2189 - val_accuracy: 0.9053 - val_loss: 0.2228 - mcc: 0.8105\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9064 - loss: 0.2211\n","Epoch 8 - MCC: 0.8120\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9066 - loss: 0.2206 - val_accuracy: 0.9061 - val_loss: 0.2158 - mcc: 0.8120\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9144 - loss: 0.2031\n","Epoch 9 - MCC: 0.8195\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9144 - loss: 0.2032 - val_accuracy: 0.9094 - val_loss: 0.2124 - mcc: 0.8195\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9160 - loss: 0.1994\n","Epoch 10 - MCC: 0.8175\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9160 - loss: 0.1995 - val_accuracy: 0.9089 - val_loss: 0.2094 - mcc: 0.8175\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9205 - loss: 0.1895\n","Epoch 11 - MCC: 0.8275\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9205 - loss: 0.1897 - val_accuracy: 0.9135 - val_loss: 0.2080 - mcc: 0.8275\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9178 - loss: 0.1962\n","Epoch 12 - MCC: 0.8300\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9179 - loss: 0.1960 - val_accuracy: 0.9151 - val_loss: 0.1988 - mcc: 0.8300\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9204 - loss: 0.1901\n","Epoch 13 - MCC: 0.8353\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9204 - loss: 0.1900 - val_accuracy: 0.9178 - val_loss: 0.1941 - mcc: 0.8353\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9198 - loss: 0.1905\n","Epoch 14 - MCC: 0.8367\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9199 - loss: 0.1902 - val_accuracy: 0.9183 - val_loss: 0.1962 - mcc: 0.8367\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9230 - loss: 0.1826\n","Epoch 15 - MCC: 0.8414\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9231 - loss: 0.1824 - val_accuracy: 0.9207 - val_loss: 0.1883 - mcc: 0.8414\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9286 - loss: 0.1696\n","Epoch 16 - MCC: 0.8396\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9285 - loss: 0.1699 - val_accuracy: 0.9199 - val_loss: 0.1881 - mcc: 0.8396\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9242 - loss: 0.1793\n","Epoch 17 - MCC: 0.8426\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9242 - loss: 0.1792 - val_accuracy: 0.9214 - val_loss: 0.1842 - mcc: 0.8426\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9284 - loss: 0.1721\n","Epoch 18 - MCC: 0.8467\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9283 - loss: 0.1722 - val_accuracy: 0.9233 - val_loss: 0.1836 - mcc: 0.8467\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9287 - loss: 0.1695\n","Epoch 19 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9286 - loss: 0.1695 - val_accuracy: 0.9230 - val_loss: 0.1820 - mcc: 0.8458\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9328 - loss: 0.1601\n","Epoch 20 - MCC: 0.8452\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9327 - loss: 0.1605 - val_accuracy: 0.9227 - val_loss: 0.1832 - mcc: 0.8452\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9307 - loss: 0.1653\n","Epoch 21 - MCC: 0.8462\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9307 - loss: 0.1654 - val_accuracy: 0.9226 - val_loss: 0.1875 - mcc: 0.8462\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9323 - loss: 0.1633\n","Epoch 22 - MCC: 0.8506\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9322 - loss: 0.1634 - val_accuracy: 0.9254 - val_loss: 0.1756 - mcc: 0.8506\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9314 - loss: 0.1645\n","Epoch 23 - MCC: 0.8490\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9314 - loss: 0.1646 - val_accuracy: 0.9246 - val_loss: 0.1762 - mcc: 0.8490\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9305 - loss: 0.1650\n","Epoch 24 - MCC: 0.8551\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9306 - loss: 0.1649 - val_accuracy: 0.9274 - val_loss: 0.1742 - mcc: 0.8551\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9355 - loss: 0.1567\n","Epoch 25 - MCC: 0.8534\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9354 - loss: 0.1569 - val_accuracy: 0.9268 - val_loss: 0.1714 - mcc: 0.8534\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9300 - loss: 0.1683\n","Epoch 26 - MCC: 0.8522\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9301 - loss: 0.1680 - val_accuracy: 0.9262 - val_loss: 0.1725 - mcc: 0.8522\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9326 - loss: 0.1620\n","Epoch 27 - MCC: 0.8582\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9326 - loss: 0.1620 - val_accuracy: 0.9291 - val_loss: 0.1707 - mcc: 0.8582\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9331 - loss: 0.1611\n","Epoch 28 - MCC: 0.8540\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.9331 - loss: 0.1611 - val_accuracy: 0.9271 - val_loss: 0.1701 - mcc: 0.8540\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9364 - loss: 0.1530\n","Epoch 29 - MCC: 0.8563\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9364 - loss: 0.1531 - val_accuracy: 0.9280 - val_loss: 0.1735 - mcc: 0.8563\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9364 - loss: 0.1536\n","Epoch 30 - MCC: 0.8624\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9364 - loss: 0.1536 - val_accuracy: 0.9313 - val_loss: 0.1638 - mcc: 0.8624\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9420266666666667,\n","              'mean': 0.9343813333333333,\n","              'min': 0.9283266666666666,\n","              'std': 0.004735864862936879},\n"," 'Inference Time (s/sample)': {'max': 0.003828601837158203,\n","                               'mean': 0.0035223646163940426,\n","                               'min': 0.0029312753677368164,\n","                               'std': 0.0003181768961333224},\n"," 'MCC': {'max': 0.8836668068216007,\n","         'mean': 0.8685800835347355,\n","         'min': 0.8564938061036276,\n","         'std': 0.009357628197338135},\n"," 'Parameters': 9025,\n"," 'Train Time (s)': {'max': 92.18240761756897,\n","                    'mean': 85.65437779426574,\n","                    'min': 80.82061314582825,\n","                    'std': 4.534305570211356},\n"," 'Training Accuracy': [[0.5968366861343384,\n","                        0.8111516833305359,\n","                        0.8742765784263611,\n","                        0.8942532539367676,\n","                        0.9009748697280884,\n","                        0.9051415920257568,\n","                        0.9080950021743774,\n","                        0.9091283679008484,\n","                        0.9119965434074402,\n","                        0.9133016467094421,\n","                        0.9150150418281555,\n","                        0.9134599566459656,\n","                        0.9151149988174438,\n","                        0.9168300628662109,\n","                        0.9190217852592468,\n","                        0.9210583567619324,\n","                        0.9218399524688721,\n","                        0.9230449795722961,\n","                        0.9250682592391968,\n","                        0.9259567856788635,\n","                        0.9272201061248779,\n","                        0.9269084334373474,\n","                        0.9281200170516968,\n","                        0.9288817048072815,\n","                        0.9300199747085571,\n","                        0.9310749769210815,\n","                        0.9321183562278748,\n","                        0.9339466691017151,\n","                        0.9333051443099976,\n","                        0.9337716102600098],\n","                       [0.5441216826438904,\n","                        0.7857715487480164,\n","                        0.8606215715408325,\n","                        0.8829134106636047,\n","                        0.8972167372703552,\n","                        0.9036882519721985,\n","                        0.9079801440238953,\n","                        0.9118850231170654,\n","                        0.9138749837875366,\n","                        0.916003406047821,\n","                        0.9183866381645203,\n","                        0.92066490650177,\n","                        0.9221482872962952,\n","                        0.9245467185974121,\n","                        0.926401674747467,\n","                        0.9278817176818848,\n","                        0.9293034076690674,\n","                        0.9317100048065186,\n","                        0.9324083924293518,\n","                        0.9333184957504272,\n","                        0.933370053768158,\n","                        0.9347482919692993,\n","                        0.9349666833877563,\n","                        0.9360150098800659,\n","                        0.9375800490379333,\n","                        0.9386366009712219,\n","                        0.9391049146652222,\n","                        0.9390699863433838,\n","                        0.9391200542449951,\n","                        0.939818263053894],\n","                       [0.5603116154670715,\n","                        0.8134766221046448,\n","                        0.8759716749191284,\n","                        0.8964816331863403,\n","                        0.9028149247169495,\n","                        0.9072600603103638,\n","                        0.9105300307273865,\n","                        0.913266658782959,\n","                        0.9128166437149048,\n","                        0.9153117537498474,\n","                        0.9181867241859436,\n","                        0.9199668169021606,\n","                        0.9216284155845642,\n","                        0.9221650958061218,\n","                        0.9236584305763245,\n","                        0.9246750473976135,\n","                        0.9255667328834534,\n","                        0.9274218082427979,\n","                        0.9284101128578186,\n","                        0.9285498857498169,\n","                        0.9291349053382874,\n","                        0.9298700094223022,\n","                        0.931149959564209,\n","                        0.9319882988929749,\n","                        0.9321833848953247,\n","                        0.9337717294692993,\n","                        0.9344333410263062,\n","                        0.9341766834259033,\n","                        0.935231626033783,\n","                        0.934773325920105],\n","                       [0.518500030040741,\n","                        0.7520899772644043,\n","                        0.8531050086021423,\n","                        0.8903031945228577,\n","                        0.8996732831001282,\n","                        0.9042248725891113,\n","                        0.9097049832344055,\n","                        0.9094716906547546,\n","                        0.9133148789405823,\n","                        0.9152433276176453,\n","                        0.9161350131034851,\n","                        0.9172767400741577,\n","                        0.9174749255180359,\n","                        0.9201216101646423,\n","                        0.9203832149505615,\n","                        0.9214183688163757,\n","                        0.9213600158691406,\n","                        0.9215050339698792,\n","                        0.9244033098220825,\n","                        0.9260032176971436,\n","                        0.9272216558456421,\n","                        0.9257633686065674,\n","                        0.9249817132949829,\n","                        0.9278599619865417,\n","                        0.9294466376304626,\n","                        0.9299217462539673,\n","                        0.9291200041770935,\n","                        0.9313817024230957,\n","                        0.9313600063323975,\n","                        0.930733323097229],\n","                       [0.553879976272583,\n","                        0.7702633142471313,\n","                        0.8534533977508545,\n","                        0.8925084471702576,\n","                        0.9033216238021851,\n","                        0.9058317542076111,\n","                        0.9085984230041504,\n","                        0.9121633768081665,\n","                        0.913963258266449,\n","                        0.9149882793426514,\n","                        0.9186848998069763,\n","                        0.9195549488067627,\n","                        0.9204883575439453,\n","                        0.9230349659919739,\n","                        0.9246166944503784,\n","                        0.9249967932701111,\n","                        0.9258632659912109,\n","                        0.9268100261688232,\n","                        0.9279983639717102,\n","                        0.9286901354789734,\n","                        0.9301099181175232,\n","                        0.9303634166717529,\n","                        0.9309383630752563,\n","                        0.9319398403167725,\n","                        0.9328733682632446,\n","                        0.933753252029419,\n","                        0.9334298968315125,\n","                        0.9336381554603577,\n","                        0.9353348612785339,\n","                        0.936690092086792]],\n"," 'Training Loss': [[0.6344727873802185,\n","                    0.4694603979587555,\n","                    0.29878824949264526,\n","                    0.25742462277412415,\n","                    0.2397719919681549,\n","                    0.22783908247947693,\n","                    0.22044230997562408,\n","                    0.21585030853748322,\n","                    0.2102014124393463,\n","                    0.20608879625797272,\n","                    0.2022290825843811,\n","                    0.20430707931518555,\n","                    0.20138786733150482,\n","                    0.1961394101381302,\n","                    0.19203698635101318,\n","                    0.187538743019104,\n","                    0.1855958104133606,\n","                    0.1834767907857895,\n","                    0.17854908108711243,\n","                    0.176788330078125,\n","                    0.17429548501968384,\n","                    0.17478139698505402,\n","                    0.17177759110927582,\n","                    0.16917158663272858,\n","                    0.16679304838180542,\n","                    0.1644945591688156,\n","                    0.16232794523239136,\n","                    0.15828284621238708,\n","                    0.15893982350826263,\n","                    0.15736226737499237],\n","                   [0.6470527648925781,\n","                    0.49651339650154114,\n","                    0.33244776725769043,\n","                    0.27952706813812256,\n","                    0.25009119510650635,\n","                    0.23368766903877258,\n","                    0.22030267119407654,\n","                    0.20963014662265778,\n","                    0.20464564859867096,\n","                    0.19989292323589325,\n","                    0.19343507289886475,\n","                    0.18988199532032013,\n","                    0.1848447024822235,\n","                    0.17943227291107178,\n","                    0.17633862793445587,\n","                    0.1725471466779709,\n","                    0.16974249482154846,\n","                    0.16466324031352997,\n","                    0.16160279512405396,\n","                    0.16035686433315277,\n","                    0.1606277972459793,\n","                    0.15852078795433044,\n","                    0.15733879804611206,\n","                    0.15401794016361237,\n","                    0.15074823796749115,\n","                    0.1484435349702835,\n","                    0.1469709724187851,\n","                    0.14712519943714142,\n","                    0.14638544619083405,\n","                    0.14515118300914764],\n","                   [0.6259171962738037,\n","                    0.4465480446815491,\n","                    0.2941286265850067,\n","                    0.24992139637470245,\n","                    0.2308773398399353,\n","                    0.2190607488155365,\n","                    0.20993752777576447,\n","                    0.20517010986804962,\n","                    0.20496253669261932,\n","                    0.19978541135787964,\n","                    0.19287995994091034,\n","                    0.18966735899448395,\n","                    0.1859007626771927,\n","                    0.1843133568763733,\n","                    0.1804855912923813,\n","                    0.17767754197120667,\n","                    0.17680421471595764,\n","                    0.17201320827007294,\n","                    0.16990777850151062,\n","                    0.16856813430786133,\n","                    0.16865459084510803,\n","                    0.16657499969005585,\n","                    0.1629968285560608,\n","                    0.16100871562957764,\n","                    0.15976472198963165,\n","                    0.15781570971012115,\n","                    0.15584087371826172,\n","                    0.15561062097549438,\n","                    0.1547093689441681,\n","                    0.1543811857700348],\n","                   [0.6674973368644714,\n","                    0.5296498537063599,\n","                    0.3547922968864441,\n","                    0.26561006903648376,\n","                    0.24017910659313202,\n","                    0.22831065952777863,\n","                    0.2162591814994812,\n","                    0.21541990339756012,\n","                    0.2071515917778015,\n","                    0.20249484479427338,\n","                    0.1980857253074646,\n","                    0.19698329269886017,\n","                    0.19593988358974457,\n","                    0.18899105489253998,\n","                    0.18998666107654572,\n","                    0.18577806651592255,\n","                    0.1864270269870758,\n","                    0.18578565120697021,\n","                    0.17941147089004517,\n","                    0.17610953748226166,\n","                    0.17352497577667236,\n","                    0.17649990320205688,\n","                    0.17703361809253693,\n","                    0.17145706713199615,\n","                    0.1684199869632721,\n","                    0.16756175458431244,\n","                    0.1690826416015625,\n","                    0.16442519426345825,\n","                    0.1642647683620453,\n","                    0.16481433808803558],\n","                   [0.6713734269142151,\n","                    0.5357858538627625,\n","                    0.34651756286621094,\n","                    0.2613525688648224,\n","                    0.23604051768779755,\n","                    0.22499045729637146,\n","                    0.21702806651592255,\n","                    0.20924003422260284,\n","                    0.2046435922384262,\n","                    0.20067569613456726,\n","                    0.1935548037290573,\n","                    0.191081240773201,\n","                    0.1884869784116745,\n","                    0.18270306289196014,\n","                    0.17895247042179108,\n","                    0.17828328907489777,\n","                    0.17579567432403564,\n","                    0.1749851256608963,\n","                    0.17103208601474762,\n","                    0.17009133100509644,\n","                    0.16733448207378387,\n","                    0.1671331226825714,\n","                    0.16614966094493866,\n","                    0.1625465601682663,\n","                    0.16150759160518646,\n","                    0.1602933555841446,\n","                    0.16032074391841888,\n","                    0.1597040593624115,\n","                    0.15554490685462952,\n","                    0.15260936319828033]],\n"," 'Validation Accuracy': [[0.7660067081451416,\n","                          0.8564267158508301,\n","                          0.8906600475311279,\n","                          0.9002532958984375,\n","                          0.9068400859832764,\n","                          0.9109066724777222,\n","                          0.9126600027084351,\n","                          0.9141200184822083,\n","                          0.9163134098052979,\n","                          0.9188265800476074,\n","                          0.9191532731056213,\n","                          0.916100025177002,\n","                          0.9212000370025635,\n","                          0.9248467087745667,\n","                          0.9242600202560425,\n","                          0.9268200397491455,\n","                          0.929253339767456,\n","                          0.9255265593528748,\n","                          0.9317134022712708,\n","                          0.931119978427887,\n","                          0.9341066479682922,\n","                          0.9317666888237,\n","                          0.9354400634765625,\n","                          0.9374133348464966,\n","                          0.936500072479248,\n","                          0.9379466772079468,\n","                          0.9390866756439209,\n","                          0.9400266408920288,\n","                          0.9398000240325928,\n","                          0.9420266151428223],\n","                         [0.6621600389480591,\n","                          0.8233133554458618,\n","                          0.851419985294342,\n","                          0.8809866905212402,\n","                          0.890186607837677,\n","                          0.8936933279037476,\n","                          0.899679958820343,\n","                          0.9039133191108704,\n","                          0.9009732604026794,\n","                          0.9080066680908203,\n","                          0.9107533097267151,\n","                          0.9126133918762207,\n","                          0.9148133993148804,\n","                          0.9154600501060486,\n","                          0.9161600470542908,\n","                          0.9188133478164673,\n","                          0.920053243637085,\n","                          0.921779990196228,\n","                          0.923799991607666,\n","                          0.9204333424568176,\n","                          0.9230533838272095,\n","                          0.9207866787910461,\n","                          0.9261466860771179,\n","                          0.9258733987808228,\n","                          0.9242199659347534,\n","                          0.9285333156585693,\n","                          0.9278333783149719,\n","                          0.9278599619865417,\n","                          0.9276933073997498,\n","                          0.9283266663551331],\n","                         [0.765906572341919,\n","                          0.8517267107963562,\n","                          0.8928066492080688,\n","                          0.9059200286865234,\n","                          0.9080666899681091,\n","                          0.9117333292961121,\n","                          0.914813220500946,\n","                          0.9087600111961365,\n","                          0.9147399663925171,\n","                          0.9163267016410828,\n","                          0.9205333590507507,\n","                          0.9210466146469116,\n","                          0.9233599901199341,\n","                          0.9220867156982422,\n","                          0.9251400232315063,\n","                          0.9276466369628906,\n","                          0.9276533722877502,\n","                          0.9298133850097656,\n","                          0.9295799732208252,\n","                          0.9324400424957275,\n","                          0.9291200041770935,\n","                          0.9320266842842102,\n","                          0.9325932860374451,\n","                          0.9325733184814453,\n","                          0.9345866441726685,\n","                          0.93531334400177,\n","                          0.9348133206367493,\n","                          0.9360200762748718,\n","                          0.9364800453186035,\n","                          0.9369266629219055],\n","                         [0.596673309803009,\n","                          0.8387733697891235,\n","                          0.8880332708358765,\n","                          0.9030532836914062,\n","                          0.9041266441345215,\n","                          0.9139599800109863,\n","                          0.9144200086593628,\n","                          0.9177133440971375,\n","                          0.9199867248535156,\n","                          0.9214066863059998,\n","                          0.9188534021377563,\n","                          0.9246134161949158,\n","                          0.9227465987205505,\n","                          0.9249799847602844,\n","                          0.9257266521453857,\n","                          0.9286733269691467,\n","                          0.9268733263015747,\n","                          0.9256265759468079,\n","                          0.9286333322525024,\n","                          0.9311800599098206,\n","                          0.9318133592605591,\n","                          0.9324199557304382,\n","                          0.9310933947563171,\n","                          0.9314932823181152,\n","                          0.9333932995796204,\n","                          0.9287199974060059,\n","                          0.9333798885345459,\n","                          0.9362000226974487,\n","                          0.9334732890129089,\n","                          0.9333133101463318],\n","                         [0.6131266951560974,\n","                          0.803486704826355,\n","                          0.8805732727050781,\n","                          0.8948800563812256,\n","                          0.8969200253486633,\n","                          0.9017333388328552,\n","                          0.9053066968917847,\n","                          0.9061333537101746,\n","                          0.909426748752594,\n","                          0.9089334011077881,\n","                          0.9135133624076843,\n","                          0.9151399731636047,\n","                          0.9177733659744263,\n","                          0.9183067083358765,\n","                          0.9206600189208984,\n","                          0.9198866486549377,\n","                          0.9214399456977844,\n","                          0.9232999682426453,\n","                          0.9230200052261353,\n","                          0.9227266907691956,\n","                          0.9226133227348328,\n","                          0.9254199862480164,\n","                          0.9246265888214111,\n","                          0.9274266958236694,\n","                          0.9268199801445007,\n","                          0.9261733293533325,\n","                          0.9291200041770935,\n","                          0.9271466135978699,\n","                          0.9280000329017639,\n","                          0.9313133358955383]],\n"," 'Validation Loss': [[0.5611274242401123,\n","                      0.3435269892215729,\n","                      0.25920456647872925,\n","                      0.23679569363594055,\n","                      0.22166641056537628,\n","                      0.21072736382484436,\n","                      0.20519132912158966,\n","                      0.2006048560142517,\n","                      0.19725048542022705,\n","                      0.19143322110176086,\n","                      0.1905011534690857,\n","                      0.19514784216880798,\n","                      0.18376216292381287,\n","                      0.17846930027008057,\n","                      0.17689737677574158,\n","                      0.1727723330259323,\n","                      0.1677003800868988,\n","                      0.17536625266075134,\n","                      0.1620938628911972,\n","                      0.16380566358566284,\n","                      0.1565188765525818,\n","                      0.16122189164161682,\n","                      0.1544615477323532,\n","                      0.15167121589183807,\n","                      0.15048235654830933,\n","                      0.14855745434761047,\n","                      0.14619947969913483,\n","                      0.14451274275779724,\n","                      0.14347906410694122,\n","                      0.14327268302440643],\n","                     [0.5952406525611877,\n","                      0.41321250796318054,\n","                      0.3447975516319275,\n","                      0.28062519431114197,\n","                      0.25959256291389465,\n","                      0.2499309480190277,\n","                      0.2346893697977066,\n","                      0.22637176513671875,\n","                      0.23071622848510742,\n","                      0.21764832735061646,\n","                      0.21258306503295898,\n","                      0.2078360766172409,\n","                      0.20356057584285736,\n","                      0.20185840129852295,\n","                      0.20105570554733276,\n","                      0.19376347959041595,\n","                      0.19037248194217682,\n","                      0.18809999525547028,\n","                      0.18309111893177032,\n","                      0.19103562831878662,\n","                      0.18494632840156555,\n","                      0.1897839605808258,\n","                      0.1756771206855774,\n","                      0.17732422053813934,\n","                      0.1784086674451828,\n","                      0.17088079452514648,\n","                      0.1721743792295456,\n","                      0.1718258261680603,\n","                      0.17214956879615784,\n","                      0.17126430571079254],\n","                     [0.5525840520858765,\n","                      0.34395936131477356,\n","                      0.25554823875427246,\n","                      0.2300880402326584,\n","                      0.2194029837846756,\n","                      0.20864811539649963,\n","                      0.20174621045589447,\n","                      0.21142590045928955,\n","                      0.20048823952674866,\n","                      0.19540637731552124,\n","                      0.18721504509449005,\n","                      0.185635507106781,\n","                      0.1806940883398056,\n","                      0.1808975785970688,\n","                      0.17396461963653564,\n","                      0.17171508073806763,\n","                      0.16812977194786072,\n","                      0.16614049673080444,\n","                      0.16523237526416779,\n","                      0.16149546205997467,\n","                      0.16634224355220795,\n","                      0.1603311002254486,\n","                      0.15849819779396057,\n","                      0.15976354479789734,\n","                      0.1548881232738495,\n","                      0.1537407785654068,\n","                      0.15246446430683136,\n","                      0.15030504763126373,\n","                      0.15053819119930267,\n","                      0.14758498966693878],\n","                     [0.6065334677696228,\n","                      0.4065316319465637,\n","                      0.2777426838874817,\n","                      0.23988334834575653,\n","                      0.23048830032348633,\n","                      0.20962491631507874,\n","                      0.20832103490829468,\n","                      0.20164386928081512,\n","                      0.19590499997138977,\n","                      0.1898861825466156,\n","                      0.19560737907886505,\n","                      0.18413759768009186,\n","                      0.18487310409545898,\n","                      0.1817082166671753,\n","                      0.178868368268013,\n","                      0.17576999962329865,\n","                      0.17546042799949646,\n","                      0.17894719541072845,\n","                      0.17047740519046783,\n","                      0.16557736694812775,\n","                      0.16541482508182526,\n","                      0.16568905115127563,\n","                      0.16681937873363495,\n","                      0.16376067698001862,\n","                      0.1597595363855362,\n","                      0.16808681190013885,\n","                      0.15982592105865479,\n","                      0.15423007309436798,\n","                      0.1591499298810959,\n","                      0.1637764871120453],\n","                     [0.61203932762146,\n","                      0.44580188393592834,\n","                      0.2908162474632263,\n","                      0.25439831614494324,\n","                      0.24289493262767792,\n","                      0.22739894688129425,\n","                      0.2228374034166336,\n","                      0.21582446992397308,\n","                      0.21235160529613495,\n","                      0.2093556821346283,\n","                      0.2079729437828064,\n","                      0.19884373247623444,\n","                      0.19405120611190796,\n","                      0.19622619450092316,\n","                      0.18833793699741364,\n","                      0.18813548982143402,\n","                      0.18420110642910004,\n","                      0.18362660706043243,\n","                      0.1820426881313324,\n","                      0.18316234648227692,\n","                      0.1875009685754776,\n","                      0.17555813491344452,\n","                      0.176238551735878,\n","                      0.17417970299720764,\n","                      0.17142389714717865,\n","                      0.17252406477928162,\n","                      0.17069481313228607,\n","                      0.17012140154838562,\n","                      0.1734904646873474,\n","                      0.16380718350410461]],\n"," 'Validation MCC': [[0.5791972499627823,\n","                     0.7116433761472222,\n","                     0.7804176507501438,\n","                     0.79987396485676,\n","                     0.8131950318358949,\n","                     0.8211427365977423,\n","                     0.8246643820753354,\n","                     0.8278277964452336,\n","                     0.8321075611717169,\n","                     0.8373386965133303,\n","                     0.8384018984564652,\n","                     0.8319518897068728,\n","                     0.8418117305112027,\n","                     0.8491114028278216,\n","                     0.8483141855708681,\n","                     0.8532257494152083,\n","                     0.8579657595553848,\n","                     0.850523265729306,\n","                     0.862898326041017,\n","                     0.8619242444578924,\n","                     0.8678713989928242,\n","                     0.8637678090631161,\n","                     0.8704548031328282,\n","                     0.8743587341905587,\n","                     0.8726011298531708,\n","                     0.8754372973650356,\n","                     0.8777324954849725,\n","                     0.8797643047745131,\n","                     0.8791332345914245,\n","                     0.8836668068216007],\n","                    [0.4397873744981164,\n","                     0.6456801098396004,\n","                     0.7099370061759156,\n","                     0.761376733619856,\n","                     0.7798306729254486,\n","                     0.7868959565818119,\n","                     0.7993868896798261,\n","                     0.8073534805134732,\n","                     0.8018215569560885,\n","                     0.815622384132862,\n","                     0.8212007846803312,\n","                     0.8247989044377968,\n","                     0.829220949775963,\n","                     0.8305353882965628,\n","                     0.8323096340888487,\n","                     0.8373763161366727,\n","                     0.839777811005536,\n","                     0.8432483382649376,\n","                     0.8472602885224323,\n","                     0.8413355122260827,\n","                     0.8461124469895629,\n","                     0.8418565209549335,\n","                     0.8519554230369233,\n","                     0.8513900431412974,\n","                     0.8480934585064933,\n","                     0.8567453682635318,\n","                     0.8554444342420486,\n","                     0.8553932857118277,\n","                     0.8553415865637124,\n","                     0.8564938061036276],\n","                    [0.5866578331903985,\n","                     0.7025053854608491,\n","                     0.7846918011095877,\n","                     0.811189317200795,\n","                     0.815498286374201,\n","                     0.823250590188029,\n","                     0.8293392558151956,\n","                     0.8187789168598142,\n","                     0.8290203819186204,\n","                     0.833169664884326,\n","                     0.8406743020621976,\n","                     0.8419836456143875,\n","                     0.8462647918247781,\n","                     0.8436387356949288,\n","                     0.8496474305314038,\n","                     0.8547219172313605,\n","                     0.8550577012797529,\n","                     0.8590907379409543,\n","                     0.8587610456442387,\n","                     0.864312829001357,\n","                     0.8576754503333164,\n","                     0.8635291258200875,\n","                     0.8646250768699528,\n","                     0.8646099455856747,\n","                     0.868796094222622,\n","                     0.8700867666965776,\n","                     0.8690809622371269,\n","                     0.8715810318874436,\n","                     0.8724419747871431,\n","                     0.8733856874045982],\n","                    [0.34745918412363697,\n","                     0.6768860704197349,\n","                     0.7758877021205376,\n","                     0.8056750425997535,\n","                     0.8081318914233102,\n","                     0.8275767514081287,\n","                     0.8284498939763094,\n","                     0.8354868249612837,\n","                     0.8397293733376071,\n","                     0.8424221746767409,\n","                     0.8381744988497294,\n","                     0.8489031078411545,\n","                     0.8454543549706259,\n","                     0.8500303000933503,\n","                     0.8510965315688532,\n","                     0.8570095512476817,\n","                     0.8536832424830648,\n","                     0.8510835588388563,\n","                     0.8569573311983234,\n","                     0.8620352289650184,\n","                     0.8632954068867795,\n","                     0.86484166087103,\n","                     0.8618864010096022,\n","                     0.8626572493878782,\n","                     0.8664592800134575,\n","                     0.8580205968699193,\n","                     0.8665737447933639,\n","                     0.8721131757700821,\n","                     0.8668088921945403,\n","                     0.86697680550674],\n","                    [0.3584186890376671,\n","                     0.6164463382850459,\n","                     0.7610022463791115,\n","                     0.7893605100742392,\n","                     0.7936916230322419,\n","                     0.8035734725783625,\n","                     0.8104894883964253,\n","                     0.8120082045638164,\n","                     0.819458576190824,\n","                     0.8175449797147418,\n","                     0.8274794066556134,\n","                     0.8300443881498162,\n","                     0.835254384354084,\n","                     0.8367447353453145,\n","                     0.8414021867198311,\n","                     0.8395505503841028,\n","                     0.8426125455390112,\n","                     0.8467012716543493,\n","                     0.8457839064385366,\n","                     0.8452234233745853,\n","                     0.8462123251923608,\n","                     0.8505618386492417,\n","                     0.8489940200181375,\n","                     0.8551390625751807,\n","                     0.853439404244823,\n","                     0.8521560456115891,\n","                     0.8582069079209125,\n","                     0.8540357633180803,\n","                     0.856304184148578,\n","                     0.8623773118371114]]}\n","Training Model: BiLSTM_Dense, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5721 - loss: 0.6585\n","Epoch 1 - MCC: 0.5975\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 0.5739 - loss: 0.6574 - val_accuracy: 0.7803 - val_loss: 0.5546 - mcc: 0.5975\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8034 - loss: 0.5098\n","Epoch 2 - MCC: 0.7226\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8041 - loss: 0.5079 - val_accuracy: 0.8615 - val_loss: 0.3451 - mcc: 0.7226\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8628 - loss: 0.3253\n","Epoch 3 - MCC: 0.7827\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.8632 - loss: 0.3244 - val_accuracy: 0.8915 - val_loss: 0.2580 - mcc: 0.7827\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8851 - loss: 0.2701\n","Epoch 4 - MCC: 0.8036\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.8853 - loss: 0.2697 - val_accuracy: 0.9022 - val_loss: 0.2318 - mcc: 0.8036\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8996 - loss: 0.2386\n","Epoch 5 - MCC: 0.8118\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8996 - loss: 0.2385 - val_accuracy: 0.9062 - val_loss: 0.2211 - mcc: 0.8118\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9040 - loss: 0.2276\n","Epoch 6 - MCC: 0.8236\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9041 - loss: 0.2274 - val_accuracy: 0.9115 - val_loss: 0.2088 - mcc: 0.8236\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9088 - loss: 0.2165\n","Epoch 7 - MCC: 0.8369\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9089 - loss: 0.2163 - val_accuracy: 0.9187 - val_loss: 0.1921 - mcc: 0.8369\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9135 - loss: 0.2079\n","Epoch 8 - MCC: 0.8390\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9135 - loss: 0.2078 - val_accuracy: 0.9197 - val_loss: 0.1884 - mcc: 0.8390\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9123 - loss: 0.2071\n","Epoch 9 - MCC: 0.8477\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9124 - loss: 0.2068 - val_accuracy: 0.9241 - val_loss: 0.1814 - mcc: 0.8477\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9170 - loss: 0.1950\n","Epoch 10 - MCC: 0.8461\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9170 - loss: 0.1950 - val_accuracy: 0.9228 - val_loss: 0.1832 - mcc: 0.8461\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9216 - loss: 0.1872\n","Epoch 11 - MCC: 0.8580\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9216 - loss: 0.1873 - val_accuracy: 0.9293 - val_loss: 0.1693 - mcc: 0.8580\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9235 - loss: 0.1831\n","Epoch 12 - MCC: 0.8597\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9235 - loss: 0.1831 - val_accuracy: 0.9297 - val_loss: 0.1687 - mcc: 0.8597\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9258 - loss: 0.1774\n","Epoch 13 - MCC: 0.8645\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - accuracy: 0.9257 - loss: 0.1775 - val_accuracy: 0.9324 - val_loss: 0.1628 - mcc: 0.8645\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9265 - loss: 0.1775\n","Epoch 14 - MCC: 0.8640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9265 - loss: 0.1774 - val_accuracy: 0.9318 - val_loss: 0.1617 - mcc: 0.8640\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9270 - loss: 0.1744\n","Epoch 15 - MCC: 0.8684\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9270 - loss: 0.1743 - val_accuracy: 0.9344 - val_loss: 0.1567 - mcc: 0.8684\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9311 - loss: 0.1645\n","Epoch 16 - MCC: 0.8698\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9311 - loss: 0.1646 - val_accuracy: 0.9348 - val_loss: 0.1560 - mcc: 0.8698\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9321 - loss: 0.1637\n","Epoch 17 - MCC: 0.8733\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - accuracy: 0.9320 - loss: 0.1638 - val_accuracy: 0.9369 - val_loss: 0.1515 - mcc: 0.8733\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9310 - loss: 0.1647\n","Epoch 18 - MCC: 0.8778\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9311 - loss: 0.1646 - val_accuracy: 0.9391 - val_loss: 0.1473 - mcc: 0.8778\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9334 - loss: 0.1599\n","Epoch 19 - MCC: 0.8772\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9333 - loss: 0.1601 - val_accuracy: 0.9384 - val_loss: 0.1508 - mcc: 0.8772\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9308 - loss: 0.1657\n","Epoch 20 - MCC: 0.8761\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9309 - loss: 0.1655 - val_accuracy: 0.9380 - val_loss: 0.1508 - mcc: 0.8761\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9359 - loss: 0.1535\n","Epoch 21 - MCC: 0.8841\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9358 - loss: 0.1536 - val_accuracy: 0.9423 - val_loss: 0.1405 - mcc: 0.8841\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9375 - loss: 0.1497\n","Epoch 22 - MCC: 0.8801\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9375 - loss: 0.1498 - val_accuracy: 0.9400 - val_loss: 0.1439 - mcc: 0.8801\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9420 - loss: 0.1410\n","Epoch 23 - MCC: 0.8885\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9418 - loss: 0.1413 - val_accuracy: 0.9444 - val_loss: 0.1366 - mcc: 0.8885\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9398 - loss: 0.1455\n","Epoch 24 - MCC: 0.8899\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9398 - loss: 0.1457 - val_accuracy: 0.9450 - val_loss: 0.1337 - mcc: 0.8899\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9402 - loss: 0.1436\n","Epoch 25 - MCC: 0.8898\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.9401 - loss: 0.1439 - val_accuracy: 0.9450 - val_loss: 0.1333 - mcc: 0.8898\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9370 - loss: 0.1491\n","Epoch 26 - MCC: 0.8941\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9370 - loss: 0.1491 - val_accuracy: 0.9472 - val_loss: 0.1300 - mcc: 0.8941\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9402 - loss: 0.1440\n","Epoch 27 - MCC: 0.8941\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9402 - loss: 0.1440 - val_accuracy: 0.9471 - val_loss: 0.1303 - mcc: 0.8941\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9369 - loss: 0.1541\n","Epoch 28 - MCC: 0.8830\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9369 - loss: 0.1539 - val_accuracy: 0.9414 - val_loss: 0.1452 - mcc: 0.8830\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9370 - loss: 0.1528\n","Epoch 29 - MCC: 0.8915\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9371 - loss: 0.1527 - val_accuracy: 0.9460 - val_loss: 0.1313 - mcc: 0.8915\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9388 - loss: 0.1476\n","Epoch 30 - MCC: 0.8965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - accuracy: 0.9389 - loss: 0.1474 - val_accuracy: 0.9484 - val_loss: 0.1263 - mcc: 0.8965\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5095 - loss: 0.6508\n","Epoch 1 - MCC: 0.4952\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 154ms/step - accuracy: 0.5115 - loss: 0.6498 - val_accuracy: 0.7170 - val_loss: 0.5609 - mcc: 0.4952\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7830 - loss: 0.5182\n","Epoch 2 - MCC: 0.6829\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.7844 - loss: 0.5166 - val_accuracy: 0.8418 - val_loss: 0.3890 - mcc: 0.6829\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8617 - loss: 0.3368\n","Epoch 3 - MCC: 0.7378\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.8620 - loss: 0.3359 - val_accuracy: 0.8689 - val_loss: 0.3023 - mcc: 0.7378\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8831 - loss: 0.2742\n","Epoch 4 - MCC: 0.7800\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8834 - loss: 0.2736 - val_accuracy: 0.8889 - val_loss: 0.2616 - mcc: 0.7800\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9046 - loss: 0.2267\n","Epoch 5 - MCC: 0.8049\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9048 - loss: 0.2263 - val_accuracy: 0.9026 - val_loss: 0.2306 - mcc: 0.8049\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9154 - loss: 0.2028\n","Epoch 6 - MCC: 0.8106\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.9154 - loss: 0.2027 - val_accuracy: 0.9052 - val_loss: 0.2227 - mcc: 0.8106\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9158 - loss: 0.2016\n","Epoch 7 - MCC: 0.8205\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 0.9159 - loss: 0.2014 - val_accuracy: 0.9100 - val_loss: 0.2140 - mcc: 0.8205\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9193 - loss: 0.1931\n","Epoch 8 - MCC: 0.8264\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9194 - loss: 0.1929 - val_accuracy: 0.9134 - val_loss: 0.2048 - mcc: 0.8264\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9261 - loss: 0.1763\n","Epoch 9 - MCC: 0.8309\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9261 - loss: 0.1763 - val_accuracy: 0.9157 - val_loss: 0.1989 - mcc: 0.8309\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9299 - loss: 0.1696\n","Epoch 10 - MCC: 0.8328\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9298 - loss: 0.1698 - val_accuracy: 0.9165 - val_loss: 0.1977 - mcc: 0.8328\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9303 - loss: 0.1667\n","Epoch 11 - MCC: 0.8330\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9303 - loss: 0.1668 - val_accuracy: 0.9165 - val_loss: 0.1958 - mcc: 0.8330\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9281 - loss: 0.1731\n","Epoch 12 - MCC: 0.8376\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9281 - loss: 0.1731 - val_accuracy: 0.9184 - val_loss: 0.1952 - mcc: 0.8376\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9302 - loss: 0.1684\n","Epoch 13 - MCC: 0.8420\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9302 - loss: 0.1683 - val_accuracy: 0.9209 - val_loss: 0.1872 - mcc: 0.8420\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9322 - loss: 0.1627\n","Epoch 14 - MCC: 0.8453\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9323 - loss: 0.1625 - val_accuracy: 0.9228 - val_loss: 0.1834 - mcc: 0.8453\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9339 - loss: 0.1595\n","Epoch 15 - MCC: 0.8484\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9339 - loss: 0.1595 - val_accuracy: 0.9244 - val_loss: 0.1799 - mcc: 0.8484\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9387 - loss: 0.1501\n","Epoch 16 - MCC: 0.8508\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9386 - loss: 0.1503 - val_accuracy: 0.9256 - val_loss: 0.1781 - mcc: 0.8508\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9383 - loss: 0.1491\n","Epoch 17 - MCC: 0.8519\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9383 - loss: 0.1492 - val_accuracy: 0.9257 - val_loss: 0.1787 - mcc: 0.8519\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9381 - loss: 0.1492\n","Epoch 18 - MCC: 0.8584\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9380 - loss: 0.1493 - val_accuracy: 0.9294 - val_loss: 0.1712 - mcc: 0.8584\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9373 - loss: 0.1513\n","Epoch 19 - MCC: 0.8550\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9374 - loss: 0.1512 - val_accuracy: 0.9276 - val_loss: 0.1730 - mcc: 0.8550\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9398 - loss: 0.1452\n","Epoch 20 - MCC: 0.8572\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9397 - loss: 0.1453 - val_accuracy: 0.9284 - val_loss: 0.1737 - mcc: 0.8572\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9408 - loss: 0.1448\n","Epoch 21 - MCC: 0.8586\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9407 - loss: 0.1449 - val_accuracy: 0.9294 - val_loss: 0.1714 - mcc: 0.8586\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9373 - loss: 0.1520\n","Epoch 22 - MCC: 0.8616\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9375 - loss: 0.1517 - val_accuracy: 0.9309 - val_loss: 0.1676 - mcc: 0.8616\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9394 - loss: 0.1468\n","Epoch 23 - MCC: 0.8533\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9395 - loss: 0.1467 - val_accuracy: 0.9262 - val_loss: 0.1782 - mcc: 0.8533\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9459 - loss: 0.1325\n","Epoch 24 - MCC: 0.8578\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9457 - loss: 0.1329 - val_accuracy: 0.9287 - val_loss: 0.1701 - mcc: 0.8578\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9416 - loss: 0.1415\n","Epoch 25 - MCC: 0.8560\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - accuracy: 0.9416 - loss: 0.1415 - val_accuracy: 0.9282 - val_loss: 0.1694 - mcc: 0.8560\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9427 - loss: 0.1392\n","Epoch 26 - MCC: 0.8600\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9426 - loss: 0.1393 - val_accuracy: 0.9298 - val_loss: 0.1697 - mcc: 0.8600\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9364 - loss: 0.1525\n","Epoch 27 - MCC: 0.8635\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9366 - loss: 0.1521 - val_accuracy: 0.9319 - val_loss: 0.1635 - mcc: 0.8635\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9406 - loss: 0.1412\n","Epoch 28 - MCC: 0.8610\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9406 - loss: 0.1411 - val_accuracy: 0.9306 - val_loss: 0.1668 - mcc: 0.8610\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9400 - loss: 0.1456\n","Epoch 29 - MCC: 0.8611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9402 - loss: 0.1454 - val_accuracy: 0.9302 - val_loss: 0.1677 - mcc: 0.8611\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9439 - loss: 0.1364\n","Epoch 30 - MCC: 0.8640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9439 - loss: 0.1364 - val_accuracy: 0.9321 - val_loss: 0.1647 - mcc: 0.8640\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5583 - loss: 0.6514\n","Epoch 1 - MCC: 0.5393\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 162ms/step - accuracy: 0.5602 - loss: 0.6504 - val_accuracy: 0.7553 - val_loss: 0.5442 - mcc: 0.5393\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7994 - loss: 0.4919\n","Epoch 2 - MCC: 0.7195\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8000 - loss: 0.4900 - val_accuracy: 0.8602 - val_loss: 0.3290 - mcc: 0.7195\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8720 - loss: 0.3043\n","Epoch 3 - MCC: 0.7987\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.8723 - loss: 0.3035 - val_accuracy: 0.8998 - val_loss: 0.2387 - mcc: 0.7987\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8990 - loss: 0.2381\n","Epoch 4 - MCC: 0.8200\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8992 - loss: 0.2377 - val_accuracy: 0.9103 - val_loss: 0.2085 - mcc: 0.8200\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9118 - loss: 0.2069\n","Epoch 5 - MCC: 0.8244\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9116 - loss: 0.2071 - val_accuracy: 0.9120 - val_loss: 0.2033 - mcc: 0.8244\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9107 - loss: 0.2093\n","Epoch 6 - MCC: 0.8323\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9108 - loss: 0.2091 - val_accuracy: 0.9165 - val_loss: 0.1971 - mcc: 0.8323\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9149 - loss: 0.1987\n","Epoch 7 - MCC: 0.8403\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9149 - loss: 0.1987 - val_accuracy: 0.9205 - val_loss: 0.1886 - mcc: 0.8403\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9188 - loss: 0.1938\n","Epoch 8 - MCC: 0.8446\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9189 - loss: 0.1937 - val_accuracy: 0.9226 - val_loss: 0.1835 - mcc: 0.8446\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9229 - loss: 0.1836\n","Epoch 9 - MCC: 0.8492\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9228 - loss: 0.1837 - val_accuracy: 0.9249 - val_loss: 0.1769 - mcc: 0.8492\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9255 - loss: 0.1784\n","Epoch 10 - MCC: 0.8507\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9254 - loss: 0.1785 - val_accuracy: 0.9255 - val_loss: 0.1785 - mcc: 0.8507\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9204 - loss: 0.1891\n","Epoch 11 - MCC: 0.8499\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9206 - loss: 0.1887 - val_accuracy: 0.9248 - val_loss: 0.1754 - mcc: 0.8499\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9288 - loss: 0.1694\n","Epoch 12 - MCC: 0.8548\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9287 - loss: 0.1696 - val_accuracy: 0.9277 - val_loss: 0.1718 - mcc: 0.8548\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9265 - loss: 0.1745\n","Epoch 13 - MCC: 0.8560\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9265 - loss: 0.1745 - val_accuracy: 0.9281 - val_loss: 0.1695 - mcc: 0.8560\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9261 - loss: 0.1735\n","Epoch 14 - MCC: 0.8578\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9261 - loss: 0.1735 - val_accuracy: 0.9292 - val_loss: 0.1678 - mcc: 0.8578\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9299 - loss: 0.1663\n","Epoch 15 - MCC: 0.8636\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9298 - loss: 0.1664 - val_accuracy: 0.9321 - val_loss: 0.1616 - mcc: 0.8636\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9232 - loss: 0.1806\n","Epoch 16 - MCC: 0.8661\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9234 - loss: 0.1800 - val_accuracy: 0.9333 - val_loss: 0.1585 - mcc: 0.8661\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9313 - loss: 0.1652\n","Epoch 17 - MCC: 0.8687\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9313 - loss: 0.1652 - val_accuracy: 0.9346 - val_loss: 0.1563 - mcc: 0.8687\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9322 - loss: 0.1607\n","Epoch 18 - MCC: 0.8688\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9322 - loss: 0.1607 - val_accuracy: 0.9345 - val_loss: 0.1552 - mcc: 0.8688\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9332 - loss: 0.1600\n","Epoch 19 - MCC: 0.8689\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9332 - loss: 0.1600 - val_accuracy: 0.9344 - val_loss: 0.1554 - mcc: 0.8689\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9317 - loss: 0.1615\n","Epoch 20 - MCC: 0.8705\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9318 - loss: 0.1614 - val_accuracy: 0.9352 - val_loss: 0.1551 - mcc: 0.8705\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9344 - loss: 0.1571\n","Epoch 21 - MCC: 0.8761\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9344 - loss: 0.1571 - val_accuracy: 0.9383 - val_loss: 0.1480 - mcc: 0.8761\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9352 - loss: 0.1541\n","Epoch 22 - MCC: 0.8754\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9352 - loss: 0.1541 - val_accuracy: 0.9380 - val_loss: 0.1491 - mcc: 0.8754\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9342 - loss: 0.1559\n","Epoch 23 - MCC: 0.8795\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9343 - loss: 0.1557 - val_accuracy: 0.9400 - val_loss: 0.1455 - mcc: 0.8795\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9381 - loss: 0.1493\n","Epoch 24 - MCC: 0.8786\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9380 - loss: 0.1493 - val_accuracy: 0.9396 - val_loss: 0.1461 - mcc: 0.8786\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9352 - loss: 0.1526\n","Epoch 25 - MCC: 0.8808\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9353 - loss: 0.1525 - val_accuracy: 0.9406 - val_loss: 0.1421 - mcc: 0.8808\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9386 - loss: 0.1463\n","Epoch 26 - MCC: 0.8795\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9385 - loss: 0.1464 - val_accuracy: 0.9400 - val_loss: 0.1452 - mcc: 0.8795\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9379 - loss: 0.1505\n","Epoch 27 - MCC: 0.8798\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9379 - loss: 0.1503 - val_accuracy: 0.9401 - val_loss: 0.1425 - mcc: 0.8798\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9405 - loss: 0.1427\n","Epoch 28 - MCC: 0.8777\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9404 - loss: 0.1428 - val_accuracy: 0.9390 - val_loss: 0.1448 - mcc: 0.8777\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9375 - loss: 0.1476\n","Epoch 29 - MCC: 0.8819\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - accuracy: 0.9375 - loss: 0.1475 - val_accuracy: 0.9412 - val_loss: 0.1399 - mcc: 0.8819\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9388 - loss: 0.1458\n","Epoch 30 - MCC: 0.8836\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.9389 - loss: 0.1457 - val_accuracy: 0.9420 - val_loss: 0.1388 - mcc: 0.8836\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5150 - loss: 0.6486\n","Epoch 1 - MCC: 0.6234\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 170ms/step - accuracy: 0.5178 - loss: 0.6472 - val_accuracy: 0.8030 - val_loss: 0.5250 - mcc: 0.6234\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8194 - loss: 0.4905\n","Epoch 2 - MCC: 0.7267\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.8198 - loss: 0.4888 - val_accuracy: 0.8637 - val_loss: 0.3336 - mcc: 0.7267\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8682 - loss: 0.3106\n","Epoch 3 - MCC: 0.7710\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.8683 - loss: 0.3103 - val_accuracy: 0.8857 - val_loss: 0.2699 - mcc: 0.7710\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8793 - loss: 0.2824\n","Epoch 4 - MCC: 0.8010\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.8795 - loss: 0.2818 - val_accuracy: 0.9005 - val_loss: 0.2373 - mcc: 0.8010\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8988 - loss: 0.2379\n","Epoch 5 - MCC: 0.8256\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8988 - loss: 0.2379 - val_accuracy: 0.9129 - val_loss: 0.2077 - mcc: 0.8256\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9084 - loss: 0.2173\n","Epoch 6 - MCC: 0.8205\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9084 - loss: 0.2172 - val_accuracy: 0.9086 - val_loss: 0.2121 - mcc: 0.8205\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9088 - loss: 0.2128\n","Epoch 7 - MCC: 0.8395\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9089 - loss: 0.2127 - val_accuracy: 0.9196 - val_loss: 0.1926 - mcc: 0.8395\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9156 - loss: 0.1984\n","Epoch 8 - MCC: 0.8473\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9156 - loss: 0.1984 - val_accuracy: 0.9238 - val_loss: 0.1822 - mcc: 0.8473\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9142 - loss: 0.2010\n","Epoch 9 - MCC: 0.8453\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9142 - loss: 0.2008 - val_accuracy: 0.9228 - val_loss: 0.1841 - mcc: 0.8453\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9202 - loss: 0.1878\n","Epoch 10 - MCC: 0.8500\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9202 - loss: 0.1879 - val_accuracy: 0.9249 - val_loss: 0.1772 - mcc: 0.8500\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9251 - loss: 0.1757\n","Epoch 11 - MCC: 0.8475\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9249 - loss: 0.1761 - val_accuracy: 0.9234 - val_loss: 0.1786 - mcc: 0.8475\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9192 - loss: 0.1892\n","Epoch 12 - MCC: 0.8594\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9194 - loss: 0.1889 - val_accuracy: 0.9298 - val_loss: 0.1691 - mcc: 0.8594\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9286 - loss: 0.1705\n","Epoch 13 - MCC: 0.8638\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9285 - loss: 0.1708 - val_accuracy: 0.9321 - val_loss: 0.1638 - mcc: 0.8638\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9299 - loss: 0.1678\n","Epoch 14 - MCC: 0.8546\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9297 - loss: 0.1680 - val_accuracy: 0.9271 - val_loss: 0.1723 - mcc: 0.8546\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9289 - loss: 0.1682\n","Epoch 15 - MCC: 0.8638\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9288 - loss: 0.1684 - val_accuracy: 0.9319 - val_loss: 0.1619 - mcc: 0.8638\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9281 - loss: 0.1690\n","Epoch 16 - MCC: 0.8704\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9282 - loss: 0.1689 - val_accuracy: 0.9353 - val_loss: 0.1575 - mcc: 0.8704\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9300 - loss: 0.1658\n","Epoch 17 - MCC: 0.8731\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9300 - loss: 0.1657 - val_accuracy: 0.9367 - val_loss: 0.1554 - mcc: 0.8731\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9323 - loss: 0.1597\n","Epoch 18 - MCC: 0.8749\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9323 - loss: 0.1598 - val_accuracy: 0.9376 - val_loss: 0.1517 - mcc: 0.8749\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9287 - loss: 0.1686\n","Epoch 19 - MCC: 0.8766\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9289 - loss: 0.1682 - val_accuracy: 0.9385 - val_loss: 0.1482 - mcc: 0.8766\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9345 - loss: 0.1560\n","Epoch 20 - MCC: 0.8784\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9345 - loss: 0.1560 - val_accuracy: 0.9393 - val_loss: 0.1474 - mcc: 0.8784\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9389 - loss: 0.1472\n","Epoch 21 - MCC: 0.8807\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 129ms/step - accuracy: 0.9387 - loss: 0.1475 - val_accuracy: 0.9405 - val_loss: 0.1437 - mcc: 0.8807\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9361 - loss: 0.1532\n","Epoch 22 - MCC: 0.8800\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9361 - loss: 0.1533 - val_accuracy: 0.9401 - val_loss: 0.1456 - mcc: 0.8800\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9376 - loss: 0.1488\n","Epoch 23 - MCC: 0.8817\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9376 - loss: 0.1489 - val_accuracy: 0.9409 - val_loss: 0.1440 - mcc: 0.8817\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9376 - loss: 0.1483\n","Epoch 24 - MCC: 0.8862\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9376 - loss: 0.1484 - val_accuracy: 0.9432 - val_loss: 0.1395 - mcc: 0.8862\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9374 - loss: 0.1499\n","Epoch 25 - MCC: 0.8842\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9374 - loss: 0.1499 - val_accuracy: 0.9422 - val_loss: 0.1407 - mcc: 0.8842\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9348 - loss: 0.1536\n","Epoch 26 - MCC: 0.8854\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9350 - loss: 0.1533 - val_accuracy: 0.9428 - val_loss: 0.1381 - mcc: 0.8854\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9379 - loss: 0.1478\n","Epoch 27 - MCC: 0.8880\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9380 - loss: 0.1477 - val_accuracy: 0.9441 - val_loss: 0.1363 - mcc: 0.8880\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9427 - loss: 0.1368\n","Epoch 28 - MCC: 0.8872\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9426 - loss: 0.1370 - val_accuracy: 0.9437 - val_loss: 0.1363 - mcc: 0.8872\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9398 - loss: 0.1446\n","Epoch 29 - MCC: 0.8865\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9399 - loss: 0.1446 - val_accuracy: 0.9434 - val_loss: 0.1369 - mcc: 0.8865\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9427 - loss: 0.1380\n","Epoch 30 - MCC: 0.8888\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9426 - loss: 0.1381 - val_accuracy: 0.9444 - val_loss: 0.1358 - mcc: 0.8888\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5818 - loss: 0.6487\n","Epoch 1 - MCC: 0.5275\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - accuracy: 0.5842 - loss: 0.6474 - val_accuracy: 0.7600 - val_loss: 0.5363 - mcc: 0.5275\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7884 - loss: 0.4913\n","Epoch 2 - MCC: 0.6839\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - accuracy: 0.7892 - loss: 0.4894 - val_accuracy: 0.8423 - val_loss: 0.3655 - mcc: 0.6839\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8551 - loss: 0.3375\n","Epoch 3 - MCC: 0.7382\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.8553 - loss: 0.3367 - val_accuracy: 0.8683 - val_loss: 0.3045 - mcc: 0.7382\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8836 - loss: 0.2719\n","Epoch 4 - MCC: 0.7721\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8835 - loss: 0.2721 - val_accuracy: 0.8862 - val_loss: 0.2679 - mcc: 0.7721\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8933 - loss: 0.2515\n","Epoch 5 - MCC: 0.7998\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.8934 - loss: 0.2511 - val_accuracy: 0.9000 - val_loss: 0.2357 - mcc: 0.7998\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9039 - loss: 0.2287\n","Epoch 6 - MCC: 0.8046\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9040 - loss: 0.2285 - val_accuracy: 0.9023 - val_loss: 0.2262 - mcc: 0.8046\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9096 - loss: 0.2124\n","Epoch 7 - MCC: 0.8090\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9096 - loss: 0.2125 - val_accuracy: 0.9045 - val_loss: 0.2190 - mcc: 0.8090\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9136 - loss: 0.2045\n","Epoch 8 - MCC: 0.8229\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9136 - loss: 0.2043 - val_accuracy: 0.9116 - val_loss: 0.2065 - mcc: 0.8229\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9206 - loss: 0.1892\n","Epoch 9 - MCC: 0.8262\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9205 - loss: 0.1894 - val_accuracy: 0.9132 - val_loss: 0.2019 - mcc: 0.8262\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9199 - loss: 0.1887\n","Epoch 10 - MCC: 0.8258\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9199 - loss: 0.1888 - val_accuracy: 0.9129 - val_loss: 0.2007 - mcc: 0.8258\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9213 - loss: 0.1869\n","Epoch 11 - MCC: 0.8349\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.9213 - loss: 0.1868 - val_accuracy: 0.9174 - val_loss: 0.1919 - mcc: 0.8349\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9247 - loss: 0.1803\n","Epoch 12 - MCC: 0.8434\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9247 - loss: 0.1803 - val_accuracy: 0.9214 - val_loss: 0.1900 - mcc: 0.8434\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9255 - loss: 0.1767\n","Epoch 13 - MCC: 0.8391\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9255 - loss: 0.1767 - val_accuracy: 0.9196 - val_loss: 0.1875 - mcc: 0.8391\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9274 - loss: 0.1730\n","Epoch 14 - MCC: 0.8503\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9274 - loss: 0.1730 - val_accuracy: 0.9252 - val_loss: 0.1776 - mcc: 0.8503\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9325 - loss: 0.1616\n","Epoch 15 - MCC: 0.8499\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9325 - loss: 0.1617 - val_accuracy: 0.9248 - val_loss: 0.1771 - mcc: 0.8499\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9313 - loss: 0.1667\n","Epoch 16 - MCC: 0.8524\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9313 - loss: 0.1666 - val_accuracy: 0.9261 - val_loss: 0.1751 - mcc: 0.8524\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9317 - loss: 0.1635\n","Epoch 17 - MCC: 0.8570\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9318 - loss: 0.1634 - val_accuracy: 0.9283 - val_loss: 0.1719 - mcc: 0.8570\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9349 - loss: 0.1571\n","Epoch 18 - MCC: 0.8587\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9349 - loss: 0.1571 - val_accuracy: 0.9290 - val_loss: 0.1712 - mcc: 0.8587\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9350 - loss: 0.1565\n","Epoch 19 - MCC: 0.8578\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9350 - loss: 0.1565 - val_accuracy: 0.9290 - val_loss: 0.1685 - mcc: 0.8578\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9371 - loss: 0.1513\n","Epoch 20 - MCC: 0.8587\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9371 - loss: 0.1515 - val_accuracy: 0.9295 - val_loss: 0.1630 - mcc: 0.8587\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9413 - loss: 0.1424\n","Epoch 21 - MCC: 0.8653\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9412 - loss: 0.1426 - val_accuracy: 0.9327 - val_loss: 0.1608 - mcc: 0.8653\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9350 - loss: 0.1564\n","Epoch 22 - MCC: 0.8558\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9351 - loss: 0.1561 - val_accuracy: 0.9278 - val_loss: 0.1647 - mcc: 0.8558\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9368 - loss: 0.1522\n","Epoch 23 - MCC: 0.8616\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9368 - loss: 0.1522 - val_accuracy: 0.9309 - val_loss: 0.1594 - mcc: 0.8616\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9377 - loss: 0.1482\n","Epoch 24 - MCC: 0.8654\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9377 - loss: 0.1482 - val_accuracy: 0.9325 - val_loss: 0.1581 - mcc: 0.8654\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9411 - loss: 0.1416\n","Epoch 25 - MCC: 0.8718\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9410 - loss: 0.1417 - val_accuracy: 0.9359 - val_loss: 0.1526 - mcc: 0.8718\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9425 - loss: 0.1407\n","Epoch 26 - MCC: 0.8714\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.9424 - loss: 0.1408 - val_accuracy: 0.9355 - val_loss: 0.1543 - mcc: 0.8714\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9403 - loss: 0.1441\n","Epoch 27 - MCC: 0.8716\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9404 - loss: 0.1440 - val_accuracy: 0.9356 - val_loss: 0.1539 - mcc: 0.8716\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9432 - loss: 0.1362\n","Epoch 28 - MCC: 0.8718\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9432 - loss: 0.1363 - val_accuracy: 0.9360 - val_loss: 0.1503 - mcc: 0.8718\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9439 - loss: 0.1362\n","Epoch 29 - MCC: 0.8715\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9439 - loss: 0.1363 - val_accuracy: 0.9357 - val_loss: 0.1524 - mcc: 0.8715\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9398 - loss: 0.1454\n","Epoch 30 - MCC: 0.8731\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9399 - loss: 0.1452 - val_accuracy: 0.9367 - val_loss: 0.1496 - mcc: 0.8731\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9483666666666667,\n","              'mean': 0.9407066666666666,\n","              'min': 0.9320533333333333,\n","              'std': 0.00574808180565001},\n"," 'Inference Time (s/sample)': {'max': 0.0036112594604492186,\n","                               'mean': 0.003192037582397461,\n","                               'min': 0.0021795177459716796,\n","                               'std': 0.0005407108074675919},\n"," 'MCC': {'max': 0.8964878810208075,\n","         'mean': 0.8811989097036573,\n","         'min': 0.8640135408032064,\n","         'std': 0.011461701953627463},\n"," 'Parameters': 10017,\n"," 'Train Time (s)': {'max': 141.41014695167542,\n","                    'mean': 99.35629744529724,\n","                    'min': 85.70549750328064,\n","                    'std': 21.262382085745557},\n"," 'Training Accuracy': [[0.618963360786438,\n","                        0.8212433457374573,\n","                        0.8722733855247498,\n","                        0.8911851644515991,\n","                        0.8997367024421692,\n","                        0.9064933061599731,\n","                        0.9106783866882324,\n","                        0.9130150079727173,\n","                        0.916088342666626,\n","                        0.9171249866485596,\n","                        0.9201483130455017,\n","                        0.921896755695343,\n","                        0.9249400496482849,\n","                        0.9263635277748108,\n","                        0.9278183579444885,\n","                        0.929878294467926,\n","                        0.9294750094413757,\n","                        0.9319517016410828,\n","                        0.931326687335968,\n","                        0.9334365725517273,\n","                        0.934499979019165,\n","                        0.9362033009529114,\n","                        0.937218189239502,\n","                        0.9378498792648315,\n","                        0.9369484186172485,\n","                        0.938589870929718,\n","                        0.9391200542449951,\n","                        0.9383849501609802,\n","                        0.9384250640869141,\n","                        0.9407065510749817],\n","                       [0.5612333416938782,\n","                        0.8208248615264893,\n","                        0.8675450682640076,\n","                        0.8906615972518921,\n","                        0.9105666875839233,\n","                        0.915983259677887,\n","                        0.9178082942962646,\n","                        0.9216882586479187,\n","                        0.9259633421897888,\n","                        0.9277549982070923,\n","                        0.9297633171081543,\n","                        0.9278534650802612,\n","                        0.9307533502578735,\n","                        0.9338233470916748,\n","                        0.9345182180404663,\n","                        0.9367098808288574,\n","                        0.9377149939537048,\n","                        0.9373167157173157,\n","                        0.9383599758148193,\n","                        0.9380750060081482,\n","                        0.9384399652481079,\n","                        0.9405066967010498,\n","                        0.9402568340301514,\n","                        0.9413849115371704,\n","                        0.9413166046142578,\n","                        0.9415633678436279,\n","                        0.9418917298316956,\n","                        0.9427332878112793,\n","                        0.9430782794952393,\n","                        0.9433518052101135],\n","                       [0.6098799705505371,\n","                        0.8165299892425537,\n","                        0.8801066875457764,\n","                        0.9030716419219971,\n","                        0.90850830078125,\n","                        0.9133949279785156,\n","                        0.9157866835594177,\n","                        0.9193933010101318,\n","                        0.9214000105857849,\n","                        0.9237766265869141,\n","                        0.9246366620063782,\n","                        0.9267532825469971,\n","                        0.926194965839386,\n","                        0.9262349605560303,\n","                        0.9286566376686096,\n","                        0.9306399822235107,\n","                        0.9303932785987854,\n","                        0.9322750568389893,\n","                        0.9334933757781982,\n","                        0.9338833093643188,\n","                        0.9340648651123047,\n","                        0.9359250664710999,\n","                        0.936316728591919,\n","                        0.9368383288383484,\n","                        0.9367882609367371,\n","                        0.9371850490570068,\n","                        0.9387383460998535,\n","                        0.9391449093818665,\n","                        0.9398467540740967,\n","                        0.9399433732032776],\n","                       [0.5881667137145996,\n","                        0.8288500308990479,\n","                        0.8702566623687744,\n","                        0.8862083554267883,\n","                        0.8991867303848267,\n","                        0.9086000323295593,\n","                        0.9109982848167419,\n","                        0.9157333374023438,\n","                        0.9165283441543579,\n","                        0.9192700386047363,\n","                        0.92073655128479,\n","                        0.9230200052261353,\n","                        0.9254984855651855,\n","                        0.9257500171661377,\n","                        0.9275084137916565,\n","                        0.9295499920845032,\n","                        0.9300433993339539,\n","                        0.9318649172782898,\n","                        0.933411717414856,\n","                        0.9343899488449097,\n","                        0.9349767565727234,\n","                        0.9349417090415955,\n","                        0.9371784329414368,\n","                        0.9369050860404968,\n","                        0.9371615648269653,\n","                        0.9383100867271423,\n","                        0.9400951266288757,\n","                        0.9400749802589417,\n","                        0.940706729888916,\n","                        0.9407632946968079],\n","                       [0.6431499719619751,\n","                        0.8107698559761047,\n","                        0.8624067902565002,\n","                        0.8813300132751465,\n","                        0.8974867463111877,\n","                        0.9060500264167786,\n","                        0.9093217253684998,\n","                        0.9154166579246521,\n","                        0.9176382422447205,\n","                        0.9190399050712585,\n","                        0.9217766523361206,\n","                        0.9242133498191833,\n","                        0.9254265427589417,\n","                        0.9274217486381531,\n","                        0.9313082098960876,\n","                        0.9314866662025452,\n","                        0.9334582686424255,\n","                        0.935363233089447,\n","                        0.9350332021713257,\n","                        0.9350948333740234,\n","                        0.9384517669677734,\n","                        0.938010036945343,\n","                        0.9372550249099731,\n","                        0.9375582933425903,\n","                        0.9407116770744324,\n","                        0.9413083791732788,\n","                        0.9418649077415466,\n","                        0.9421166777610779,\n","                        0.9424582719802856,\n","                        0.9422432780265808]],\n"," 'Training Loss': [[0.6293176412582397,\n","                    0.4616581201553345,\n","                    0.3018622398376465,\n","                    0.25797608494758606,\n","                    0.2368820607662201,\n","                    0.22225795686244965,\n","                    0.21156877279281616,\n","                    0.2061871737241745,\n","                    0.19910813868045807,\n","                    0.1954670548439026,\n","                    0.1894001066684723,\n","                    0.18545210361480713,\n","                    0.1790623515844345,\n","                    0.17508678138256073,\n","                    0.17186538875102997,\n","                    0.16712802648544312,\n","                    0.16852547228336334,\n","                    0.1638830006122589,\n","                    0.16392265260219574,\n","                    0.1604941487312317,\n","                    0.15720143914222717,\n","                    0.1531917154788971,\n","                    0.1508745700120926,\n","                    0.15030187368392944,\n","                    0.1517084836959839,\n","                    0.14686910808086395,\n","                    0.1462070196866989,\n","                    0.1496071219444275,\n","                    0.1496782749891281,\n","                    0.14371348917484283],\n","                   [0.6255399584770203,\n","                    0.4760925769805908,\n","                    0.31565019488334656,\n","                    0.25843697786331177,\n","                    0.21404436230659485,\n","                    0.20014768838882446,\n","                    0.1963696926832199,\n","                    0.1878812462091446,\n","                    0.17704755067825317,\n","                    0.1730474978685379,\n","                    0.169081911444664,\n","                    0.17367887496948242,\n","                    0.1672961413860321,\n","                    0.1593109667301178,\n","                    0.15853358805179596,\n","                    0.15407603979110718,\n","                    0.15018302202224731,\n","                    0.1504109501838684,\n","                    0.14924310147762299,\n","                    0.14887657761573792,\n","                    0.14881740510463715,\n","                    0.1445276290178299,\n","                    0.14558038115501404,\n","                    0.1427750140428543,\n","                    0.1419687122106552,\n","                    0.14095813035964966,\n","                    0.1408623903989792,\n","                    0.13851578533649445,\n","                    0.13841842114925385,\n","                    0.13749825954437256],\n","                   [0.6239295601844788,\n","                    0.442848801612854,\n","                    0.28372588753700256,\n","                    0.22932706773281097,\n","                    0.21345476806163788,\n","                    0.20401622354984283,\n","                    0.1975531429052353,\n","                    0.19071850180625916,\n","                    0.1861094832420349,\n","                    0.1812981367111206,\n","                    0.18054187297821045,\n","                    0.17412610352039337,\n","                    0.17544503509998322,\n","                    0.173860564827919,\n","                    0.16933214664459229,\n","                    0.16466599702835083,\n","                    0.16570262610912323,\n","                    0.1613495945930481,\n","                    0.15922583639621735,\n","                    0.15750476717948914,\n","                    0.1582246720790863,\n","                    0.1529470980167389,\n","                    0.1518283188343048,\n","                    0.15157297253608704,\n","                    0.15054923295974731,\n","                    0.14947760105133057,\n","                    0.14738382399082184,\n","                    0.14614543318748474,\n","                    0.1442338526248932,\n","                    0.14363215863704681],\n","                   [0.6120187640190125,\n","                    0.446493923664093,\n","                    0.30418670177459717,\n","                    0.2665773332118988,\n","                    0.23743711411952972,\n","                    0.21454325318336487,\n","                    0.20801015198230743,\n","                    0.19783250987529755,\n","                    0.19507795572280884,\n","                    0.1889963299036026,\n","                    0.18567590415477753,\n","                    0.18092016875743866,\n","                    0.17618055641651154,\n","                    0.17465916275978088,\n","                    0.1714143604040146,\n","                    0.16631776094436646,\n","                    0.16501733660697937,\n","                    0.16062268614768982,\n","                    0.1582445502281189,\n","                    0.15589119493961334,\n","                    0.1547265201807022,\n","                    0.15487739443778992,\n","                    0.14977458119392395,\n","                    0.14940182864665985,\n","                    0.15088775753974915,\n","                    0.14685942232608795,\n","                    0.14307722449302673,\n","                    0.14278407394886017,\n","                    0.14288300275802612,\n","                    0.14238809049129486],\n","                   [0.6147790551185608,\n","                    0.4429573118686676,\n","                    0.3186919689178467,\n","                    0.27722588181495667,\n","                    0.24268034100532532,\n","                    0.22252628207206726,\n","                    0.213401660323143,\n","                    0.20036351680755615,\n","                    0.19500713050365448,\n","                    0.19187234342098236,\n","                    0.1851525902748108,\n","                    0.18078410625457764,\n","                    0.17650167644023895,\n","                    0.17307178676128387,\n","                    0.16470631957054138,\n","                    0.16431862115859985,\n","                    0.16044332087039948,\n","                    0.1556747555732727,\n","                    0.1565893590450287,\n","                    0.1553400605916977,\n","                    0.1487315148115158,\n","                    0.14909528195858002,\n","                    0.15128354728221893,\n","                    0.14944028854370117,\n","                    0.14275018870830536,\n","                    0.1416042000055313,\n","                    0.14042414724826813,\n","                    0.13920220732688904,\n","                    0.13999710977077484,\n","                    0.1395290493965149]],\n"," 'Validation Accuracy': [[0.7803066968917847,\n","                          0.8614932298660278,\n","                          0.8914533257484436,\n","                          0.9021933674812317,\n","                          0.906166672706604,\n","                          0.9115000367164612,\n","                          0.9187467098236084,\n","                          0.9196799397468567,\n","                          0.9241266846656799,\n","                          0.9227866530418396,\n","                          0.9292600154876709,\n","                          0.9296733140945435,\n","                          0.932379961013794,\n","                          0.9318466782569885,\n","                          0.9344000816345215,\n","                          0.9347732663154602,\n","                          0.9368667006492615,\n","                          0.9391266703605652,\n","                          0.9384467601776123,\n","                          0.9379934072494507,\n","                          0.9422599673271179,\n","                          0.9400399923324585,\n","                          0.9444333910942078,\n","                          0.9449732899665833,\n","                          0.9449999332427979,\n","                          0.947233259677887,\n","                          0.9471333026885986,\n","                          0.9413666725158691,\n","                          0.945953369140625,\n","                          0.9483667016029358],\n","                         [0.71697998046875,\n","                          0.8417533040046692,\n","                          0.8689466714859009,\n","                          0.8888866305351257,\n","                          0.90256667137146,\n","                          0.9052466750144958,\n","                          0.9100400805473328,\n","                          0.9133666753768921,\n","                          0.9156666398048401,\n","                          0.9165199398994446,\n","                          0.9165400862693787,\n","                          0.9184066653251648,\n","                          0.9209132194519043,\n","                          0.9228200316429138,\n","                          0.9243533611297607,\n","                          0.9255532622337341,\n","                          0.9257000088691711,\n","                          0.9293665885925293,\n","                          0.9275665879249573,\n","                          0.9283933043479919,\n","                          0.9293934106826782,\n","                          0.9309400320053101,\n","                          0.926240086555481,\n","                          0.9286733269691467,\n","                          0.9281866550445557,\n","                          0.929806649684906,\n","                          0.9319201111793518,\n","                          0.9306000471115112,\n","                          0.9301866292953491,\n","                          0.9320533871650696],\n","                         [0.7553266882896423,\n","                          0.8602200150489807,\n","                          0.899773359298706,\n","                          0.9102533459663391,\n","                          0.9120399951934814,\n","                          0.9164733290672302,\n","                          0.9204933047294617,\n","                          0.9226133823394775,\n","                          0.9248733520507812,\n","                          0.9254667162895203,\n","                          0.9247999787330627,\n","                          0.9277200102806091,\n","                          0.9280733466148376,\n","                          0.9291732907295227,\n","                          0.9320799112319946,\n","                          0.9333466291427612,\n","                          0.9346199631690979,\n","                          0.9345266819000244,\n","                          0.934406578540802,\n","                          0.9352066516876221,\n","                          0.9383266568183899,\n","                          0.9379733204841614,\n","                          0.9399933815002441,\n","                          0.9395732879638672,\n","                          0.940619945526123,\n","                          0.9399932622909546,\n","                          0.9401466846466064,\n","                          0.9390000104904175,\n","                          0.9411666989326477,\n","                          0.942026674747467],\n","                         [0.8029866814613342,\n","                          0.8637332916259766,\n","                          0.8857399821281433,\n","                          0.9005333781242371,\n","                          0.9129266142845154,\n","                          0.9086067080497742,\n","                          0.9195801019668579,\n","                          0.9237533807754517,\n","                          0.9228333234786987,\n","                          0.9249399304389954,\n","                          0.9233798980712891,\n","                          0.9298466444015503,\n","                          0.9320599436759949,\n","                          0.9270533919334412,\n","                          0.9319400787353516,\n","                          0.9352599382400513,\n","                          0.936686635017395,\n","                          0.9375799298286438,\n","                          0.9384533166885376,\n","                          0.9393399953842163,\n","                          0.9404799938201904,\n","                          0.9401400089263916,\n","                          0.9408932328224182,\n","                          0.9432400465011597,\n","                          0.9422199726104736,\n","                          0.9428133368492126,\n","                          0.9440866708755493,\n","                          0.9436933994293213,\n","                          0.9434067010879517,\n","                          0.9444066882133484],\n","                         [0.7599599957466125,\n","                          0.8422999382019043,\n","                          0.8682733178138733,\n","                          0.8862133622169495,\n","                          0.9000399708747864,\n","                          0.9023399949073792,\n","                          0.9044733643531799,\n","                          0.9115734100341797,\n","                          0.9132399559020996,\n","                          0.9128600358963013,\n","                          0.917419970035553,\n","                          0.9214000701904297,\n","                          0.9196265935897827,\n","                          0.9251933097839355,\n","                          0.9248400330543518,\n","                          0.9261466264724731,\n","                          0.9283466935157776,\n","                          0.9290333390235901,\n","                          0.9289667010307312,\n","                          0.9294866919517517,\n","                          0.9326667189598083,\n","                          0.9278066754341125,\n","                          0.9309333562850952,\n","                          0.9325466156005859,\n","                          0.9358599781990051,\n","                          0.9355400204658508,\n","                          0.9355934262275696,\n","                          0.9360067248344421,\n","                          0.9357000589370728,\n","                          0.936680018901825]],\n"," 'Validation Loss': [[0.5545694231987,\n","                      0.34514227509498596,\n","                      0.25804799795150757,\n","                      0.23180757462978363,\n","                      0.22105447947978973,\n","                      0.20875228941440582,\n","                      0.19211439788341522,\n","                      0.18836785852909088,\n","                      0.18139323592185974,\n","                      0.18323402106761932,\n","                      0.1692860722541809,\n","                      0.1686973124742508,\n","                      0.16283346712589264,\n","                      0.16165733337402344,\n","                      0.15666979551315308,\n","                      0.1560177206993103,\n","                      0.151511549949646,\n","                      0.1472504436969757,\n","                      0.1507742553949356,\n","                      0.1508457064628601,\n","                      0.14052173495292664,\n","                      0.14387738704681396,\n","                      0.13664881885051727,\n","                      0.13372448086738586,\n","                      0.13334697484970093,\n","                      0.12999944388866425,\n","                      0.13033239543437958,\n","                      0.145155131816864,\n","                      0.13127611577510834,\n","                      0.12633906304836273],\n","                     [0.5608527660369873,\n","                      0.38898035883903503,\n","                      0.30228546261787415,\n","                      0.2616228759288788,\n","                      0.23056526482105255,\n","                      0.22272184491157532,\n","                      0.2140025645494461,\n","                      0.20475074648857117,\n","                      0.1989484280347824,\n","                      0.19770082831382751,\n","                      0.1957704722881317,\n","                      0.19521188735961914,\n","                      0.18716342747211456,\n","                      0.18339377641677856,\n","                      0.17990431189537048,\n","                      0.17809243500232697,\n","                      0.1787397563457489,\n","                      0.17120584845542908,\n","                      0.17297792434692383,\n","                      0.17367517948150635,\n","                      0.17137110233306885,\n","                      0.16760453581809998,\n","                      0.17822805047035217,\n","                      0.1701451689004898,\n","                      0.16939325630664825,\n","                      0.1697380691766739,\n","                      0.163521409034729,\n","                      0.16675201058387756,\n","                      0.1677159070968628,\n","                      0.16474246978759766],\n","                     [0.5442051887512207,\n","                      0.3290270268917084,\n","                      0.23870749771595,\n","                      0.2085200697183609,\n","                      0.20330318808555603,\n","                      0.19710133969783783,\n","                      0.18859153985977173,\n","                      0.18345820903778076,\n","                      0.17693765461444855,\n","                      0.17852096259593964,\n","                      0.17536941170692444,\n","                      0.17182624340057373,\n","                      0.16949385404586792,\n","                      0.1678403615951538,\n","                      0.16155286133289337,\n","                      0.15854568779468536,\n","                      0.15629178285598755,\n","                      0.15515288710594177,\n","                      0.15539711713790894,\n","                      0.15507906675338745,\n","                      0.14800520241260529,\n","                      0.14913545548915863,\n","                      0.14548149704933167,\n","                      0.1460890918970108,\n","                      0.14208970963954926,\n","                      0.14515776932239532,\n","                      0.1424800604581833,\n","                      0.14475570619106293,\n","                      0.13993462920188904,\n","                      0.13879075646400452],\n","                     [0.525032639503479,\n","                      0.3335723876953125,\n","                      0.2699395716190338,\n","                      0.2372993528842926,\n","                      0.20770879089832306,\n","                      0.2121182233095169,\n","                      0.19261369109153748,\n","                      0.18215912580490112,\n","                      0.18412435054779053,\n","                      0.1771937757730484,\n","                      0.17859691381454468,\n","                      0.16909366846084595,\n","                      0.16377082467079163,\n","                      0.17230087518692017,\n","                      0.16185323894023895,\n","                      0.15746568143367767,\n","                      0.15541939437389374,\n","                      0.15174001455307007,\n","                      0.14824756979942322,\n","                      0.14735451340675354,\n","                      0.1436794251203537,\n","                      0.145553320646286,\n","                      0.14396999776363373,\n","                      0.1394728571176529,\n","                      0.1406555026769638,\n","                      0.13810200989246368,\n","                      0.1363060474395752,\n","                      0.13628549873828888,\n","                      0.13691484928131104,\n","                      0.13582004606723785],\n","                     [0.5362858772277832,\n","                      0.36547568440437317,\n","                      0.30445951223373413,\n","                      0.267872154712677,\n","                      0.23572227358818054,\n","                      0.22624102234840393,\n","                      0.21904915571212769,\n","                      0.206454798579216,\n","                      0.2019200325012207,\n","                      0.20065636932849884,\n","                      0.1918838918209076,\n","                      0.1900128871202469,\n","                      0.18753650784492493,\n","                      0.17760486900806427,\n","                      0.17705745995044708,\n","                      0.17506912350654602,\n","                      0.1718924194574356,\n","                      0.17121875286102295,\n","                      0.16853532195091248,\n","                      0.16298940777778625,\n","                      0.16077938675880432,\n","                      0.1646661013364792,\n","                      0.15937122702598572,\n","                      0.15807873010635376,\n","                      0.1526019424200058,\n","                      0.15428870916366577,\n","                      0.15388938784599304,\n","                      0.1503494828939438,\n","                      0.1524047702550888,\n","                      0.14958901703357697]],\n"," 'Validation MCC': [[0.597499328578195,\n","                     0.7225684679043161,\n","                     0.7827073828229653,\n","                     0.8036092438211367,\n","                     0.8118229686562799,\n","                     0.8236356550355364,\n","                     0.836897395733325,\n","                     0.838989757817237,\n","                     0.84771170957121,\n","                     0.8460643174420392,\n","                     0.8579797588177084,\n","                     0.8596694558281627,\n","                     0.8644933348906408,\n","                     0.8639725313819808,\n","                     0.8684418303056688,\n","                     0.8698270938145096,\n","                     0.8733218014914031,\n","                     0.8778029313368104,\n","                     0.8772397327163397,\n","                     0.876066578950894,\n","                     0.884128405579053,\n","                     0.8801068134795056,\n","                     0.8884642586089738,\n","                     0.8898789508894985,\n","                     0.8897622974218342,\n","                     0.8940689180785399,\n","                     0.8940986217270234,\n","                     0.8829636906073854,\n","                     0.8914935412324134,\n","                     0.8964878810208075],\n","                    [0.49519083408535364,\n","                     0.6828506644087701,\n","                     0.7377951625938317,\n","                     0.779961359611215,\n","                     0.8048993083867291,\n","                     0.810608209407283,\n","                     0.8204571614441775,\n","                     0.826368376479676,\n","                     0.8309228193846543,\n","                     0.8327654361524127,\n","                     0.8329547018587022,\n","                     0.8375662517209672,\n","                     0.8420432205922017,\n","                     0.8452705309674028,\n","                     0.8483572943383871,\n","                     0.8507752864607374,\n","                     0.8519217922854762,\n","                     0.8584128601745131,\n","                     0.8550376612710369,\n","                     0.8571550675616684,\n","                     0.8585889939188349,\n","                     0.8615650560605804,\n","                     0.8532815708319772,\n","                     0.8578428570193425,\n","                     0.856034442786832,\n","                     0.8600371807950868,\n","                     0.8635193184986619,\n","                     0.8609835894400509,\n","                     0.8610644307045361,\n","                     0.8640135408032064],\n","                    [0.5393241584133951,\n","                     0.7194705799920658,\n","                     0.7986831877529714,\n","                     0.8200270302856586,\n","                     0.8243973004355394,\n","                     0.8322524946531977,\n","                     0.8403387640313692,\n","                     0.8445694556977658,\n","                     0.8491556158549597,\n","                     0.8506946312935588,\n","                     0.8498503733771544,\n","                     0.8548396734253595,\n","                     0.8560002016322763,\n","                     0.8577501755741536,\n","                     0.8636408937535048,\n","                     0.8661418022427564,\n","                     0.868745984562584,\n","                     0.8687863724793955,\n","                     0.8689412960959418,\n","                     0.8704853660917945,\n","                     0.8761393976577514,\n","                     0.8754352692833726,\n","                     0.8794919618721954,\n","                     0.8786492511287969,\n","                     0.8808425404395377,\n","                     0.8794882939676346,\n","                     0.8798052205413237,\n","                     0.8776572772349356,\n","                     0.8819191172110195,\n","                     0.8835766582078036],\n","                    [0.6233889415442091,\n","                     0.7267306531332643,\n","                     0.7710488524540668,\n","                     0.8010263358204511,\n","                     0.825613148198271,\n","                     0.8204967954087548,\n","                     0.8394621424616359,\n","                     0.8472589551482743,\n","                     0.8452822310367712,\n","                     0.8500009202956177,\n","                     0.847458936222724,\n","                     0.8594241299156495,\n","                     0.8638002643666584,\n","                     0.8545695543257877,\n","                     0.863832650738652,\n","                     0.8703828460663654,\n","                     0.8731116180800385,\n","                     0.8748772220053171,\n","                     0.8766155675950014,\n","                     0.8783856343651513,\n","                     0.8806756210759076,\n","                     0.8799899816116241,\n","                     0.8817337136152238,\n","                     0.8862400530208078,\n","                     0.8842080838590543,\n","                     0.8853562375527938,\n","                     0.8879844509070609,\n","                     0.8871555337793897,\n","                     0.8865384886095014,\n","                     0.8887682909934842],\n","                    [0.5274569034456936,\n","                     0.6839338734029682,\n","                     0.7381674529288456,\n","                     0.7720567368518806,\n","                     0.7997594858652418,\n","                     0.8046026742777687,\n","                     0.8090420991258057,\n","                     0.8228819965814352,\n","                     0.8261637551093305,\n","                     0.8257812556433864,\n","                     0.834880413685521,\n","                     0.8433581805111565,\n","                     0.8390663953124035,\n","                     0.8502706185919437,\n","                     0.849850758944321,\n","                     0.8523748173046467,\n","                     0.8570435041115105,\n","                     0.8586550745189037,\n","                     0.8578234678760627,\n","                     0.8587236954045129,\n","                     0.8652777891416179,\n","                     0.8558443630955836,\n","                     0.8616168578958195,\n","                     0.8653656108405972,\n","                     0.8717854175196413,\n","                     0.8713973162187599,\n","                     0.8715875336430593,\n","                     0.8717806559559799,\n","                     0.8714957718085306,\n","                     0.8731481774929848]]}\n","Training Model: BiLSTM_Deep, Fold: 1\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.6405 - loss: 0.5931\n","Epoch 1 - MCC: 0.7002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 230ms/step - accuracy: 0.6442 - loss: 0.5895 - val_accuracy: 0.8482 - val_loss: 0.3423 - mcc: 0.7002\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8569 - loss: 0.3250\n","Epoch 2 - MCC: 0.7955\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.8574 - loss: 0.3239 - val_accuracy: 0.8975 - val_loss: 0.2382 - mcc: 0.7955\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8988 - loss: 0.2383\n","Epoch 3 - MCC: 0.8356\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 193ms/step - accuracy: 0.8990 - loss: 0.2378 - val_accuracy: 0.9181 - val_loss: 0.1947 - mcc: 0.8356\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9148 - loss: 0.1999\n","Epoch 4 - MCC: 0.8559\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9148 - loss: 0.1999 - val_accuracy: 0.9282 - val_loss: 0.1729 - mcc: 0.8559\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9250 - loss: 0.1791\n","Epoch 5 - MCC: 0.8669\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - accuracy: 0.9249 - loss: 0.1793 - val_accuracy: 0.9337 - val_loss: 0.1598 - mcc: 0.8669\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9252 - loss: 0.1780\n","Epoch 6 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9252 - loss: 0.1779 - val_accuracy: 0.9353 - val_loss: 0.1562 - mcc: 0.8701\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9260 - loss: 0.1764\n","Epoch 7 - MCC: 0.8722\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9262 - loss: 0.1761 - val_accuracy: 0.9363 - val_loss: 0.1498 - mcc: 0.8722\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9310 - loss: 0.1640\n","Epoch 8 - MCC: 0.8818\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.9310 - loss: 0.1640 - val_accuracy: 0.9408 - val_loss: 0.1437 - mcc: 0.8818\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9341 - loss: 0.1561\n","Epoch 9 - MCC: 0.8874\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9341 - loss: 0.1561 - val_accuracy: 0.9439 - val_loss: 0.1350 - mcc: 0.8874\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9375 - loss: 0.1498\n","Epoch 10 - MCC: 0.8945\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.9375 - loss: 0.1499 - val_accuracy: 0.9474 - val_loss: 0.1277 - mcc: 0.8945\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.9421 - loss: 0.1402\n","Epoch 11 - MCC: 0.8964\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - accuracy: 0.9419 - loss: 0.1405 - val_accuracy: 0.9484 - val_loss: 0.1266 - mcc: 0.8964\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9425 - loss: 0.1382\n","Epoch 12 - MCC: 0.8801\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9424 - loss: 0.1384 - val_accuracy: 0.9393 - val_loss: 0.1473 - mcc: 0.8801\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9439 - loss: 0.1373\n","Epoch 13 - MCC: 0.8994\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 188ms/step - accuracy: 0.9438 - loss: 0.1376 - val_accuracy: 0.9499 - val_loss: 0.1252 - mcc: 0.8994\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9451 - loss: 0.1350\n","Epoch 14 - MCC: 0.8960\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 0.9449 - loss: 0.1353 - val_accuracy: 0.9479 - val_loss: 0.1281 - mcc: 0.8960\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9440 - loss: 0.1354\n","Epoch 15 - MCC: 0.9014\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9439 - loss: 0.1356 - val_accuracy: 0.9509 - val_loss: 0.1220 - mcc: 0.9014\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9468 - loss: 0.1292\n","Epoch 16 - MCC: 0.9037\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 208ms/step - accuracy: 0.9466 - loss: 0.1295 - val_accuracy: 0.9520 - val_loss: 0.1179 - mcc: 0.9037\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9417 - loss: 0.1412\n","Epoch 17 - MCC: 0.9081\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 157ms/step - accuracy: 0.9418 - loss: 0.1410 - val_accuracy: 0.9542 - val_loss: 0.1142 - mcc: 0.9081\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.9446 - loss: 0.1342\n","Epoch 18 - MCC: 0.9043\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 200ms/step - accuracy: 0.9445 - loss: 0.1343 - val_accuracy: 0.9522 - val_loss: 0.1188 - mcc: 0.9043\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9441 - loss: 0.1343\n","Epoch 19 - MCC: 0.9047\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9441 - loss: 0.1342 - val_accuracy: 0.9524 - val_loss: 0.1139 - mcc: 0.9047\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9448 - loss: 0.1359\n","Epoch 20 - MCC: 0.9064\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9448 - loss: 0.1356 - val_accuracy: 0.9534 - val_loss: 0.1135 - mcc: 0.9064\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9463 - loss: 0.1305\n","Epoch 21 - MCC: 0.9070\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - accuracy: 0.9463 - loss: 0.1303 - val_accuracy: 0.9534 - val_loss: 0.1159 - mcc: 0.9070\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9514 - loss: 0.1202\n","Epoch 22 - MCC: 0.9106\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9513 - loss: 0.1205 - val_accuracy: 0.9554 - val_loss: 0.1107 - mcc: 0.9106\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9481 - loss: 0.1275\n","Epoch 23 - MCC: 0.9134\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.9481 - loss: 0.1274 - val_accuracy: 0.9569 - val_loss: 0.1060 - mcc: 0.9134\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9489 - loss: 0.1241\n","Epoch 24 - MCC: 0.9111\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9490 - loss: 0.1240 - val_accuracy: 0.9557 - val_loss: 0.1088 - mcc: 0.9111\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9532 - loss: 0.1143\n","Epoch 25 - MCC: 0.9118\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9531 - loss: 0.1146 - val_accuracy: 0.9561 - val_loss: 0.1084 - mcc: 0.9118\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9480 - loss: 0.1262\n","Epoch 26 - MCC: 0.9147\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.9481 - loss: 0.1260 - val_accuracy: 0.9575 - val_loss: 0.1027 - mcc: 0.9147\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9481 - loss: 0.1269\n","Epoch 27 - MCC: 0.9132\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9482 - loss: 0.1267 - val_accuracy: 0.9567 - val_loss: 0.1066 - mcc: 0.9132\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9520 - loss: 0.1185\n","Epoch 28 - MCC: 0.9080\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 173ms/step - accuracy: 0.9519 - loss: 0.1186 - val_accuracy: 0.9538 - val_loss: 0.1114 - mcc: 0.9080\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9480 - loss: 0.1279\n","Epoch 29 - MCC: 0.9162\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - accuracy: 0.9480 - loss: 0.1277 - val_accuracy: 0.9581 - val_loss: 0.1048 - mcc: 0.9162\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9476 - loss: 0.1273\n","Epoch 30 - MCC: 0.9143\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9477 - loss: 0.1269 - val_accuracy: 0.9573 - val_loss: 0.1028 - mcc: 0.9143\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 2\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.6706 - loss: 0.5973\n","Epoch 1 - MCC: 0.6616\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - accuracy: 0.6736 - loss: 0.5939 - val_accuracy: 0.8296 - val_loss: 0.3831 - mcc: 0.6616\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8456 - loss: 0.3442\n","Epoch 2 - MCC: 0.7678\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 176ms/step - accuracy: 0.8463 - loss: 0.3429 - val_accuracy: 0.8840 - val_loss: 0.2697 - mcc: 0.7678\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9019 - loss: 0.2315\n","Epoch 3 - MCC: 0.7897\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9018 - loss: 0.2315 - val_accuracy: 0.8920 - val_loss: 0.2491 - mcc: 0.7897\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9118 - loss: 0.2059\n","Epoch 4 - MCC: 0.8224\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - accuracy: 0.9119 - loss: 0.2056 - val_accuracy: 0.9109 - val_loss: 0.2082 - mcc: 0.8224\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9218 - loss: 0.1883\n","Epoch 5 - MCC: 0.8375\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - accuracy: 0.9220 - loss: 0.1878 - val_accuracy: 0.9188 - val_loss: 0.1903 - mcc: 0.8375\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9319 - loss: 0.1648\n","Epoch 6 - MCC: 0.8458\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9319 - loss: 0.1648 - val_accuracy: 0.9231 - val_loss: 0.1791 - mcc: 0.8458\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9336 - loss: 0.1584\n","Epoch 7 - MCC: 0.8475\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.9337 - loss: 0.1584 - val_accuracy: 0.9236 - val_loss: 0.1788 - mcc: 0.8475\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9427 - loss: 0.1403\n","Epoch 8 - MCC: 0.8580\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9426 - loss: 0.1406 - val_accuracy: 0.9291 - val_loss: 0.1655 - mcc: 0.8580\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9410 - loss: 0.1416\n","Epoch 9 - MCC: 0.8537\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9409 - loss: 0.1417 - val_accuracy: 0.9260 - val_loss: 0.1746 - mcc: 0.8537\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.9436 - loss: 0.1400\n","Epoch 10 - MCC: 0.8682\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 200ms/step - accuracy: 0.9436 - loss: 0.1401 - val_accuracy: 0.9342 - val_loss: 0.1566 - mcc: 0.8682\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9421 - loss: 0.1408\n","Epoch 11 - MCC: 0.8695\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9422 - loss: 0.1406 - val_accuracy: 0.9348 - val_loss: 0.1543 - mcc: 0.8695\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9457 - loss: 0.1330\n","Epoch 12 - MCC: 0.8655\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9457 - loss: 0.1329 - val_accuracy: 0.9329 - val_loss: 0.1579 - mcc: 0.8655\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9462 - loss: 0.1330\n","Epoch 13 - MCC: 0.8646\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.9461 - loss: 0.1330 - val_accuracy: 0.9325 - val_loss: 0.1597 - mcc: 0.8646\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9462 - loss: 0.1316\n","Epoch 14 - MCC: 0.8692\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9463 - loss: 0.1315 - val_accuracy: 0.9347 - val_loss: 0.1538 - mcc: 0.8692\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9505 - loss: 0.1211\n","Epoch 15 - MCC: 0.8743\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 0.9505 - loss: 0.1212 - val_accuracy: 0.9372 - val_loss: 0.1492 - mcc: 0.8743\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9512 - loss: 0.1201\n","Epoch 16 - MCC: 0.8720\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.9511 - loss: 0.1202 - val_accuracy: 0.9357 - val_loss: 0.1545 - mcc: 0.8720\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9513 - loss: 0.1205\n","Epoch 17 - MCC: 0.8763\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9513 - loss: 0.1205 - val_accuracy: 0.9382 - val_loss: 0.1481 - mcc: 0.8763\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9512 - loss: 0.1200\n","Epoch 18 - MCC: 0.8766\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 199ms/step - accuracy: 0.9512 - loss: 0.1200 - val_accuracy: 0.9381 - val_loss: 0.1482 - mcc: 0.8766\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9518 - loss: 0.1173\n","Epoch 19 - MCC: 0.8731\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9518 - loss: 0.1174 - val_accuracy: 0.9362 - val_loss: 0.1564 - mcc: 0.8731\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9524 - loss: 0.1160\n","Epoch 20 - MCC: 0.8760\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9523 - loss: 0.1162 - val_accuracy: 0.9381 - val_loss: 0.1487 - mcc: 0.8760\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9531 - loss: 0.1147\n","Epoch 21 - MCC: 0.8765\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 200ms/step - accuracy: 0.9531 - loss: 0.1148 - val_accuracy: 0.9384 - val_loss: 0.1477 - mcc: 0.8765\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9530 - loss: 0.1156\n","Epoch 22 - MCC: 0.8797\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9530 - loss: 0.1156 - val_accuracy: 0.9400 - val_loss: 0.1441 - mcc: 0.8797\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9529 - loss: 0.1148\n","Epoch 23 - MCC: 0.8829\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9529 - loss: 0.1148 - val_accuracy: 0.9415 - val_loss: 0.1399 - mcc: 0.8829\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9550 - loss: 0.1096\n","Epoch 24 - MCC: 0.8803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 200ms/step - accuracy: 0.9549 - loss: 0.1097 - val_accuracy: 0.9403 - val_loss: 0.1407 - mcc: 0.8803\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9543 - loss: 0.1123\n","Epoch 25 - MCC: 0.8821\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9543 - loss: 0.1124 - val_accuracy: 0.9412 - val_loss: 0.1394 - mcc: 0.8821\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9579 - loss: 0.1044\n","Epoch 26 - MCC: 0.8859\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 180ms/step - accuracy: 0.9578 - loss: 0.1046 - val_accuracy: 0.9431 - val_loss: 0.1393 - mcc: 0.8859\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.9586 - loss: 0.1037\n","Epoch 27 - MCC: 0.8845\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 189ms/step - accuracy: 0.9585 - loss: 0.1038 - val_accuracy: 0.9423 - val_loss: 0.1372 - mcc: 0.8845\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9623 - loss: 0.0930\n","Epoch 28 - MCC: 0.8856\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9621 - loss: 0.0935 - val_accuracy: 0.9429 - val_loss: 0.1382 - mcc: 0.8856\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9532 - loss: 0.1137\n","Epoch 29 - MCC: 0.8869\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - accuracy: 0.9534 - loss: 0.1134 - val_accuracy: 0.9436 - val_loss: 0.1404 - mcc: 0.8869\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9556 - loss: 0.1095\n","Epoch 30 - MCC: 0.8836\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.9557 - loss: 0.1093 - val_accuracy: 0.9418 - val_loss: 0.1383 - mcc: 0.8836\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 3\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.6478 - loss: 0.6184\n","Epoch 1 - MCC: 0.6700\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 256ms/step - accuracy: 0.6511 - loss: 0.6152 - val_accuracy: 0.8321 - val_loss: 0.3769 - mcc: 0.6700\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.8490 - loss: 0.3457\n","Epoch 2 - MCC: 0.7696\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 157ms/step - accuracy: 0.8492 - loss: 0.3452 - val_accuracy: 0.8852 - val_loss: 0.2750 - mcc: 0.7696\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8866 - loss: 0.2682\n","Epoch 3 - MCC: 0.8194\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 205ms/step - accuracy: 0.8869 - loss: 0.2675 - val_accuracy: 0.9101 - val_loss: 0.2142 - mcc: 0.8194\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9110 - loss: 0.2121\n","Epoch 4 - MCC: 0.8387\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9110 - loss: 0.2119 - val_accuracy: 0.9197 - val_loss: 0.1902 - mcc: 0.8387\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9219 - loss: 0.1864\n","Epoch 5 - MCC: 0.8432\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - accuracy: 0.9217 - loss: 0.1867 - val_accuracy: 0.9212 - val_loss: 0.1883 - mcc: 0.8432\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9155 - loss: 0.1960\n","Epoch 6 - MCC: 0.8545\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 186ms/step - accuracy: 0.9156 - loss: 0.1958 - val_accuracy: 0.9275 - val_loss: 0.1748 - mcc: 0.8545\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9240 - loss: 0.1824\n","Epoch 7 - MCC: 0.8630\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9241 - loss: 0.1822 - val_accuracy: 0.9318 - val_loss: 0.1634 - mcc: 0.8630\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9298 - loss: 0.1679\n","Epoch 8 - MCC: 0.8734\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - accuracy: 0.9298 - loss: 0.1678 - val_accuracy: 0.9370 - val_loss: 0.1525 - mcc: 0.8734\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.9331 - loss: 0.1618\n","Epoch 9 - MCC: 0.8768\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 173ms/step - accuracy: 0.9331 - loss: 0.1617 - val_accuracy: 0.9386 - val_loss: 0.1506 - mcc: 0.8768\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9348 - loss: 0.1566\n","Epoch 10 - MCC: 0.8820\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9350 - loss: 0.1564 - val_accuracy: 0.9412 - val_loss: 0.1410 - mcc: 0.8820\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9359 - loss: 0.1526\n","Epoch 11 - MCC: 0.8846\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.9360 - loss: 0.1524 - val_accuracy: 0.9425 - val_loss: 0.1389 - mcc: 0.8846\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9439 - loss: 0.1346\n","Epoch 12 - MCC: 0.8843\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9438 - loss: 0.1349 - val_accuracy: 0.9424 - val_loss: 0.1402 - mcc: 0.8843\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9447 - loss: 0.1336\n","Epoch 13 - MCC: 0.8929\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 174ms/step - accuracy: 0.9447 - loss: 0.1339 - val_accuracy: 0.9467 - val_loss: 0.1311 - mcc: 0.8929\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9445 - loss: 0.1339\n","Epoch 14 - MCC: 0.8939\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 196ms/step - accuracy: 0.9445 - loss: 0.1339 - val_accuracy: 0.9470 - val_loss: 0.1285 - mcc: 0.8939\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9426 - loss: 0.1412\n","Epoch 15 - MCC: 0.8910\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9427 - loss: 0.1410 - val_accuracy: 0.9456 - val_loss: 0.1324 - mcc: 0.8910\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9446 - loss: 0.1344\n","Epoch 16 - MCC: 0.8945\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 182ms/step - accuracy: 0.9446 - loss: 0.1342 - val_accuracy: 0.9474 - val_loss: 0.1298 - mcc: 0.8945\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9401 - loss: 0.1468\n","Epoch 17 - MCC: 0.8939\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 175ms/step - accuracy: 0.9402 - loss: 0.1465 - val_accuracy: 0.9469 - val_loss: 0.1318 - mcc: 0.8939\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9452 - loss: 0.1334\n","Epoch 18 - MCC: 0.8962\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9452 - loss: 0.1335 - val_accuracy: 0.9483 - val_loss: 0.1265 - mcc: 0.8962\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9486 - loss: 0.1256\n","Epoch 19 - MCC: 0.8984\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.9485 - loss: 0.1258 - val_accuracy: 0.9494 - val_loss: 0.1266 - mcc: 0.8984\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9483 - loss: 0.1263\n","Epoch 20 - MCC: 0.8970\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 158ms/step - accuracy: 0.9483 - loss: 0.1264 - val_accuracy: 0.9487 - val_loss: 0.1253 - mcc: 0.8970\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9472 - loss: 0.1295\n","Epoch 21 - MCC: 0.8982\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - accuracy: 0.9472 - loss: 0.1295 - val_accuracy: 0.9493 - val_loss: 0.1254 - mcc: 0.8982\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9509 - loss: 0.1192\n","Epoch 22 - MCC: 0.9003\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.9508 - loss: 0.1194 - val_accuracy: 0.9503 - val_loss: 0.1216 - mcc: 0.9003\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9502 - loss: 0.1202\n","Epoch 23 - MCC: 0.8973\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - accuracy: 0.9501 - loss: 0.1204 - val_accuracy: 0.9489 - val_loss: 0.1253 - mcc: 0.8973\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9499 - loss: 0.1224\n","Epoch 24 - MCC: 0.8977\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.9499 - loss: 0.1224 - val_accuracy: 0.9489 - val_loss: 0.1251 - mcc: 0.8977\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9471 - loss: 0.1267\n","Epoch 25 - MCC: 0.9004\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9472 - loss: 0.1265 - val_accuracy: 0.9504 - val_loss: 0.1210 - mcc: 0.9004\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9509 - loss: 0.1203\n","Epoch 26 - MCC: 0.9038\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 199ms/step - accuracy: 0.9509 - loss: 0.1204 - val_accuracy: 0.9521 - val_loss: 0.1182 - mcc: 0.9038\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9503 - loss: 0.1204\n","Epoch 27 - MCC: 0.9008\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9503 - loss: 0.1204 - val_accuracy: 0.9506 - val_loss: 0.1232 - mcc: 0.9008\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9523 - loss: 0.1170\n","Epoch 28 - MCC: 0.9013\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9523 - loss: 0.1172 - val_accuracy: 0.9507 - val_loss: 0.1227 - mcc: 0.9013\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9493 - loss: 0.1235\n","Epoch 29 - MCC: 0.8999\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 203ms/step - accuracy: 0.9493 - loss: 0.1233 - val_accuracy: 0.9501 - val_loss: 0.1225 - mcc: 0.8999\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9530 - loss: 0.1142\n","Epoch 30 - MCC: 0.8979\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.9529 - loss: 0.1144 - val_accuracy: 0.9490 - val_loss: 0.1241 - mcc: 0.8979\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 4\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.6202 - loss: 0.6177\n","Epoch 1 - MCC: 0.7110\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 262ms/step - accuracy: 0.6236 - loss: 0.6149 - val_accuracy: 0.8559 - val_loss: 0.3528 - mcc: 0.7110\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8553 - loss: 0.3415\n","Epoch 2 - MCC: 0.7952\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 181ms/step - accuracy: 0.8556 - loss: 0.3405 - val_accuracy: 0.8967 - val_loss: 0.2494 - mcc: 0.7952\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.8995 - loss: 0.2376\n","Epoch 3 - MCC: 0.8519\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 174ms/step - accuracy: 0.8997 - loss: 0.2371 - val_accuracy: 0.9260 - val_loss: 0.1824 - mcc: 0.8519\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9168 - loss: 0.1983\n","Epoch 4 - MCC: 0.8635\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9169 - loss: 0.1980 - val_accuracy: 0.9319 - val_loss: 0.1666 - mcc: 0.8635\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9225 - loss: 0.1838\n","Epoch 5 - MCC: 0.8686\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 203ms/step - accuracy: 0.9225 - loss: 0.1838 - val_accuracy: 0.9344 - val_loss: 0.1615 - mcc: 0.8686\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9270 - loss: 0.1727\n","Epoch 6 - MCC: 0.8763\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9271 - loss: 0.1726 - val_accuracy: 0.9382 - val_loss: 0.1499 - mcc: 0.8763\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9320 - loss: 0.1618\n","Epoch 7 - MCC: 0.8825\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9320 - loss: 0.1618 - val_accuracy: 0.9413 - val_loss: 0.1448 - mcc: 0.8825\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9337 - loss: 0.1596\n","Epoch 8 - MCC: 0.8812\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.9337 - loss: 0.1597 - val_accuracy: 0.9405 - val_loss: 0.1476 - mcc: 0.8812\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9312 - loss: 0.1635\n","Epoch 9 - MCC: 0.8900\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9313 - loss: 0.1633 - val_accuracy: 0.9451 - val_loss: 0.1372 - mcc: 0.8900\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9399 - loss: 0.1472\n","Epoch 10 - MCC: 0.8833\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step - accuracy: 0.9399 - loss: 0.1472 - val_accuracy: 0.9418 - val_loss: 0.1432 - mcc: 0.8833\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9395 - loss: 0.1477\n","Epoch 11 - MCC: 0.8922\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9396 - loss: 0.1475 - val_accuracy: 0.9459 - val_loss: 0.1358 - mcc: 0.8922\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9436 - loss: 0.1369\n","Epoch 12 - MCC: 0.8997\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 165ms/step - accuracy: 0.9435 - loss: 0.1371 - val_accuracy: 0.9499 - val_loss: 0.1251 - mcc: 0.8997\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9433 - loss: 0.1369\n","Epoch 13 - MCC: 0.8992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.9433 - loss: 0.1369 - val_accuracy: 0.9497 - val_loss: 0.1246 - mcc: 0.8992\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9423 - loss: 0.1391\n","Epoch 14 - MCC: 0.8984\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9424 - loss: 0.1390 - val_accuracy: 0.9490 - val_loss: 0.1284 - mcc: 0.8984\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9412 - loss: 0.1419\n","Epoch 15 - MCC: 0.8691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9413 - loss: 0.1418 - val_accuracy: 0.9340 - val_loss: 0.1579 - mcc: 0.8691\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9396 - loss: 0.1474\n","Epoch 16 - MCC: 0.8975\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - accuracy: 0.9395 - loss: 0.1476 - val_accuracy: 0.9488 - val_loss: 0.1278 - mcc: 0.8975\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9483 - loss: 0.1255\n","Epoch 17 - MCC: 0.9020\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9481 - loss: 0.1259 - val_accuracy: 0.9510 - val_loss: 0.1230 - mcc: 0.9020\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9461 - loss: 0.1315\n","Epoch 18 - MCC: 0.9004\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9461 - loss: 0.1315 - val_accuracy: 0.9502 - val_loss: 0.1250 - mcc: 0.9004\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9452 - loss: 0.1336\n","Epoch 19 - MCC: 0.9011\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.9452 - loss: 0.1335 - val_accuracy: 0.9507 - val_loss: 0.1231 - mcc: 0.9011\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9425 - loss: 0.1367\n","Epoch 20 - MCC: 0.9021\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - accuracy: 0.9427 - loss: 0.1363 - val_accuracy: 0.9511 - val_loss: 0.1218 - mcc: 0.9021\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9456 - loss: 0.1324\n","Epoch 21 - MCC: 0.9056\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - accuracy: 0.9457 - loss: 0.1321 - val_accuracy: 0.9528 - val_loss: 0.1178 - mcc: 0.9056\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9505 - loss: 0.1194\n","Epoch 22 - MCC: 0.9025\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9504 - loss: 0.1196 - val_accuracy: 0.9513 - val_loss: 0.1206 - mcc: 0.9025\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9532 - loss: 0.1159\n","Epoch 23 - MCC: 0.9022\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9531 - loss: 0.1161 - val_accuracy: 0.9512 - val_loss: 0.1204 - mcc: 0.9022\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9483 - loss: 0.1243\n","Epoch 24 - MCC: 0.9089\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.9483 - loss: 0.1244 - val_accuracy: 0.9545 - val_loss: 0.1145 - mcc: 0.9089\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9489 - loss: 0.1264\n","Epoch 25 - MCC: 0.9069\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9490 - loss: 0.1262 - val_accuracy: 0.9532 - val_loss: 0.1192 - mcc: 0.9069\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9539 - loss: 0.1126\n","Epoch 26 - MCC: 0.9117\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 164ms/step - accuracy: 0.9538 - loss: 0.1128 - val_accuracy: 0.9559 - val_loss: 0.1097 - mcc: 0.9117\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9544 - loss: 0.1108\n","Epoch 27 - MCC: 0.9125\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.9543 - loss: 0.1109 - val_accuracy: 0.9563 - val_loss: 0.1104 - mcc: 0.9125\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9505 - loss: 0.1189\n","Epoch 28 - MCC: 0.9110\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9506 - loss: 0.1187 - val_accuracy: 0.9555 - val_loss: 0.1116 - mcc: 0.9110\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9510 - loss: 0.1202\n","Epoch 29 - MCC: 0.9130\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.9511 - loss: 0.1199 - val_accuracy: 0.9566 - val_loss: 0.1098 - mcc: 0.9130\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9525 - loss: 0.1146\n","Epoch 30 - MCC: 0.9162\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9525 - loss: 0.1146 - val_accuracy: 0.9582 - val_loss: 0.1055 - mcc: 0.9162\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 5\n","Epoch 1/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.6124 - loss: 0.6132\n","Epoch 1 - MCC: 0.6408\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 264ms/step - accuracy: 0.6164 - loss: 0.6100 - val_accuracy: 0.8163 - val_loss: 0.4030 - mcc: 0.6408\n","Epoch 2/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8439 - loss: 0.3528\n","Epoch 2 - MCC: 0.7555\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.8445 - loss: 0.3517 - val_accuracy: 0.8749 - val_loss: 0.2896 - mcc: 0.7555\n","Epoch 3/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.8986 - loss: 0.2417\n","Epoch 3 - MCC: 0.8153\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 179ms/step - accuracy: 0.8988 - loss: 0.2413 - val_accuracy: 0.9076 - val_loss: 0.2201 - mcc: 0.8153\n","Epoch 4/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9191 - loss: 0.1934\n","Epoch 4 - MCC: 0.8248\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9189 - loss: 0.1937 - val_accuracy: 0.9125 - val_loss: 0.2045 - mcc: 0.8248\n","Epoch 5/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9165 - loss: 0.1957\n","Epoch 5 - MCC: 0.8258\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.9167 - loss: 0.1955 - val_accuracy: 0.9130 - val_loss: 0.2007 - mcc: 0.8258\n","Epoch 6/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9193 - loss: 0.1913\n","Epoch 6 - MCC: 0.8411\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 199ms/step - accuracy: 0.9194 - loss: 0.1909 - val_accuracy: 0.9205 - val_loss: 0.1857 - mcc: 0.8411\n","Epoch 7/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9229 - loss: 0.1831\n","Epoch 7 - MCC: 0.8484\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9230 - loss: 0.1828 - val_accuracy: 0.9239 - val_loss: 0.1785 - mcc: 0.8484\n","Epoch 8/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9307 - loss: 0.1645\n","Epoch 8 - MCC: 0.8613\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9308 - loss: 0.1643 - val_accuracy: 0.9306 - val_loss: 0.1637 - mcc: 0.8613\n","Epoch 9/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9374 - loss: 0.1500\n","Epoch 9 - MCC: 0.8557\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 198ms/step - accuracy: 0.9373 - loss: 0.1502 - val_accuracy: 0.9266 - val_loss: 0.1738 - mcc: 0.8557\n","Epoch 10/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9363 - loss: 0.1522\n","Epoch 10 - MCC: 0.8579\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9363 - loss: 0.1521 - val_accuracy: 0.9290 - val_loss: 0.1648 - mcc: 0.8579\n","Epoch 11/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9413 - loss: 0.1427\n","Epoch 11 - MCC: 0.8746\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 187ms/step - accuracy: 0.9412 - loss: 0.1429 - val_accuracy: 0.9371 - val_loss: 0.1495 - mcc: 0.8746\n","Epoch 12/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9418 - loss: 0.1406\n","Epoch 12 - MCC: 0.8640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 179ms/step - accuracy: 0.9418 - loss: 0.1405 - val_accuracy: 0.9313 - val_loss: 0.1623 - mcc: 0.8640\n","Epoch 13/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9400 - loss: 0.1456\n","Epoch 13 - MCC: 0.8773\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9402 - loss: 0.1454 - val_accuracy: 0.9384 - val_loss: 0.1461 - mcc: 0.8773\n","Epoch 14/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9439 - loss: 0.1370\n","Epoch 14 - MCC: 0.8793\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.9439 - loss: 0.1369 - val_accuracy: 0.9397 - val_loss: 0.1445 - mcc: 0.8793\n","Epoch 15/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9444 - loss: 0.1338\n","Epoch 15 - MCC: 0.8855\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9444 - loss: 0.1338 - val_accuracy: 0.9427 - val_loss: 0.1397 - mcc: 0.8855\n","Epoch 16/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9444 - loss: 0.1354\n","Epoch 16 - MCC: 0.8798\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9444 - loss: 0.1353 - val_accuracy: 0.9400 - val_loss: 0.1485 - mcc: 0.8798\n","Epoch 17/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9448 - loss: 0.1374\n","Epoch 17 - MCC: 0.8823\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 202ms/step - accuracy: 0.9448 - loss: 0.1374 - val_accuracy: 0.9410 - val_loss: 0.1412 - mcc: 0.8823\n","Epoch 18/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9481 - loss: 0.1247\n","Epoch 18 - MCC: 0.8890\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9481 - loss: 0.1248 - val_accuracy: 0.9445 - val_loss: 0.1325 - mcc: 0.8890\n","Epoch 19/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9477 - loss: 0.1282\n","Epoch 19 - MCC: 0.8851\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 179ms/step - accuracy: 0.9477 - loss: 0.1282 - val_accuracy: 0.9427 - val_loss: 0.1367 - mcc: 0.8851\n","Epoch 20/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9469 - loss: 0.1304\n","Epoch 20 - MCC: 0.8860\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 177ms/step - accuracy: 0.9470 - loss: 0.1302 - val_accuracy: 0.9425 - val_loss: 0.1410 - mcc: 0.8860\n","Epoch 21/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9501 - loss: 0.1227\n","Epoch 21 - MCC: 0.8866\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9500 - loss: 0.1229 - val_accuracy: 0.9434 - val_loss: 0.1341 - mcc: 0.8866\n","Epoch 22/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9466 - loss: 0.1273\n","Epoch 22 - MCC: 0.8878\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 187ms/step - accuracy: 0.9467 - loss: 0.1272 - val_accuracy: 0.9439 - val_loss: 0.1360 - mcc: 0.8878\n","Epoch 23/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9494 - loss: 0.1251\n","Epoch 23 - MCC: 0.8897\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9494 - loss: 0.1251 - val_accuracy: 0.9449 - val_loss: 0.1340 - mcc: 0.8897\n","Epoch 24/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9514 - loss: 0.1189\n","Epoch 24 - MCC: 0.8946\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9514 - loss: 0.1190 - val_accuracy: 0.9474 - val_loss: 0.1281 - mcc: 0.8946\n","Epoch 25/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.9532 - loss: 0.1149\n","Epoch 25 - MCC: 0.8927\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 189ms/step - accuracy: 0.9531 - loss: 0.1151 - val_accuracy: 0.9463 - val_loss: 0.1306 - mcc: 0.8927\n","Epoch 26/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9531 - loss: 0.1160\n","Epoch 26 - MCC: 0.8968\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9531 - loss: 0.1161 - val_accuracy: 0.9484 - val_loss: 0.1267 - mcc: 0.8968\n","Epoch 27/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9525 - loss: 0.1180\n","Epoch 27 - MCC: 0.8977\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9525 - loss: 0.1179 - val_accuracy: 0.9489 - val_loss: 0.1255 - mcc: 0.8977\n","Epoch 28/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9559 - loss: 0.1107\n","Epoch 28 - MCC: 0.8982\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - accuracy: 0.9558 - loss: 0.1108 - val_accuracy: 0.9491 - val_loss: 0.1252 - mcc: 0.8982\n","Epoch 29/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9590 - loss: 0.1017\n","Epoch 29 - MCC: 0.8992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9588 - loss: 0.1022 - val_accuracy: 0.9495 - val_loss: 0.1232 - mcc: 0.8992\n","Epoch 30/30\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9568 - loss: 0.1074\n","Epoch 30 - MCC: 0.8955\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 200ms/step - accuracy: 0.9567 - loss: 0.1075 - val_accuracy: 0.9477 - val_loss: 0.1260 - mcc: 0.8955\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9581933333333333,\n","              'mean': 0.9508146666666667,\n","              'min': 0.9418466666666667,\n","              'std': 0.006158864830469967},\n"," 'Inference Time (s/sample)': {'max': 0.00354550838470459,\n","                               'mean': 0.003401020050048828,\n","                               'min': 0.003199038505554199,\n","                               'std': 0.00016094781836579713},\n"," 'MCC': {'max': 0.9161910083518204,\n","         'mean': 0.901506788087494,\n","         'min': 0.8836338629566973,\n","         'std': 0.012218529779988567},\n"," 'Parameters': 76577,\n"," 'Train Time (s)': {'max': 155.4189531803131,\n","                    'mean': 152.42602577209473,\n","                    'min': 149.31457614898682,\n","                    'std': 2.463563780602621},\n"," 'Training Accuracy': [[0.7386633157730103,\n","                        0.8706033825874329,\n","                        0.9041282534599304,\n","                        0.9146900177001953,\n","                        0.9226967096328735,\n","                        0.926841676235199,\n","                        0.929236650466919,\n","                        0.931179940700531,\n","                        0.9343833923339844,\n","                        0.9367933869361877,\n","                        0.9380300045013428,\n","                        0.94089674949646,\n","                        0.9400634765625,\n","                        0.9407417178153992,\n","                        0.9422800540924072,\n","                        0.9430133700370789,\n","                        0.9433450102806091,\n","                        0.9433899521827698,\n","                        0.9446050524711609,\n","                        0.9463099837303162,\n","                        0.9478051066398621,\n","                        0.9477250576019287,\n","                        0.9484216570854187,\n","                        0.9502165913581848,\n","                        0.9503783583641052,\n","                        0.9506649971008301,\n","                        0.9497449398040771,\n","                        0.9502884745597839,\n","                        0.9496150016784668,\n","                        0.9515416622161865],\n","                       [0.7464383840560913,\n","                        0.8636815547943115,\n","                        0.9011250138282776,\n","                        0.915618360042572,\n","                        0.9270117282867432,\n","                        0.9317934513092041,\n","                        0.9337483048439026,\n","                        0.9391299486160278,\n","                        0.939663290977478,\n","                        0.9422799944877625,\n","                        0.9451133608818054,\n","                        0.9463749527931213,\n","                        0.9450432062149048,\n","                        0.947776734828949,\n","                        0.9494550228118896,\n","                        0.9489800930023193,\n","                        0.9502232074737549,\n","                        0.9510583281517029,\n","                        0.9510416388511658,\n","                        0.9506449699401855,\n","                        0.9512333869934082,\n","                        0.953091561794281,\n","                        0.9529200792312622,\n","                        0.954481840133667,\n","                        0.9542416334152222,\n","                        0.9556151032447815,\n","                        0.9572200179100037,\n","                        0.9569433331489563,\n","                        0.9571183919906616,\n","                        0.9581149220466614],\n","                       [0.7340566515922546,\n","                        0.8545000553131104,\n","                        0.8945950269699097,\n","                        0.9122200608253479,\n","                        0.9180467128753662,\n","                        0.9182201623916626,\n","                        0.9267834424972534,\n","                        0.9315418004989624,\n","                        0.9343915581703186,\n","                        0.9376933574676514,\n","                        0.9383583068847656,\n","                        0.9406133890151978,\n","                        0.9425716996192932,\n","                        0.9448866248130798,\n","                        0.9445500373840332,\n","                        0.9464199542999268,\n","                        0.9434815049171448,\n","                        0.9445682764053345,\n","                        0.9461798667907715,\n","                        0.9470732808113098,\n","                        0.9471250772476196,\n","                        0.9484683275222778,\n","                        0.9479849338531494,\n","                        0.9496598839759827,\n","                        0.9498631954193115,\n","                        0.9495198726654053,\n","                        0.9506449699401855,\n","                        0.9501217007637024,\n","                        0.950814962387085,\n","                        0.9512866735458374],\n","                       [0.7083900570869446,\n","                        0.8647517561912537,\n","                        0.904938280582428,\n","                        0.9201583862304688,\n","                        0.9226550459861755,\n","                        0.9291033744812012,\n","                        0.9314966797828674,\n","                        0.9333799481391907,\n","                        0.9338200092315674,\n","                        0.9394364953041077,\n","                        0.940786600112915,\n","                        0.9417883157730103,\n","                        0.9431751370429993,\n","                        0.9430632591247559,\n","                        0.9423283338546753,\n","                        0.9367682933807373,\n","                        0.9439215660095215,\n","                        0.9458348155021667,\n","                        0.9463316202163696,\n","                        0.9473599791526794,\n","                        0.9484348893165588,\n","                        0.949074923992157,\n","                        0.9495667219161987,\n","                        0.9473617672920227,\n","                        0.950404942035675,\n","                        0.950401782989502,\n","                        0.9527233242988586,\n","                        0.9527283310890198,\n","                        0.9536917209625244,\n","                        0.953624963760376],\n","                       [0.7155401110649109,\n","                        0.8588582873344421,\n","                        0.903249979019165,\n","                        0.9148799777030945,\n","                        0.9195016622543335,\n","                        0.9236632585525513,\n","                        0.9257215261459351,\n","                        0.9329800605773926,\n","                        0.9353951215744019,\n","                        0.93708336353302,\n","                        0.9388198852539062,\n","                        0.9422667026519775,\n","                        0.942821741104126,\n","                        0.9443832635879517,\n","                        0.9440599083900452,\n","                        0.9451000094413757,\n","                        0.944736659526825,\n","                        0.9473515152931213,\n","                        0.9479668140411377,\n","                        0.9485666751861572,\n","                        0.9483649730682373,\n","                        0.9490997791290283,\n","                        0.9502465724945068,\n","                        0.9508550763130188,\n","                        0.9499250054359436,\n","                        0.9513983130455017,\n","                        0.9532949328422546,\n","                        0.9545400142669678,\n","                        0.9542965888977051,\n","                        0.9554298520088196]],\n"," 'Training Loss': [[0.5002095103263855,\n","                    0.29775750637054443,\n","                    0.22550329566001892,\n","                    0.20075710117816925,\n","                    0.18454720079898834,\n","                    0.1747911423444748,\n","                    0.16776730120182037,\n","                    0.16411226987838745,\n","                    0.15568141639232635,\n","                    0.15298251807689667,\n","                    0.14929857850074768,\n","                    0.14353175461292267,\n","                    0.1455685943365097,\n","                    0.14327186346054077,\n","                    0.1390354186296463,\n","                    0.13755860924720764,\n","                    0.13682572543621063,\n","                    0.13719512522220612,\n","                    0.13382770121097565,\n","                    0.13054561614990234,\n","                    0.1272105723619461,\n","                    0.12817956507205963,\n","                    0.12585291266441345,\n","                    0.12332432717084885,\n","                    0.12166967988014221,\n","                    0.12066464126110077,\n","                    0.12390684336423874,\n","                    0.1224401444196701,\n","                    0.12425132095813751,\n","                    0.11844739317893982],\n","                   [0.507420003414154,\n","                    0.3105236887931824,\n","                    0.2321077585220337,\n","                    0.19887694716453552,\n","                    0.1764143407344818,\n","                    0.16458404064178467,\n","                    0.1588638424873352,\n","                    0.14841683208942413,\n","                    0.14481312036514282,\n","                    0.14255858957767487,\n","                    0.13375966250896454,\n","                    0.1307772845029831,\n","                    0.13454678654670715,\n","                    0.12865391373634338,\n","                    0.12410377711057663,\n","                    0.12455008178949356,\n","                    0.12273169308900833,\n","                    0.11991966515779495,\n","                    0.11914870142936707,\n","                    0.1203392893075943,\n","                    0.11821608245372772,\n","                    0.11646198481321335,\n","                    0.11541062593460083,\n","                    0.11161728203296661,\n","                    0.11359930783510208,\n","                    0.10896226763725281,\n","                    0.10585010796785355,\n","                    0.10653839260339737,\n","                    0.10584745556116104,\n","                    0.10368340462446213],\n","                   [0.5347179770469666,\n","                    0.33425235748291016,\n","                    0.24966873228549957,\n","                    0.20871202647686005,\n","                    0.19496236741542816,\n","                    0.19229494035243988,\n","                    0.17544421553611755,\n","                    0.16373080015182495,\n","                    0.15893203020095825,\n","                    0.15057405829429626,\n","                    0.1474732756614685,\n","                    0.14286458492279053,\n","                    0.13982315361499786,\n","                    0.1335572898387909,\n","                    0.13484391570091248,\n","                    0.13031266629695892,\n","                    0.13822390139102936,\n","                    0.13442522287368774,\n","                    0.13052251935005188,\n","                    0.12860329449176788,\n","                    0.12952014803886414,\n","                    0.12426138669252396,\n","                    0.12535464763641357,\n","                    0.12251994758844376,\n","                    0.12141919136047363,\n","                    0.12286870926618576,\n","                    0.11975812911987305,\n","                    0.12107270956039429,\n","                    0.12005161494016647,\n","                    0.1190480887889862],\n","                   [0.5458428859710693,\n","                    0.3175252079963684,\n","                    0.22575724124908447,\n","                    0.19004079699516296,\n","                    0.18406230211257935,\n","                    0.16977423429489136,\n","                    0.16315317153930664,\n","                    0.16050536930561066,\n","                    0.15804564952850342,\n","                    0.1471056193113327,\n","                    0.14397037029266357,\n","                    0.14114685356616974,\n","                    0.13659827411174774,\n","                    0.1378384530544281,\n","                    0.13849198818206787,\n","                    0.15237809717655182,\n","                    0.1361578404903412,\n","                    0.13112908601760864,\n","                    0.13047336041927338,\n","                    0.12755930423736572,\n","                    0.12469582259654999,\n","                    0.12302751839160919,\n","                    0.1225547343492508,\n","                    0.12756863236427307,\n","                    0.12160363793373108,\n","                    0.11886759847402573,\n","                    0.11461833864450455,\n","                    0.11436831206083298,\n","                    0.11261753737926483,\n","                    0.11323937773704529],\n","                   [0.5313650369644165,\n","                    0.32361435890197754,\n","                    0.2304103821516037,\n","                    0.2010011076927185,\n","                    0.1911536604166031,\n","                    0.18105900287628174,\n","                    0.17619770765304565,\n","                    0.15994717180728912,\n","                    0.15464797616004944,\n","                    0.15188288688659668,\n","                    0.1475476771593094,\n","                    0.13828885555267334,\n","                    0.13990703225135803,\n","                    0.1345776617527008,\n","                    0.1345892995595932,\n","                    0.1328878104686737,\n","                    0.13552047312259674,\n","                    0.1278403103351593,\n","                    0.12743407487869263,\n","                    0.12503427267074585,\n","                    0.12643451988697052,\n","                    0.123198501765728,\n","                    0.12314639240503311,\n","                    0.12022366374731064,\n","                    0.12210610508918762,\n","                    0.11804193258285522,\n","                    0.11511306464672089,\n","                    0.1128278523683548,\n","                    0.11283104121685028,\n","                    0.10952921956777573]],\n"," 'Validation Accuracy': [[0.848153293132782,\n","                          0.8974867463111877,\n","                          0.9181066751480103,\n","                          0.9281932711601257,\n","                          0.9337199926376343,\n","                          0.935259997844696,\n","                          0.9362866282463074,\n","                          0.9407800436019897,\n","                          0.9439000487327576,\n","                          0.9474467635154724,\n","                          0.9483733177185059,\n","                          0.9392533302307129,\n","                          0.949906587600708,\n","                          0.9478867053985596,\n","                          0.9508733153343201,\n","                          0.9520334005355835,\n","                          0.9542199969291687,\n","                          0.9522332549095154,\n","                          0.9524000287055969,\n","                          0.9533599019050598,\n","                          0.9534400105476379,\n","                          0.9553933143615723,\n","                          0.9568800330162048,\n","                          0.9557400345802307,\n","                          0.9560799598693848,\n","                          0.9575133323669434,\n","                          0.956706702709198,\n","                          0.9538266062736511,\n","                          0.958139955997467,\n","                          0.9572933912277222],\n","                         [0.8295866250991821,\n","                          0.8839666843414307,\n","                          0.8919999599456787,\n","                          0.910860002040863,\n","                          0.9188467264175415,\n","                          0.9230732917785645,\n","                          0.9236066341400146,\n","                          0.9291466474533081,\n","                          0.925986647605896,\n","                          0.9342200756072998,\n","                          0.9348199367523193,\n","                          0.9329066276550293,\n","                          0.9324666857719421,\n","                          0.9347400069236755,\n","                          0.9371532201766968,\n","                          0.9356600046157837,\n","                          0.9382266402244568,\n","                          0.9380733966827393,\n","                          0.9362133741378784,\n","                          0.9381400346755981,\n","                          0.9383999705314636,\n","                          0.9399666786193848,\n","                          0.9415333271026611,\n","                          0.9402866363525391,\n","                          0.9411666989326477,\n","                          0.9430932402610779,\n","                          0.9423467516899109,\n","                          0.9429466128349304,\n","                          0.943560004234314,\n","                          0.9418466687202454],\n","                         [0.8321200013160706,\n","                          0.8852066993713379,\n","                          0.9100534319877625,\n","                          0.9196933507919312,\n","                          0.9212400913238525,\n","                          0.9275400042533875,\n","                          0.9317933917045593,\n","                          0.9369798898696899,\n","                          0.9385867118835449,\n","                          0.9411600232124329,\n","                          0.9425399899482727,\n","                          0.9423733353614807,\n","                          0.9466666579246521,\n","                          0.9469932317733765,\n","                          0.9455999732017517,\n","                          0.9474266767501831,\n","                          0.9469000101089478,\n","                          0.9482933282852173,\n","                          0.949400007724762,\n","                          0.9486933350563049,\n","                          0.9492866396903992,\n","                          0.9503200054168701,\n","                          0.948866605758667,\n","                          0.9489133358001709,\n","                          0.9503732919692993,\n","                          0.9520933628082275,\n","                          0.9506065845489502,\n","                          0.9507266879081726,\n","                          0.9501466155052185,\n","                          0.9490266442298889],\n","                         [0.8558666706085205,\n","                          0.8967400193214417,\n","                          0.9259999990463257,\n","                          0.9318533539772034,\n","                          0.9343999624252319,\n","                          0.9381799101829529,\n","                          0.9413334131240845,\n","                          0.9405266642570496,\n","                          0.94514000415802,\n","                          0.9417601227760315,\n","                          0.9459200501441956,\n","                          0.9498999714851379,\n","                          0.9497132897377014,\n","                          0.9490067362785339,\n","                          0.9340066313743591,\n","                          0.948759913444519,\n","                          0.9510200023651123,\n","                          0.9502000212669373,\n","                          0.950659990310669,\n","                          0.9510933756828308,\n","                          0.952826738357544,\n","                          0.9513400197029114,\n","                          0.9512200355529785,\n","                          0.954466700553894,\n","                          0.9531533122062683,\n","                          0.9559333324432373,\n","                          0.9562667012214661,\n","                          0.9554866552352905,\n","                          0.9566200375556946,\n","                          0.958193302154541],\n","                         [0.8162932395935059,\n","                          0.8749200701713562,\n","                          0.9076467156410217,\n","                          0.9124600291252136,\n","                          0.9130067229270935,\n","                          0.9205000400543213,\n","                          0.9238799214363098,\n","                          0.9306401014328003,\n","                          0.9265599846839905,\n","                          0.9289533495903015,\n","                          0.9370533227920532,\n","                          0.9313399791717529,\n","                          0.9383599162101746,\n","                          0.9397400617599487,\n","                          0.9426866173744202,\n","                          0.9399866461753845,\n","                          0.9409599900245667,\n","                          0.9445266723632812,\n","                          0.9426532983779907,\n","                          0.9425399899482727,\n","                          0.9434067010879517,\n","                          0.9439399838447571,\n","                          0.9449466466903687,\n","                          0.9473599195480347,\n","                          0.946286678314209,\n","                          0.9484266042709351,\n","                          0.9488933086395264,\n","                          0.9490732550621033,\n","                          0.94951993227005,\n","                          0.947713315486908]],\n"," 'Validation Loss': [[0.3422839641571045,\n","                      0.23823416233062744,\n","                      0.19465693831443787,\n","                      0.172885000705719,\n","                      0.1597616970539093,\n","                      0.1561817228794098,\n","                      0.14982518553733826,\n","                      0.14366039633750916,\n","                      0.1350097358226776,\n","                      0.12767942249774933,\n","                      0.12659670412540436,\n","                      0.14732438325881958,\n","                      0.12521414458751678,\n","                      0.12809985876083374,\n","                      0.12199599295854568,\n","                      0.11792420595884323,\n","                      0.11419786512851715,\n","                      0.11877483129501343,\n","                      0.11392896622419357,\n","                      0.11348726600408554,\n","                      0.11586683988571167,\n","                      0.11065606772899628,\n","                      0.1060076355934143,\n","                      0.1088290885090828,\n","                      0.10835543274879456,\n","                      0.10274669528007507,\n","                      0.10662270337343216,\n","                      0.11142290383577347,\n","                      0.10475695878267288,\n","                      0.102751724421978],\n","                     [0.3830966055393219,\n","                      0.26966801285743713,\n","                      0.2491295039653778,\n","                      0.20822429656982422,\n","                      0.1903422325849533,\n","                      0.1790841817855835,\n","                      0.1788499653339386,\n","                      0.16552837193012238,\n","                      0.1745736002922058,\n","                      0.15663672983646393,\n","                      0.15426896512508392,\n","                      0.15793336927890778,\n","                      0.1596745252609253,\n","                      0.1538308709859848,\n","                      0.14919006824493408,\n","                      0.1545390486717224,\n","                      0.14809773862361908,\n","                      0.14821010828018188,\n","                      0.15636472404003143,\n","                      0.1486922800540924,\n","                      0.1477421522140503,\n","                      0.1441311538219452,\n","                      0.13988220691680908,\n","                      0.14069323241710663,\n","                      0.13944104313850403,\n","                      0.13929076492786407,\n","                      0.13715694844722748,\n","                      0.13818061351776123,\n","                      0.14036253094673157,\n","                      0.13829730451107025],\n","                     [0.3768770098686218,\n","                      0.2750409245491028,\n","                      0.21415668725967407,\n","                      0.19020886719226837,\n","                      0.18834716081619263,\n","                      0.17480586469173431,\n","                      0.16338036954402924,\n","                      0.15252168476581573,\n","                      0.15064163506031036,\n","                      0.14097610116004944,\n","                      0.1388547122478485,\n","                      0.14023278653621674,\n","                      0.13107053935527802,\n","                      0.12852787971496582,\n","                      0.13241851329803467,\n","                      0.12975014746189117,\n","                      0.1317988634109497,\n","                      0.12645426392555237,\n","                      0.1265767216682434,\n","                      0.12531417608261108,\n","                      0.12542791664600372,\n","                      0.12161466479301453,\n","                      0.12533529102802277,\n","                      0.1250673234462738,\n","                      0.12101602554321289,\n","                      0.1181618869304657,\n","                      0.12323420494794846,\n","                      0.12272419035434723,\n","                      0.12249238044023514,\n","                      0.12413150072097778],\n","                     [0.3527776598930359,\n","                      0.24944105744361877,\n","                      0.18239350616931915,\n","                      0.1665601283311844,\n","                      0.161505788564682,\n","                      0.14987702667713165,\n","                      0.1448114663362503,\n","                      0.14761149883270264,\n","                      0.13720592856407166,\n","                      0.14321114122867584,\n","                      0.13584204018115997,\n","                      0.12511007487773895,\n","                      0.12455087900161743,\n","                      0.1284356266260147,\n","                      0.15786661207675934,\n","                      0.12783430516719818,\n","                      0.12296581268310547,\n","                      0.12502218782901764,\n","                      0.123147152364254,\n","                      0.12179285287857056,\n","                      0.11778665333986282,\n","                      0.1206449344754219,\n","                      0.1203528568148613,\n","                      0.11445573717355728,\n","                      0.11919286847114563,\n","                      0.1096932515501976,\n","                      0.11041345447301865,\n","                      0.11158771812915802,\n","                      0.10984015464782715,\n","                      0.105537049472332],\n","                     [0.4029867649078369,\n","                      0.2896154522895813,\n","                      0.22007204592227936,\n","                      0.20447157323360443,\n","                      0.20072992146015167,\n","                      0.18573720753192902,\n","                      0.17845676839351654,\n","                      0.16370005905628204,\n","                      0.17381733655929565,\n","                      0.16484171152114868,\n","                      0.1494847685098648,\n","                      0.16234546899795532,\n","                      0.14608660340309143,\n","                      0.1445452719926834,\n","                      0.13965977728366852,\n","                      0.14852233231067657,\n","                      0.14123325049877167,\n","                      0.13249196112155914,\n","                      0.13667921721935272,\n","                      0.14099609851837158,\n","                      0.1340516209602356,\n","                      0.1359877735376358,\n","                      0.1339908242225647,\n","                      0.12811635434627533,\n","                      0.13059352338314056,\n","                      0.12666448950767517,\n","                      0.12548863887786865,\n","                      0.12519340217113495,\n","                      0.12316125631332397,\n","                      0.12602712213993073]],\n"," 'Validation MCC': [[0.7002431490667165,\n","                     0.7955012428578546,\n","                     0.8355640340139534,\n","                     0.8559403164001728,\n","                     0.866922699111501,\n","                     0.8701432105967815,\n","                     0.8721956338814036,\n","                     0.8818414314816393,\n","                     0.8873671804116118,\n","                     0.8945029016794654,\n","                     0.8964384050156708,\n","                     0.8800578748324903,\n","                     0.8994391317398847,\n","                     0.8960195965353125,\n","                     0.9013961079936698,\n","                     0.9037236918537992,\n","                     0.9080913013252708,\n","                     0.9043024389472447,\n","                     0.9047427592906656,\n","                     0.9063689020154568,\n","                     0.9069871459261961,\n","                     0.9106161883431488,\n","                     0.9134331026587525,\n","                     0.9111470182561028,\n","                     0.9118485736081925,\n","                     0.9147303808712802,\n","                     0.9131874331816487,\n","                     0.90795742128323,\n","                     0.9162133212738425,\n","                     0.9142624543658732],\n","                    [0.661577311350335,\n","                     0.7678133669964833,\n","                     0.7897167188374719,\n","                     0.8224002867013032,\n","                     0.8375428112651191,\n","                     0.8458033171606155,\n","                     0.8475263464307765,\n","                     0.8579905235643073,\n","                     0.8537177703443344,\n","                     0.8682352959698832,\n","                     0.8694934144389699,\n","                     0.8655199287566663,\n","                     0.8646118548496904,\n","                     0.8691749770486672,\n","                     0.8743279439912599,\n","                     0.8719639160249041,\n","                     0.8762557738325714,\n","                     0.8765755759222345,\n","                     0.8730757774636725,\n","                     0.8759907213061866,\n","                     0.8765366886331227,\n","                     0.8796941442578856,\n","                     0.8828541156028132,\n","                     0.8803122993193178,\n","                     0.8820744155657965,\n","                     0.8859194107505491,\n","                     0.884492195465319,\n","                     0.8856320746363713,\n","                     0.8868986653867873,\n","                     0.8836338629566973],\n","                    [0.6699569565644848,\n","                     0.7695550749366029,\n","                     0.8194446810886745,\n","                     0.8387018963229405,\n","                     0.8432262396630897,\n","                     0.8545057230184497,\n","                     0.863036237513007,\n","                     0.8734459942823728,\n","                     0.8767660520554343,\n","                     0.8819955235803452,\n","                     0.8846040730859539,\n","                     0.8842901718803174,\n","                     0.89289397408786,\n","                     0.8938731120012623,\n","                     0.890970162092525,\n","                     0.8944856591028274,\n","                     0.8939273804691793,\n","                     0.8961614776283128,\n","                     0.8984071980369905,\n","                     0.8969715976692949,\n","                     0.8981585611991414,\n","                     0.9002547890788224,\n","                     0.8973124778907705,\n","                     0.8977416771468811,\n","                     0.9003837905187744,\n","                     0.9038270952461852,\n","                     0.9008498104065595,\n","                     0.9013349750090934,\n","                     0.8998901560366103,\n","                     0.8979324754604585],\n","                    [0.711033599040299,\n","                     0.7951712229154928,\n","                     0.8519001321706202,\n","                     0.8634597432013797,\n","                     0.8686405566742252,\n","                     0.8763135833719278,\n","                     0.8824512020755148,\n","                     0.8811809653215381,\n","                     0.8900229807266464,\n","                     0.8832759984370123,\n","                     0.8922052836103398,\n","                     0.8996681487465282,\n","                     0.8991847766798987,\n","                     0.8984071930089577,\n","                     0.869074195950663,\n","                     0.8974925688617641,\n","                     0.9019803592374238,\n","                     0.9003911000389146,\n","                     0.901089624221873,\n","                     0.9021137606691391,\n","                     0.9056453158096657,\n","                     0.9024666356672405,\n","                     0.9022429502077376,\n","                     0.9088694828209535,\n","                     0.9068936219593013,\n","                     0.9116982971101977,\n","                     0.9124706033688547,\n","                     0.9110286224305486,\n","                     0.9130329159850871,\n","                     0.9161910083518204],\n","                    [0.6408020830665453,\n","                     0.755467489010542,\n","                     0.8153498736882525,\n","                     0.824823885761807,\n","                     0.8257748543022563,\n","                     0.8410725412069983,\n","                     0.8483652309592206,\n","                     0.8612748344066035,\n","                     0.8556942536166915,\n","                     0.8578983896252985,\n","                     0.8746031791475712,\n","                     0.8639752895130446,\n","                     0.8773153140583737,\n","                     0.879319521993224,\n","                     0.8854656369153652,\n","                     0.8798107561132719,\n","                     0.8822950149331881,\n","                     0.888957955755906,\n","                     0.8851242486692488,\n","                     0.8859846688645003,\n","                     0.886643866947934,\n","                     0.887777712929812,\n","                     0.8896964783722382,\n","                     0.8945730498008668,\n","                     0.8926710116988317,\n","                     0.8968336400160052,\n","                     0.8977499083599848,\n","                     0.8982238330031258,\n","                     0.8991565901194443,\n","                     0.8955141393026198]]}\n"]}]},{"cell_type":"markdown","source":["Signal Beta Binary"],"metadata":{"id":"dwnf8KLCncpR"}},{"cell_type":"code","source":["signal_binary_results, trained_models = train_and_evaluate(simple_models_dict, X=signal_data_vec, y=label_binary, epochs=n_epochs, dir_name=\"signal_binary\")\n","\n","basePath = \"/content/drive/MyDrive/Colab Notebooks/Bachelor Thesis/Data/Model Comparisons/LSTM Models\"\n","\n","filePath = f\"{basePath}/Signal_Binary_Model_Results.json\"\n","\n","with open(filePath, 'w') as f:\n","        json.dump(signal_binary_results, f, indent=4)  # indent=4 for pretty formatting"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVtPK2yucWiu","executionInfo":{"status":"ok","timestamp":1740323586066,"user_tz":-60,"elapsed":4275886,"user":{"displayName":"Jasper Angl","userId":"13853951665807648258"}},"outputId":"3cb368fb-20e0-4678-9634-18ddcc8baaff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 1\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5527 - loss: 0.6897\n","Epoch 1 - MCC: 0.2208\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.5534 - loss: 0.6895 - val_accuracy: 0.6025 - val_loss: 0.6687 - mcc: 0.2208\n","Epoch 2/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6288 - loss: 0.6541\n","Epoch 2 - MCC: 0.4983\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.6320 - loss: 0.6522 - val_accuracy: 0.7490 - val_loss: 0.5636 - mcc: 0.4983\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7548 - loss: 0.5328\n","Epoch 3 - MCC: 0.5841\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.7552 - loss: 0.5319 - val_accuracy: 0.7930 - val_loss: 0.4604 - mcc: 0.5841\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7921 - loss: 0.4605\n","Epoch 4 - MCC: 0.6401\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.7925 - loss: 0.4599 - val_accuracy: 0.8209 - val_loss: 0.4172 - mcc: 0.6401\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8212 - loss: 0.4116\n","Epoch 5 - MCC: 0.6686\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8212 - loss: 0.4115 - val_accuracy: 0.8350 - val_loss: 0.3869 - mcc: 0.6686\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8320 - loss: 0.3887\n","Epoch 6 - MCC: 0.6962\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8320 - loss: 0.3885 - val_accuracy: 0.8488 - val_loss: 0.3630 - mcc: 0.6962\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8444 - loss: 0.3643\n","Epoch 7 - MCC: 0.7072\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8444 - loss: 0.3642 - val_accuracy: 0.8542 - val_loss: 0.3419 - mcc: 0.7072\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8520 - loss: 0.3471\n","Epoch 8 - MCC: 0.7309\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8521 - loss: 0.3470 - val_accuracy: 0.8660 - val_loss: 0.3251 - mcc: 0.7309\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8604 - loss: 0.3312\n","Epoch 9 - MCC: 0.7463\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8604 - loss: 0.3311 - val_accuracy: 0.8736 - val_loss: 0.3098 - mcc: 0.7463\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8654 - loss: 0.3215\n","Epoch 10 - MCC: 0.7585\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8656 - loss: 0.3212 - val_accuracy: 0.8798 - val_loss: 0.2969 - mcc: 0.7585\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8742 - loss: 0.3048\n","Epoch 11 - MCC: 0.7609\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.8742 - loss: 0.3047 - val_accuracy: 0.8810 - val_loss: 0.2920 - mcc: 0.7609\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8763 - loss: 0.2998\n","Epoch 12 - MCC: 0.7727\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8764 - loss: 0.2997 - val_accuracy: 0.8868 - val_loss: 0.2804 - mcc: 0.7727\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8783 - loss: 0.2925\n","Epoch 13 - MCC: 0.7789\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8783 - loss: 0.2925 - val_accuracy: 0.8899 - val_loss: 0.2741 - mcc: 0.7789\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8815 - loss: 0.2859\n","Epoch 14 - MCC: 0.7811\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8815 - loss: 0.2859 - val_accuracy: 0.8909 - val_loss: 0.2708 - mcc: 0.7811\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8828 - loss: 0.2838\n","Epoch 15 - MCC: 0.7785\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8828 - loss: 0.2837 - val_accuracy: 0.8897 - val_loss: 0.2715 - mcc: 0.7785\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8832 - loss: 0.2813\n","Epoch 16 - MCC: 0.7869\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8832 - loss: 0.2812 - val_accuracy: 0.8939 - val_loss: 0.2642 - mcc: 0.7869\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8901 - loss: 0.2689\n","Epoch 17 - MCC: 0.7855\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8899 - loss: 0.2692 - val_accuracy: 0.8932 - val_loss: 0.2640 - mcc: 0.7855\n","Epoch 18/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8923 - loss: 0.2646\n","Epoch 18 - MCC: 0.7900\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8919 - loss: 0.2653 - val_accuracy: 0.8954 - val_loss: 0.2594 - mcc: 0.7900\n","Epoch 19/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8828 - loss: 0.2808\n","Epoch 19 - MCC: 0.7912\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8832 - loss: 0.2801 - val_accuracy: 0.8960 - val_loss: 0.2583 - mcc: 0.7912\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8848 - loss: 0.2764\n","Epoch 20 - MCC: 0.7827\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8850 - loss: 0.2761 - val_accuracy: 0.8916 - val_loss: 0.2655 - mcc: 0.7827\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8914 - loss: 0.2663\n","Epoch 21 - MCC: 0.7917\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8913 - loss: 0.2664 - val_accuracy: 0.8962 - val_loss: 0.2561 - mcc: 0.7917\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8864 - loss: 0.2735\n","Epoch 22 - MCC: 0.7903\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8866 - loss: 0.2732 - val_accuracy: 0.8956 - val_loss: 0.2572 - mcc: 0.7903\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8892 - loss: 0.2677\n","Epoch 23 - MCC: 0.7926\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8892 - loss: 0.2676 - val_accuracy: 0.8967 - val_loss: 0.2541 - mcc: 0.7926\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8988 - loss: 0.2499\n","Epoch 24 - MCC: 0.7970\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8986 - loss: 0.2504 - val_accuracy: 0.8989 - val_loss: 0.2510 - mcc: 0.7970\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8952 - loss: 0.2574\n","Epoch 25 - MCC: 0.7936\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8951 - loss: 0.2576 - val_accuracy: 0.8972 - val_loss: 0.2535 - mcc: 0.7936\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8912 - loss: 0.2627\n","Epoch 26 - MCC: 0.7960\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8912 - loss: 0.2628 - val_accuracy: 0.8984 - val_loss: 0.2507 - mcc: 0.7960\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8891 - loss: 0.2676\n","Epoch 27 - MCC: 0.7975\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8892 - loss: 0.2673 - val_accuracy: 0.8992 - val_loss: 0.2493 - mcc: 0.7975\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8907 - loss: 0.2648\n","Epoch 28 - MCC: 0.7983\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.8908 - loss: 0.2646 - val_accuracy: 0.8995 - val_loss: 0.2475 - mcc: 0.7983\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8928 - loss: 0.2588\n","Epoch 29 - MCC: 0.7988\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8928 - loss: 0.2588 - val_accuracy: 0.8998 - val_loss: 0.2468 - mcc: 0.7988\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8964 - loss: 0.2537\n","Epoch 30 - MCC: 0.7994\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8963 - loss: 0.2539 - val_accuracy: 0.9000 - val_loss: 0.2462 - mcc: 0.7994\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8917 - loss: 0.2627\n","Epoch 31 - MCC: 0.7929\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8917 - loss: 0.2627 - val_accuracy: 0.8967 - val_loss: 0.2518 - mcc: 0.7929\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8914 - loss: 0.2651\n","Epoch 32 - MCC: 0.7963\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8915 - loss: 0.2648 - val_accuracy: 0.8986 - val_loss: 0.2477 - mcc: 0.7963\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8894 - loss: 0.2657\n","Epoch 33 - MCC: 0.7981\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8896 - loss: 0.2653 - val_accuracy: 0.8994 - val_loss: 0.2462 - mcc: 0.7981\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8988 - loss: 0.2474\n","Epoch 34 - MCC: 0.8007\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8986 - loss: 0.2477 - val_accuracy: 0.9008 - val_loss: 0.2439 - mcc: 0.8007\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8959 - loss: 0.2532\n","Epoch 35 - MCC: 0.8003\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8959 - loss: 0.2532 - val_accuracy: 0.9006 - val_loss: 0.2441 - mcc: 0.8003\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8985 - loss: 0.2497\n","Epoch 36 - MCC: 0.8015\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8984 - loss: 0.2499 - val_accuracy: 0.9012 - val_loss: 0.2433 - mcc: 0.8015\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8965 - loss: 0.2510\n","Epoch 37 - MCC: 0.7958\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8965 - loss: 0.2511 - val_accuracy: 0.8983 - val_loss: 0.2469 - mcc: 0.7958\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8932 - loss: 0.2591\n","Epoch 38 - MCC: 0.8028\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.8933 - loss: 0.2589 - val_accuracy: 0.9018 - val_loss: 0.2423 - mcc: 0.8028\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8921 - loss: 0.2594\n","Epoch 39 - MCC: 0.7995\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8923 - loss: 0.2591 - val_accuracy: 0.9001 - val_loss: 0.2440 - mcc: 0.7995\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8995 - loss: 0.2456\n","Epoch 40 - MCC: 0.8031\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8994 - loss: 0.2458 - val_accuracy: 0.9019 - val_loss: 0.2410 - mcc: 0.8031\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8988 - loss: 0.2467\n","Epoch 41 - MCC: 0.8029\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8987 - loss: 0.2469 - val_accuracy: 0.9017 - val_loss: 0.2410 - mcc: 0.8029\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8969 - loss: 0.2507\n","Epoch 42 - MCC: 0.8008\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8970 - loss: 0.2507 - val_accuracy: 0.9008 - val_loss: 0.2417 - mcc: 0.8008\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8926 - loss: 0.2598\n","Epoch 43 - MCC: 0.8051\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8928 - loss: 0.2594 - val_accuracy: 0.9030 - val_loss: 0.2393 - mcc: 0.8051\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8990 - loss: 0.2461\n","Epoch 44 - MCC: 0.8045\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8989 - loss: 0.2462 - val_accuracy: 0.9025 - val_loss: 0.2400 - mcc: 0.8045\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8950 - loss: 0.2537\n","Epoch 45 - MCC: 0.8026\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8951 - loss: 0.2534 - val_accuracy: 0.9017 - val_loss: 0.2397 - mcc: 0.8026\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8921 - loss: 0.2589\n","Epoch 46 - MCC: 0.8054\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8923 - loss: 0.2585 - val_accuracy: 0.9031 - val_loss: 0.2381 - mcc: 0.8054\n","Epoch 47/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9016 - loss: 0.2422\n","Epoch 47 - MCC: 0.8059\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9013 - loss: 0.2427 - val_accuracy: 0.9033 - val_loss: 0.2373 - mcc: 0.8059\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8971 - loss: 0.2488\n","Epoch 48 - MCC: 0.8058\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.8972 - loss: 0.2487 - val_accuracy: 0.9033 - val_loss: 0.2388 - mcc: 0.8058\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9002 - loss: 0.2441\n","Epoch 49 - MCC: 0.8021\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9001 - loss: 0.2442 - val_accuracy: 0.9013 - val_loss: 0.2411 - mcc: 0.8021\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8962 - loss: 0.2512\n","Epoch 50 - MCC: 0.8047\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8963 - loss: 0.2511 - val_accuracy: 0.9027 - val_loss: 0.2371 - mcc: 0.8047\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 2\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5789 - loss: 0.6847\n","Epoch 1 - MCC: 0.2497\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.5796 - loss: 0.6844 - val_accuracy: 0.6130 - val_loss: 0.6604 - mcc: 0.2497\n","Epoch 2/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6683 - loss: 0.6358\n","Epoch 2 - MCC: 0.4794\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.6714 - loss: 0.6337 - val_accuracy: 0.7354 - val_loss: 0.5651 - mcc: 0.4794\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7756 - loss: 0.5134\n","Epoch 3 - MCC: 0.5897\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.7759 - loss: 0.5125 - val_accuracy: 0.7954 - val_loss: 0.4630 - mcc: 0.5897\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8142 - loss: 0.4285\n","Epoch 4 - MCC: 0.6322\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8143 - loss: 0.4281 - val_accuracy: 0.8164 - val_loss: 0.4204 - mcc: 0.6322\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8332 - loss: 0.3882\n","Epoch 5 - MCC: 0.6566\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8332 - loss: 0.3882 - val_accuracy: 0.8287 - val_loss: 0.3968 - mcc: 0.6566\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8449 - loss: 0.3642\n","Epoch 6 - MCC: 0.6734\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8449 - loss: 0.3641 - val_accuracy: 0.8372 - val_loss: 0.3776 - mcc: 0.6734\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8584 - loss: 0.3379\n","Epoch 7 - MCC: 0.6850\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8581 - loss: 0.3382 - val_accuracy: 0.8429 - val_loss: 0.3641 - mcc: 0.6850\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8590 - loss: 0.3342\n","Epoch 8 - MCC: 0.6976\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8591 - loss: 0.3340 - val_accuracy: 0.8489 - val_loss: 0.3530 - mcc: 0.6976\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8612 - loss: 0.3267\n","Epoch 9 - MCC: 0.7147\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8614 - loss: 0.3263 - val_accuracy: 0.8577 - val_loss: 0.3383 - mcc: 0.7147\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8689 - loss: 0.3147\n","Epoch 10 - MCC: 0.7169\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8690 - loss: 0.3144 - val_accuracy: 0.8587 - val_loss: 0.3292 - mcc: 0.7169\n","Epoch 11/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8800 - loss: 0.2918\n","Epoch 11 - MCC: 0.7206\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.8797 - loss: 0.2923 - val_accuracy: 0.8604 - val_loss: 0.3255 - mcc: 0.7206\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8774 - loss: 0.2940\n","Epoch 12 - MCC: 0.7322\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8774 - loss: 0.2939 - val_accuracy: 0.8662 - val_loss: 0.3160 - mcc: 0.7322\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8729 - loss: 0.3004\n","Epoch 13 - MCC: 0.7382\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8733 - loss: 0.2998 - val_accuracy: 0.8694 - val_loss: 0.3100 - mcc: 0.7382\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8882 - loss: 0.2744\n","Epoch 14 - MCC: 0.7389\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8880 - loss: 0.2746 - val_accuracy: 0.8697 - val_loss: 0.3089 - mcc: 0.7389\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8818 - loss: 0.2847\n","Epoch 15 - MCC: 0.7378\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8819 - loss: 0.2845 - val_accuracy: 0.8693 - val_loss: 0.3093 - mcc: 0.7378\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8915 - loss: 0.2668\n","Epoch 16 - MCC: 0.7464\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8913 - loss: 0.2672 - val_accuracy: 0.8735 - val_loss: 0.3013 - mcc: 0.7464\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8831 - loss: 0.2829\n","Epoch 17 - MCC: 0.7468\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8833 - loss: 0.2825 - val_accuracy: 0.8737 - val_loss: 0.2996 - mcc: 0.7468\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8904 - loss: 0.2673\n","Epoch 18 - MCC: 0.7514\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8903 - loss: 0.2674 - val_accuracy: 0.8760 - val_loss: 0.2951 - mcc: 0.7514\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8861 - loss: 0.2754\n","Epoch 19 - MCC: 0.7508\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8863 - loss: 0.2751 - val_accuracy: 0.8757 - val_loss: 0.2953 - mcc: 0.7508\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8925 - loss: 0.2634\n","Epoch 20 - MCC: 0.7504\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8925 - loss: 0.2635 - val_accuracy: 0.8755 - val_loss: 0.2953 - mcc: 0.7504\n","Epoch 21/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8956 - loss: 0.2560\n","Epoch 21 - MCC: 0.7518\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8953 - loss: 0.2566 - val_accuracy: 0.8762 - val_loss: 0.2947 - mcc: 0.7518\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8912 - loss: 0.2648\n","Epoch 22 - MCC: 0.7546\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8912 - loss: 0.2647 - val_accuracy: 0.8772 - val_loss: 0.2931 - mcc: 0.7546\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8942 - loss: 0.2589\n","Epoch 23 - MCC: 0.7581\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8941 - loss: 0.2590 - val_accuracy: 0.8793 - val_loss: 0.2875 - mcc: 0.7581\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8890 - loss: 0.2694\n","Epoch 24 - MCC: 0.7572\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8892 - loss: 0.2690 - val_accuracy: 0.8788 - val_loss: 0.2904 - mcc: 0.7572\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8922 - loss: 0.2631\n","Epoch 25 - MCC: 0.7569\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8923 - loss: 0.2629 - val_accuracy: 0.8787 - val_loss: 0.2894 - mcc: 0.7569\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8887 - loss: 0.2695\n","Epoch 26 - MCC: 0.7607\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8889 - loss: 0.2690 - val_accuracy: 0.8807 - val_loss: 0.2854 - mcc: 0.7607\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8935 - loss: 0.2590\n","Epoch 27 - MCC: 0.7618\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8936 - loss: 0.2589 - val_accuracy: 0.8811 - val_loss: 0.2837 - mcc: 0.7618\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8982 - loss: 0.2503\n","Epoch 28 - MCC: 0.7620\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8981 - loss: 0.2504 - val_accuracy: 0.8813 - val_loss: 0.2826 - mcc: 0.7620\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8934 - loss: 0.2594\n","Epoch 29 - MCC: 0.7617\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8935 - loss: 0.2592 - val_accuracy: 0.8811 - val_loss: 0.2851 - mcc: 0.7617\n","Epoch 30/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8962 - loss: 0.2552\n","Epoch 30 - MCC: 0.7611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.8963 - loss: 0.2550 - val_accuracy: 0.8807 - val_loss: 0.2844 - mcc: 0.7611\n","Epoch 31/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8958 - loss: 0.2543\n","Epoch 31 - MCC: 0.7649\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.8959 - loss: 0.2541 - val_accuracy: 0.8827 - val_loss: 0.2820 - mcc: 0.7649\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8975 - loss: 0.2504\n","Epoch 32 - MCC: 0.7652\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8975 - loss: 0.2505 - val_accuracy: 0.8829 - val_loss: 0.2798 - mcc: 0.7652\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8968 - loss: 0.2534\n","Epoch 33 - MCC: 0.7643\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8968 - loss: 0.2533 - val_accuracy: 0.8824 - val_loss: 0.2845 - mcc: 0.7643\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9032 - loss: 0.2395\n","Epoch 34 - MCC: 0.7648\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9030 - loss: 0.2399 - val_accuracy: 0.8826 - val_loss: 0.2842 - mcc: 0.7648\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8979 - loss: 0.2508\n","Epoch 35 - MCC: 0.7663\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8979 - loss: 0.2508 - val_accuracy: 0.8833 - val_loss: 0.2786 - mcc: 0.7663\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9023 - loss: 0.2443\n","Epoch 36 - MCC: 0.7650\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9022 - loss: 0.2445 - val_accuracy: 0.8828 - val_loss: 0.2796 - mcc: 0.7650\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9001 - loss: 0.2451\n","Epoch 37 - MCC: 0.7661\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9000 - loss: 0.2453 - val_accuracy: 0.8831 - val_loss: 0.2784 - mcc: 0.7661\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8932 - loss: 0.2593\n","Epoch 38 - MCC: 0.7682\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8934 - loss: 0.2589 - val_accuracy: 0.8843 - val_loss: 0.2770 - mcc: 0.7682\n","Epoch 39/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9017 - loss: 0.2415\n","Epoch 39 - MCC: 0.7677\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9015 - loss: 0.2419 - val_accuracy: 0.8841 - val_loss: 0.2780 - mcc: 0.7677\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8987 - loss: 0.2471\n","Epoch 40 - MCC: 0.7675\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.8987 - loss: 0.2471 - val_accuracy: 0.8841 - val_loss: 0.2762 - mcc: 0.7675\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8920 - loss: 0.2601\n","Epoch 41 - MCC: 0.7678\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8923 - loss: 0.2596 - val_accuracy: 0.8841 - val_loss: 0.2765 - mcc: 0.7678\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9003 - loss: 0.2445\n","Epoch 42 - MCC: 0.7691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9003 - loss: 0.2445 - val_accuracy: 0.8847 - val_loss: 0.2781 - mcc: 0.7691\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8991 - loss: 0.2487\n","Epoch 43 - MCC: 0.7684\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8992 - loss: 0.2485 - val_accuracy: 0.8845 - val_loss: 0.2764 - mcc: 0.7684\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9034 - loss: 0.2378\n","Epoch 44 - MCC: 0.7677\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9033 - loss: 0.2379 - val_accuracy: 0.8840 - val_loss: 0.2793 - mcc: 0.7677\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8999 - loss: 0.2437\n","Epoch 45 - MCC: 0.7666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9000 - loss: 0.2437 - val_accuracy: 0.8832 - val_loss: 0.2785 - mcc: 0.7666\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9015 - loss: 0.2418\n","Epoch 46 - MCC: 0.7677\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9015 - loss: 0.2418 - val_accuracy: 0.8841 - val_loss: 0.2763 - mcc: 0.7677\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9028 - loss: 0.2393\n","Epoch 47 - MCC: 0.7707\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9027 - loss: 0.2394 - val_accuracy: 0.8856 - val_loss: 0.2733 - mcc: 0.7707\n","Epoch 48/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9015 - loss: 0.2399\n","Epoch 48 - MCC: 0.7702\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9015 - loss: 0.2400 - val_accuracy: 0.8853 - val_loss: 0.2748 - mcc: 0.7702\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9022 - loss: 0.2404\n","Epoch 49 - MCC: 0.7730\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.9022 - loss: 0.2404 - val_accuracy: 0.8867 - val_loss: 0.2727 - mcc: 0.7730\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9026 - loss: 0.2392\n","Epoch 50 - MCC: 0.7725\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9025 - loss: 0.2393 - val_accuracy: 0.8865 - val_loss: 0.2733 - mcc: 0.7725\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 3\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5275 - loss: 0.6916\n","Epoch 1 - MCC: 0.2664\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.5291 - loss: 0.6914 - val_accuracy: 0.6283 - val_loss: 0.6677 - mcc: 0.2664\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6394 - loss: 0.6541\n","Epoch 2 - MCC: 0.4800\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.6404 - loss: 0.6534 - val_accuracy: 0.7388 - val_loss: 0.5741 - mcc: 0.4800\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7558 - loss: 0.5376\n","Epoch 3 - MCC: 0.6150\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.7564 - loss: 0.5363 - val_accuracy: 0.8081 - val_loss: 0.4534 - mcc: 0.6150\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8095 - loss: 0.4417\n","Epoch 4 - MCC: 0.6359\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8095 - loss: 0.4414 - val_accuracy: 0.8188 - val_loss: 0.4093 - mcc: 0.6359\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8175 - loss: 0.4122\n","Epoch 5 - MCC: 0.6675\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - accuracy: 0.8177 - loss: 0.4118 - val_accuracy: 0.8345 - val_loss: 0.3819 - mcc: 0.6675\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8404 - loss: 0.3757\n","Epoch 6 - MCC: 0.6890\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.8402 - loss: 0.3758 - val_accuracy: 0.8452 - val_loss: 0.3604 - mcc: 0.6890\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8440 - loss: 0.3635\n","Epoch 7 - MCC: 0.7096\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8441 - loss: 0.3634 - val_accuracy: 0.8552 - val_loss: 0.3447 - mcc: 0.7096\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8523 - loss: 0.3457\n","Epoch 8 - MCC: 0.7257\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8523 - loss: 0.3456 - val_accuracy: 0.8631 - val_loss: 0.3303 - mcc: 0.7257\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8576 - loss: 0.3368\n","Epoch 9 - MCC: 0.7428\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8578 - loss: 0.3365 - val_accuracy: 0.8720 - val_loss: 0.3083 - mcc: 0.7428\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8627 - loss: 0.3256\n","Epoch 10 - MCC: 0.7542\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8629 - loss: 0.3252 - val_accuracy: 0.8777 - val_loss: 0.2951 - mcc: 0.7542\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8735 - loss: 0.3037\n","Epoch 11 - MCC: 0.7596\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8734 - loss: 0.3038 - val_accuracy: 0.8803 - val_loss: 0.2894 - mcc: 0.7596\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8786 - loss: 0.2937\n","Epoch 12 - MCC: 0.7602\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8784 - loss: 0.2939 - val_accuracy: 0.8799 - val_loss: 0.2936 - mcc: 0.7602\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8801 - loss: 0.2921\n","Epoch 13 - MCC: 0.7680\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8800 - loss: 0.2922 - val_accuracy: 0.8842 - val_loss: 0.2799 - mcc: 0.7680\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8870 - loss: 0.2752\n","Epoch 14 - MCC: 0.7712\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8867 - loss: 0.2757 - val_accuracy: 0.8860 - val_loss: 0.2738 - mcc: 0.7712\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8802 - loss: 0.2893\n","Epoch 15 - MCC: 0.7714\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.8803 - loss: 0.2893 - val_accuracy: 0.8860 - val_loss: 0.2799 - mcc: 0.7714\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8829 - loss: 0.2837\n","Epoch 16 - MCC: 0.7770\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8829 - loss: 0.2836 - val_accuracy: 0.8889 - val_loss: 0.2728 - mcc: 0.7770\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8884 - loss: 0.2743\n","Epoch 17 - MCC: 0.7839\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8882 - loss: 0.2745 - val_accuracy: 0.8924 - val_loss: 0.2626 - mcc: 0.7839\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8832 - loss: 0.2816\n","Epoch 18 - MCC: 0.7846\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8833 - loss: 0.2814 - val_accuracy: 0.8925 - val_loss: 0.2614 - mcc: 0.7846\n","Epoch 19/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8840 - loss: 0.2798\n","Epoch 19 - MCC: 0.7879\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8842 - loss: 0.2793 - val_accuracy: 0.8943 - val_loss: 0.2589 - mcc: 0.7879\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8873 - loss: 0.2729\n","Epoch 20 - MCC: 0.7892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8873 - loss: 0.2729 - val_accuracy: 0.8950 - val_loss: 0.2582 - mcc: 0.7892\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8928 - loss: 0.2642\n","Epoch 21 - MCC: 0.7925\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8926 - loss: 0.2643 - val_accuracy: 0.8967 - val_loss: 0.2535 - mcc: 0.7925\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8908 - loss: 0.2686\n","Epoch 22 - MCC: 0.7902\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8908 - loss: 0.2686 - val_accuracy: 0.8952 - val_loss: 0.2562 - mcc: 0.7902\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8867 - loss: 0.2731\n","Epoch 23 - MCC: 0.7937\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8869 - loss: 0.2728 - val_accuracy: 0.8973 - val_loss: 0.2496 - mcc: 0.7937\n","Epoch 24/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8907 - loss: 0.2643\n","Epoch 24 - MCC: 0.7920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.8908 - loss: 0.2643 - val_accuracy: 0.8963 - val_loss: 0.2528 - mcc: 0.7920\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8925 - loss: 0.2609\n","Epoch 25 - MCC: 0.7927\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8925 - loss: 0.2610 - val_accuracy: 0.8967 - val_loss: 0.2497 - mcc: 0.7927\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8943 - loss: 0.2579\n","Epoch 26 - MCC: 0.7992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8942 - loss: 0.2580 - val_accuracy: 0.9000 - val_loss: 0.2447 - mcc: 0.7992\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8977 - loss: 0.2524\n","Epoch 27 - MCC: 0.7962\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8975 - loss: 0.2527 - val_accuracy: 0.8984 - val_loss: 0.2483 - mcc: 0.7962\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8976 - loss: 0.2518\n","Epoch 28 - MCC: 0.7987\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8974 - loss: 0.2522 - val_accuracy: 0.8997 - val_loss: 0.2439 - mcc: 0.7987\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8965 - loss: 0.2531\n","Epoch 29 - MCC: 0.8002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8964 - loss: 0.2533 - val_accuracy: 0.9004 - val_loss: 0.2427 - mcc: 0.8002\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8930 - loss: 0.2593\n","Epoch 30 - MCC: 0.8010\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8931 - loss: 0.2593 - val_accuracy: 0.9009 - val_loss: 0.2413 - mcc: 0.8010\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8950 - loss: 0.2567\n","Epoch 31 - MCC: 0.7989\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8950 - loss: 0.2567 - val_accuracy: 0.8997 - val_loss: 0.2433 - mcc: 0.7989\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8959 - loss: 0.2539\n","Epoch 32 - MCC: 0.7965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.8958 - loss: 0.2541 - val_accuracy: 0.8984 - val_loss: 0.2473 - mcc: 0.7965\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8943 - loss: 0.2573\n","Epoch 33 - MCC: 0.8032\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8943 - loss: 0.2572 - val_accuracy: 0.9020 - val_loss: 0.2405 - mcc: 0.8032\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8947 - loss: 0.2562\n","Epoch 34 - MCC: 0.8016\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8947 - loss: 0.2562 - val_accuracy: 0.9012 - val_loss: 0.2400 - mcc: 0.8016\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8991 - loss: 0.2476\n","Epoch 35 - MCC: 0.8040\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8990 - loss: 0.2479 - val_accuracy: 0.9024 - val_loss: 0.2382 - mcc: 0.8040\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8978 - loss: 0.2494\n","Epoch 36 - MCC: 0.8008\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8977 - loss: 0.2496 - val_accuracy: 0.9008 - val_loss: 0.2426 - mcc: 0.8008\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9011 - loss: 0.2443\n","Epoch 37 - MCC: 0.8056\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9009 - loss: 0.2447 - val_accuracy: 0.9032 - val_loss: 0.2359 - mcc: 0.8056\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9016 - loss: 0.2413\n","Epoch 38 - MCC: 0.8036\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9014 - loss: 0.2417 - val_accuracy: 0.9022 - val_loss: 0.2375 - mcc: 0.8036\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8961 - loss: 0.2510\n","Epoch 39 - MCC: 0.8031\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8962 - loss: 0.2510 - val_accuracy: 0.9019 - val_loss: 0.2386 - mcc: 0.8031\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8971 - loss: 0.2507\n","Epoch 40 - MCC: 0.8054\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8971 - loss: 0.2507 - val_accuracy: 0.9031 - val_loss: 0.2361 - mcc: 0.8054\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8971 - loss: 0.2498\n","Epoch 41 - MCC: 0.8064\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8971 - loss: 0.2498 - val_accuracy: 0.9036 - val_loss: 0.2356 - mcc: 0.8064\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8990 - loss: 0.2461\n","Epoch 42 - MCC: 0.8086\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.8989 - loss: 0.2462 - val_accuracy: 0.9047 - val_loss: 0.2336 - mcc: 0.8086\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9001 - loss: 0.2436\n","Epoch 43 - MCC: 0.8038\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9000 - loss: 0.2438 - val_accuracy: 0.9021 - val_loss: 0.2389 - mcc: 0.8038\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8983 - loss: 0.2468\n","Epoch 44 - MCC: 0.8069\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8983 - loss: 0.2468 - val_accuracy: 0.9038 - val_loss: 0.2332 - mcc: 0.8069\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9016 - loss: 0.2403\n","Epoch 45 - MCC: 0.8079\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9014 - loss: 0.2406 - val_accuracy: 0.9044 - val_loss: 0.2327 - mcc: 0.8079\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8944 - loss: 0.2520\n","Epoch 46 - MCC: 0.8061\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8945 - loss: 0.2518 - val_accuracy: 0.9034 - val_loss: 0.2347 - mcc: 0.8061\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9049 - loss: 0.2342\n","Epoch 47 - MCC: 0.8094\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9046 - loss: 0.2346 - val_accuracy: 0.9051 - val_loss: 0.2316 - mcc: 0.8094\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8981 - loss: 0.2479\n","Epoch 48 - MCC: 0.8097\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8982 - loss: 0.2477 - val_accuracy: 0.9052 - val_loss: 0.2310 - mcc: 0.8097\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9061 - loss: 0.2310\n","Epoch 49 - MCC: 0.8100\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9058 - loss: 0.2315 - val_accuracy: 0.9054 - val_loss: 0.2310 - mcc: 0.8100\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9025 - loss: 0.2398\n","Epoch 50 - MCC: 0.8091\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9024 - loss: 0.2400 - val_accuracy: 0.9049 - val_loss: 0.2313 - mcc: 0.8091\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 4\n","Epoch 1/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5625 - loss: 0.6864\n","Epoch 1 - MCC: 0.2432\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.5651 - loss: 0.6858 - val_accuracy: 0.6141 - val_loss: 0.6594 - mcc: 0.2432\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6436 - loss: 0.6413\n","Epoch 2 - MCC: 0.5156\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.6450 - loss: 0.6405 - val_accuracy: 0.7544 - val_loss: 0.5616 - mcc: 0.5156\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7635 - loss: 0.5298\n","Epoch 3 - MCC: 0.6203\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.7640 - loss: 0.5287 - val_accuracy: 0.8106 - val_loss: 0.4411 - mcc: 0.6203\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8138 - loss: 0.4340\n","Epoch 4 - MCC: 0.6634\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8138 - loss: 0.4338 - val_accuracy: 0.8322 - val_loss: 0.4015 - mcc: 0.6634\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8330 - loss: 0.3923\n","Epoch 5 - MCC: 0.6838\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8329 - loss: 0.3923 - val_accuracy: 0.8423 - val_loss: 0.3696 - mcc: 0.6838\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8408 - loss: 0.3698\n","Epoch 6 - MCC: 0.7130\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8409 - loss: 0.3697 - val_accuracy: 0.8568 - val_loss: 0.3500 - mcc: 0.7130\n","Epoch 7/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8465 - loss: 0.3590\n","Epoch 7 - MCC: 0.7275\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.8472 - loss: 0.3578 - val_accuracy: 0.8641 - val_loss: 0.3285 - mcc: 0.7275\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8672 - loss: 0.3223\n","Epoch 8 - MCC: 0.7473\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8671 - loss: 0.3224 - val_accuracy: 0.8738 - val_loss: 0.3118 - mcc: 0.7473\n","Epoch 9/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8710 - loss: 0.3130\n","Epoch 9 - MCC: 0.7513\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8711 - loss: 0.3128 - val_accuracy: 0.8760 - val_loss: 0.3021 - mcc: 0.7513\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8759 - loss: 0.2988\n","Epoch 10 - MCC: 0.7587\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8758 - loss: 0.2989 - val_accuracy: 0.8795 - val_loss: 0.2975 - mcc: 0.7587\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8731 - loss: 0.3047\n","Epoch 11 - MCC: 0.7575\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8732 - loss: 0.3045 - val_accuracy: 0.8788 - val_loss: 0.2935 - mcc: 0.7575\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8756 - loss: 0.2970\n","Epoch 12 - MCC: 0.7641\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8758 - loss: 0.2968 - val_accuracy: 0.8823 - val_loss: 0.2873 - mcc: 0.7641\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8835 - loss: 0.2820\n","Epoch 13 - MCC: 0.7709\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8834 - loss: 0.2822 - val_accuracy: 0.8857 - val_loss: 0.2831 - mcc: 0.7709\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8865 - loss: 0.2766\n","Epoch 14 - MCC: 0.7713\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8864 - loss: 0.2768 - val_accuracy: 0.8859 - val_loss: 0.2798 - mcc: 0.7713\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8776 - loss: 0.2918\n","Epoch 15 - MCC: 0.7633\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.8778 - loss: 0.2914 - val_accuracy: 0.8819 - val_loss: 0.2879 - mcc: 0.7633\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8821 - loss: 0.2834\n","Epoch 16 - MCC: 0.7736\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.8821 - loss: 0.2833 - val_accuracy: 0.8870 - val_loss: 0.2770 - mcc: 0.7736\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8892 - loss: 0.2691\n","Epoch 17 - MCC: 0.7783\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8891 - loss: 0.2693 - val_accuracy: 0.8894 - val_loss: 0.2723 - mcc: 0.7783\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8876 - loss: 0.2720\n","Epoch 18 - MCC: 0.7778\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8876 - loss: 0.2721 - val_accuracy: 0.8891 - val_loss: 0.2729 - mcc: 0.7778\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8869 - loss: 0.2731\n","Epoch 19 - MCC: 0.7823\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8869 - loss: 0.2730 - val_accuracy: 0.8914 - val_loss: 0.2700 - mcc: 0.7823\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8881 - loss: 0.2701\n","Epoch 20 - MCC: 0.7828\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8881 - loss: 0.2701 - val_accuracy: 0.8916 - val_loss: 0.2670 - mcc: 0.7828\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8865 - loss: 0.2741\n","Epoch 21 - MCC: 0.7850\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8866 - loss: 0.2738 - val_accuracy: 0.8928 - val_loss: 0.2652 - mcc: 0.7850\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8946 - loss: 0.2590\n","Epoch 22 - MCC: 0.7858\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8944 - loss: 0.2593 - val_accuracy: 0.8932 - val_loss: 0.2623 - mcc: 0.7858\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8906 - loss: 0.2670\n","Epoch 23 - MCC: 0.7834\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8906 - loss: 0.2669 - val_accuracy: 0.8919 - val_loss: 0.2665 - mcc: 0.7834\n","Epoch 24/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8868 - loss: 0.2718\n","Epoch 24 - MCC: 0.7874\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.8871 - loss: 0.2713 - val_accuracy: 0.8939 - val_loss: 0.2613 - mcc: 0.7874\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8873 - loss: 0.2714\n","Epoch 25 - MCC: 0.7822\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8875 - loss: 0.2712 - val_accuracy: 0.8914 - val_loss: 0.2671 - mcc: 0.7822\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8929 - loss: 0.2619\n","Epoch 26 - MCC: 0.7907\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8928 - loss: 0.2619 - val_accuracy: 0.8956 - val_loss: 0.2584 - mcc: 0.7907\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8922 - loss: 0.2604\n","Epoch 27 - MCC: 0.7908\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8922 - loss: 0.2604 - val_accuracy: 0.8956 - val_loss: 0.2582 - mcc: 0.7908\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8911 - loss: 0.2654\n","Epoch 28 - MCC: 0.7909\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8912 - loss: 0.2652 - val_accuracy: 0.8957 - val_loss: 0.2574 - mcc: 0.7909\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8949 - loss: 0.2567\n","Epoch 29 - MCC: 0.7909\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8948 - loss: 0.2568 - val_accuracy: 0.8955 - val_loss: 0.2579 - mcc: 0.7909\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8909 - loss: 0.2637\n","Epoch 30 - MCC: 0.7883\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8910 - loss: 0.2635 - val_accuracy: 0.8944 - val_loss: 0.2603 - mcc: 0.7883\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8955 - loss: 0.2536\n","Epoch 31 - MCC: 0.7920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8954 - loss: 0.2538 - val_accuracy: 0.8963 - val_loss: 0.2563 - mcc: 0.7920\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8929 - loss: 0.2598\n","Epoch 32 - MCC: 0.7909\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8930 - loss: 0.2596 - val_accuracy: 0.8957 - val_loss: 0.2579 - mcc: 0.7909\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8927 - loss: 0.2605\n","Epoch 33 - MCC: 0.7919\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.8928 - loss: 0.2604 - val_accuracy: 0.8962 - val_loss: 0.2557 - mcc: 0.7919\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9010 - loss: 0.2428\n","Epoch 34 - MCC: 0.7946\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.9007 - loss: 0.2433 - val_accuracy: 0.8976 - val_loss: 0.2532 - mcc: 0.7946\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8999 - loss: 0.2454\n","Epoch 35 - MCC: 0.7935\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8997 - loss: 0.2458 - val_accuracy: 0.8967 - val_loss: 0.2550 - mcc: 0.7935\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8936 - loss: 0.2562\n","Epoch 36 - MCC: 0.7948\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8936 - loss: 0.2562 - val_accuracy: 0.8976 - val_loss: 0.2535 - mcc: 0.7948\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8964 - loss: 0.2525\n","Epoch 37 - MCC: 0.7961\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8964 - loss: 0.2525 - val_accuracy: 0.8982 - val_loss: 0.2510 - mcc: 0.7961\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8992 - loss: 0.2453\n","Epoch 38 - MCC: 0.7985\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8990 - loss: 0.2456 - val_accuracy: 0.8994 - val_loss: 0.2497 - mcc: 0.7985\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8918 - loss: 0.2613\n","Epoch 39 - MCC: 0.7961\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8920 - loss: 0.2609 - val_accuracy: 0.8983 - val_loss: 0.2529 - mcc: 0.7961\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8978 - loss: 0.2498\n","Epoch 40 - MCC: 0.7959\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8978 - loss: 0.2499 - val_accuracy: 0.8981 - val_loss: 0.2532 - mcc: 0.7959\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8961 - loss: 0.2546\n","Epoch 41 - MCC: 0.7970\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8961 - loss: 0.2545 - val_accuracy: 0.8987 - val_loss: 0.2503 - mcc: 0.7970\n","Epoch 42/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8949 - loss: 0.2545\n","Epoch 42 - MCC: 0.7943\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.8951 - loss: 0.2543 - val_accuracy: 0.8974 - val_loss: 0.2530 - mcc: 0.7943\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8970 - loss: 0.2511\n","Epoch 43 - MCC: 0.7959\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8970 - loss: 0.2511 - val_accuracy: 0.8982 - val_loss: 0.2503 - mcc: 0.7959\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8991 - loss: 0.2472\n","Epoch 44 - MCC: 0.7951\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8991 - loss: 0.2474 - val_accuracy: 0.8978 - val_loss: 0.2505 - mcc: 0.7951\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8940 - loss: 0.2564\n","Epoch 45 - MCC: 0.8003\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8941 - loss: 0.2561 - val_accuracy: 0.9004 - val_loss: 0.2464 - mcc: 0.8003\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8974 - loss: 0.2493\n","Epoch 46 - MCC: 0.7988\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8974 - loss: 0.2492 - val_accuracy: 0.8995 - val_loss: 0.2492 - mcc: 0.7988\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8948 - loss: 0.2550\n","Epoch 47 - MCC: 0.7988\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8948 - loss: 0.2549 - val_accuracy: 0.8996 - val_loss: 0.2485 - mcc: 0.7988\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8978 - loss: 0.2494\n","Epoch 48 - MCC: 0.8004\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8978 - loss: 0.2493 - val_accuracy: 0.9003 - val_loss: 0.2466 - mcc: 0.8004\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8984 - loss: 0.2483\n","Epoch 49 - MCC: 0.8003\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8984 - loss: 0.2482 - val_accuracy: 0.9003 - val_loss: 0.2451 - mcc: 0.8003\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9016 - loss: 0.2419\n","Epoch 50 - MCC: 0.7997\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9015 - loss: 0.2421 - val_accuracy: 0.9001 - val_loss: 0.2467 - mcc: 0.7997\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM, Fold: 5\n","Epoch 1/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5478 - loss: 0.6896\n","Epoch 1 - MCC: 0.1929\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.5489 - loss: 0.6892 - val_accuracy: 0.5878 - val_loss: 0.6673 - mcc: 0.1929\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6080 - loss: 0.6560\n","Epoch 2 - MCC: 0.4624\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.6096 - loss: 0.6553 - val_accuracy: 0.7279 - val_loss: 0.5802 - mcc: 0.4624\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7391 - loss: 0.5525\n","Epoch 3 - MCC: 0.5965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.7400 - loss: 0.5510 - val_accuracy: 0.7987 - val_loss: 0.4473 - mcc: 0.5965\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8040 - loss: 0.4383\n","Epoch 4 - MCC: 0.6438\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8043 - loss: 0.4378 - val_accuracy: 0.8222 - val_loss: 0.4023 - mcc: 0.6438\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8251 - loss: 0.3976\n","Epoch 5 - MCC: 0.6684\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8253 - loss: 0.3972 - val_accuracy: 0.8345 - val_loss: 0.3758 - mcc: 0.6684\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8384 - loss: 0.3761\n","Epoch 6 - MCC: 0.6912\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8386 - loss: 0.3756 - val_accuracy: 0.8459 - val_loss: 0.3517 - mcc: 0.6912\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8526 - loss: 0.3502\n","Epoch 7 - MCC: 0.7129\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8528 - loss: 0.3498 - val_accuracy: 0.8568 - val_loss: 0.3331 - mcc: 0.7129\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8616 - loss: 0.3277\n","Epoch 8 - MCC: 0.7352\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8618 - loss: 0.3275 - val_accuracy: 0.8679 - val_loss: 0.3170 - mcc: 0.7352\n","Epoch 9/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8768 - loss: 0.3017\n","Epoch 9 - MCC: 0.7466\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.8765 - loss: 0.3021 - val_accuracy: 0.8735 - val_loss: 0.3039 - mcc: 0.7466\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8730 - loss: 0.3056\n","Epoch 10 - MCC: 0.7483\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8731 - loss: 0.3053 - val_accuracy: 0.8744 - val_loss: 0.2988 - mcc: 0.7483\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8814 - loss: 0.2896\n","Epoch 11 - MCC: 0.7552\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8813 - loss: 0.2897 - val_accuracy: 0.8778 - val_loss: 0.2920 - mcc: 0.7552\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8810 - loss: 0.2871\n","Epoch 12 - MCC: 0.7591\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8811 - loss: 0.2870 - val_accuracy: 0.8798 - val_loss: 0.2887 - mcc: 0.7591\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8858 - loss: 0.2798\n","Epoch 13 - MCC: 0.7627\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8858 - loss: 0.2799 - val_accuracy: 0.8816 - val_loss: 0.2848 - mcc: 0.7627\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8851 - loss: 0.2803\n","Epoch 14 - MCC: 0.7598\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8851 - loss: 0.2802 - val_accuracy: 0.8801 - val_loss: 0.2891 - mcc: 0.7598\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8876 - loss: 0.2738\n","Epoch 15 - MCC: 0.7660\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8877 - loss: 0.2737 - val_accuracy: 0.8832 - val_loss: 0.2793 - mcc: 0.7660\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8872 - loss: 0.2752\n","Epoch 16 - MCC: 0.7707\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8872 - loss: 0.2750 - val_accuracy: 0.8855 - val_loss: 0.2773 - mcc: 0.7707\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8910 - loss: 0.2675\n","Epoch 17 - MCC: 0.7629\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8909 - loss: 0.2676 - val_accuracy: 0.8817 - val_loss: 0.2847 - mcc: 0.7629\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8926 - loss: 0.2637\n","Epoch 18 - MCC: 0.7714\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.8925 - loss: 0.2639 - val_accuracy: 0.8859 - val_loss: 0.2756 - mcc: 0.7714\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8866 - loss: 0.2742\n","Epoch 19 - MCC: 0.7725\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8867 - loss: 0.2739 - val_accuracy: 0.8864 - val_loss: 0.2757 - mcc: 0.7725\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8936 - loss: 0.2608\n","Epoch 20 - MCC: 0.7746\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8935 - loss: 0.2609 - val_accuracy: 0.8875 - val_loss: 0.2713 - mcc: 0.7746\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8901 - loss: 0.2677\n","Epoch 21 - MCC: 0.7741\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8902 - loss: 0.2674 - val_accuracy: 0.8872 - val_loss: 0.2712 - mcc: 0.7741\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8944 - loss: 0.2592\n","Epoch 22 - MCC: 0.7727\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.8943 - loss: 0.2593 - val_accuracy: 0.8865 - val_loss: 0.2739 - mcc: 0.7727\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8942 - loss: 0.2599\n","Epoch 23 - MCC: 0.7751\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8942 - loss: 0.2599 - val_accuracy: 0.8877 - val_loss: 0.2707 - mcc: 0.7751\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8918 - loss: 0.2623\n","Epoch 24 - MCC: 0.7777\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8919 - loss: 0.2623 - val_accuracy: 0.8891 - val_loss: 0.2679 - mcc: 0.7777\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8902 - loss: 0.2670\n","Epoch 25 - MCC: 0.7777\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 62ms/step - accuracy: 0.8903 - loss: 0.2667 - val_accuracy: 0.8890 - val_loss: 0.2670 - mcc: 0.7777\n","Epoch 26/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8990 - loss: 0.2504\n","Epoch 26 - MCC: 0.7785\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.8987 - loss: 0.2509 - val_accuracy: 0.8893 - val_loss: 0.2673 - mcc: 0.7785\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8961 - loss: 0.2540\n","Epoch 27 - MCC: 0.7793\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8961 - loss: 0.2540 - val_accuracy: 0.8897 - val_loss: 0.2665 - mcc: 0.7793\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8946 - loss: 0.2579\n","Epoch 28 - MCC: 0.7799\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8947 - loss: 0.2578 - val_accuracy: 0.8900 - val_loss: 0.2652 - mcc: 0.7799\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8983 - loss: 0.2506\n","Epoch 29 - MCC: 0.7784\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8982 - loss: 0.2508 - val_accuracy: 0.8890 - val_loss: 0.2662 - mcc: 0.7784\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8957 - loss: 0.2553\n","Epoch 30 - MCC: 0.7780\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8957 - loss: 0.2552 - val_accuracy: 0.8892 - val_loss: 0.2682 - mcc: 0.7780\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8929 - loss: 0.2599\n","Epoch 31 - MCC: 0.7815\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8930 - loss: 0.2597 - val_accuracy: 0.8910 - val_loss: 0.2632 - mcc: 0.7815\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8910 - loss: 0.2644\n","Epoch 32 - MCC: 0.7811\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8912 - loss: 0.2639 - val_accuracy: 0.8907 - val_loss: 0.2634 - mcc: 0.7811\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8937 - loss: 0.2583\n","Epoch 33 - MCC: 0.7826\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8938 - loss: 0.2581 - val_accuracy: 0.8915 - val_loss: 0.2620 - mcc: 0.7826\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8952 - loss: 0.2553\n","Epoch 34 - MCC: 0.7822\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.8953 - loss: 0.2552 - val_accuracy: 0.8913 - val_loss: 0.2623 - mcc: 0.7822\n","Epoch 35/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8942 - loss: 0.2577\n","Epoch 35 - MCC: 0.7808\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.8945 - loss: 0.2571 - val_accuracy: 0.8906 - val_loss: 0.2634 - mcc: 0.7808\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8980 - loss: 0.2488\n","Epoch 36 - MCC: 0.7846\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8980 - loss: 0.2488 - val_accuracy: 0.8925 - val_loss: 0.2595 - mcc: 0.7846\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9003 - loss: 0.2451\n","Epoch 37 - MCC: 0.7838\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9003 - loss: 0.2452 - val_accuracy: 0.8921 - val_loss: 0.2613 - mcc: 0.7838\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9006 - loss: 0.2440\n","Epoch 38 - MCC: 0.7837\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9005 - loss: 0.2442 - val_accuracy: 0.8920 - val_loss: 0.2607 - mcc: 0.7837\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8982 - loss: 0.2489\n","Epoch 39 - MCC: 0.7842\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8983 - loss: 0.2488 - val_accuracy: 0.8923 - val_loss: 0.2607 - mcc: 0.7842\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9039 - loss: 0.2382\n","Epoch 40 - MCC: 0.7849\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9038 - loss: 0.2385 - val_accuracy: 0.8926 - val_loss: 0.2597 - mcc: 0.7849\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8960 - loss: 0.2538\n","Epoch 41 - MCC: 0.7845\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8961 - loss: 0.2535 - val_accuracy: 0.8924 - val_loss: 0.2589 - mcc: 0.7845\n","Epoch 42/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9060 - loss: 0.2326\n","Epoch 42 - MCC: 0.7863\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9055 - loss: 0.2336 - val_accuracy: 0.8933 - val_loss: 0.2576 - mcc: 0.7863\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8974 - loss: 0.2516\n","Epoch 43 - MCC: 0.7867\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8975 - loss: 0.2514 - val_accuracy: 0.8936 - val_loss: 0.2564 - mcc: 0.7867\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9022 - loss: 0.2414\n","Epoch 44 - MCC: 0.7841\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9021 - loss: 0.2415 - val_accuracy: 0.8921 - val_loss: 0.2589 - mcc: 0.7841\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9048 - loss: 0.2352\n","Epoch 45 - MCC: 0.7877\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9046 - loss: 0.2356 - val_accuracy: 0.8940 - val_loss: 0.2560 - mcc: 0.7877\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9007 - loss: 0.2430\n","Epoch 46 - MCC: 0.7858\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9007 - loss: 0.2431 - val_accuracy: 0.8931 - val_loss: 0.2580 - mcc: 0.7858\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8971 - loss: 0.2518\n","Epoch 47 - MCC: 0.7882\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8972 - loss: 0.2515 - val_accuracy: 0.8943 - val_loss: 0.2555 - mcc: 0.7882\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9025 - loss: 0.2421\n","Epoch 48 - MCC: 0.7885\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9025 - loss: 0.2421 - val_accuracy: 0.8944 - val_loss: 0.2553 - mcc: 0.7885\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9013 - loss: 0.2422\n","Epoch 49 - MCC: 0.7880\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9013 - loss: 0.2422 - val_accuracy: 0.8941 - val_loss: 0.2562 - mcc: 0.7880\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8997 - loss: 0.2462\n","Epoch 50 - MCC: 0.7837\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8997 - loss: 0.2461 - val_accuracy: 0.8920 - val_loss: 0.2591 - mcc: 0.7837\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.90492,\n","              'mean': 0.8972453333333335,\n","              'min': 0.8865133333333334,\n","              'std': 0.006924449950076231},\n"," 'Inference Time (s/sample)': {'max': 0.0022170424461364748,\n","                               'mean': 0.0019390316009521486,\n","                               'min': 0.0013763642311096192,\n","                               'std': 0.00029919715064554434},\n"," 'MCC': {'max': 0.8091316381087936,\n","         'mean': 0.7939328786881179,\n","         'min': 0.772499662458005,\n","         'std': 0.013740010422870192},\n"," 'Parameters': 4385,\n"," 'Train Time (s)': {'max': 74.7742166519165,\n","                    'mean': 73.20595126152038,\n","                    'min': 72.0775854587555,\n","                    'std': 1.0446407729267222},\n"," 'Training Accuracy': [[0.5706233978271484,\n","                        0.6699332594871521,\n","                        0.7632783651351929,\n","                        0.8022816181182861,\n","                        0.8208434581756592,\n","                        0.8335883617401123,\n","                        0.8451534509658813,\n","                        0.8536516427993774,\n","                        0.861508309841156,\n","                        0.8691167235374451,\n","                        0.8746882677078247,\n","                        0.8779582977294922,\n","                        0.8797749280929565,\n","                        0.8822048902511597,\n","                        0.8833866119384766,\n","                        0.8846266865730286,\n","                        0.8859016299247742,\n","                        0.8869966268539429,\n","                        0.8881034255027771,\n","                        0.889376699924469,\n","                        0.8893483877182007,\n","                        0.8903883099555969,\n","                        0.8911033868789673,\n","                        0.8915550112724304,\n","                        0.8921416401863098,\n","                        0.8913800120353699,\n","                        0.8930132389068604,\n","                        0.89322829246521,\n","                        0.8932783603668213,\n","                        0.893695056438446,\n","                        0.8921517133712769,\n","                        0.8937284350395203,\n","                        0.8945667147636414,\n","                        0.8950350880622864,\n","                        0.8957266807556152,\n","                        0.8957166075706482,\n","                        0.8952718377113342,\n","                        0.8963300585746765,\n","                        0.8959583044052124,\n","                        0.8968983292579651,\n","                        0.896649956703186,\n","                        0.897433340549469,\n","                        0.8979616761207581,\n","                        0.897826611995697,\n","                        0.897971510887146,\n","                        0.8983250260353088,\n","                        0.8981049060821533,\n","                        0.898080050945282,\n","                        0.8986900448799133,\n","                        0.898261547088623],\n","                       [0.5961184501647949,\n","                        0.7078582048416138,\n","                        0.7825067043304443,\n","                        0.8172866106033325,\n","                        0.8330467343330383,\n","                        0.8449783325195312,\n","                        0.8530182838439941,\n","                        0.860251784324646,\n","                        0.8668850660324097,\n","                        0.8721083998680115,\n","                        0.8765484094619751,\n","                        0.8784334063529968,\n","                        0.8816949725151062,\n","                        0.8844850063323975,\n","                        0.8848416805267334,\n","                        0.885846734046936,\n","                        0.888393223285675,\n","                        0.8890382647514343,\n","                        0.8899833559989929,\n","                        0.8910500407218933,\n","                        0.8916900157928467,\n","                        0.892696738243103,\n","                        0.8932266235351562,\n","                        0.8942266702651978,\n","                        0.894713282585144,\n","                        0.8947399854660034,\n","                        0.8955350518226624,\n","                        0.8963066339492798,\n","                        0.8965499997138977,\n","                        0.8974217176437378,\n","                        0.8974167108535767,\n","                        0.8966100215911865,\n","                        0.8970900177955627,\n","                        0.8980867862701416,\n","                        0.8980333805084229,\n","                        0.8985183835029602,\n","                        0.8984382748603821,\n","                        0.89910489320755,\n","                        0.8991432785987854,\n","                        0.8992483615875244,\n","                        0.8997266888618469,\n","                        0.9004483222961426,\n","                        0.900981605052948,\n","                        0.9009416699409485,\n","                        0.9007984399795532,\n","                        0.9008034467697144,\n","                        0.9014049768447876,\n","                        0.9016132950782776,\n","                        0.9024600982666016,\n","                        0.9010416269302368],\n","                       [0.5673216581344604,\n","                        0.6659549474716187,\n","                        0.7708399891853333,\n","                        0.8094133138656616,\n","                        0.822731614112854,\n","                        0.8359066843986511,\n","                        0.8458534479141235,\n","                        0.8535598516464233,\n","                        0.8618099689483643,\n","                        0.8682817220687866,\n","                        0.8724183440208435,\n","                        0.8748999238014221,\n","                        0.87718665599823,\n","                        0.8797150254249573,\n","                        0.8813283443450928,\n","                        0.8831616044044495,\n","                        0.8844767808914185,\n","                        0.8861699104309082,\n","                        0.8871451020240784,\n","                        0.8879033923149109,\n","                        0.8895982503890991,\n","                        0.8900418281555176,\n","                        0.8905583024024963,\n","                        0.8915117383003235,\n","                        0.892091691493988,\n","                        0.8926267027854919,\n","                        0.8930299282073975,\n","                        0.8929316997528076,\n","                        0.8939234018325806,\n","                        0.8942081928253174,\n","                        0.8950067758560181,\n","                        0.8934750556945801,\n","                        0.8951116800308228,\n","                        0.8952299356460571,\n","                        0.8954049944877625,\n","                        0.8957033753395081,\n","                        0.8960532546043396,\n","                        0.8967832922935486,\n","                        0.8965750336647034,\n","                        0.8969766497612,\n","                        0.8969416618347168,\n","                        0.8977664709091187,\n","                        0.8968949317932129,\n","                        0.8980266451835632,\n","                        0.8981999754905701,\n","                        0.8980799317359924,\n","                        0.898945152759552,\n","                        0.8996484279632568,\n","                        0.8992516994476318,\n","                        0.8994165062904358],\n","                       [0.595496654510498,\n","                        0.681233286857605,\n","                        0.7760566473007202,\n","                        0.8149798512458801,\n","                        0.8305916786193848,\n","                        0.8420583605766296,\n","                        0.8550050258636475,\n","                        0.8652583360671997,\n","                        0.8715367317199707,\n","                        0.8744084239006042,\n","                        0.8760601282119751,\n","                        0.8787365555763245,\n","                        0.8811016082763672,\n","                        0.8823034167289734,\n","                        0.8831601142883301,\n","                        0.8835950493812561,\n","                        0.8859348893165588,\n","                        0.886929988861084,\n","                        0.8881784081459045,\n","                        0.8882166147232056,\n","                        0.8893216848373413,\n","                        0.889849841594696,\n","                        0.890561580657959,\n","                        0.8906598091125488,\n","                        0.8909599781036377,\n","                        0.8919999599456787,\n","                        0.8928849101066589,\n","                        0.8933050632476807,\n","                        0.8936149477958679,\n","                        0.8930266499519348,\n","                        0.8933950066566467,\n","                        0.8950248956680298,\n","                        0.894788384437561,\n","                        0.8944050073623657,\n","                        0.8952165842056274,\n","                        0.8947433233261108,\n","                        0.8964566588401794,\n","                        0.8962116241455078,\n","                        0.8969265818595886,\n","                        0.8970566391944885,\n","                        0.8973749279975891,\n","                        0.8967416286468506,\n","                        0.8963450789451599,\n","                        0.8972317576408386,\n","                        0.8979532718658447,\n","                        0.8981849551200867,\n","                        0.8963966369628906,\n","                        0.8985300660133362,\n","                        0.898734986782074,\n","                        0.8987632989883423],\n","                       [0.5626950263977051,\n","                        0.6514782905578613,\n","                        0.7638915777206421,\n","                        0.8113000392913818,\n","                        0.8305866718292236,\n","                        0.8449183106422424,\n","                        0.8570383191108704,\n","                        0.8667583465576172,\n","                        0.8728567361831665,\n","                        0.8772000670433044,\n","                        0.8805000185966492,\n","                        0.8829817771911621,\n","                        0.8840866684913635,\n","                        0.8860750794410706,\n","                        0.8882900476455688,\n","                        0.8890266418457031,\n","                        0.8891983032226562,\n","                        0.8894833326339722,\n","                        0.8905131816864014,\n","                        0.8913915753364563,\n","                        0.8930750489234924,\n","                        0.8930217027664185,\n","                        0.8932881951332092,\n","                        0.8934434652328491,\n","                        0.8937832713127136,\n","                        0.8951815962791443,\n","                        0.8957266807556152,\n","                        0.8961599469184875,\n","                        0.8959633708000183,\n","                        0.8965233564376831,\n","                        0.8964633941650391,\n","                        0.8972566723823547,\n","                        0.8969416618347168,\n","                        0.8970333337783813,\n","                        0.8978816866874695,\n","                        0.8987783789634705,\n","                        0.8993282914161682,\n","                        0.8984950184822083,\n","                        0.8992384076118469,\n","                        0.8990300893783569,\n","                        0.899505078792572,\n","                        0.900153398513794,\n","                        0.9001250267028809,\n","                        0.9001549482345581,\n","                        0.9006599187850952,\n","                        0.9002200961112976,\n","                        0.9009082913398743,\n","                        0.9014366865158081,\n","                        0.9006249904632568,\n","                        0.9006065130233765]],\n"," 'Training Loss': [[0.6845068335533142,\n","                    0.6296850442886353,\n","                    0.5097090005874634,\n","                    0.44474753737449646,\n","                    0.4086492657661438,\n","                    0.38333767652511597,\n","                    0.36122313141822815,\n","                    0.3447345793247223,\n","                    0.3285287916660309,\n","                    0.31571730971336365,\n","                    0.30365321040153503,\n","                    0.29633718729019165,\n","                    0.2903025150299072,\n","                    0.28500306606292725,\n","                    0.28151631355285645,\n","                    0.27868416905403137,\n","                    0.2761007249355316,\n","                    0.2741747200489044,\n","                    0.2711262106895447,\n","                    0.26841434836387634,\n","                    0.269700825214386,\n","                    0.26678168773651123,\n","                    0.2651086449623108,\n","                    0.26354002952575684,\n","                    0.2627273499965668,\n","                    0.2640063166618347,\n","                    0.25984686613082886,\n","                    0.26001015305519104,\n","                    0.2589929401874542,\n","                    0.25839486718177795,\n","                    0.26162782311439514,\n","                    0.2584051787853241,\n","                    0.25574448704719543,\n","                    0.2547273337841034,\n","                    0.25428271293640137,\n","                    0.25417792797088623,\n","                    0.25386473536491394,\n","                    0.2524814307689667,\n","                    0.25237467885017395,\n","                    0.25093191862106323,\n","                    0.2514904737472534,\n","                    0.24954283237457275,\n","                    0.24930837750434875,\n","                    0.2484879344701767,\n","                    0.24830645322799683,\n","                    0.24773536622524261,\n","                    0.24911516904830933,\n","                    0.2474580556154251,\n","                    0.24716606736183167,\n","                    0.24672776460647583],\n","                   [0.6776540279388428,\n","                    0.6074419617652893,\n","                    0.48882755637168884,\n","                    0.41915127635002136,\n","                    0.38676467537879944,\n","                    0.3630007803440094,\n","                    0.3455091714859009,\n","                    0.33072251081466675,\n","                    0.3179578483104706,\n","                    0.30855098366737366,\n","                    0.29828977584838867,\n","                    0.2921755313873291,\n","                    0.28576451539993286,\n","                    0.2800624966621399,\n","                    0.2787797451019287,\n","                    0.2764182388782501,\n","                    0.27158990502357483,\n","                    0.26904964447021484,\n","                    0.2673657536506653,\n","                    0.26578083634376526,\n","                    0.26364535093307495,\n","                    0.26240989565849304,\n","                    0.2607077360153198,\n","                    0.2589106261730194,\n","                    0.2579783499240875,\n","                    0.25711894035339355,\n","                    0.25564509630203247,\n","                    0.25361141562461853,\n","                    0.2529027462005615,\n","                    0.25172924995422363,\n","                    0.25081080198287964,\n","                    0.25237491726875305,\n","                    0.25174906849861145,\n","                    0.24925704300403595,\n","                    0.2497752606868744,\n","                    0.24856054782867432,\n","                    0.24876874685287476,\n","                    0.24691259860992432,\n","                    0.24680902063846588,\n","                    0.2466856986284256,\n","                    0.24608030915260315,\n","                    0.24411962926387787,\n","                    0.2436203509569168,\n","                    0.24271047115325928,\n","                    0.2433239221572876,\n","                    0.2430846095085144,\n","                    0.24254268407821655,\n","                    0.24087591469287872,\n","                    0.23993444442749023,\n","                    0.24257361888885498],\n","                   [0.6848819255828857,\n","                    0.6348900198936462,\n","                    0.5034487247467041,\n","                    0.4343145787715912,\n","                    0.4019871950149536,\n","                    0.3798684775829315,\n","                    0.35976359248161316,\n","                    0.34408360719680786,\n","                    0.3293631374835968,\n","                    0.3160277307033539,\n","                    0.3062467575073242,\n","                    0.29980021715164185,\n","                    0.2962583303451538,\n","                    0.2896144688129425,\n","                    0.28730249404907227,\n","                    0.28275975584983826,\n","                    0.28085628151893616,\n","                    0.2760392427444458,\n","                    0.2732289433479309,\n","                    0.27194613218307495,\n","                    0.2686644196510315,\n","                    0.26765382289886475,\n","                    0.2664841413497925,\n","                    0.26361605525016785,\n","                    0.2626400291919708,\n","                    0.26167622208595276,\n","                    0.26000699400901794,\n","                    0.2605120837688446,\n","                    0.2580015957355499,\n","                    0.25722989439964294,\n","                    0.25650662183761597,\n","                    0.25908225774765015,\n","                    0.2549055218696594,\n","                    0.25442829728126526,\n","                    0.25480416417121887,\n","                    0.2535426616668701,\n","                    0.25341424345970154,\n","                    0.2512000501155853,\n","                    0.25158825516700745,\n","                    0.25053587555885315,\n","                    0.24975669384002686,\n","                    0.2485606074333191,\n","                    0.2505594789981842,\n","                    0.2481590062379837,\n","                    0.2477855235338211,\n","                    0.2467917948961258,\n","                    0.24521741271018982,\n","                    0.24447020888328552,\n","                    0.2449238896369934,\n","                    0.24436573684215546],\n","                   [0.6788591742515564,\n","                    0.6206926703453064,\n","                    0.5014647245407104,\n","                    0.42822301387786865,\n","                    0.3927552103996277,\n","                    0.36661213636398315,\n","                    0.34301039576530457,\n","                    0.32335802912712097,\n","                    0.3113420009613037,\n","                    0.30109289288520813,\n","                    0.29883378744125366,\n","                    0.2931326627731323,\n","                    0.2861603796482086,\n","                    0.2833419740200043,\n","                    0.28192028403282166,\n","                    0.2802467942237854,\n","                    0.2748934030532837,\n","                    0.27370980381965637,\n","                    0.27076253294944763,\n","                    0.2702168822288513,\n","                    0.26774340867996216,\n","                    0.2669292390346527,\n","                    0.2656669318675995,\n","                    0.2650446593761444,\n","                    0.2647409737110138,\n","                    0.26254141330718994,\n","                    0.2599183917045593,\n","                    0.2595544159412384,\n","                    0.2589353621006012,\n","                    0.25986728072166443,\n","                    0.25884827971458435,\n","                    0.25585418939590454,\n","                    0.2562301754951477,\n","                    0.2559284269809723,\n","                    0.25535452365875244,\n","                    0.2552863359451294,\n","                    0.2525779604911804,\n","                    0.25220295786857605,\n","                    0.25175297260284424,\n","                    0.2513423562049866,\n","                    0.2513503134250641,\n","                    0.2516791820526123,\n","                    0.25170889496803284,\n","                    0.2502899765968323,\n","                    0.2492721676826477,\n","                    0.24761606752872467,\n","                    0.2516104578971863,\n","                    0.2479773759841919,\n","                    0.24707278609275818,\n","                    0.24686414003372192],\n","                   [0.6842890381813049,\n","                    0.6380106806755066,\n","                    0.5131042003631592,\n","                    0.4253474473953247,\n","                    0.38776034116744995,\n","                    0.3622598946094513,\n","                    0.33921921253204346,\n","                    0.3206510841846466,\n","                    0.3076108396053314,\n","                    0.29753342270851135,\n","                    0.2903505563735962,\n","                    0.28407442569732666,\n","                    0.28186169266700745,\n","                    0.27785301208496094,\n","                    0.2726229727268219,\n","                    0.2707039713859558,\n","                    0.27016329765319824,\n","                    0.26950955390930176,\n","                    0.26693907380104065,\n","                    0.26456019282341003,\n","                    0.2613096237182617,\n","                    0.26180315017700195,\n","                    0.26115018129348755,\n","                    0.26114967465400696,\n","                    0.25949060916900635,\n","                    0.25663483142852783,\n","                    0.2556517422199249,\n","                    0.254972368478775,\n","                    0.2549521327018738,\n","                    0.2531290352344513,\n","                    0.25355464220046997,\n","                    0.2518863081932068,\n","                    0.25214648246765137,\n","                    0.2518376410007477,\n","                    0.2502618134021759,\n","                    0.2490232288837433,\n","                    0.2474287450313568,\n","                    0.24882888793945312,\n","                    0.24691501259803772,\n","                    0.2470780909061432,\n","                    0.2463042140007019,\n","                    0.24566546082496643,\n","                    0.24521276354789734,\n","                    0.24517516791820526,\n","                    0.24401366710662842,\n","                    0.24490246176719666,\n","                    0.2437075972557068,\n","                    0.24244995415210724,\n","                    0.2431313544511795,\n","                    0.24393168091773987]],\n"," 'Validation Accuracy': [[0.6024866700172424,\n","                          0.7490466833114624,\n","                          0.7929933071136475,\n","                          0.8208533525466919,\n","                          0.8349734544754028,\n","                          0.8487666845321655,\n","                          0.8541866540908813,\n","                          0.8659866452217102,\n","                          0.8736332654953003,\n","                          0.8797599673271179,\n","                          0.8809599876403809,\n","                          0.8868399858474731,\n","                          0.8899133205413818,\n","                          0.890946626663208,\n","                          0.8896732926368713,\n","                          0.8938800096511841,\n","                          0.8931533694267273,\n","                          0.8954200148582458,\n","                          0.8960066437721252,\n","                          0.8916400074958801,\n","                          0.8961732983589172,\n","                          0.8955866098403931,\n","                          0.8967067003250122,\n","                          0.8988733887672424,\n","                          0.8972132802009583,\n","                          0.8983867168426514,\n","                          0.8991666436195374,\n","                          0.8995199799537659,\n","                          0.8997600674629211,\n","                          0.900046706199646,\n","                          0.8966600894927979,\n","                          0.898566722869873,\n","                          0.8994333148002625,\n","                          0.9007667303085327,\n","                          0.9005666971206665,\n","                          0.9011666774749756,\n","                          0.8982799649238586,\n","                          0.9017933011054993,\n","                          0.9001400470733643,\n","                          0.9019200205802917,\n","                          0.9017133116722107,\n","                          0.9007666707038879,\n","                          0.9029600024223328,\n","                          0.9025466442108154,\n","                          0.901699960231781,\n","                          0.9030600190162659,\n","                          0.9033399820327759,\n","                          0.9032599925994873,\n","                          0.9013066291809082,\n","                          0.9027132391929626],\n","                         [0.6129800081253052,\n","                          0.7353666424751282,\n","                          0.7954399585723877,\n","                          0.8164399862289429,\n","                          0.8287067413330078,\n","                          0.8371666669845581,\n","                          0.8428733944892883,\n","                          0.8489066362380981,\n","                          0.8577466607093811,\n","                          0.8587133288383484,\n","                          0.8603733777999878,\n","                          0.8662333488464355,\n","                          0.8694266676902771,\n","                          0.869713306427002,\n","                          0.8692599534988403,\n","                          0.8734599351882935,\n","                          0.8736667037010193,\n","                          0.876039981842041,\n","                          0.8756866455078125,\n","                          0.8754866123199463,\n","                          0.8762000203132629,\n","                          0.8771799206733704,\n","                          0.8792932629585266,\n","                          0.8788334131240845,\n","                          0.8787333965301514,\n","                          0.8806800246238708,\n","                          0.8810666799545288,\n","                          0.8813000321388245,\n","                          0.8810866475105286,\n","                          0.8807000517845154,\n","                          0.8826800584793091,\n","                          0.882860004901886,\n","                          0.8823866844177246,\n","                          0.8826332688331604,\n","                          0.8833400011062622,\n","                          0.8827800154685974,\n","                          0.8830599784851074,\n","                          0.884320080280304,\n","                          0.8840799927711487,\n","                          0.8840600848197937,\n","                          0.8841066956520081,\n","                          0.8846666812896729,\n","                          0.8844999670982361,\n","                          0.8840332627296448,\n","                          0.8832066059112549,\n","                          0.8841000199317932,\n","                          0.8855533599853516,\n","                          0.8853133320808411,\n","                          0.886680006980896,\n","                          0.886513352394104],\n","                         [0.6282867193222046,\n","                          0.7388266921043396,\n","                          0.8081400394439697,\n","                          0.8188000321388245,\n","                          0.8344666957855225,\n","                          0.8451666831970215,\n","                          0.8551866412162781,\n","                          0.8631466031074524,\n","                          0.8719599843025208,\n","                          0.877666711807251,\n","                          0.8803333044052124,\n","                          0.8799466490745544,\n","                          0.8841866254806519,\n","                          0.8860466480255127,\n","                          0.8859533071517944,\n","                          0.8889467716217041,\n","                          0.8924132585525513,\n","                          0.8925333619117737,\n","                          0.8943399786949158,\n","                          0.895046591758728,\n","                          0.8966800570487976,\n","                          0.8951666355133057,\n","                          0.897320032119751,\n","                          0.8963199853897095,\n","                          0.8966732621192932,\n","                          0.900046706199646,\n","                          0.898360013961792,\n","                          0.8996732831001282,\n","                          0.9003867506980896,\n","                          0.9009199738502502,\n","                          0.8996533155441284,\n","                          0.8984465599060059,\n","                          0.9020000696182251,\n","                          0.9011666774749756,\n","                          0.9024066925048828,\n","                          0.9007800221443176,\n","                          0.9031866192817688,\n","                          0.9022399187088013,\n","                          0.9018734097480774,\n","                          0.9031266570091248,\n","                          0.903613269329071,\n","                          0.9046599864959717,\n","                          0.9020733833312988,\n","                          0.9038465619087219,\n","                          0.9043732285499573,\n","                          0.9033600091934204,\n","                          0.9051333665847778,\n","                          0.905239999294281,\n","                          0.905386745929718,\n","                          0.9049199819564819],\n","                         [0.6141066551208496,\n","                          0.7543799877166748,\n","                          0.8105533719062805,\n","                          0.8322001099586487,\n","                          0.8423200249671936,\n","                          0.8568333387374878,\n","                          0.8640733361244202,\n","                          0.8737999796867371,\n","                          0.875986635684967,\n","                          0.8794733285903931,\n","                          0.8788067102432251,\n","                          0.8823200464248657,\n","                          0.8856533765792847,\n","                          0.8859133124351501,\n","                          0.8819467425346375,\n","                          0.8870200514793396,\n","                          0.8894467353820801,\n","                          0.8890866637229919,\n","                          0.8914133310317993,\n","                          0.8916200399398804,\n","                          0.8927534222602844,\n","                          0.8931533694267273,\n","                          0.8918533325195312,\n","                          0.8939467072486877,\n","                          0.8913933038711548,\n","                          0.8955999612808228,\n","                          0.895639955997467,\n","                          0.8957133293151855,\n","                          0.8954933881759644,\n","                          0.8944200277328491,\n","                          0.89628005027771,\n","                          0.8956933617591858,\n","                          0.8962200284004211,\n","                          0.8975666165351868,\n","                          0.8966600894927979,\n","                          0.8975866436958313,\n","                          0.898246705532074,\n","                          0.8994466662406921,\n","                          0.8982666730880737,\n","                          0.898099958896637,\n","                          0.8987200856208801,\n","                          0.8974066376686096,\n","                          0.8982200026512146,\n","                          0.8977532982826233,\n","                          0.9003867506980896,\n","                          0.8995199799537659,\n","                          0.8995600342750549,\n","                          0.9003133177757263,\n","                          0.9003067016601562,\n","                          0.9000933170318604],\n","                         [0.5877532958984375,\n","                          0.7279066443443298,\n","                          0.7986999750137329,\n","                          0.8222399950027466,\n","                          0.8344866037368774,\n","                          0.8458999395370483,\n","                          0.8567532896995544,\n","                          0.8678533434867859,\n","                          0.8735267519950867,\n","                          0.8743533492088318,\n","                          0.8778199553489685,\n","                          0.8797999620437622,\n","                          0.8815533518791199,\n","                          0.8801332712173462,\n","                          0.8831666707992554,\n","                          0.8855399489402771,\n","                          0.8816599249839783,\n","                          0.8859399557113647,\n","                          0.8864266872406006,\n","                          0.8875000476837158,\n","                          0.8872132897377014,\n","                          0.8865333795547485,\n","                          0.8877333998680115,\n","                          0.8890599608421326,\n","                          0.8889866471290588,\n","                          0.889259934425354,\n","                          0.889666736125946,\n","                          0.8900400400161743,\n","                          0.8890266418457031,\n","                          0.8892333507537842,\n","                          0.8909599781036377,\n","                          0.8906866312026978,\n","                          0.891486644744873,\n","                          0.8913200497627258,\n","                          0.8905532956123352,\n","                          0.8924933075904846,\n","                          0.8920667171478271,\n","                          0.8919533491134644,\n","                          0.8922865986824036,\n","                          0.8926133513450623,\n","                          0.892373263835907,\n","                          0.8932999968528748,\n","                          0.8935666680335999,\n","                          0.8921467065811157,\n","                          0.8940132856369019,\n","                          0.8930932879447937,\n","                          0.8943066596984863,\n","                          0.8944132924079895,\n","                          0.8941200375556946,\n","                          0.891986608505249]],\n"," 'Validation Loss': [[0.6686935424804688,\n","                      0.5636478662490845,\n","                      0.4604055881500244,\n","                      0.4171704351902008,\n","                      0.3868701159954071,\n","                      0.36301907896995544,\n","                      0.34186241030693054,\n","                      0.3250506520271301,\n","                      0.3097839951515198,\n","                      0.2968987226486206,\n","                      0.29201316833496094,\n","                      0.28041768074035645,\n","                      0.2740938067436218,\n","                      0.27075764536857605,\n","                      0.2714751958847046,\n","                      0.2642190158367157,\n","                      0.2640456259250641,\n","                      0.2593769133090973,\n","                      0.25832512974739075,\n","                      0.26545450091362,\n","                      0.2561414837837219,\n","                      0.2571711838245392,\n","                      0.25408101081848145,\n","                      0.2509500980377197,\n","                      0.2535262703895569,\n","                      0.25071799755096436,\n","                      0.24928349256515503,\n","                      0.24752746522426605,\n","                      0.24681106209754944,\n","                      0.24617061018943787,\n","                      0.25178876519203186,\n","                      0.24766458570957184,\n","                      0.24622277915477753,\n","                      0.24388699233531952,\n","                      0.2440740019083023,\n","                      0.2432856410741806,\n","                      0.24686433374881744,\n","                      0.2422545850276947,\n","                      0.24403615295886993,\n","                      0.2410132735967636,\n","                      0.24101538956165314,\n","                      0.24170315265655518,\n","                      0.23933957517147064,\n","                      0.23998503386974335,\n","                      0.23972803354263306,\n","                      0.238148033618927,\n","                      0.2372954934835434,\n","                      0.2387860119342804,\n","                      0.24113614857196808,\n","                      0.23707975447177887],\n","                     [0.6604187488555908,\n","                      0.5651127696037292,\n","                      0.462999165058136,\n","                      0.42036914825439453,\n","                      0.3967597484588623,\n","                      0.37757110595703125,\n","                      0.36411696672439575,\n","                      0.3529832363128662,\n","                      0.3382701873779297,\n","                      0.3292108178138733,\n","                      0.3254799544811249,\n","                      0.3160276710987091,\n","                      0.3100128173828125,\n","                      0.3088586926460266,\n","                      0.3092789053916931,\n","                      0.3013262152671814,\n","                      0.29956406354904175,\n","                      0.29508841037750244,\n","                      0.2953415513038635,\n","                      0.2953483760356903,\n","                      0.2946595847606659,\n","                      0.2931118309497833,\n","                      0.2874959111213684,\n","                      0.2903767228126526,\n","                      0.2894069254398346,\n","                      0.2854081094264984,\n","                      0.2836560010910034,\n","                      0.2826197147369385,\n","                      0.28507882356643677,\n","                      0.2844499349594116,\n","                      0.2819816768169403,\n","                      0.2797623574733734,\n","                      0.28445303440093994,\n","                      0.28415533900260925,\n","                      0.2785918712615967,\n","                      0.27961620688438416,\n","                      0.2784324884414673,\n","                      0.27698972821235657,\n","                      0.27797457575798035,\n","                      0.27620285749435425,\n","                      0.27651816606521606,\n","                      0.2781244218349457,\n","                      0.27641817927360535,\n","                      0.27927789092063904,\n","                      0.2785343825817108,\n","                      0.27629339694976807,\n","                      0.2733309864997864,\n","                      0.2748156785964966,\n","                      0.27274295687675476,\n","                      0.27329808473587036],\n","                     [0.6677425503730774,\n","                      0.5740516185760498,\n","                      0.45344552397727966,\n","                      0.40934839844703674,\n","                      0.38186514377593994,\n","                      0.36036112904548645,\n","                      0.34472042322158813,\n","                      0.33029961585998535,\n","                      0.3083057403564453,\n","                      0.2951411306858063,\n","                      0.2894016206264496,\n","                      0.29361480474472046,\n","                      0.2799209654331207,\n","                      0.27377697825431824,\n","                      0.27987977862358093,\n","                      0.2728337049484253,\n","                      0.26257967948913574,\n","                      0.26139792799949646,\n","                      0.25889164209365845,\n","                      0.2582204043865204,\n","                      0.25353842973709106,\n","                      0.2562200725078583,\n","                      0.24960824847221375,\n","                      0.2527918517589569,\n","                      0.24969534575939178,\n","                      0.2447308897972107,\n","                      0.2483282834291458,\n","                      0.24385908246040344,\n","                      0.2427012026309967,\n","                      0.24131441116333008,\n","                      0.24333155155181885,\n","                      0.24734358489513397,\n","                      0.24045999348163605,\n","                      0.23999644815921783,\n","                      0.2382480949163437,\n","                      0.242557093501091,\n","                      0.23589013516902924,\n","                      0.2375471144914627,\n","                      0.23855046927928925,\n","                      0.23609617352485657,\n","                      0.23560769855976105,\n","                      0.23356588184833527,\n","                      0.23894336819648743,\n","                      0.2331618070602417,\n","                      0.2326640486717224,\n","                      0.23465442657470703,\n","                      0.23162691295146942,\n","                      0.23101726174354553,\n","                      0.23097486793994904,\n","                      0.23132914304733276],\n","                     [0.6593736410140991,\n","                      0.561579704284668,\n","                      0.44109004735946655,\n","                      0.40153735876083374,\n","                      0.369629830121994,\n","                      0.3499637246131897,\n","                      0.32849356532096863,\n","                      0.31176403164863586,\n","                      0.30212312936782837,\n","                      0.29750820994377136,\n","                      0.2934704124927521,\n","                      0.2873484492301941,\n","                      0.2830858528614044,\n","                      0.27982768416404724,\n","                      0.2878781259059906,\n","                      0.276962012052536,\n","                      0.27228638529777527,\n","                      0.27287670969963074,\n","                      0.2700026035308838,\n","                      0.2670356333255768,\n","                      0.2651858627796173,\n","                      0.2622731924057007,\n","                      0.2665497660636902,\n","                      0.26127246022224426,\n","                      0.26714688539505005,\n","                      0.25843676924705505,\n","                      0.25822219252586365,\n","                      0.25743746757507324,\n","                      0.25794699788093567,\n","                      0.2603203058242798,\n","                      0.25632214546203613,\n","                      0.2578689754009247,\n","                      0.25573986768722534,\n","                      0.2531989812850952,\n","                      0.255035400390625,\n","                      0.2535179555416107,\n","                      0.2509910464286804,\n","                      0.24974796175956726,\n","                      0.25291404128074646,\n","                      0.2532382905483246,\n","                      0.2503475546836853,\n","                      0.2529851198196411,\n","                      0.25026535987854004,\n","                      0.250510573387146,\n","                      0.24643707275390625,\n","                      0.249160036444664,\n","                      0.2485029399394989,\n","                      0.24656431376934052,\n","                      0.24510419368743896,\n","                      0.2467062622308731],\n","                     [0.6673066020011902,\n","                      0.5802269577980042,\n","                      0.44728007912635803,\n","                      0.4023033380508423,\n","                      0.3758021295070648,\n","                      0.35167354345321655,\n","                      0.3330537676811218,\n","                      0.3169908821582794,\n","                      0.3038746416568756,\n","                      0.29884254932403564,\n","                      0.29196351766586304,\n","                      0.28868022561073303,\n","                      0.28480440378189087,\n","                      0.28905248641967773,\n","                      0.27925601601600647,\n","                      0.2772819399833679,\n","                      0.28473103046417236,\n","                      0.27555808424949646,\n","                      0.2756836414337158,\n","                      0.2712806463241577,\n","                      0.2712017297744751,\n","                      0.27387621998786926,\n","                      0.27067333459854126,\n","                      0.26791560649871826,\n","                      0.26700788736343384,\n","                      0.2672942578792572,\n","                      0.26651471853256226,\n","                      0.26518452167510986,\n","                      0.2661589980125427,\n","                      0.2681853175163269,\n","                      0.26322251558303833,\n","                      0.26342129707336426,\n","                      0.26198288798332214,\n","                      0.2623143196105957,\n","                      0.26338374614715576,\n","                      0.25954800844192505,\n","                      0.2613280117511749,\n","                      0.2606661915779114,\n","                      0.2607017457485199,\n","                      0.2596740424633026,\n","                      0.2589474320411682,\n","                      0.2576127350330353,\n","                      0.25641530752182007,\n","                      0.25891464948654175,\n","                      0.2559935748577118,\n","                      0.25801560282707214,\n","                      0.25551438331604004,\n","                      0.25531601905822754,\n","                      0.256239116191864,\n","                      0.25914621353149414]],\n"," 'Validation MCC': [[0.22084420960880746,\n","                     0.4982789727215572,\n","                     0.5840995721925721,\n","                     0.6400779745429417,\n","                     0.6685700164019193,\n","                     0.6962201649768421,\n","                     0.7071664506119862,\n","                     0.730882591570982,\n","                     0.7463083239079448,\n","                     0.7585390570148136,\n","                     0.7609301874496381,\n","                     0.7727386980528869,\n","                     0.7789205106396047,\n","                     0.7810823373585739,\n","                     0.778547850097376,\n","                     0.7869143780142583,\n","                     0.7854592857622708,\n","                     0.790013611952033,\n","                     0.7911793762612596,\n","                     0.7827103095648347,\n","                     0.7917074889171232,\n","                     0.7903211016935462,\n","                     0.7926345206520148,\n","                     0.7969527150266836,\n","                     0.7935928200762066,\n","                     0.7960271574065342,\n","                     0.7975089815328018,\n","                     0.798254740202336,\n","                     0.7987590891605701,\n","                     0.7993799349284348,\n","                     0.7929289402397565,\n","                     0.7963120454379438,\n","                     0.7980723724224476,\n","                     0.8007247291801292,\n","                     0.8003232890055038,\n","                     0.801549552578799,\n","                     0.795799963011294,\n","                     0.8028246793338999,\n","                     0.7995222095476254,\n","                     0.8031005304827162,\n","                     0.8028580144042948,\n","                     0.8008227278292945,\n","                     0.8051308121754109,\n","                     0.8044580078537237,\n","                     0.8026036664091195,\n","                     0.8053729880836483,\n","                     0.8059100019556747,\n","                     0.8057573038684647,\n","                     0.8021398576899017,\n","                     0.804667077300133],\n","                    [0.24965760066331502,\n","                     0.47939730419605714,\n","                     0.5896872944668491,\n","                     0.6322361716203756,\n","                     0.6565727766716568,\n","                     0.67344578517141,\n","                     0.6849743807608293,\n","                     0.6976138645302591,\n","                     0.7147382766202743,\n","                     0.716868435112885,\n","                     0.7206277973766521,\n","                     0.7321670816856457,\n","                     0.7382210009166945,\n","                     0.7389417816648817,\n","                     0.737841008800565,\n","                     0.7463886655178816,\n","                     0.746840799009935,\n","                     0.7514409111092917,\n","                     0.7508316940373059,\n","                     0.7504274534243208,\n","                     0.7517665176566137,\n","                     0.7546491968520666,\n","                     0.758134360882379,\n","                     0.7572142524625505,\n","                     0.7568528317194901,\n","                     0.7607499385436008,\n","                     0.7617702476793862,\n","                     0.7620149124984323,\n","                     0.7617108368329404,\n","                     0.761122961888835,\n","                     0.7649085392191053,\n","                     0.7651594915035234,\n","                     0.7642586270234517,\n","                     0.7648008358094806,\n","                     0.7663156710158547,\n","                     0.7649617830330467,\n","                     0.7661264918535029,\n","                     0.7681650642901923,\n","                     0.7677258130662262,\n","                     0.7675307472453332,\n","                     0.7677869626226927,\n","                     0.7690752041579583,\n","                     0.7684234898352509,\n","                     0.7676761872177963,\n","                     0.7666174545106219,\n","                     0.7677080508403796,\n","                     0.7707171549144303,\n","                     0.7701754557836746,\n","                     0.7730449613006388,\n","                     0.772499662458005],\n","                    [0.2663891650163556,\n","                     0.48004087212714175,\n","                     0.615042636751116,\n","                     0.6359027745928589,\n","                     0.6674920998447816,\n","                     0.6890159135571706,\n","                     0.7095917668203238,\n","                     0.7257157863915544,\n","                     0.742755036490227,\n","                     0.7542412222895715,\n","                     0.7596468895023331,\n","                     0.7602422519415782,\n","                     0.7680482582536701,\n","                     0.7711693602563565,\n","                     0.7713715238384059,\n","                     0.7769848406508992,\n","                     0.7838966394450906,\n","                     0.7846202259206252,\n","                     0.7878912998766521,\n","                     0.7891807205311877,\n","                     0.7924506137155054,\n","                     0.7901822936541852,\n","                     0.7937470960811981,\n","                     0.791963179695378,\n","                     0.7926850970263366,\n","                     0.7992394019789566,\n","                     0.7962240792067407,\n","                     0.7986751132841409,\n","                     0.8002326370432775,\n","                     0.8010233769665984,\n","                     0.7988926899692458,\n","                     0.7964567273639261,\n","                     0.8032374524587215,\n","                     0.8016104801339166,\n","                     0.8040337101864506,\n","                     0.8007786934792607,\n","                     0.8055539101062181,\n","                     0.8036326923480733,\n","                     0.8030947526030594,\n","                     0.8054129628472347,\n","                     0.8063859679457911,\n","                     0.8086304709774835,\n","                     0.8038325615214738,\n","                     0.8069215760030228,\n","                     0.8079379295460637,\n","                     0.8060579449264725,\n","                     0.8094412392980379,\n","                     0.8096878043786462,\n","                     0.8099533713764583,\n","                     0.8091316381087936],\n","                    [0.24315876903288933,\n","                     0.5155900167995435,\n","                     0.6203011406033874,\n","                     0.6634452343856548,\n","                     0.6837592002794914,\n","                     0.7130222610601851,\n","                     0.7274992797561068,\n","                     0.7473069288499791,\n","                     0.7513210281204046,\n","                     0.7586720393114692,\n","                     0.7574996376991401,\n","                     0.7640587181695079,\n","                     0.7708578033902856,\n","                     0.7713031386152842,\n","                     0.7632787273531377,\n","                     0.7736253583360486,\n","                     0.778335487921963,\n","                     0.7777544791345342,\n","                     0.7823153385712857,\n","                     0.782807817896226,\n","                     0.7849564907127753,\n","                     0.785758863156787,\n","                     0.7834493733898137,\n","                     0.7873612619608507,\n","                     0.7822346256954074,\n","                     0.7907467429360249,\n","                     0.7907830723542928,\n","                     0.7909094452044682,\n","                     0.7908657084296649,\n","                     0.7883144909549862,\n","                     0.7920414642368154,\n","                     0.7908608832096462,\n","                     0.7919046641104708,\n","                     0.7946162113180397,\n","                     0.7935213037718176,\n","                     0.7948111732981454,\n","                     0.7960860990763814,\n","                     0.7984812684646512,\n","                     0.79613162865371,\n","                     0.7959048946171947,\n","                     0.7970070089090241,\n","                     0.794291530632321,\n","                     0.7959307699988057,\n","                     0.795124823377808,\n","                     0.8002848289759698,\n","                     0.7987542960572592,\n","                     0.7988331822476095,\n","                     0.8003764152337045,\n","                     0.8003112009777658,\n","                     0.7996894904172178],\n","                    [0.1929131006268434,\n","                     0.46239475918942685,\n","                     0.5964940326080871,\n","                     0.6437795805837291,\n","                     0.6684419255202769,\n","                     0.6911615796489337,\n","                     0.7129283429492096,\n","                     0.735220023045218,\n","                     0.7466202662434306,\n","                     0.7483212783536877,\n","                     0.7551640563787337,\n","                     0.7591468743673784,\n","                     0.7626515006360979,\n","                     0.7598096642736555,\n","                     0.7659922694290888,\n","                     0.7706696252597702,\n","                     0.7628813218100182,\n","                     0.7714420919992029,\n","                     0.7724558821747454,\n","                     0.77457167888147,\n","                     0.7741002921049218,\n","                     0.7726547584197228,\n","                     0.7750546490270276,\n","                     0.7776920172561869,\n","                     0.7776615112185206,\n","                     0.7784642827007109,\n","                     0.7792730872562643,\n","                     0.7798539422873654,\n","                     0.7784175685001,\n","                     0.7780341873394004,\n","                     0.7814947089929833,\n","                     0.7811081975967554,\n","                     0.7825520538078021,\n","                     0.7822166424752248,\n","                     0.7808462453280332,\n","                     0.7845883262266429,\n","                     0.7837617602238311,\n","                     0.7836733920377195,\n","                     0.7841905182195947,\n","                     0.7848777691707621,\n","                     0.7845017211203408,\n","                     0.7862600395220865,\n","                     0.7867299491811813,\n","                     0.7840796772272093,\n","                     0.7876593928227245,\n","                     0.785775233720931,\n","                     0.7882153055098168,\n","                     0.7884881623045353,\n","                     0.7879828003244906,\n","                     0.7836765251564402]]}\n","Training Model: LSTM_Dense, Fold: 1\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5645 - loss: 0.6846\n","Epoch 1 - MCC: 0.3722\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 70ms/step - accuracy: 0.5663 - loss: 0.6843 - val_accuracy: 0.6880 - val_loss: 0.6539 - mcc: 0.3722\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6984 - loss: 0.6346\n","Epoch 2 - MCC: 0.5253\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.6988 - loss: 0.6337 - val_accuracy: 0.7636 - val_loss: 0.5425 - mcc: 0.5253\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.7766 - loss: 0.5103\n","Epoch 3 - MCC: 0.6391\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.7772 - loss: 0.5087 - val_accuracy: 0.8194 - val_loss: 0.4124 - mcc: 0.6391\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8201 - loss: 0.4114\n","Epoch 4 - MCC: 0.6860\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8204 - loss: 0.4107 - val_accuracy: 0.8437 - val_loss: 0.3627 - mcc: 0.6860\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8305 - loss: 0.3794\n","Epoch 5 - MCC: 0.7142\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8310 - loss: 0.3786 - val_accuracy: 0.8577 - val_loss: 0.3367 - mcc: 0.7142\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8490 - loss: 0.3473\n","Epoch 6 - MCC: 0.7234\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8492 - loss: 0.3470 - val_accuracy: 0.8621 - val_loss: 0.3196 - mcc: 0.7234\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8615 - loss: 0.3233\n","Epoch 7 - MCC: 0.7416\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.8615 - loss: 0.3233 - val_accuracy: 0.8713 - val_loss: 0.3030 - mcc: 0.7416\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8688 - loss: 0.3074\n","Epoch 8 - MCC: 0.7627\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8688 - loss: 0.3073 - val_accuracy: 0.8818 - val_loss: 0.2855 - mcc: 0.7627\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8757 - loss: 0.2941\n","Epoch 9 - MCC: 0.7722\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8758 - loss: 0.2941 - val_accuracy: 0.8866 - val_loss: 0.2767 - mcc: 0.7722\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8811 - loss: 0.2849\n","Epoch 10 - MCC: 0.7777\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8810 - loss: 0.2850 - val_accuracy: 0.8893 - val_loss: 0.2706 - mcc: 0.7777\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8894 - loss: 0.2698\n","Epoch 11 - MCC: 0.7681\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8891 - loss: 0.2702 - val_accuracy: 0.8840 - val_loss: 0.2770 - mcc: 0.7681\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8770 - loss: 0.2939\n","Epoch 12 - MCC: 0.7828\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8772 - loss: 0.2934 - val_accuracy: 0.8916 - val_loss: 0.2652 - mcc: 0.7828\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8838 - loss: 0.2783\n","Epoch 13 - MCC: 0.7892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.8839 - loss: 0.2781 - val_accuracy: 0.8949 - val_loss: 0.2586 - mcc: 0.7892\n","Epoch 14/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8813 - loss: 0.2847\n","Epoch 14 - MCC: 0.7871\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - accuracy: 0.8817 - loss: 0.2839 - val_accuracy: 0.8938 - val_loss: 0.2590 - mcc: 0.7871\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8863 - loss: 0.2760\n","Epoch 15 - MCC: 0.7907\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8864 - loss: 0.2758 - val_accuracy: 0.8957 - val_loss: 0.2565 - mcc: 0.7907\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8904 - loss: 0.2655\n","Epoch 16 - MCC: 0.7943\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8904 - loss: 0.2656 - val_accuracy: 0.8976 - val_loss: 0.2535 - mcc: 0.7943\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8837 - loss: 0.2784\n","Epoch 17 - MCC: 0.7905\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.8839 - loss: 0.2780 - val_accuracy: 0.8957 - val_loss: 0.2554 - mcc: 0.7905\n","Epoch 18/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8912 - loss: 0.2634\n","Epoch 18 - MCC: 0.7919\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.8911 - loss: 0.2637 - val_accuracy: 0.8960 - val_loss: 0.2554 - mcc: 0.7919\n","Epoch 19/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8918 - loss: 0.2655\n","Epoch 19 - MCC: 0.7961\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8917 - loss: 0.2655 - val_accuracy: 0.8984 - val_loss: 0.2506 - mcc: 0.7961\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8924 - loss: 0.2598\n","Epoch 20 - MCC: 0.7964\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.8924 - loss: 0.2600 - val_accuracy: 0.8986 - val_loss: 0.2500 - mcc: 0.7964\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8938 - loss: 0.2597\n","Epoch 21 - MCC: 0.7974\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.8938 - loss: 0.2598 - val_accuracy: 0.8990 - val_loss: 0.2490 - mcc: 0.7974\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8986 - loss: 0.2495\n","Epoch 22 - MCC: 0.7937\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8984 - loss: 0.2500 - val_accuracy: 0.8966 - val_loss: 0.2565 - mcc: 0.7937\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8917 - loss: 0.2631\n","Epoch 23 - MCC: 0.7988\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8918 - loss: 0.2631 - val_accuracy: 0.8998 - val_loss: 0.2486 - mcc: 0.7988\n","Epoch 24/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8908 - loss: 0.2646\n","Epoch 24 - MCC: 0.7914\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8909 - loss: 0.2644 - val_accuracy: 0.8960 - val_loss: 0.2555 - mcc: 0.7914\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8937 - loss: 0.2580\n","Epoch 25 - MCC: 0.7992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8936 - loss: 0.2582 - val_accuracy: 0.8999 - val_loss: 0.2467 - mcc: 0.7992\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8872 - loss: 0.2720\n","Epoch 26 - MCC: 0.7994\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8874 - loss: 0.2716 - val_accuracy: 0.9000 - val_loss: 0.2472 - mcc: 0.7994\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8914 - loss: 0.2623\n","Epoch 27 - MCC: 0.8012\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.8915 - loss: 0.2622 - val_accuracy: 0.9009 - val_loss: 0.2461 - mcc: 0.8012\n","Epoch 28/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8940 - loss: 0.2587\n","Epoch 28 - MCC: 0.8023\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.8940 - loss: 0.2588 - val_accuracy: 0.9016 - val_loss: 0.2441 - mcc: 0.8023\n","Epoch 29/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8949 - loss: 0.2570\n","Epoch 29 - MCC: 0.8024\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8949 - loss: 0.2570 - val_accuracy: 0.9016 - val_loss: 0.2439 - mcc: 0.8024\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8971 - loss: 0.2517\n","Epoch 30 - MCC: 0.7967\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8970 - loss: 0.2519 - val_accuracy: 0.8982 - val_loss: 0.2486 - mcc: 0.7967\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8954 - loss: 0.2549\n","Epoch 31 - MCC: 0.8020\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8954 - loss: 0.2551 - val_accuracy: 0.9013 - val_loss: 0.2443 - mcc: 0.8020\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8959 - loss: 0.2532\n","Epoch 32 - MCC: 0.8040\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8958 - loss: 0.2533 - val_accuracy: 0.9024 - val_loss: 0.2418 - mcc: 0.8040\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8973 - loss: 0.2509\n","Epoch 33 - MCC: 0.8015\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8972 - loss: 0.2511 - val_accuracy: 0.9012 - val_loss: 0.2440 - mcc: 0.8015\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8891 - loss: 0.2682\n","Epoch 34 - MCC: 0.7986\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8893 - loss: 0.2677 - val_accuracy: 0.8996 - val_loss: 0.2472 - mcc: 0.7986\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8951 - loss: 0.2556\n","Epoch 35 - MCC: 0.8039\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8951 - loss: 0.2556 - val_accuracy: 0.9024 - val_loss: 0.2427 - mcc: 0.8039\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8967 - loss: 0.2518\n","Epoch 36 - MCC: 0.8032\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.8966 - loss: 0.2519 - val_accuracy: 0.9020 - val_loss: 0.2415 - mcc: 0.8032\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9044 - loss: 0.2370\n","Epoch 37 - MCC: 0.8059\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9040 - loss: 0.2377 - val_accuracy: 0.9033 - val_loss: 0.2393 - mcc: 0.8059\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9009 - loss: 0.2436\n","Epoch 38 - MCC: 0.8010\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9007 - loss: 0.2440 - val_accuracy: 0.9008 - val_loss: 0.2427 - mcc: 0.8010\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8986 - loss: 0.2492\n","Epoch 39 - MCC: 0.8031\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.8985 - loss: 0.2493 - val_accuracy: 0.9018 - val_loss: 0.2409 - mcc: 0.8031\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8941 - loss: 0.2569\n","Epoch 40 - MCC: 0.8035\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - accuracy: 0.8942 - loss: 0.2567 - val_accuracy: 0.9020 - val_loss: 0.2411 - mcc: 0.8035\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8973 - loss: 0.2496\n","Epoch 41 - MCC: 0.8077\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.8972 - loss: 0.2497 - val_accuracy: 0.9042 - val_loss: 0.2377 - mcc: 0.8077\n","Epoch 42/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8949 - loss: 0.2557\n","Epoch 42 - MCC: 0.8055\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.8950 - loss: 0.2554 - val_accuracy: 0.9031 - val_loss: 0.2390 - mcc: 0.8055\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8942 - loss: 0.2558\n","Epoch 43 - MCC: 0.8087\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.8943 - loss: 0.2556 - val_accuracy: 0.9047 - val_loss: 0.2362 - mcc: 0.8087\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8934 - loss: 0.2572\n","Epoch 44 - MCC: 0.8067\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8935 - loss: 0.2570 - val_accuracy: 0.9036 - val_loss: 0.2385 - mcc: 0.8067\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8963 - loss: 0.2532\n","Epoch 45 - MCC: 0.8076\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8964 - loss: 0.2531 - val_accuracy: 0.9042 - val_loss: 0.2362 - mcc: 0.8076\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8956 - loss: 0.2545\n","Epoch 46 - MCC: 0.8080\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8957 - loss: 0.2543 - val_accuracy: 0.9043 - val_loss: 0.2369 - mcc: 0.8080\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8970 - loss: 0.2520\n","Epoch 47 - MCC: 0.8082\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.8970 - loss: 0.2519 - val_accuracy: 0.9044 - val_loss: 0.2355 - mcc: 0.8082\n","Epoch 48/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9025 - loss: 0.2421\n","Epoch 48 - MCC: 0.8044\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9021 - loss: 0.2427 - val_accuracy: 0.9026 - val_loss: 0.2383 - mcc: 0.8044\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8954 - loss: 0.2524\n","Epoch 49 - MCC: 0.8051\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.8955 - loss: 0.2523 - val_accuracy: 0.9030 - val_loss: 0.2374 - mcc: 0.8051\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8991 - loss: 0.2470\n","Epoch 50 - MCC: 0.8088\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8991 - loss: 0.2470 - val_accuracy: 0.9047 - val_loss: 0.2345 - mcc: 0.8088\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 2\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6162 - loss: 0.6848\n","Epoch 1 - MCC: 0.3535\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.6176 - loss: 0.6844 - val_accuracy: 0.6594 - val_loss: 0.6595 - mcc: 0.3535\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6999 - loss: 0.6312\n","Epoch 2 - MCC: 0.5364\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.7008 - loss: 0.6302 - val_accuracy: 0.7636 - val_loss: 0.5509 - mcc: 0.5364\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7945 - loss: 0.4961\n","Epoch 3 - MCC: 0.6319\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7948 - loss: 0.4950 - val_accuracy: 0.8162 - val_loss: 0.4351 - mcc: 0.6319\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8281 - loss: 0.4052\n","Epoch 4 - MCC: 0.6701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.8284 - loss: 0.4046 - val_accuracy: 0.8352 - val_loss: 0.3825 - mcc: 0.6701\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8526 - loss: 0.3460\n","Epoch 5 - MCC: 0.6946\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8525 - loss: 0.3460 - val_accuracy: 0.8477 - val_loss: 0.3538 - mcc: 0.6946\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8655 - loss: 0.3163\n","Epoch 6 - MCC: 0.7137\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.8655 - loss: 0.3165 - val_accuracy: 0.8572 - val_loss: 0.3345 - mcc: 0.7137\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8738 - loss: 0.3031\n","Epoch 7 - MCC: 0.7174\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8737 - loss: 0.3031 - val_accuracy: 0.8587 - val_loss: 0.3288 - mcc: 0.7174\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8764 - loss: 0.2943\n","Epoch 8 - MCC: 0.7338\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8764 - loss: 0.2942 - val_accuracy: 0.8672 - val_loss: 0.3108 - mcc: 0.7338\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8857 - loss: 0.2766\n","Epoch 9 - MCC: 0.7397\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8856 - loss: 0.2768 - val_accuracy: 0.8701 - val_loss: 0.3052 - mcc: 0.7397\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8830 - loss: 0.2802\n","Epoch 10 - MCC: 0.7450\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8830 - loss: 0.2801 - val_accuracy: 0.8728 - val_loss: 0.3014 - mcc: 0.7450\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8896 - loss: 0.2676\n","Epoch 11 - MCC: 0.7467\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8895 - loss: 0.2678 - val_accuracy: 0.8737 - val_loss: 0.3013 - mcc: 0.7467\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8872 - loss: 0.2734\n","Epoch 12 - MCC: 0.7473\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.8873 - loss: 0.2732 - val_accuracy: 0.8739 - val_loss: 0.2994 - mcc: 0.7473\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8890 - loss: 0.2686\n","Epoch 13 - MCC: 0.7514\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.8890 - loss: 0.2685 - val_accuracy: 0.8759 - val_loss: 0.2965 - mcc: 0.7514\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8885 - loss: 0.2684\n","Epoch 14 - MCC: 0.7538\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.8886 - loss: 0.2682 - val_accuracy: 0.8772 - val_loss: 0.2907 - mcc: 0.7538\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8927 - loss: 0.2616\n","Epoch 15 - MCC: 0.7546\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8927 - loss: 0.2616 - val_accuracy: 0.8773 - val_loss: 0.2928 - mcc: 0.7546\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8962 - loss: 0.2525\n","Epoch 16 - MCC: 0.7550\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8960 - loss: 0.2528 - val_accuracy: 0.8775 - val_loss: 0.2936 - mcc: 0.7550\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8842 - loss: 0.2761\n","Epoch 17 - MCC: 0.7591\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8845 - loss: 0.2755 - val_accuracy: 0.8798 - val_loss: 0.2863 - mcc: 0.7591\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8925 - loss: 0.2603\n","Epoch 18 - MCC: 0.7593\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8925 - loss: 0.2602 - val_accuracy: 0.8799 - val_loss: 0.2861 - mcc: 0.7593\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8892 - loss: 0.2671\n","Epoch 19 - MCC: 0.7596\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8894 - loss: 0.2667 - val_accuracy: 0.8799 - val_loss: 0.2858 - mcc: 0.7596\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8982 - loss: 0.2481\n","Epoch 20 - MCC: 0.7599\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8981 - loss: 0.2484 - val_accuracy: 0.8799 - val_loss: 0.2856 - mcc: 0.7599\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8994 - loss: 0.2461\n","Epoch 21 - MCC: 0.7627\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8992 - loss: 0.2464 - val_accuracy: 0.8816 - val_loss: 0.2821 - mcc: 0.7627\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8922 - loss: 0.2601\n","Epoch 22 - MCC: 0.7625\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.8924 - loss: 0.2598 - val_accuracy: 0.8815 - val_loss: 0.2831 - mcc: 0.7625\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8960 - loss: 0.2525\n","Epoch 23 - MCC: 0.7630\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8960 - loss: 0.2525 - val_accuracy: 0.8817 - val_loss: 0.2823 - mcc: 0.7630\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9010 - loss: 0.2420\n","Epoch 24 - MCC: 0.7622\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9008 - loss: 0.2424 - val_accuracy: 0.8814 - val_loss: 0.2874 - mcc: 0.7622\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8982 - loss: 0.2485\n","Epoch 25 - MCC: 0.7640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8981 - loss: 0.2486 - val_accuracy: 0.8823 - val_loss: 0.2820 - mcc: 0.7640\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8944 - loss: 0.2567\n","Epoch 26 - MCC: 0.7654\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8945 - loss: 0.2564 - val_accuracy: 0.8829 - val_loss: 0.2804 - mcc: 0.7654\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8962 - loss: 0.2529\n","Epoch 27 - MCC: 0.7644\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8962 - loss: 0.2527 - val_accuracy: 0.8823 - val_loss: 0.2805 - mcc: 0.7644\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9006 - loss: 0.2437\n","Epoch 28 - MCC: 0.7631\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9005 - loss: 0.2439 - val_accuracy: 0.8812 - val_loss: 0.2869 - mcc: 0.7631\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9010 - loss: 0.2451\n","Epoch 29 - MCC: 0.7661\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9009 - loss: 0.2453 - val_accuracy: 0.8831 - val_loss: 0.2808 - mcc: 0.7661\n","Epoch 30/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8936 - loss: 0.2559\n","Epoch 30 - MCC: 0.7677\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.8939 - loss: 0.2553 - val_accuracy: 0.8841 - val_loss: 0.2768 - mcc: 0.7677\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8995 - loss: 0.2454\n","Epoch 31 - MCC: 0.7675\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8995 - loss: 0.2455 - val_accuracy: 0.8840 - val_loss: 0.2766 - mcc: 0.7675\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8979 - loss: 0.2494\n","Epoch 32 - MCC: 0.7677\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8980 - loss: 0.2492 - val_accuracy: 0.8841 - val_loss: 0.2773 - mcc: 0.7677\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8973 - loss: 0.2497\n","Epoch 33 - MCC: 0.7692\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8974 - loss: 0.2496 - val_accuracy: 0.8849 - val_loss: 0.2760 - mcc: 0.7692\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9060 - loss: 0.2330\n","Epoch 34 - MCC: 0.7675\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9058 - loss: 0.2334 - val_accuracy: 0.8840 - val_loss: 0.2769 - mcc: 0.7675\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9036 - loss: 0.2371\n","Epoch 35 - MCC: 0.7674\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9035 - loss: 0.2374 - val_accuracy: 0.8839 - val_loss: 0.2765 - mcc: 0.7674\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8983 - loss: 0.2491\n","Epoch 36 - MCC: 0.7691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8983 - loss: 0.2489 - val_accuracy: 0.8848 - val_loss: 0.2755 - mcc: 0.7691\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9030 - loss: 0.2396\n","Epoch 37 - MCC: 0.7695\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9029 - loss: 0.2398 - val_accuracy: 0.8850 - val_loss: 0.2770 - mcc: 0.7695\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8986 - loss: 0.2480\n","Epoch 38 - MCC: 0.7685\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.8987 - loss: 0.2478 - val_accuracy: 0.8844 - val_loss: 0.2756 - mcc: 0.7685\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9028 - loss: 0.2394\n","Epoch 39 - MCC: 0.7694\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9027 - loss: 0.2395 - val_accuracy: 0.8847 - val_loss: 0.2754 - mcc: 0.7694\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8964 - loss: 0.2499\n","Epoch 40 - MCC: 0.7696\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.8966 - loss: 0.2496 - val_accuracy: 0.8850 - val_loss: 0.2748 - mcc: 0.7696\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9007 - loss: 0.2418\n","Epoch 41 - MCC: 0.7708\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9008 - loss: 0.2418 - val_accuracy: 0.8856 - val_loss: 0.2746 - mcc: 0.7708\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9009 - loss: 0.2426\n","Epoch 42 - MCC: 0.7723\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9009 - loss: 0.2426 - val_accuracy: 0.8864 - val_loss: 0.2721 - mcc: 0.7723\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8942 - loss: 0.2568\n","Epoch 43 - MCC: 0.7707\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8944 - loss: 0.2562 - val_accuracy: 0.8856 - val_loss: 0.2726 - mcc: 0.7707\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8995 - loss: 0.2447\n","Epoch 44 - MCC: 0.7693\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.8996 - loss: 0.2446 - val_accuracy: 0.8849 - val_loss: 0.2748 - mcc: 0.7693\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9026 - loss: 0.2376\n","Epoch 45 - MCC: 0.7712\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.9026 - loss: 0.2377 - val_accuracy: 0.8859 - val_loss: 0.2730 - mcc: 0.7712\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8934 - loss: 0.2579\n","Epoch 46 - MCC: 0.7726\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8938 - loss: 0.2572 - val_accuracy: 0.8865 - val_loss: 0.2708 - mcc: 0.7726\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9037 - loss: 0.2363\n","Epoch 47 - MCC: 0.7729\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9037 - loss: 0.2364 - val_accuracy: 0.8865 - val_loss: 0.2704 - mcc: 0.7729\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9043 - loss: 0.2348\n","Epoch 48 - MCC: 0.7728\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9042 - loss: 0.2349 - val_accuracy: 0.8866 - val_loss: 0.2703 - mcc: 0.7728\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9017 - loss: 0.2406\n","Epoch 49 - MCC: 0.7734\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.9017 - loss: 0.2405 - val_accuracy: 0.8870 - val_loss: 0.2701 - mcc: 0.7734\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9093 - loss: 0.2245\n","Epoch 50 - MCC: 0.7695\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9091 - loss: 0.2250 - val_accuracy: 0.8850 - val_loss: 0.2728 - mcc: 0.7695\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 3\n","Epoch 1/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6458 - loss: 0.6848\n","Epoch 1 - MCC: 0.5865\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - accuracy: 0.6511 - loss: 0.6842 - val_accuracy: 0.7930 - val_loss: 0.6541 - mcc: 0.5865\n","Epoch 2/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7862 - loss: 0.6376\n","Epoch 2 - MCC: 0.6037\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 53ms/step - accuracy: 0.7863 - loss: 0.6355 - val_accuracy: 0.8008 - val_loss: 0.5509 - mcc: 0.6037\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8119 - loss: 0.5132\n","Epoch 3 - MCC: 0.6531\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8119 - loss: 0.5120 - val_accuracy: 0.8273 - val_loss: 0.4021 - mcc: 0.6531\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8345 - loss: 0.3866\n","Epoch 4 - MCC: 0.7044\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.8346 - loss: 0.3863 - val_accuracy: 0.8529 - val_loss: 0.3451 - mcc: 0.7044\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8516 - loss: 0.3477\n","Epoch 5 - MCC: 0.7221\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8517 - loss: 0.3475 - val_accuracy: 0.8613 - val_loss: 0.3223 - mcc: 0.7221\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8627 - loss: 0.3231\n","Epoch 6 - MCC: 0.7433\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8627 - loss: 0.3230 - val_accuracy: 0.8722 - val_loss: 0.3011 - mcc: 0.7433\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8670 - loss: 0.3133\n","Epoch 7 - MCC: 0.7556\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.8672 - loss: 0.3130 - val_accuracy: 0.8780 - val_loss: 0.2903 - mcc: 0.7556\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8754 - loss: 0.2967\n","Epoch 8 - MCC: 0.7693\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.8755 - loss: 0.2967 - val_accuracy: 0.8852 - val_loss: 0.2785 - mcc: 0.7693\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8839 - loss: 0.2822\n","Epoch 9 - MCC: 0.7765\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8838 - loss: 0.2824 - val_accuracy: 0.8887 - val_loss: 0.2672 - mcc: 0.7765\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8857 - loss: 0.2794\n","Epoch 10 - MCC: 0.7798\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8857 - loss: 0.2795 - val_accuracy: 0.8904 - val_loss: 0.2674 - mcc: 0.7798\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8847 - loss: 0.2820\n","Epoch 11 - MCC: 0.7829\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8848 - loss: 0.2818 - val_accuracy: 0.8919 - val_loss: 0.2634 - mcc: 0.7829\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8811 - loss: 0.2859\n","Epoch 12 - MCC: 0.7827\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8813 - loss: 0.2854 - val_accuracy: 0.8918 - val_loss: 0.2598 - mcc: 0.7827\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8905 - loss: 0.2707\n","Epoch 13 - MCC: 0.7874\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.8904 - loss: 0.2708 - val_accuracy: 0.8942 - val_loss: 0.2587 - mcc: 0.7874\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8884 - loss: 0.2728\n","Epoch 14 - MCC: 0.7896\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8885 - loss: 0.2728 - val_accuracy: 0.8952 - val_loss: 0.2554 - mcc: 0.7896\n","Epoch 15/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8923 - loss: 0.2655\n","Epoch 15 - MCC: 0.7897\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.8921 - loss: 0.2659 - val_accuracy: 0.8951 - val_loss: 0.2540 - mcc: 0.7897\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8904 - loss: 0.2663\n","Epoch 16 - MCC: 0.7926\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8904 - loss: 0.2663 - val_accuracy: 0.8966 - val_loss: 0.2529 - mcc: 0.7926\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8859 - loss: 0.2764\n","Epoch 17 - MCC: 0.7939\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8861 - loss: 0.2760 - val_accuracy: 0.8974 - val_loss: 0.2511 - mcc: 0.7939\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8896 - loss: 0.2702\n","Epoch 18 - MCC: 0.7917\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8896 - loss: 0.2700 - val_accuracy: 0.8959 - val_loss: 0.2538 - mcc: 0.7917\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8921 - loss: 0.2640\n","Epoch 19 - MCC: 0.7940\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8921 - loss: 0.2640 - val_accuracy: 0.8972 - val_loss: 0.2546 - mcc: 0.7940\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8917 - loss: 0.2655\n","Epoch 20 - MCC: 0.7932\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8917 - loss: 0.2655 - val_accuracy: 0.8969 - val_loss: 0.2489 - mcc: 0.7932\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8893 - loss: 0.2709\n","Epoch 21 - MCC: 0.7995\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8894 - loss: 0.2706 - val_accuracy: 0.9002 - val_loss: 0.2437 - mcc: 0.7995\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8964 - loss: 0.2568\n","Epoch 22 - MCC: 0.7976\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8963 - loss: 0.2570 - val_accuracy: 0.8992 - val_loss: 0.2485 - mcc: 0.7976\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8905 - loss: 0.2668\n","Epoch 23 - MCC: 0.7997\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.8906 - loss: 0.2666 - val_accuracy: 0.8999 - val_loss: 0.2459 - mcc: 0.7997\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8959 - loss: 0.2574\n","Epoch 24 - MCC: 0.7930\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.8958 - loss: 0.2576 - val_accuracy: 0.8965 - val_loss: 0.2519 - mcc: 0.7930\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8999 - loss: 0.2477\n","Epoch 25 - MCC: 0.8005\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8997 - loss: 0.2482 - val_accuracy: 0.9007 - val_loss: 0.2416 - mcc: 0.8005\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8876 - loss: 0.2737\n","Epoch 26 - MCC: 0.7972\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8878 - loss: 0.2732 - val_accuracy: 0.8989 - val_loss: 0.2446 - mcc: 0.7972\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8937 - loss: 0.2601\n","Epoch 27 - MCC: 0.7998\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8938 - loss: 0.2601 - val_accuracy: 0.9001 - val_loss: 0.2449 - mcc: 0.7998\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8982 - loss: 0.2519\n","Epoch 28 - MCC: 0.8017\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8981 - loss: 0.2522 - val_accuracy: 0.9012 - val_loss: 0.2415 - mcc: 0.8017\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8982 - loss: 0.2519\n","Epoch 29 - MCC: 0.8036\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8980 - loss: 0.2522 - val_accuracy: 0.9022 - val_loss: 0.2419 - mcc: 0.8036\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8958 - loss: 0.2551\n","Epoch 30 - MCC: 0.7993\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8958 - loss: 0.2552 - val_accuracy: 0.9001 - val_loss: 0.2442 - mcc: 0.7993\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8992 - loss: 0.2473\n","Epoch 31 - MCC: 0.8027\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8990 - loss: 0.2476 - val_accuracy: 0.9018 - val_loss: 0.2400 - mcc: 0.8027\n","Epoch 32/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9036 - loss: 0.2387\n","Epoch 32 - MCC: 0.8027\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.9030 - loss: 0.2399 - val_accuracy: 0.9018 - val_loss: 0.2404 - mcc: 0.8027\n","Epoch 33/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8978 - loss: 0.2518\n","Epoch 33 - MCC: 0.8002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.8977 - loss: 0.2520 - val_accuracy: 0.9005 - val_loss: 0.2444 - mcc: 0.8002\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8923 - loss: 0.2618\n","Epoch 34 - MCC: 0.8038\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8924 - loss: 0.2616 - val_accuracy: 0.9023 - val_loss: 0.2396 - mcc: 0.8038\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8969 - loss: 0.2522\n","Epoch 35 - MCC: 0.8034\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8968 - loss: 0.2522 - val_accuracy: 0.9021 - val_loss: 0.2389 - mcc: 0.8034\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8961 - loss: 0.2555\n","Epoch 36 - MCC: 0.8038\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8961 - loss: 0.2554 - val_accuracy: 0.9023 - val_loss: 0.2398 - mcc: 0.8038\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8924 - loss: 0.2613\n","Epoch 37 - MCC: 0.8032\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8925 - loss: 0.2609 - val_accuracy: 0.9020 - val_loss: 0.2376 - mcc: 0.8032\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8966 - loss: 0.2529\n","Epoch 38 - MCC: 0.8025\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.8966 - loss: 0.2528 - val_accuracy: 0.9015 - val_loss: 0.2399 - mcc: 0.8025\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8965 - loss: 0.2528\n","Epoch 39 - MCC: 0.8042\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8965 - loss: 0.2528 - val_accuracy: 0.9025 - val_loss: 0.2363 - mcc: 0.8042\n","Epoch 40/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8903 - loss: 0.2635\n","Epoch 40 - MCC: 0.8052\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.8907 - loss: 0.2627 - val_accuracy: 0.9030 - val_loss: 0.2362 - mcc: 0.8052\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8939 - loss: 0.2560\n","Epoch 41 - MCC: 0.8073\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8941 - loss: 0.2557 - val_accuracy: 0.9041 - val_loss: 0.2348 - mcc: 0.8073\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8984 - loss: 0.2471\n","Epoch 42 - MCC: 0.8032\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8984 - loss: 0.2472 - val_accuracy: 0.9020 - val_loss: 0.2367 - mcc: 0.8032\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8964 - loss: 0.2513\n","Epoch 43 - MCC: 0.8058\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8965 - loss: 0.2513 - val_accuracy: 0.9031 - val_loss: 0.2346 - mcc: 0.8058\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8926 - loss: 0.2595\n","Epoch 44 - MCC: 0.8031\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8928 - loss: 0.2591 - val_accuracy: 0.9019 - val_loss: 0.2393 - mcc: 0.8031\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9002 - loss: 0.2447\n","Epoch 45 - MCC: 0.8091\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9001 - loss: 0.2449 - val_accuracy: 0.9049 - val_loss: 0.2316 - mcc: 0.8091\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8975 - loss: 0.2474\n","Epoch 46 - MCC: 0.8037\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - accuracy: 0.8975 - loss: 0.2474 - val_accuracy: 0.9023 - val_loss: 0.2388 - mcc: 0.8037\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9007 - loss: 0.2439\n","Epoch 47 - MCC: 0.8102\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.9006 - loss: 0.2441 - val_accuracy: 0.9055 - val_loss: 0.2308 - mcc: 0.8102\n","Epoch 48/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8994 - loss: 0.2463\n","Epoch 48 - MCC: 0.8101\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.8994 - loss: 0.2462 - val_accuracy: 0.9054 - val_loss: 0.2306 - mcc: 0.8101\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8997 - loss: 0.2448\n","Epoch 49 - MCC: 0.8051\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8997 - loss: 0.2448 - val_accuracy: 0.9030 - val_loss: 0.2352 - mcc: 0.8051\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8959 - loss: 0.2521\n","Epoch 50 - MCC: 0.8074\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8960 - loss: 0.2518 - val_accuracy: 0.9041 - val_loss: 0.2335 - mcc: 0.8074\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 4\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5885 - loss: 0.6816\n","Epoch 1 - MCC: 0.3952\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.5908 - loss: 0.6813 - val_accuracy: 0.6783 - val_loss: 0.6508 - mcc: 0.3952\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.6913 - loss: 0.6306\n","Epoch 2 - MCC: 0.5789\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.6923 - loss: 0.6296 - val_accuracy: 0.7823 - val_loss: 0.5353 - mcc: 0.5789\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7924 - loss: 0.5033\n","Epoch 3 - MCC: 0.6648\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7929 - loss: 0.5022 - val_accuracy: 0.8322 - val_loss: 0.4240 - mcc: 0.6648\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8301 - loss: 0.4090\n","Epoch 4 - MCC: 0.6986\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.8303 - loss: 0.4085 - val_accuracy: 0.8494 - val_loss: 0.3647 - mcc: 0.6986\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8423 - loss: 0.3677\n","Epoch 5 - MCC: 0.7257\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8426 - loss: 0.3671 - val_accuracy: 0.8632 - val_loss: 0.3298 - mcc: 0.7257\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8585 - loss: 0.3294\n","Epoch 6 - MCC: 0.7546\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8586 - loss: 0.3290 - val_accuracy: 0.8776 - val_loss: 0.2998 - mcc: 0.7546\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8733 - loss: 0.3014\n","Epoch 7 - MCC: 0.7647\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8733 - loss: 0.3013 - val_accuracy: 0.8826 - val_loss: 0.2860 - mcc: 0.7647\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8760 - loss: 0.2935\n","Epoch 8 - MCC: 0.7696\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8761 - loss: 0.2934 - val_accuracy: 0.8849 - val_loss: 0.2791 - mcc: 0.7696\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8807 - loss: 0.2845\n","Epoch 9 - MCC: 0.7708\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8807 - loss: 0.2843 - val_accuracy: 0.8856 - val_loss: 0.2749 - mcc: 0.7708\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8834 - loss: 0.2761\n","Epoch 10 - MCC: 0.7803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.8834 - loss: 0.2760 - val_accuracy: 0.8904 - val_loss: 0.2677 - mcc: 0.7803\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8872 - loss: 0.2703\n","Epoch 11 - MCC: 0.7826\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8872 - loss: 0.2702 - val_accuracy: 0.8916 - val_loss: 0.2645 - mcc: 0.7826\n","Epoch 12/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8929 - loss: 0.2587\n","Epoch 12 - MCC: 0.7871\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.8925 - loss: 0.2594 - val_accuracy: 0.8938 - val_loss: 0.2614 - mcc: 0.7871\n","Epoch 13/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8894 - loss: 0.2656\n","Epoch 13 - MCC: 0.7881\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8895 - loss: 0.2655 - val_accuracy: 0.8941 - val_loss: 0.2617 - mcc: 0.7881\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8939 - loss: 0.2581\n","Epoch 14 - MCC: 0.7910\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8938 - loss: 0.2583 - val_accuracy: 0.8957 - val_loss: 0.2576 - mcc: 0.7910\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8864 - loss: 0.2722\n","Epoch 15 - MCC: 0.7903\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8866 - loss: 0.2719 - val_accuracy: 0.8954 - val_loss: 0.2569 - mcc: 0.7903\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8926 - loss: 0.2603\n","Epoch 16 - MCC: 0.7923\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8925 - loss: 0.2604 - val_accuracy: 0.8964 - val_loss: 0.2554 - mcc: 0.7923\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8938 - loss: 0.2560\n","Epoch 17 - MCC: 0.7915\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8938 - loss: 0.2561 - val_accuracy: 0.8960 - val_loss: 0.2550 - mcc: 0.7915\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8951 - loss: 0.2527\n","Epoch 18 - MCC: 0.7928\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8950 - loss: 0.2529 - val_accuracy: 0.8966 - val_loss: 0.2547 - mcc: 0.7928\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8956 - loss: 0.2540\n","Epoch 19 - MCC: 0.7924\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.8955 - loss: 0.2542 - val_accuracy: 0.8964 - val_loss: 0.2546 - mcc: 0.7924\n","Epoch 20/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8958 - loss: 0.2550\n","Epoch 20 - MCC: 0.7920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.8957 - loss: 0.2552 - val_accuracy: 0.8963 - val_loss: 0.2546 - mcc: 0.7920\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8905 - loss: 0.2640\n","Epoch 21 - MCC: 0.7886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8906 - loss: 0.2638 - val_accuracy: 0.8945 - val_loss: 0.2579 - mcc: 0.7886\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8938 - loss: 0.2568\n","Epoch 22 - MCC: 0.7946\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8938 - loss: 0.2568 - val_accuracy: 0.8976 - val_loss: 0.2520 - mcc: 0.7946\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8936 - loss: 0.2566\n","Epoch 23 - MCC: 0.7933\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8936 - loss: 0.2566 - val_accuracy: 0.8965 - val_loss: 0.2569 - mcc: 0.7933\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8877 - loss: 0.2716\n","Epoch 24 - MCC: 0.7968\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8880 - loss: 0.2710 - val_accuracy: 0.8985 - val_loss: 0.2505 - mcc: 0.7968\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8981 - loss: 0.2477\n","Epoch 25 - MCC: 0.7974\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8980 - loss: 0.2478 - val_accuracy: 0.8990 - val_loss: 0.2497 - mcc: 0.7974\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8923 - loss: 0.2600\n","Epoch 26 - MCC: 0.7995\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8924 - loss: 0.2597 - val_accuracy: 0.8999 - val_loss: 0.2473 - mcc: 0.7995\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8947 - loss: 0.2526\n","Epoch 27 - MCC: 0.7966\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8948 - loss: 0.2525 - val_accuracy: 0.8985 - val_loss: 0.2498 - mcc: 0.7966\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8964 - loss: 0.2504\n","Epoch 28 - MCC: 0.7961\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.8965 - loss: 0.2504 - val_accuracy: 0.8983 - val_loss: 0.2514 - mcc: 0.7961\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8961 - loss: 0.2532\n","Epoch 29 - MCC: 0.7982\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.8961 - loss: 0.2531 - val_accuracy: 0.8994 - val_loss: 0.2481 - mcc: 0.7982\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8946 - loss: 0.2548\n","Epoch 30 - MCC: 0.7965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8947 - loss: 0.2546 - val_accuracy: 0.8985 - val_loss: 0.2496 - mcc: 0.7965\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8976 - loss: 0.2482\n","Epoch 31 - MCC: 0.7961\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8976 - loss: 0.2483 - val_accuracy: 0.8983 - val_loss: 0.2495 - mcc: 0.7961\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8976 - loss: 0.2490\n","Epoch 32 - MCC: 0.8001\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8976 - loss: 0.2490 - val_accuracy: 0.9003 - val_loss: 0.2457 - mcc: 0.8001\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8933 - loss: 0.2553\n","Epoch 33 - MCC: 0.8008\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8935 - loss: 0.2550 - val_accuracy: 0.9006 - val_loss: 0.2446 - mcc: 0.8008\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8924 - loss: 0.2589\n","Epoch 34 - MCC: 0.7982\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8926 - loss: 0.2585 - val_accuracy: 0.8994 - val_loss: 0.2464 - mcc: 0.7982\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9021 - loss: 0.2399\n","Epoch 35 - MCC: 0.7987\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.9019 - loss: 0.2402 - val_accuracy: 0.8996 - val_loss: 0.2467 - mcc: 0.7987\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8986 - loss: 0.2467\n","Epoch 36 - MCC: 0.8020\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8986 - loss: 0.2467 - val_accuracy: 0.9011 - val_loss: 0.2442 - mcc: 0.8020\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8972 - loss: 0.2475\n","Epoch 37 - MCC: 0.8029\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.8973 - loss: 0.2474 - val_accuracy: 0.9017 - val_loss: 0.2431 - mcc: 0.8029\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9010 - loss: 0.2405\n","Epoch 38 - MCC: 0.7996\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.9010 - loss: 0.2407 - val_accuracy: 0.9000 - val_loss: 0.2451 - mcc: 0.7996\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9032 - loss: 0.2378\n","Epoch 39 - MCC: 0.8031\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9030 - loss: 0.2381 - val_accuracy: 0.9018 - val_loss: 0.2426 - mcc: 0.8031\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8985 - loss: 0.2464\n","Epoch 40 - MCC: 0.8029\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.8985 - loss: 0.2463 - val_accuracy: 0.9017 - val_loss: 0.2420 - mcc: 0.8029\n","Epoch 41/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9021 - loss: 0.2389\n","Epoch 41 - MCC: 0.8046\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9019 - loss: 0.2393 - val_accuracy: 0.9025 - val_loss: 0.2400 - mcc: 0.8046\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8953 - loss: 0.2529\n","Epoch 42 - MCC: 0.8002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8955 - loss: 0.2525 - val_accuracy: 0.9004 - val_loss: 0.2464 - mcc: 0.8002\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9066 - loss: 0.2289\n","Epoch 43 - MCC: 0.8050\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9064 - loss: 0.2295 - val_accuracy: 0.9028 - val_loss: 0.2410 - mcc: 0.8050\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9024 - loss: 0.2362\n","Epoch 44 - MCC: 0.8000\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9023 - loss: 0.2365 - val_accuracy: 0.9002 - val_loss: 0.2433 - mcc: 0.8000\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9004 - loss: 0.2419\n","Epoch 45 - MCC: 0.8050\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.9004 - loss: 0.2419 - val_accuracy: 0.9025 - val_loss: 0.2402 - mcc: 0.8050\n","Epoch 46/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9029 - loss: 0.2362\n","Epoch 46 - MCC: 0.8065\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9027 - loss: 0.2366 - val_accuracy: 0.9035 - val_loss: 0.2384 - mcc: 0.8065\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8974 - loss: 0.2455\n","Epoch 47 - MCC: 0.8064\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.8976 - loss: 0.2453 - val_accuracy: 0.9033 - val_loss: 0.2391 - mcc: 0.8064\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9017 - loss: 0.2383\n","Epoch 48 - MCC: 0.8062\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9016 - loss: 0.2383 - val_accuracy: 0.9032 - val_loss: 0.2390 - mcc: 0.8062\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9018 - loss: 0.2382\n","Epoch 49 - MCC: 0.8041\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9018 - loss: 0.2383 - val_accuracy: 0.9023 - val_loss: 0.2396 - mcc: 0.8041\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8936 - loss: 0.2531\n","Epoch 50 - MCC: 0.8067\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8938 - loss: 0.2527 - val_accuracy: 0.9036 - val_loss: 0.2373 - mcc: 0.8067\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Dense, Fold: 5\n","Epoch 1/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.5592 - loss: 0.6896\n","Epoch 1 - MCC: 0.2977\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.5598 - loss: 0.6890 - val_accuracy: 0.6020 - val_loss: 0.6622 - mcc: 0.2977\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6443 - loss: 0.6460\n","Epoch 2 - MCC: 0.4832\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.6454 - loss: 0.6455 - val_accuracy: 0.7332 - val_loss: 0.5883 - mcc: 0.4832\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7598 - loss: 0.5554\n","Epoch 3 - MCC: 0.6068\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.7605 - loss: 0.5542 - val_accuracy: 0.8036 - val_loss: 0.4693 - mcc: 0.6068\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8062 - loss: 0.4593\n","Epoch 4 - MCC: 0.6398\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8064 - loss: 0.4586 - val_accuracy: 0.8202 - val_loss: 0.4103 - mcc: 0.6398\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8297 - loss: 0.3937\n","Epoch 5 - MCC: 0.6784\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8298 - loss: 0.3934 - val_accuracy: 0.8393 - val_loss: 0.3729 - mcc: 0.6784\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8441 - loss: 0.3635\n","Epoch 6 - MCC: 0.7051\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8442 - loss: 0.3632 - val_accuracy: 0.8529 - val_loss: 0.3435 - mcc: 0.7051\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8539 - loss: 0.3414\n","Epoch 7 - MCC: 0.7227\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8541 - loss: 0.3409 - val_accuracy: 0.8616 - val_loss: 0.3215 - mcc: 0.7227\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8655 - loss: 0.3150\n","Epoch 8 - MCC: 0.7404\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8657 - loss: 0.3147 - val_accuracy: 0.8703 - val_loss: 0.3064 - mcc: 0.7404\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8721 - loss: 0.3039\n","Epoch 9 - MCC: 0.7486\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8722 - loss: 0.3036 - val_accuracy: 0.8746 - val_loss: 0.2980 - mcc: 0.7486\n","Epoch 10/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8789 - loss: 0.2906\n","Epoch 10 - MCC: 0.7559\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.8789 - loss: 0.2905 - val_accuracy: 0.8781 - val_loss: 0.2912 - mcc: 0.7559\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8878 - loss: 0.2725\n","Epoch 11 - MCC: 0.7624\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.8876 - loss: 0.2729 - val_accuracy: 0.8814 - val_loss: 0.2847 - mcc: 0.7624\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8856 - loss: 0.2762\n","Epoch 12 - MCC: 0.7655\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - accuracy: 0.8855 - loss: 0.2763 - val_accuracy: 0.8829 - val_loss: 0.2816 - mcc: 0.7655\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8892 - loss: 0.2701\n","Epoch 13 - MCC: 0.7708\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8891 - loss: 0.2702 - val_accuracy: 0.8856 - val_loss: 0.2756 - mcc: 0.7708\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8875 - loss: 0.2719\n","Epoch 14 - MCC: 0.7715\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8875 - loss: 0.2718 - val_accuracy: 0.8856 - val_loss: 0.2747 - mcc: 0.7715\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8899 - loss: 0.2673\n","Epoch 15 - MCC: 0.7745\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8898 - loss: 0.2674 - val_accuracy: 0.8874 - val_loss: 0.2734 - mcc: 0.7745\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8893 - loss: 0.2682\n","Epoch 16 - MCC: 0.7753\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8893 - loss: 0.2681 - val_accuracy: 0.8877 - val_loss: 0.2707 - mcc: 0.7753\n","Epoch 17/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8873 - loss: 0.2718\n","Epoch 17 - MCC: 0.7757\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8876 - loss: 0.2712 - val_accuracy: 0.8881 - val_loss: 0.2683 - mcc: 0.7757\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8917 - loss: 0.2631\n","Epoch 18 - MCC: 0.7776\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.8917 - loss: 0.2631 - val_accuracy: 0.8890 - val_loss: 0.2705 - mcc: 0.7776\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8898 - loss: 0.2685\n","Epoch 19 - MCC: 0.7786\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8898 - loss: 0.2683 - val_accuracy: 0.8895 - val_loss: 0.2675 - mcc: 0.7786\n","Epoch 20/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8950 - loss: 0.2563\n","Epoch 20 - MCC: 0.7760\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8949 - loss: 0.2565 - val_accuracy: 0.8882 - val_loss: 0.2699 - mcc: 0.7760\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8930 - loss: 0.2615\n","Epoch 21 - MCC: 0.7777\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8930 - loss: 0.2614 - val_accuracy: 0.8889 - val_loss: 0.2671 - mcc: 0.7777\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8948 - loss: 0.2565\n","Epoch 22 - MCC: 0.7786\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8947 - loss: 0.2566 - val_accuracy: 0.8893 - val_loss: 0.2664 - mcc: 0.7786\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8961 - loss: 0.2544\n","Epoch 23 - MCC: 0.7815\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8961 - loss: 0.2545 - val_accuracy: 0.8908 - val_loss: 0.2646 - mcc: 0.7815\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8956 - loss: 0.2559\n","Epoch 24 - MCC: 0.7803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8955 - loss: 0.2559 - val_accuracy: 0.8904 - val_loss: 0.2644 - mcc: 0.7803\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8962 - loss: 0.2510\n","Epoch 25 - MCC: 0.7818\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8962 - loss: 0.2512 - val_accuracy: 0.8911 - val_loss: 0.2633 - mcc: 0.7818\n","Epoch 26/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8955 - loss: 0.2550\n","Epoch 26 - MCC: 0.7816\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.8955 - loss: 0.2550 - val_accuracy: 0.8910 - val_loss: 0.2631 - mcc: 0.7816\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8960 - loss: 0.2545\n","Epoch 27 - MCC: 0.7842\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.8960 - loss: 0.2545 - val_accuracy: 0.8923 - val_loss: 0.2613 - mcc: 0.7842\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8983 - loss: 0.2487\n","Epoch 28 - MCC: 0.7829\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8982 - loss: 0.2489 - val_accuracy: 0.8912 - val_loss: 0.2632 - mcc: 0.7829\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8954 - loss: 0.2526\n","Epoch 29 - MCC: 0.7834\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8955 - loss: 0.2527 - val_accuracy: 0.8919 - val_loss: 0.2608 - mcc: 0.7834\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8916 - loss: 0.2616\n","Epoch 30 - MCC: 0.7824\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.8918 - loss: 0.2612 - val_accuracy: 0.8910 - val_loss: 0.2635 - mcc: 0.7824\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8965 - loss: 0.2516\n","Epoch 31 - MCC: 0.7839\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8965 - loss: 0.2516 - val_accuracy: 0.8922 - val_loss: 0.2628 - mcc: 0.7839\n","Epoch 32/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8958 - loss: 0.2544\n","Epoch 32 - MCC: 0.7821\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8958 - loss: 0.2543 - val_accuracy: 0.8904 - val_loss: 0.2650 - mcc: 0.7821\n","Epoch 33/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8952 - loss: 0.2537\n","Epoch 33 - MCC: 0.7846\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - accuracy: 0.8952 - loss: 0.2538 - val_accuracy: 0.8921 - val_loss: 0.2615 - mcc: 0.7846\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8958 - loss: 0.2526\n","Epoch 34 - MCC: 0.7822\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8959 - loss: 0.2525 - val_accuracy: 0.8913 - val_loss: 0.2609 - mcc: 0.7822\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8973 - loss: 0.2503\n","Epoch 35 - MCC: 0.7842\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8972 - loss: 0.2503 - val_accuracy: 0.8920 - val_loss: 0.2611 - mcc: 0.7842\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9006 - loss: 0.2431\n","Epoch 36 - MCC: 0.7874\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9005 - loss: 0.2433 - val_accuracy: 0.8939 - val_loss: 0.2592 - mcc: 0.7874\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9003 - loss: 0.2436\n","Epoch 37 - MCC: 0.7863\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9003 - loss: 0.2438 - val_accuracy: 0.8933 - val_loss: 0.2587 - mcc: 0.7863\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9003 - loss: 0.2444\n","Epoch 38 - MCC: 0.7883\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9002 - loss: 0.2445 - val_accuracy: 0.8943 - val_loss: 0.2572 - mcc: 0.7883\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9003 - loss: 0.2439\n","Epoch 39 - MCC: 0.7840\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9002 - loss: 0.2441 - val_accuracy: 0.8921 - val_loss: 0.2600 - mcc: 0.7840\n","Epoch 40/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8932 - loss: 0.2576\n","Epoch 40 - MCC: 0.7864\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.8935 - loss: 0.2569 - val_accuracy: 0.8933 - val_loss: 0.2587 - mcc: 0.7864\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9014 - loss: 0.2428\n","Epoch 41 - MCC: 0.7893\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9013 - loss: 0.2430 - val_accuracy: 0.8948 - val_loss: 0.2565 - mcc: 0.7893\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8994 - loss: 0.2454\n","Epoch 42 - MCC: 0.7909\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8993 - loss: 0.2455 - val_accuracy: 0.8955 - val_loss: 0.2544 - mcc: 0.7909\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8971 - loss: 0.2484\n","Epoch 43 - MCC: 0.7860\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.8972 - loss: 0.2483 - val_accuracy: 0.8923 - val_loss: 0.2617 - mcc: 0.7860\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8991 - loss: 0.2447\n","Epoch 44 - MCC: 0.7892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8991 - loss: 0.2448 - val_accuracy: 0.8946 - val_loss: 0.2555 - mcc: 0.7892\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8952 - loss: 0.2546\n","Epoch 45 - MCC: 0.7913\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - accuracy: 0.8953 - loss: 0.2543 - val_accuracy: 0.8956 - val_loss: 0.2545 - mcc: 0.7913\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9046 - loss: 0.2340\n","Epoch 46 - MCC: 0.7920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9044 - loss: 0.2344 - val_accuracy: 0.8961 - val_loss: 0.2535 - mcc: 0.7920\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8991 - loss: 0.2455\n","Epoch 47 - MCC: 0.7925\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.8991 - loss: 0.2454 - val_accuracy: 0.8964 - val_loss: 0.2526 - mcc: 0.7925\n","Epoch 48/50\n","\u001b[1m24/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9028 - loss: 0.2378\n","Epoch 48 - MCC: 0.7913\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - accuracy: 0.9025 - loss: 0.2383 - val_accuracy: 0.8956 - val_loss: 0.2551 - mcc: 0.7913\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9064 - loss: 0.2313\n","Epoch 49 - MCC: 0.7883\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9062 - loss: 0.2317 - val_accuracy: 0.8937 - val_loss: 0.2557 - mcc: 0.7883\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8956 - loss: 0.2502\n","Epoch 50 - MCC: 0.7937\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.8957 - loss: 0.2499 - val_accuracy: 0.8970 - val_loss: 0.2512 - mcc: 0.7937\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9047133333333334,\n","              'mean': 0.8988826666666666,\n","              'min': 0.8849533333333334,\n","              'std': 0.007504723964129371},\n"," 'Inference Time (s/sample)': {'max': 0.0019326448440551759,\n","                               'mean': 0.0019187083244323732,\n","                               'min': 0.0018892478942871094,\n","                               'std': 1.563739711179627e-05},\n"," 'MCC': {'max': 0.8087747766899881,\n","         'mean': 0.7972200532078857,\n","         'min': 0.7694620801752171,\n","         'std': 0.01491181028285565},\n"," 'Parameters': 4897,\n"," 'Train Time (s)': {'max': 96.69378447532654,\n","                    'mean': 85.17904925346375,\n","                    'min': 78.34442186355591,\n","                    'std': 6.348495084176636},\n"," 'Training Accuracy': [[0.6116183400154114,\n","                        0.710421621799469,\n","                        0.7924183011054993,\n","                        0.8285133838653564,\n","                        0.8424183130264282,\n","                        0.8529084920883179,\n","                        0.8604899644851685,\n","                        0.8687917590141296,\n","                        0.8763984441757202,\n","                        0.880246639251709,\n","                        0.8834784030914307,\n","                        0.8835567235946655,\n","                        0.8863450884819031,\n","                        0.8872649669647217,\n","                        0.8880733251571655,\n","                        0.8895650506019592,\n","                        0.8899067044258118,\n","                        0.8899617195129395,\n","                        0.8908032774925232,\n","                        0.8912016153335571,\n","                        0.8922533392906189,\n","                        0.8922699093818665,\n","                        0.8923182487487793,\n","                        0.8924784064292908,\n","                        0.8909849524497986,\n","                        0.892988383769989,\n","                        0.8928148746490479,\n","                        0.8935917019844055,\n","                        0.8945750594139099,\n","                        0.894623339176178,\n","                        0.893730103969574,\n","                        0.8948799967765808,\n","                        0.8953016400337219,\n","                        0.8951699733734131,\n","                        0.894746720790863,\n","                        0.8956866264343262,\n","                        0.8960283398628235,\n","                        0.8958500623703003,\n","                        0.8961833119392395,\n","                        0.8969866037368774,\n","                        0.8965849280357361,\n","                        0.8965365886688232,\n","                        0.897331714630127,\n","                        0.8977082967758179,\n","                        0.897871732711792,\n","                        0.8980566263198853,\n","                        0.8979566693305969,\n","                        0.8976300954818726,\n","                        0.8979249596595764,\n","                        0.8992266058921814],\n","                       [0.6525616645812988,\n","                        0.7215633392333984,\n","                        0.8031349778175354,\n","                        0.8348535299301147,\n","                        0.8512017130851746,\n","                        0.8638766407966614,\n","                        0.872768223285675,\n","                        0.8773349523544312,\n","                        0.8833783864974976,\n","                        0.8844115734100342,\n","                        0.8874166011810303,\n","                        0.8889883160591125,\n","                        0.8901882767677307,\n","                        0.8916350603103638,\n","                        0.8920900225639343,\n","                        0.892591655254364,\n","                        0.8917998671531677,\n","                        0.8932051062583923,\n","                        0.8936151266098022,\n","                        0.8953151106834412,\n","                        0.8958867788314819,\n","                        0.8963749408721924,\n","                        0.8963882923126221,\n","                        0.8962284922599792,\n","                        0.8966816067695618,\n","                        0.8977566361427307,\n","                        0.8979049921035767,\n","                        0.8982399702072144,\n","                        0.8972282409667969,\n","                        0.8979916572570801,\n","                        0.8990516662597656,\n","                        0.8995649814605713,\n","                        0.8992434740066528,\n","                        0.8999649882316589,\n","                        0.9001882076263428,\n","                        0.8999999761581421,\n","                        0.9007101655006409,\n","                        0.9007116556167603,\n","                        0.9011533260345459,\n","                        0.9005216360092163,\n","                        0.9014467000961304,\n","                        0.9016433954238892,\n","                        0.9013450741767883,\n","                        0.9018366932868958,\n","                        0.9020849466323853,\n","                        0.9026950001716614,\n","                        0.9026550054550171,\n","                        0.903053343296051,\n","                        0.9033516645431519,\n","                        0.9031931161880493],\n","                       [0.7155933380126953,\n","                        0.7869000434875488,\n","                        0.8128683567047119,\n","                        0.8372383117675781,\n","                        0.8529567122459412,\n","                        0.8624666333198547,\n","                        0.8717133402824402,\n","                        0.8766815662384033,\n","                        0.8814067244529724,\n","                        0.8843750953674316,\n","                        0.8862866759300232,\n","                        0.8875800967216492,\n","                        0.8882217407226562,\n","                        0.8886217474937439,\n","                        0.8895715475082397,\n","                        0.8907397985458374,\n","                        0.8907166123390198,\n","                        0.8916732668876648,\n","                        0.8924067616462708,\n","                        0.8916383981704712,\n","                        0.8929382562637329,\n","                        0.8927833437919617,\n","                        0.8932048678398132,\n","                        0.8933966159820557,\n","                        0.8938165307044983,\n","                        0.893779993057251,\n","                        0.8938266634941101,\n","                        0.8949200510978699,\n","                        0.8938949704170227,\n","                        0.8951817154884338,\n","                        0.8954282402992249,\n","                        0.8959465622901917,\n","                        0.8964616656303406,\n","                        0.8953867554664612,\n","                        0.8964349627494812,\n","                        0.8966849446296692,\n","                        0.8974316120147705,\n","                        0.8973133563995361,\n","                        0.8971083164215088,\n","                        0.8963683247566223,\n","                        0.8978967070579529,\n","                        0.8976148366928101,\n","                        0.8972481489181519,\n","                        0.8976367115974426,\n","                        0.8982700109481812,\n","                        0.8977600932121277,\n","                        0.8974983096122742,\n","                        0.8994783163070679,\n","                        0.8990349769592285,\n","                        0.8989666700363159],\n","                       [0.6480849981307983,\n","                        0.7174432873725891,\n","                        0.8041250109672546,\n","                        0.8340917825698853,\n","                        0.8504898548126221,\n","                        0.8629716634750366,\n","                        0.8734649419784546,\n","                        0.8776800036430359,\n","                        0.8815600872039795,\n","                        0.8842365741729736,\n","                        0.8873699903488159,\n","                        0.8884783983230591,\n","                        0.8899883031845093,\n","                        0.8906867504119873,\n","                        0.890946626663208,\n","                        0.8913816809654236,\n","                        0.8927931785583496,\n","                        0.893256664276123,\n","                        0.8928850293159485,\n","                        0.893833339214325,\n","                        0.8932982683181763,\n","                        0.8944200277328491,\n","                        0.893703281879425,\n","                        0.8941734433174133,\n","                        0.8956982493400574,\n","                        0.8958415985107422,\n","                        0.8966099619865417,\n","                        0.8971598744392395,\n","                        0.896818220615387,\n","                        0.8971533179283142,\n","                        0.8967816829681396,\n","                        0.8979099988937378,\n","                        0.8977882862091064,\n","                        0.8982100486755371,\n","                        0.8982400298118591,\n","                        0.8987516760826111,\n","                        0.8989465832710266,\n","                        0.8994516134262085,\n","                        0.8982666730880737,\n","                        0.8993183374404907,\n","                        0.8997417688369751,\n","                        0.899869978427887,\n","                        0.9001984596252441,\n","                        0.8995283246040344,\n","                        0.9001283049583435,\n","                        0.9004416465759277,\n","                        0.9010733962059021,\n","                        0.9007167220115662,\n","                        0.9003750681877136,\n","                        0.8990132808685303],\n","                       [0.5666833519935608,\n","                        0.6737383008003235,\n","                        0.7776567935943604,\n","                        0.8112599849700928,\n","                        0.8322133421897888,\n","                        0.8480650186538696,\n","                        0.8596649765968323,\n","                        0.8695116639137268,\n","                        0.8762383460998535,\n","                        0.8790434002876282,\n","                        0.8825783729553223,\n","                        0.8845716714859009,\n","                        0.8871933817863464,\n","                        0.88787841796875,\n","                        0.8883334398269653,\n","                        0.8909149169921875,\n","                        0.8913583159446716,\n","                        0.8912750482559204,\n","                        0.8916083574295044,\n","                        0.8932015299797058,\n","                        0.8937416076660156,\n","                        0.8936900496482849,\n","                        0.8946599364280701,\n","                        0.8949867486953735,\n","                        0.8950549960136414,\n","                        0.8954083323478699,\n","                        0.8953465819358826,\n","                        0.8959800601005554,\n","                        0.8958683609962463,\n","                        0.8963032364845276,\n","                        0.8964933156967163,\n","                        0.8958233594894409,\n","                        0.89499831199646,\n","                        0.8968499898910522,\n","                        0.8969315886497498,\n","                        0.8977982997894287,\n","                        0.8978400230407715,\n","                        0.8981850147247314,\n","                        0.8982366919517517,\n","                        0.89784836769104,\n","                        0.8981133103370667,\n","                        0.8982250094413757,\n","                        0.8977699875831604,\n","                        0.8989516496658325,\n","                        0.8987465500831604,\n","                        0.8996732831001282,\n","                        0.9000282287597656,\n","                        0.8996568322181702,\n","                        0.9000732898712158,\n","                        0.9002249836921692]],\n"," 'Training Loss': [[0.6765419244766235,\n","                    0.6113150715827942,\n","                    0.47055584192276,\n","                    0.3952639102935791,\n","                    0.36012911796569824,\n","                    0.33939918875694275,\n","                    0.32298070192337036,\n","                    0.30638888478279114,\n","                    0.293936163187027,\n","                    0.2870872914791107,\n","                    0.28007376194000244,\n","                    0.2807529866695404,\n","                    0.2740078568458557,\n","                    0.27369967103004456,\n","                    0.27136340737342834,\n","                    0.268014520406723,\n","                    0.26690369844436646,\n","                    0.2672359049320221,\n","                    0.26557934284210205,\n","                    0.2640050947666168,\n","                    0.2624588906764984,\n","                    0.2624896764755249,\n","                    0.26231804490089417,\n","                    0.2617560625076294,\n","                    0.2639038860797882,\n","                    0.2610568702220917,\n","                    0.26013848185539246,\n","                    0.2596116065979004,\n","                    0.25713086128234863,\n","                    0.25702542066574097,\n","                    0.25886890292167664,\n","                    0.2556985914707184,\n","                    0.2553274929523468,\n","                    0.25581881403923035,\n","                    0.2560221552848816,\n","                    0.25364527106285095,\n","                    0.25368383526802063,\n","                    0.25348591804504395,\n","                    0.25262463092803955,\n","                    0.25180065631866455,\n","                    0.2522202432155609,\n","                    0.2522929608821869,\n","                    0.2505635619163513,\n","                    0.2506173551082611,\n","                    0.24962928891181946,\n","                    0.24883855879306793,\n","                    0.24907532334327698,\n","                    0.2502569854259491,\n","                    0.24845851957798004,\n","                    0.24671989679336548],\n","                   [0.6761108636856079,\n","                    0.6051444411277771,\n","                    0.46678292751312256,\n","                    0.3886067867279053,\n","                    0.3478430509567261,\n","                    0.31946510076522827,\n","                    0.30337345600128174,\n","                    0.2918670177459717,\n","                    0.28165680170059204,\n","                    0.2769950330257416,\n","                    0.2718527317047119,\n","                    0.26886874437332153,\n","                    0.2661777436733246,\n","                    0.2626480162143707,\n","                    0.26219817996025085,\n","                    0.2603820860385895,\n","                    0.261712908744812,\n","                    0.25864216685295105,\n","                    0.25764572620391846,\n","                    0.2546089291572571,\n","                    0.25342199206352234,\n","                    0.25280871987342834,\n","                    0.25210657715797424,\n","                    0.2526216506958008,\n","                    0.2517695128917694,\n","                    0.2494586706161499,\n","                    0.24922242760658264,\n","                    0.24860776960849762,\n","                    0.25126004219055176,\n","                    0.24842345714569092,\n","                    0.24619530141353607,\n","                    0.2457737922668457,\n","                    0.24569693207740784,\n","                    0.24414177238941193,\n","                    0.2438424825668335,\n","                    0.24400772154331207,\n","                    0.24312454462051392,\n","                    0.24296057224273682,\n","                    0.24182046949863434,\n","                    0.2431010603904724,\n","                    0.24096493422985077,\n","                    0.24169328808784485,\n","                    0.24069790542125702,\n","                    0.24029433727264404,\n","                    0.23956964910030365,\n","                    0.2383958399295807,\n","                    0.23830950260162354,\n","                    0.23739250004291534,\n","                    0.23688232898712158,\n","                    0.23716577887535095],\n","                   [0.6764618158340454,\n","                    0.6111170053482056,\n","                    0.4803982079029083,\n","                    0.37776079773902893,\n","                    0.3426886796951294,\n","                    0.32263579964637756,\n","                    0.305574506521225,\n","                    0.29584577679634094,\n","                    0.2867370843887329,\n","                    0.2808363139629364,\n","                    0.2775404453277588,\n","                    0.2735409736633301,\n","                    0.2733613848686218,\n","                    0.2718510329723358,\n","                    0.27032652497291565,\n","                    0.2665586471557617,\n","                    0.26731130480766296,\n","                    0.2658146917819977,\n","                    0.26365453004837036,\n","                    0.2651752829551697,\n","                    0.2631756067276001,\n","                    0.26201796531677246,\n","                    0.2611900866031647,\n","                    0.26061293482780457,\n","                    0.2596968710422516,\n","                    0.2604069113731384,\n","                    0.2591640055179596,\n","                    0.25747668743133545,\n","                    0.25955691933631897,\n","                    0.2566916048526764,\n","                    0.2551692724227905,\n","                    0.2548690736293793,\n","                    0.2538752853870392,\n","                    0.25516819953918457,\n","                    0.25272753834724426,\n","                    0.25264179706573486,\n","                    0.250882089138031,\n","                    0.2507822513580322,\n","                    0.25098443031311035,\n","                    0.25201496481895447,\n","                    0.24912717938423157,\n","                    0.24860738217830658,\n","                    0.24958939850330353,\n","                    0.248774453997612,\n","                    0.24794796109199524,\n","                    0.24818405508995056,\n","                    0.24938775599002838,\n","                    0.24519743025302887,\n","                    0.24530909955501556,\n","                    0.2449486404657364],\n","                   [0.6731674671173096,\n","                    0.6035385727882385,\n","                    0.4747834801673889,\n","                    0.3953244686126709,\n","                    0.352298378944397,\n","                    0.3206503987312317,\n","                    0.29961952567100525,\n","                    0.28928741812705994,\n","                    0.28157517313957214,\n","                    0.2755010426044464,\n","                    0.269848108291626,\n","                    0.2673715353012085,\n","                    0.2651655972003937,\n","                    0.2634366452693939,\n","                    0.26262423396110535,\n","                    0.26194241642951965,\n","                    0.25928443670272827,\n","                    0.257946640253067,\n","                    0.2585909962654114,\n","                    0.25732433795928955,\n","                    0.2582169771194458,\n","                    0.2562845051288605,\n","                    0.25650206208229065,\n","                    0.2573491632938385,\n","                    0.25231605768203735,\n","                    0.25281432271003723,\n","                    0.25133177638053894,\n","                    0.24989937245845795,\n","                    0.25129270553588867,\n","                    0.24998894333839417,\n","                    0.25060638785362244,\n","                    0.2483699917793274,\n","                    0.24821728467941284,\n","                    0.24757876992225647,\n","                    0.24737168848514557,\n","                    0.24615493416786194,\n","                    0.24536076188087463,\n","                    0.24414990842342377,\n","                    0.24659433960914612,\n","                    0.24438375234603882,\n","                    0.24329714477062225,\n","                    0.24301357567310333,\n","                    0.24285343289375305,\n","                    0.243107870221138,\n","                    0.24158217012882233,\n","                    0.24144956469535828,\n","                    0.24010156095027924,\n","                    0.2405964434146881,\n","                    0.24072682857513428,\n","                    0.2443312406539917],\n","                   [0.6818969249725342,\n","                    0.6320048570632935,\n","                    0.5246431827545166,\n","                    0.4426668882369995,\n","                    0.3868972659111023,\n","                    0.3553119897842407,\n","                    0.32955485582351685,\n","                    0.30923566222190857,\n","                    0.29686567187309265,\n","                    0.28959399461746216,\n","                    0.2823118567466736,\n","                    0.2788597643375397,\n","                    0.2729845643043518,\n","                    0.27139952778816223,\n","                    0.27030280232429504,\n","                    0.26506853103637695,\n","                    0.26377373933792114,\n","                    0.2637176811695099,\n","                    0.2634497284889221,\n","                    0.2593499422073364,\n","                    0.2588025629520416,\n","                    0.2587730884552002,\n","                    0.257101833820343,\n","                    0.2564617097377777,\n","                    0.2549338638782501,\n","                    0.2549266219139099,\n","                    0.2547018826007843,\n","                    0.25346365571022034,\n","                    0.2536159157752991,\n","                    0.25220730900764465,\n","                    0.2518137991428375,\n","                    0.253590852022171,\n","                    0.25466421246528625,\n","                    0.2515692710876465,\n","                    0.25045716762542725,\n","                    0.24864980578422546,\n","                    0.24881210923194885,\n","                    0.24809147417545319,\n","                    0.24822847545146942,\n","                    0.24861648678779602,\n","                    0.24874809384346008,\n","                    0.24781133234500885,\n","                    0.24809303879737854,\n","                    0.2463015615940094,\n","                    0.24629975855350494,\n","                    0.24435852468013763,\n","                    0.24314585328102112,\n","                    0.24438199400901794,\n","                    0.24319323897361755,\n","                    0.24290484189987183]],\n"," 'Validation Accuracy': [[0.6880333423614502,\n","                          0.7635600566864014,\n","                          0.8193733096122742,\n","                          0.8436933159828186,\n","                          0.8577266931533813,\n","                          0.862060010433197,\n","                          0.8712799549102783,\n","                          0.8817665576934814,\n","                          0.8865800499916077,\n","                          0.8892732858657837,\n","                          0.884006679058075,\n","                          0.8916266560554504,\n","                          0.8949400186538696,\n","                          0.8938133120536804,\n","                          0.8956799507141113,\n","                          0.8975599408149719,\n","                          0.895673394203186,\n","                          0.8960133194923401,\n","                          0.8983865976333618,\n","                          0.898586630821228,\n","                          0.899013340473175,\n","                          0.8965999484062195,\n","                          0.8997867107391357,\n","                          0.895966649055481,\n","                          0.8999399542808533,\n","                          0.900013267993927,\n","                          0.9009333848953247,\n","                          0.9015533328056335,\n","                          0.9015733599662781,\n","                          0.8982066512107849,\n","                          0.901253342628479,\n","                          0.9023666977882385,\n","                          0.9011533260345459,\n","                          0.8995799422264099,\n","                          0.9023600220680237,\n","                          0.9019799828529358,\n","                          0.9033267498016357,\n","                          0.9008266925811768,\n","                          0.9018266797065735,\n","                          0.9020066261291504,\n","                          0.9042133092880249,\n","                          0.90312659740448,\n","                          0.9047466516494751,\n","                          0.9035733938217163,\n","                          0.9041866064071655,\n","                          0.9043399691581726,\n","                          0.9044399857521057,\n","                          0.9025599956512451,\n","                          0.9029600024223328,\n","                          0.9047133922576904],\n","                         [0.6593733429908752,\n","                          0.7636067271232605,\n","                          0.8161599636077881,\n","                          0.8351800441741943,\n","                          0.8476865887641907,\n","                          0.8571866750717163,\n","                          0.8586866855621338,\n","                          0.8671932816505432,\n","                          0.8701133728027344,\n","                          0.8727667331695557,\n","                          0.873680055141449,\n","                          0.8738799095153809,\n","                          0.8759332895278931,\n","                          0.8771601319313049,\n","                          0.8772798776626587,\n","                          0.8774800300598145,\n","                          0.8798400163650513,\n","                          0.8799199461936951,\n","                          0.8799199461936951,\n","                          0.8798533082008362,\n","                          0.8816133737564087,\n","                          0.8815400004386902,\n","                          0.8816800713539124,\n","                          0.881380021572113,\n","                          0.8822599649429321,\n","                          0.8828666806221008,\n","                          0.8823133111000061,\n","                          0.8811599612236023,\n","                          0.8830733299255371,\n","                          0.8841332793235779,\n","                          0.8840200304985046,\n","                          0.8841066956520081,\n","                          0.8848798871040344,\n","                          0.8840199112892151,\n","                          0.8839133977890015,\n","                          0.8848400712013245,\n","                          0.8850198984146118,\n","                          0.884433388710022,\n","                          0.8846933245658875,\n","                          0.8850000500679016,\n","                          0.8856000304222107,\n","                          0.886406660079956,\n","                          0.885646641254425,\n","                          0.8848733305931091,\n","                          0.8858866691589355,\n","                          0.8865200281143188,\n","                          0.8865066766738892,\n","                          0.8866267204284668,\n","                          0.8869933485984802,\n","                          0.8849533200263977],\n","                         [0.7930333614349365,\n","                          0.8007800579071045,\n","                          0.8273466229438782,\n","                          0.8528799414634705,\n","                          0.8612599968910217,\n","                          0.8722266554832458,\n","                          0.8779934048652649,\n","                          0.8851867914199829,\n","                          0.888719916343689,\n","                          0.8903933167457581,\n","                          0.8918666839599609,\n","                          0.8917667269706726,\n","                          0.8941799402236938,\n","                          0.895193338394165,\n","                          0.8950866460800171,\n","                          0.8966400027275085,\n","                          0.897379994392395,\n","                          0.8959266543388367,\n","                          0.8971800208091736,\n","                          0.896893322467804,\n","                          0.9001867175102234,\n","                          0.8991999626159668,\n","                          0.8999000787734985,\n","                          0.8965466022491455,\n","                          0.9006800055503845,\n","                          0.8989333510398865,\n","                          0.9000732898712158,\n","                          0.9012132883071899,\n","                          0.9021999835968018,\n","                          0.9000999927520752,\n","                          0.9017732739448547,\n","                          0.90177321434021,\n","                          0.9005132913589478,\n","                          0.90229332447052,\n","                          0.902126669883728,\n","                          0.9023134112358093,\n","                          0.9020332098007202,\n","                          0.9014533162117004,\n","                          0.9025399684906006,\n","                          0.9030200242996216,\n","                          0.904086709022522,\n","                          0.9019733667373657,\n","                          0.9030666947364807,\n","                          0.9019333720207214,\n","                          0.9048799872398376,\n","                          0.9022600054740906,\n","                          0.9055333733558655,\n","                          0.9054332971572876,\n","                          0.9029933214187622,\n","                          0.9041333794593811],\n","                         [0.6783466339111328,\n","                          0.7823066115379333,\n","                          0.8322332501411438,\n","                          0.8493666648864746,\n","                          0.8632400035858154,\n","                          0.877613365650177,\n","                          0.8825600147247314,\n","                          0.8849133253097534,\n","                          0.8855533599853516,\n","                          0.8904334306716919,\n","                          0.891593337059021,\n","                          0.893833339214325,\n","                          0.8941400051116943,\n","                          0.8957200050354004,\n","                          0.8954334259033203,\n","                          0.896399974822998,\n","                          0.895953357219696,\n","                          0.8966465592384338,\n","                          0.8964333534240723,\n","                          0.8962799906730652,\n","                          0.8945266604423523,\n","                          0.8975599408149719,\n","                          0.896506667137146,\n","                          0.8984999656677246,\n","                          0.898953378200531,\n","                          0.8999333381652832,\n","                          0.8985466957092285,\n","                          0.8983200192451477,\n","                          0.8993800282478333,\n","                          0.8984665870666504,\n","                          0.8982866406440735,\n","                          0.9003067016601562,\n","                          0.9005999565124512,\n","                          0.8993532657623291,\n","                          0.8995932936668396,\n","                          0.9011200666427612,\n","                          0.901699960231781,\n","                          0.9000266194343567,\n","                          0.9017733931541443,\n","                          0.9016533493995667,\n","                          0.9024999737739563,\n","                          0.9003732800483704,\n","                          0.9027600288391113,\n","                          0.9002400040626526,\n","                          0.9025067090988159,\n","                          0.903493344783783,\n","                          0.9033266305923462,\n","                          0.9032132625579834,\n","                          0.9023067355155945,\n","                          0.9036200046539307],\n","                         [0.6019733548164368,\n","                          0.7332133650779724,\n","                          0.8036067485809326,\n","                          0.8202400207519531,\n","                          0.8392666578292847,\n","                          0.8528599739074707,\n","                          0.861613392829895,\n","                          0.8702533841133118,\n","                          0.8745533227920532,\n","                          0.8780534267425537,\n","                          0.8814199566841125,\n","                          0.882860004901886,\n","                          0.8855932354927063,\n","                          0.885640025138855,\n","                          0.8874466419219971,\n","                          0.8876599669456482,\n","                          0.8880732655525208,\n","                          0.8890333771705627,\n","                          0.8895266652107239,\n","                          0.8881999850273132,\n","                          0.8888800144195557,\n","                          0.8893200159072876,\n","                          0.8908132910728455,\n","                          0.8903800249099731,\n","                          0.8911332488059998,\n","                          0.8910333514213562,\n","                          0.8923133611679077,\n","                          0.8912333846092224,\n","                          0.8919267058372498,\n","                          0.8909733295440674,\n","                          0.8921533226966858,\n","                          0.8904133439064026,\n","                          0.892073392868042,\n","                          0.8912933468818665,\n","                          0.8920333385467529,\n","                          0.8938866257667542,\n","                          0.8933465480804443,\n","                          0.8943066596984863,\n","                          0.8921400308609009,\n","                          0.8933266997337341,\n","                          0.8947599530220032,\n","                          0.8954866528511047,\n","                          0.8922532796859741,\n","                          0.8945866227149963,\n","                          0.8955866098403931,\n","                          0.8960999846458435,\n","                          0.8964266180992126,\n","                          0.8956066966056824,\n","                          0.8936933279037476,\n","                          0.8969933390617371]],\n"," 'Validation Loss': [[0.6539187431335449,\n","                      0.5425115823745728,\n","                      0.4124102294445038,\n","                      0.36271631717681885,\n","                      0.33665013313293457,\n","                      0.319621205329895,\n","                      0.30304646492004395,\n","                      0.285457581281662,\n","                      0.2767379879951477,\n","                      0.2705918550491333,\n","                      0.277006596326828,\n","                      0.26523077487945557,\n","                      0.25855231285095215,\n","                      0.25898534059524536,\n","                      0.25652989745140076,\n","                      0.25345006585121155,\n","                      0.2554064691066742,\n","                      0.25537481904029846,\n","                      0.2506445646286011,\n","                      0.24999429285526276,\n","                      0.24903561174869537,\n","                      0.2564854919910431,\n","                      0.24855275452136993,\n","                      0.25550103187561035,\n","                      0.24668282270431519,\n","                      0.2471698522567749,\n","                      0.24613012373447418,\n","                      0.24412506818771362,\n","                      0.2438974827528,\n","                      0.24859324097633362,\n","                      0.24428509175777435,\n","                      0.24177314341068268,\n","                      0.24396179616451263,\n","                      0.24718262255191803,\n","                      0.2426680326461792,\n","                      0.2414528876543045,\n","                      0.23928004503250122,\n","                      0.2426503300666809,\n","                      0.24089492857456207,\n","                      0.24112674593925476,\n","                      0.237709179520607,\n","                      0.23900528252124786,\n","                      0.23623505234718323,\n","                      0.23848003149032593,\n","                      0.2361830323934555,\n","                      0.23686522245407104,\n","                      0.23547255992889404,\n","                      0.23829855024814606,\n","                      0.23740100860595703,\n","                      0.2344721257686615],\n","                     [0.6595451235771179,\n","                      0.5509456396102905,\n","                      0.4350734353065491,\n","                      0.38248616456985474,\n","                      0.35381123423576355,\n","                      0.334483802318573,\n","                      0.3288300335407257,\n","                      0.3108363449573517,\n","                      0.3051525354385376,\n","                      0.3014086186885834,\n","                      0.3013063371181488,\n","                      0.2993711233139038,\n","                      0.2965461015701294,\n","                      0.2907401919364929,\n","                      0.29275694489479065,\n","                      0.29357728362083435,\n","                      0.2862842381000519,\n","                      0.2860695719718933,\n","                      0.2857862412929535,\n","                      0.28559577465057373,\n","                      0.28208789229393005,\n","                      0.28307101130485535,\n","                      0.28227394819259644,\n","                      0.28741252422332764,\n","                      0.2820223569869995,\n","                      0.2804245948791504,\n","                      0.2805067002773285,\n","                      0.2868598401546478,\n","                      0.2808416783809662,\n","                      0.27683889865875244,\n","                      0.2766290605068207,\n","                      0.2773028016090393,\n","                      0.2759948968887329,\n","                      0.2768884599208832,\n","                      0.2764688730239868,\n","                      0.2754676342010498,\n","                      0.27698075771331787,\n","                      0.2756384611129761,\n","                      0.2753995954990387,\n","                      0.27476686239242554,\n","                      0.2746085822582245,\n","                      0.27213093638420105,\n","                      0.27258196473121643,\n","                      0.2748444080352783,\n","                      0.27295368909835815,\n","                      0.2707642912864685,\n","                      0.27043911814689636,\n","                      0.27025648951530457,\n","                      0.2700899839401245,\n","                      0.2728342115879059],\n","                     [0.6540809869766235,\n","                      0.5509452223777771,\n","                      0.40209460258483887,\n","                      0.34507983922958374,\n","                      0.3222818076610565,\n","                      0.3011416494846344,\n","                      0.2903096377849579,\n","                      0.278537780046463,\n","                      0.2672039270401001,\n","                      0.2673821747303009,\n","                      0.2634148895740509,\n","                      0.25977957248687744,\n","                      0.25870993733406067,\n","                      0.25544461607933044,\n","                      0.2539743185043335,\n","                      0.25287243723869324,\n","                      0.251123309135437,\n","                      0.2538134455680847,\n","                      0.2545611560344696,\n","                      0.24886123836040497,\n","                      0.24366962909698486,\n","                      0.24848352372646332,\n","                      0.2458782196044922,\n","                      0.2519291043281555,\n","                      0.2415902316570282,\n","                      0.24461570382118225,\n","                      0.24490459263324738,\n","                      0.24151207506656647,\n","                      0.24188736081123352,\n","                      0.24421659111976624,\n","                      0.23997874557971954,\n","                      0.24038952589035034,\n","                      0.24441371858119965,\n","                      0.23964430391788483,\n","                      0.2389153242111206,\n","                      0.2398453652858734,\n","                      0.2376321405172348,\n","                      0.2399173378944397,\n","                      0.23634731769561768,\n","                      0.2361764758825302,\n","                      0.23476915061473846,\n","                      0.23671847581863403,\n","                      0.2345934361219406,\n","                      0.23934736847877502,\n","                      0.23156601190567017,\n","                      0.2387617826461792,\n","                      0.2308128923177719,\n","                      0.2306080311536789,\n","                      0.23519496619701385,\n","                      0.2334800660610199],\n","                     [0.6507508754730225,\n","                      0.5353126525878906,\n","                      0.4240395426750183,\n","                      0.364708811044693,\n","                      0.3298192620277405,\n","                      0.2997821867465973,\n","                      0.2860442101955414,\n","                      0.2791045308113098,\n","                      0.2749464809894562,\n","                      0.26769572496414185,\n","                      0.26448044180870056,\n","                      0.2614189088344574,\n","                      0.26174959540367126,\n","                      0.2575949728488922,\n","                      0.2569102942943573,\n","                      0.25536665320396423,\n","                      0.2549833357334137,\n","                      0.25470197200775146,\n","                      0.25460129976272583,\n","                      0.25460878014564514,\n","                      0.257900595664978,\n","                      0.2520400881767273,\n","                      0.2568584382534027,\n","                      0.25054848194122314,\n","                      0.24968644976615906,\n","                      0.24730660021305084,\n","                      0.2497861236333847,\n","                      0.2513562738895416,\n","                      0.2480597347021103,\n","                      0.24962963163852692,\n","                      0.2495097517967224,\n","                      0.24569706618785858,\n","                      0.24459299445152283,\n","                      0.2464364469051361,\n","                      0.24674449861049652,\n","                      0.24419012665748596,\n","                      0.24313673377037048,\n","                      0.24510835111141205,\n","                      0.24256044626235962,\n","                      0.24197150766849518,\n","                      0.2400452047586441,\n","                      0.2464471012353897,\n","                      0.24101105332374573,\n","                      0.24330134689807892,\n","                      0.24021966755390167,\n","                      0.23842716217041016,\n","                      0.23914805054664612,\n","                      0.2389884889125824,\n","                      0.23963487148284912,\n","                      0.23728103935718536],\n","                     [0.6621981263160706,\n","                      0.5882967114448547,\n","                      0.46928784251213074,\n","                      0.41026973724365234,\n","                      0.37286320328712463,\n","                      0.3434693515300751,\n","                      0.3215292990207672,\n","                      0.3064205050468445,\n","                      0.2980220317840576,\n","                      0.29122549295425415,\n","                      0.28474894165992737,\n","                      0.2816067039966583,\n","                      0.27562853693962097,\n","                      0.27471762895584106,\n","                      0.2733587324619293,\n","                      0.27071377635002136,\n","                      0.26834744215011597,\n","                      0.2705039083957672,\n","                      0.2675495445728302,\n","                      0.2698615789413452,\n","                      0.2671167254447937,\n","                      0.2664128541946411,\n","                      0.26461756229400635,\n","                      0.2644158601760864,\n","                      0.26332786679267883,\n","                      0.26307213306427,\n","                      0.261255145072937,\n","                      0.2632438838481903,\n","                      0.26081007719039917,\n","                      0.2634880542755127,\n","                      0.2627958059310913,\n","                      0.2650066912174225,\n","                      0.2614933252334595,\n","                      0.2608850598335266,\n","                      0.2611010670661926,\n","                      0.25921204686164856,\n","                      0.2587134540081024,\n","                      0.25722813606262207,\n","                      0.2599734365940094,\n","                      0.258701354265213,\n","                      0.25647619366645813,\n","                      0.25442153215408325,\n","                      0.2617037296295166,\n","                      0.2554871439933777,\n","                      0.25454843044281006,\n","                      0.2535232603549957,\n","                      0.25258970260620117,\n","                      0.2550951838493347,\n","                      0.25570839643478394,\n","                      0.2511577904224396]],\n"," 'Validation MCC': [[0.37223098107618435,\n","                     0.5252758289486009,\n","                     0.6390564439230363,\n","                     0.6860103978234389,\n","                     0.7142376820831707,\n","                     0.7233810714732015,\n","                     0.741555222713579,\n","                     0.7627215088128113,\n","                     0.7722350951435328,\n","                     0.7776955430824282,\n","                     0.7681268767289371,\n","                     0.782804595592999,\n","                     0.7891642647928775,\n","                     0.7870566358657398,\n","                     0.7906998099348078,\n","                     0.7942990589071809,\n","                     0.7905166278688438,\n","                     0.7918527593106398,\n","                     0.7961487759665866,\n","                     0.7963625348009115,\n","                     0.7974399936103304,\n","                     0.7936787788208866,\n","                     0.798798614216266,\n","                     0.7913629647309999,\n","                     0.7991744670919175,\n","                     0.7993508603776316,\n","                     0.801150510870253,\n","                     0.8023238437764348,\n","                     0.802371081469389,\n","                     0.7966825039524371,\n","                     0.8019787604288222,\n","                     0.8039576747008002,\n","                     0.8015033175386542,\n","                     0.7985855768590743,\n","                     0.803938387276383,\n","                     0.8032091530198898,\n","                     0.8058880667822214,\n","                     0.8009554975264446,\n","                     0.8030927564009894,\n","                     0.8034549259464145,\n","                     0.8076521700447356,\n","                     0.8054660516963341,\n","                     0.8087215758118749,\n","                     0.8066645705961264,\n","                     0.8076236588276455,\n","                     0.8079812623925556,\n","                     0.8081758751678997,\n","                     0.8043542871844656,\n","                     0.8051404157992011,\n","                     0.8087747766899881],\n","                    [0.3534728639346752,\n","                     0.536428426465261,\n","                     0.6319402108061482,\n","                     0.6700598372783773,\n","                     0.6946142639132245,\n","                     0.7136926102038418,\n","                     0.7174106582565641,\n","                     0.7338257571954593,\n","                     0.7396928201761825,\n","                     0.7450047257143799,\n","                     0.7467063058163983,\n","                     0.7472535534432105,\n","                     0.7513993648383622,\n","                     0.7537590939413839,\n","                     0.7546350133463959,\n","                     0.7550020132045306,\n","                     0.7590737910169595,\n","                     0.7593385616752524,\n","                     0.7596259614543944,\n","                     0.7599372333773226,\n","                     0.7626705521766597,\n","                     0.7624947571063961,\n","                     0.7629875056223987,\n","                     0.7621750085278227,\n","                     0.7639629549844927,\n","                     0.7654161311400987,\n","                     0.7643667023915426,\n","                     0.7631448288489762,\n","                     0.7660772877279376,\n","                     0.7676884515873063,\n","                     0.7674550804063394,\n","                     0.7676851214688016,\n","                     0.7692283040485538,\n","                     0.7674689908573807,\n","                     0.7674142126134896,\n","                     0.7690939149909228,\n","                     0.7694852752416256,\n","                     0.7685113096340236,\n","                     0.7693502510656195,\n","                     0.7696006161572391,\n","                     0.7708325389460972,\n","                     0.7722532382996055,\n","                     0.7707304020378783,\n","                     0.7692963027261096,\n","                     0.7712106584395422,\n","                     0.7725969844550966,\n","                     0.7728797233603499,\n","                     0.7728370288810514,\n","                     0.7734295698313707,\n","                     0.7694620801752171],\n","                    [0.5864515210150957,\n","                     0.6037483162707556,\n","                     0.6531011791413298,\n","                     0.7043863080367931,\n","                     0.7220962665445237,\n","                     0.7433303780738235,\n","                     0.7555752266692469,\n","                     0.769345253490699,\n","                     0.7764678106701477,\n","                     0.7798183126647799,\n","                     0.7829376222470381,\n","                     0.7826669149236216,\n","                     0.7874355461431857,\n","                     0.7896010434503553,\n","                     0.7897293044136467,\n","                     0.7925729617610828,\n","                     0.7939193955514673,\n","                     0.7916711463586515,\n","                     0.7939786540557859,\n","                     0.7931591281443259,\n","                     0.7995142646062018,\n","                     0.7975860761479512,\n","                     0.7996898182269168,\n","                     0.7929723065498631,\n","                     0.800494473792461,\n","                     0.7971769579052951,\n","                     0.7998072576047277,\n","                     0.801748363785921,\n","                     0.8035585627776894,\n","                     0.799324458463173,\n","                     0.8027285015025908,\n","                     0.8027078360244982,\n","                     0.8001636598336841,\n","                     0.8038421081619334,\n","                     0.8034056667735567,\n","                     0.8038129669944069,\n","                     0.803210579751085,\n","                     0.8024552049067379,\n","                     0.8042464937317265,\n","                     0.8051934689907831,\n","                     0.807337865079018,\n","                     0.8031516993521034,\n","                     0.8057805042334643,\n","                     0.8031398018178436,\n","                     0.8090788104303012,\n","                     0.8037197663459824,\n","                     0.8102461081906848,\n","                     0.8101215920134571,\n","                     0.8051402163612209,\n","                     0.8074388152345238],\n","                    [0.395177777881855,\n","                     0.5789277524070705,\n","                     0.6647658001069209,\n","                     0.6986192312006142,\n","                     0.7257379849624199,\n","                     0.7546436124389726,\n","                     0.7647150822345719,\n","                     0.769579350209875,\n","                     0.770808626683744,\n","                     0.7802972646522913,\n","                     0.7826482198247028,\n","                     0.7871397856102644,\n","                     0.7881316448762454,\n","                     0.7910208284762018,\n","                     0.7903302119874809,\n","                     0.7923397150557128,\n","                     0.7914816816318888,\n","                     0.7927766245795755,\n","                     0.7924016824794856,\n","                     0.7920435356955752,\n","                     0.7885868021578443,\n","                     0.7946142761099395,\n","                     0.7933138456588698,\n","                     0.7968021408365976,\n","                     0.7973878986031447,\n","                     0.7995111259291336,\n","                     0.7966109434221516,\n","                     0.7961179489620248,\n","                     0.7982442013203662,\n","                     0.7965047991848052,\n","                     0.796089823839611,\n","                     0.8001336787167921,\n","                     0.800764505034445,\n","                     0.7982075280032334,\n","                     0.7986934918750908,\n","                     0.802020925356621,\n","                     0.802917445675376,\n","                     0.7995523934559365,\n","                     0.8030935064008596,\n","                     0.8029425479326403,\n","                     0.8046371693773381,\n","                     0.8002446633002923,\n","                     0.8050223820206867,\n","                     0.7999824681565006,\n","                     0.8050222602918659,\n","                     0.8065400278753364,\n","                     0.8064218434007265,\n","                     0.8061759234032203,\n","                     0.8041265755760334,\n","                     0.8067492528787613],\n","                    [0.2977347234641503,\n","                     0.4832048399560612,\n","                     0.6067771204972292,\n","                     0.6398363285709562,\n","                     0.6783652705231779,\n","                     0.7051104969394266,\n","                     0.7226609249436631,\n","                     0.74042787581931,\n","                     0.7486068079582792,\n","                     0.7559290538034755,\n","                     0.7623983351421211,\n","                     0.7655264409649783,\n","                     0.7707511905097594,\n","                     0.7715133574759585,\n","                     0.7744748455674274,\n","                     0.7752546506758591,\n","                     0.7757446487606258,\n","                     0.7776320904233274,\n","                     0.7786270769601042,\n","                     0.7760153621539927,\n","                     0.7776775656717532,\n","                     0.7786496684807749,\n","                     0.7814628704219623,\n","                     0.7803419178933486,\n","                     0.7818410211889099,\n","                     0.7816451750774338,\n","                     0.7842073267204924,\n","                     0.7828537668089247,\n","                     0.7834369973058652,\n","                     0.782354452062054,\n","                     0.7839361934435246,\n","                     0.7821252933298023,\n","                     0.7846284449303079,\n","                     0.7821744927589186,\n","                     0.7841836645432652,\n","                     0.7874245563624976,\n","                     0.7863477739356832,\n","                     0.7882788261488185,\n","                     0.7840367199985667,\n","                     0.7864236137216726,\n","                     0.7892736429285496,\n","                     0.790928431021778,\n","                     0.7860449727469587,\n","                     0.789201032477408,\n","                     0.7912810003181572,\n","                     0.7920439741489029,\n","                     0.7925185361701556,\n","                     0.7913416922907108,\n","                     0.788342498562131,\n","                     0.7936753410609383]]}\n","Training Model: LSTM_Deep, Fold: 1\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5667 - loss: 0.6813\n","Epoch 1 - MCC: 0.4292\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.5687 - loss: 0.6805 - val_accuracy: 0.7163 - val_loss: 0.5843 - mcc: 0.4292\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.7433 - loss: 0.5370\n","Epoch 2 - MCC: 0.6321\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.7443 - loss: 0.5354 - val_accuracy: 0.8169 - val_loss: 0.4105 - mcc: 0.6321\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8194 - loss: 0.4021\n","Epoch 3 - MCC: 0.6892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8195 - loss: 0.4018 - val_accuracy: 0.8451 - val_loss: 0.3540 - mcc: 0.6892\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8497 - loss: 0.3435\n","Epoch 4 - MCC: 0.7277\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8497 - loss: 0.3436 - val_accuracy: 0.8644 - val_loss: 0.3171 - mcc: 0.7277\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8621 - loss: 0.3238\n","Epoch 5 - MCC: 0.7568\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.8624 - loss: 0.3234 - val_accuracy: 0.8789 - val_loss: 0.2916 - mcc: 0.7568\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8760 - loss: 0.2959\n","Epoch 6 - MCC: 0.7710\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8760 - loss: 0.2959 - val_accuracy: 0.8857 - val_loss: 0.2839 - mcc: 0.7710\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8797 - loss: 0.2900\n","Epoch 7 - MCC: 0.7786\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.8797 - loss: 0.2900 - val_accuracy: 0.8897 - val_loss: 0.2719 - mcc: 0.7786\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8791 - loss: 0.2894\n","Epoch 8 - MCC: 0.7844\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.8792 - loss: 0.2891 - val_accuracy: 0.8925 - val_loss: 0.2650 - mcc: 0.7844\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8857 - loss: 0.2763\n","Epoch 9 - MCC: 0.7840\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.8857 - loss: 0.2763 - val_accuracy: 0.8925 - val_loss: 0.2624 - mcc: 0.7840\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8876 - loss: 0.2727\n","Epoch 10 - MCC: 0.7845\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8876 - loss: 0.2728 - val_accuracy: 0.8926 - val_loss: 0.2589 - mcc: 0.7845\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8893 - loss: 0.2685\n","Epoch 11 - MCC: 0.7832\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.8892 - loss: 0.2689 - val_accuracy: 0.8911 - val_loss: 0.2614 - mcc: 0.7832\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8856 - loss: 0.2770\n","Epoch 12 - MCC: 0.7961\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.8858 - loss: 0.2767 - val_accuracy: 0.8984 - val_loss: 0.2500 - mcc: 0.7961\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8917 - loss: 0.2640\n","Epoch 13 - MCC: 0.7965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.8917 - loss: 0.2639 - val_accuracy: 0.8986 - val_loss: 0.2482 - mcc: 0.7965\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8925 - loss: 0.2593\n","Epoch 14 - MCC: 0.7965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.8925 - loss: 0.2594 - val_accuracy: 0.8985 - val_loss: 0.2481 - mcc: 0.7965\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8907 - loss: 0.2637\n","Epoch 15 - MCC: 0.8021\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8908 - loss: 0.2635 - val_accuracy: 0.9014 - val_loss: 0.2424 - mcc: 0.8021\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8960 - loss: 0.2535\n","Epoch 16 - MCC: 0.7998\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8960 - loss: 0.2536 - val_accuracy: 0.9003 - val_loss: 0.2425 - mcc: 0.7998\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8973 - loss: 0.2502\n","Epoch 17 - MCC: 0.7995\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8972 - loss: 0.2503 - val_accuracy: 0.8999 - val_loss: 0.2464 - mcc: 0.7995\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8943 - loss: 0.2550\n","Epoch 18 - MCC: 0.8056\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8943 - loss: 0.2550 - val_accuracy: 0.9032 - val_loss: 0.2380 - mcc: 0.8056\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9038 - loss: 0.2363\n","Epoch 19 - MCC: 0.8046\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9035 - loss: 0.2369 - val_accuracy: 0.9027 - val_loss: 0.2388 - mcc: 0.8046\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8939 - loss: 0.2558\n","Epoch 20 - MCC: 0.8057\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.8940 - loss: 0.2555 - val_accuracy: 0.9032 - val_loss: 0.2366 - mcc: 0.8057\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9035 - loss: 0.2355\n","Epoch 21 - MCC: 0.8024\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9031 - loss: 0.2361 - val_accuracy: 0.9015 - val_loss: 0.2449 - mcc: 0.8024\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8941 - loss: 0.2542\n","Epoch 22 - MCC: 0.8085\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8942 - loss: 0.2540 - val_accuracy: 0.9046 - val_loss: 0.2343 - mcc: 0.8085\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8986 - loss: 0.2454\n","Epoch 23 - MCC: 0.8097\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8986 - loss: 0.2454 - val_accuracy: 0.9050 - val_loss: 0.2335 - mcc: 0.8097\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8908 - loss: 0.2597\n","Epoch 24 - MCC: 0.8108\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.8911 - loss: 0.2591 - val_accuracy: 0.9058 - val_loss: 0.2290 - mcc: 0.8108\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9037 - loss: 0.2350\n","Epoch 25 - MCC: 0.8082\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9035 - loss: 0.2352 - val_accuracy: 0.9044 - val_loss: 0.2317 - mcc: 0.8082\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9026 - loss: 0.2379\n","Epoch 26 - MCC: 0.8058\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.9025 - loss: 0.2381 - val_accuracy: 0.9032 - val_loss: 0.2362 - mcc: 0.8058\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8981 - loss: 0.2456\n","Epoch 27 - MCC: 0.8112\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.8982 - loss: 0.2455 - val_accuracy: 0.9059 - val_loss: 0.2288 - mcc: 0.8112\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9019 - loss: 0.2376\n","Epoch 28 - MCC: 0.8131\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9018 - loss: 0.2376 - val_accuracy: 0.9068 - val_loss: 0.2263 - mcc: 0.8131\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9039 - loss: 0.2331\n","Epoch 29 - MCC: 0.8130\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9039 - loss: 0.2332 - val_accuracy: 0.9068 - val_loss: 0.2247 - mcc: 0.8130\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8988 - loss: 0.2448\n","Epoch 30 - MCC: 0.8094\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.8990 - loss: 0.2444 - val_accuracy: 0.9051 - val_loss: 0.2300 - mcc: 0.8094\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9071 - loss: 0.2285\n","Epoch 31 - MCC: 0.8153\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9069 - loss: 0.2288 - val_accuracy: 0.9080 - val_loss: 0.2236 - mcc: 0.8153\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9034 - loss: 0.2339\n","Epoch 32 - MCC: 0.8089\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9033 - loss: 0.2340 - val_accuracy: 0.9047 - val_loss: 0.2297 - mcc: 0.8089\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9052 - loss: 0.2302\n","Epoch 33 - MCC: 0.8118\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9051 - loss: 0.2305 - val_accuracy: 0.9062 - val_loss: 0.2265 - mcc: 0.8118\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9007 - loss: 0.2376\n","Epoch 34 - MCC: 0.8161\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9007 - loss: 0.2375 - val_accuracy: 0.9083 - val_loss: 0.2239 - mcc: 0.8161\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8974 - loss: 0.2436\n","Epoch 35 - MCC: 0.8181\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.8976 - loss: 0.2432 - val_accuracy: 0.9094 - val_loss: 0.2201 - mcc: 0.8181\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9029 - loss: 0.2333\n","Epoch 36 - MCC: 0.8203\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9029 - loss: 0.2333 - val_accuracy: 0.9105 - val_loss: 0.2197 - mcc: 0.8203\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9033 - loss: 0.2336\n","Epoch 37 - MCC: 0.8190\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9033 - loss: 0.2334 - val_accuracy: 0.9098 - val_loss: 0.2179 - mcc: 0.8190\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9077 - loss: 0.2237\n","Epoch 38 - MCC: 0.8182\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9076 - loss: 0.2239 - val_accuracy: 0.9095 - val_loss: 0.2200 - mcc: 0.8182\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9135 - loss: 0.2133\n","Epoch 39 - MCC: 0.8236\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9132 - loss: 0.2139 - val_accuracy: 0.9121 - val_loss: 0.2145 - mcc: 0.8236\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9062 - loss: 0.2239\n","Epoch 40 - MCC: 0.8199\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9062 - loss: 0.2240 - val_accuracy: 0.9103 - val_loss: 0.2181 - mcc: 0.8199\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9006 - loss: 0.2373\n","Epoch 41 - MCC: 0.8204\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9008 - loss: 0.2369 - val_accuracy: 0.9105 - val_loss: 0.2182 - mcc: 0.8204\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9060 - loss: 0.2276\n","Epoch 42 - MCC: 0.8153\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9060 - loss: 0.2275 - val_accuracy: 0.9078 - val_loss: 0.2214 - mcc: 0.8153\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9001 - loss: 0.2376\n","Epoch 43 - MCC: 0.8224\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.9004 - loss: 0.2371 - val_accuracy: 0.9115 - val_loss: 0.2153 - mcc: 0.8224\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9000 - loss: 0.2382\n","Epoch 44 - MCC: 0.8217\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.9003 - loss: 0.2377 - val_accuracy: 0.9110 - val_loss: 0.2162 - mcc: 0.8217\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9043 - loss: 0.2292\n","Epoch 45 - MCC: 0.8239\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9044 - loss: 0.2289 - val_accuracy: 0.9123 - val_loss: 0.2152 - mcc: 0.8239\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9112 - loss: 0.2146\n","Epoch 46 - MCC: 0.8247\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9111 - loss: 0.2149 - val_accuracy: 0.9126 - val_loss: 0.2115 - mcc: 0.8247\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9060 - loss: 0.2247\n","Epoch 47 - MCC: 0.8220\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9061 - loss: 0.2246 - val_accuracy: 0.9113 - val_loss: 0.2161 - mcc: 0.8220\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9116 - loss: 0.2147\n","Epoch 48 - MCC: 0.8283\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9115 - loss: 0.2150 - val_accuracy: 0.9145 - val_loss: 0.2100 - mcc: 0.8283\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9037 - loss: 0.2299\n","Epoch 49 - MCC: 0.8279\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9039 - loss: 0.2295 - val_accuracy: 0.9143 - val_loss: 0.2097 - mcc: 0.8279\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9045 - loss: 0.2288\n","Epoch 50 - MCC: 0.8268\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9046 - loss: 0.2287 - val_accuracy: 0.9137 - val_loss: 0.2130 - mcc: 0.8268\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 2\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5957 - loss: 0.6831\n","Epoch 1 - MCC: 0.4020\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - accuracy: 0.5978 - loss: 0.6824 - val_accuracy: 0.7019 - val_loss: 0.6112 - mcc: 0.4020\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7443 - loss: 0.5555\n","Epoch 2 - MCC: 0.6079\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.7453 - loss: 0.5538 - val_accuracy: 0.8045 - val_loss: 0.4479 - mcc: 0.6079\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8291 - loss: 0.3989\n","Epoch 3 - MCC: 0.6678\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.8292 - loss: 0.3985 - val_accuracy: 0.8342 - val_loss: 0.3771 - mcc: 0.6678\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8536 - loss: 0.3416\n","Epoch 4 - MCC: 0.7138\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.8538 - loss: 0.3412 - val_accuracy: 0.8571 - val_loss: 0.3362 - mcc: 0.7138\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8749 - loss: 0.3006\n","Epoch 5 - MCC: 0.7319\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.8748 - loss: 0.3007 - val_accuracy: 0.8662 - val_loss: 0.3168 - mcc: 0.7319\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8749 - loss: 0.2968\n","Epoch 6 - MCC: 0.7322\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.8751 - loss: 0.2966 - val_accuracy: 0.8663 - val_loss: 0.3168 - mcc: 0.7322\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8811 - loss: 0.2873\n","Epoch 7 - MCC: 0.7365\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.8811 - loss: 0.2872 - val_accuracy: 0.8681 - val_loss: 0.3111 - mcc: 0.7365\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8811 - loss: 0.2873\n","Epoch 8 - MCC: 0.7460\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.8812 - loss: 0.2870 - val_accuracy: 0.8733 - val_loss: 0.2993 - mcc: 0.7460\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8844 - loss: 0.2780\n","Epoch 9 - MCC: 0.7486\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8846 - loss: 0.2777 - val_accuracy: 0.8746 - val_loss: 0.2962 - mcc: 0.7486\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8889 - loss: 0.2676\n","Epoch 10 - MCC: 0.7513\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8889 - loss: 0.2676 - val_accuracy: 0.8753 - val_loss: 0.2950 - mcc: 0.7513\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8884 - loss: 0.2705\n","Epoch 11 - MCC: 0.7540\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.8885 - loss: 0.2703 - val_accuracy: 0.8773 - val_loss: 0.2899 - mcc: 0.7540\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8930 - loss: 0.2603\n","Epoch 12 - MCC: 0.7571\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8929 - loss: 0.2604 - val_accuracy: 0.8788 - val_loss: 0.2856 - mcc: 0.7571\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8927 - loss: 0.2580\n","Epoch 13 - MCC: 0.7608\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.8928 - loss: 0.2579 - val_accuracy: 0.8806 - val_loss: 0.2814 - mcc: 0.7608\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8929 - loss: 0.2603\n","Epoch 14 - MCC: 0.7611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.8931 - loss: 0.2599 - val_accuracy: 0.8800 - val_loss: 0.2827 - mcc: 0.7611\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8935 - loss: 0.2549\n","Epoch 15 - MCC: 0.7639\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8937 - loss: 0.2547 - val_accuracy: 0.8823 - val_loss: 0.2763 - mcc: 0.7639\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9029 - loss: 0.2375\n","Epoch 16 - MCC: 0.7674\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9027 - loss: 0.2379 - val_accuracy: 0.8838 - val_loss: 0.2739 - mcc: 0.7674\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8958 - loss: 0.2507\n","Epoch 17 - MCC: 0.7680\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.8960 - loss: 0.2504 - val_accuracy: 0.8842 - val_loss: 0.2729 - mcc: 0.7680\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9047 - loss: 0.2336\n","Epoch 18 - MCC: 0.7696\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9045 - loss: 0.2339 - val_accuracy: 0.8851 - val_loss: 0.2713 - mcc: 0.7696\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8953 - loss: 0.2517\n","Epoch 19 - MCC: 0.7742\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.8955 - loss: 0.2513 - val_accuracy: 0.8874 - val_loss: 0.2658 - mcc: 0.7742\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9028 - loss: 0.2372\n","Epoch 20 - MCC: 0.7750\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9028 - loss: 0.2372 - val_accuracy: 0.8876 - val_loss: 0.2659 - mcc: 0.7750\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9064 - loss: 0.2298\n","Epoch 21 - MCC: 0.7770\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9063 - loss: 0.2300 - val_accuracy: 0.8884 - val_loss: 0.2681 - mcc: 0.7770\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9061 - loss: 0.2294\n","Epoch 22 - MCC: 0.7765\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9059 - loss: 0.2297 - val_accuracy: 0.8881 - val_loss: 0.2654 - mcc: 0.7765\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9003 - loss: 0.2412\n","Epoch 23 - MCC: 0.7825\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9004 - loss: 0.2410 - val_accuracy: 0.8912 - val_loss: 0.2583 - mcc: 0.7825\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9106 - loss: 0.2200\n","Epoch 24 - MCC: 0.7819\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9104 - loss: 0.2203 - val_accuracy: 0.8911 - val_loss: 0.2570 - mcc: 0.7819\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9063 - loss: 0.2277\n","Epoch 25 - MCC: 0.7804\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9062 - loss: 0.2278 - val_accuracy: 0.8898 - val_loss: 0.2605 - mcc: 0.7804\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9002 - loss: 0.2385\n","Epoch 26 - MCC: 0.7818\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9004 - loss: 0.2381 - val_accuracy: 0.8902 - val_loss: 0.2620 - mcc: 0.7818\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9044 - loss: 0.2319\n","Epoch 27 - MCC: 0.7868\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9045 - loss: 0.2317 - val_accuracy: 0.8935 - val_loss: 0.2549 - mcc: 0.7868\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9134 - loss: 0.2136\n","Epoch 28 - MCC: 0.7883\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9132 - loss: 0.2140 - val_accuracy: 0.8944 - val_loss: 0.2509 - mcc: 0.7883\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9122 - loss: 0.2159\n","Epoch 29 - MCC: 0.7800\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9120 - loss: 0.2162 - val_accuracy: 0.8902 - val_loss: 0.2597 - mcc: 0.7800\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9087 - loss: 0.2244\n","Epoch 30 - MCC: 0.7881\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9086 - loss: 0.2244 - val_accuracy: 0.8943 - val_loss: 0.2492 - mcc: 0.7881\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9079 - loss: 0.2220\n","Epoch 31 - MCC: 0.7823\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9080 - loss: 0.2219 - val_accuracy: 0.8904 - val_loss: 0.2579 - mcc: 0.7823\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9087 - loss: 0.2214\n","Epoch 32 - MCC: 0.7866\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9087 - loss: 0.2214 - val_accuracy: 0.8936 - val_loss: 0.2504 - mcc: 0.7866\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8984 - loss: 0.2561\n","Epoch 33 - MCC: 0.7283\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.8980 - loss: 0.2574 - val_accuracy: 0.8640 - val_loss: 0.3249 - mcc: 0.7283\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8889 - loss: 0.2746\n","Epoch 34 - MCC: 0.7651\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.8890 - loss: 0.2744 - val_accuracy: 0.8827 - val_loss: 0.2811 - mcc: 0.7651\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8998 - loss: 0.2463\n","Epoch 35 - MCC: 0.7736\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8998 - loss: 0.2463 - val_accuracy: 0.8865 - val_loss: 0.2727 - mcc: 0.7736\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8972 - loss: 0.2519\n","Epoch 36 - MCC: 0.7803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8975 - loss: 0.2513 - val_accuracy: 0.8903 - val_loss: 0.2639 - mcc: 0.7803\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9077 - loss: 0.2299\n","Epoch 37 - MCC: 0.7786\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9075 - loss: 0.2302 - val_accuracy: 0.8895 - val_loss: 0.2661 - mcc: 0.7786\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9052 - loss: 0.2333\n","Epoch 38 - MCC: 0.7838\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9052 - loss: 0.2333 - val_accuracy: 0.8921 - val_loss: 0.2589 - mcc: 0.7838\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9029 - loss: 0.2356\n","Epoch 39 - MCC: 0.7837\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9030 - loss: 0.2354 - val_accuracy: 0.8919 - val_loss: 0.2594 - mcc: 0.7837\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8968 - loss: 0.2481\n","Epoch 40 - MCC: 0.7835\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.8972 - loss: 0.2474 - val_accuracy: 0.8918 - val_loss: 0.2591 - mcc: 0.7835\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9072 - loss: 0.2265\n","Epoch 41 - MCC: 0.7826\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9072 - loss: 0.2265 - val_accuracy: 0.8915 - val_loss: 0.2586 - mcc: 0.7826\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9059 - loss: 0.2298\n","Epoch 42 - MCC: 0.7877\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9059 - loss: 0.2298 - val_accuracy: 0.8941 - val_loss: 0.2550 - mcc: 0.7877\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9122 - loss: 0.2178\n","Epoch 43 - MCC: 0.7874\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9120 - loss: 0.2181 - val_accuracy: 0.8938 - val_loss: 0.2537 - mcc: 0.7874\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9111 - loss: 0.2163\n","Epoch 44 - MCC: 0.7850\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9110 - loss: 0.2165 - val_accuracy: 0.8927 - val_loss: 0.2570 - mcc: 0.7850\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9087 - loss: 0.2231\n","Epoch 45 - MCC: 0.7882\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9087 - loss: 0.2231 - val_accuracy: 0.8941 - val_loss: 0.2543 - mcc: 0.7882\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9132 - loss: 0.2131\n","Epoch 46 - MCC: 0.7863\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9130 - loss: 0.2135 - val_accuracy: 0.8932 - val_loss: 0.2568 - mcc: 0.7863\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9111 - loss: 0.2163\n","Epoch 47 - MCC: 0.7873\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9110 - loss: 0.2166 - val_accuracy: 0.8936 - val_loss: 0.2535 - mcc: 0.7873\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9097 - loss: 0.2188\n","Epoch 48 - MCC: 0.7902\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9097 - loss: 0.2189 - val_accuracy: 0.8953 - val_loss: 0.2507 - mcc: 0.7902\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9130 - loss: 0.2139\n","Epoch 49 - MCC: 0.7885\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9129 - loss: 0.2141 - val_accuracy: 0.8943 - val_loss: 0.2511 - mcc: 0.7885\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9110 - loss: 0.2162\n","Epoch 50 - MCC: 0.7881\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9110 - loss: 0.2163 - val_accuracy: 0.8943 - val_loss: 0.2513 - mcc: 0.7881\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 3\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5916 - loss: 0.6817\n","Epoch 1 - MCC: 0.4011\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - accuracy: 0.5933 - loss: 0.6809 - val_accuracy: 0.7017 - val_loss: 0.5905 - mcc: 0.4011\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7428 - loss: 0.5360\n","Epoch 2 - MCC: 0.6301\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.7440 - loss: 0.5342 - val_accuracy: 0.8142 - val_loss: 0.4215 - mcc: 0.6301\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8217 - loss: 0.4031\n","Epoch 3 - MCC: 0.7026\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8220 - loss: 0.4025 - val_accuracy: 0.8520 - val_loss: 0.3386 - mcc: 0.7026\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8577 - loss: 0.3353\n","Epoch 4 - MCC: 0.7318\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8576 - loss: 0.3354 - val_accuracy: 0.8659 - val_loss: 0.3110 - mcc: 0.7318\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8597 - loss: 0.3290\n","Epoch 5 - MCC: 0.7509\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8599 - loss: 0.3285 - val_accuracy: 0.8755 - val_loss: 0.2975 - mcc: 0.7509\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8688 - loss: 0.3109\n","Epoch 6 - MCC: 0.7626\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.8689 - loss: 0.3106 - val_accuracy: 0.8816 - val_loss: 0.2826 - mcc: 0.7626\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8750 - loss: 0.2994\n","Epoch 7 - MCC: 0.7725\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8751 - loss: 0.2993 - val_accuracy: 0.8867 - val_loss: 0.2728 - mcc: 0.7725\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8766 - loss: 0.2943\n","Epoch 8 - MCC: 0.7800\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8767 - loss: 0.2941 - val_accuracy: 0.8905 - val_loss: 0.2657 - mcc: 0.7800\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8843 - loss: 0.2823\n","Epoch 9 - MCC: 0.7858\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 245ms/step - accuracy: 0.8842 - loss: 0.2823 - val_accuracy: 0.8934 - val_loss: 0.2596 - mcc: 0.7858\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8887 - loss: 0.2715\n","Epoch 10 - MCC: 0.7871\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 91ms/step - accuracy: 0.8886 - loss: 0.2717 - val_accuracy: 0.8940 - val_loss: 0.2574 - mcc: 0.7871\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8838 - loss: 0.2801\n","Epoch 11 - MCC: 0.7904\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8839 - loss: 0.2799 - val_accuracy: 0.8955 - val_loss: 0.2547 - mcc: 0.7904\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8804 - loss: 0.2879\n","Epoch 12 - MCC: 0.7915\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.8807 - loss: 0.2873 - val_accuracy: 0.8962 - val_loss: 0.2534 - mcc: 0.7915\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8831 - loss: 0.2835\n","Epoch 13 - MCC: 0.7920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.8833 - loss: 0.2830 - val_accuracy: 0.8964 - val_loss: 0.2521 - mcc: 0.7920\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8889 - loss: 0.2697\n","Epoch 14 - MCC: 0.7953\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8889 - loss: 0.2696 - val_accuracy: 0.8981 - val_loss: 0.2476 - mcc: 0.7953\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8914 - loss: 0.2611\n","Epoch 15 - MCC: 0.7981\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.8913 - loss: 0.2613 - val_accuracy: 0.8995 - val_loss: 0.2472 - mcc: 0.7981\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8855 - loss: 0.2748\n","Epoch 16 - MCC: 0.7969\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.8858 - loss: 0.2745 - val_accuracy: 0.8989 - val_loss: 0.2472 - mcc: 0.7969\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8932 - loss: 0.2603\n","Epoch 17 - MCC: 0.7981\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.8931 - loss: 0.2603 - val_accuracy: 0.8995 - val_loss: 0.2426 - mcc: 0.7981\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.8951 - loss: 0.2548\n","Epoch 18 - MCC: 0.8033\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.8950 - loss: 0.2550 - val_accuracy: 0.9021 - val_loss: 0.2409 - mcc: 0.8033\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8941 - loss: 0.2568\n","Epoch 19 - MCC: 0.8015\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.8941 - loss: 0.2567 - val_accuracy: 0.9012 - val_loss: 0.2399 - mcc: 0.8015\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8985 - loss: 0.2469\n","Epoch 20 - MCC: 0.8040\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.8984 - loss: 0.2472 - val_accuracy: 0.9024 - val_loss: 0.2388 - mcc: 0.8040\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8891 - loss: 0.2652\n","Epoch 21 - MCC: 0.8079\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.8894 - loss: 0.2647 - val_accuracy: 0.9043 - val_loss: 0.2348 - mcc: 0.8079\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8995 - loss: 0.2446\n","Epoch 22 - MCC: 0.7997\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.8994 - loss: 0.2449 - val_accuracy: 0.9003 - val_loss: 0.2463 - mcc: 0.7997\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8988 - loss: 0.2458\n","Epoch 23 - MCC: 0.8075\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8987 - loss: 0.2460 - val_accuracy: 0.9042 - val_loss: 0.2337 - mcc: 0.8075\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9008 - loss: 0.2405\n","Epoch 24 - MCC: 0.8130\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9006 - loss: 0.2407 - val_accuracy: 0.9069 - val_loss: 0.2282 - mcc: 0.8130\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8960 - loss: 0.2493\n","Epoch 25 - MCC: 0.8096\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.8961 - loss: 0.2491 - val_accuracy: 0.9052 - val_loss: 0.2307 - mcc: 0.8096\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8951 - loss: 0.2503\n","Epoch 26 - MCC: 0.8112\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.8952 - loss: 0.2501 - val_accuracy: 0.9060 - val_loss: 0.2282 - mcc: 0.8112\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8973 - loss: 0.2463\n","Epoch 27 - MCC: 0.8128\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8974 - loss: 0.2461 - val_accuracy: 0.9068 - val_loss: 0.2259 - mcc: 0.8128\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8966 - loss: 0.2482\n","Epoch 28 - MCC: 0.8117\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8967 - loss: 0.2479 - val_accuracy: 0.9060 - val_loss: 0.2272 - mcc: 0.8117\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9015 - loss: 0.2352\n","Epoch 29 - MCC: 0.8148\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9015 - loss: 0.2354 - val_accuracy: 0.9078 - val_loss: 0.2230 - mcc: 0.8148\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8947 - loss: 0.2515\n","Epoch 30 - MCC: 0.8128\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8950 - loss: 0.2509 - val_accuracy: 0.9068 - val_loss: 0.2237 - mcc: 0.8128\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8957 - loss: 0.2484\n","Epoch 31 - MCC: 0.8154\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8959 - loss: 0.2480 - val_accuracy: 0.9081 - val_loss: 0.2226 - mcc: 0.8154\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9031 - loss: 0.2335\n","Epoch 32 - MCC: 0.8175\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9030 - loss: 0.2336 - val_accuracy: 0.9091 - val_loss: 0.2199 - mcc: 0.8175\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9016 - loss: 0.2361\n","Epoch 33 - MCC: 0.8181\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9016 - loss: 0.2361 - val_accuracy: 0.9093 - val_loss: 0.2199 - mcc: 0.8181\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9001 - loss: 0.2390\n","Epoch 34 - MCC: 0.8126\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9002 - loss: 0.2387 - val_accuracy: 0.9067 - val_loss: 0.2242 - mcc: 0.8126\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8998 - loss: 0.2393\n","Epoch 35 - MCC: 0.8096\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.8999 - loss: 0.2392 - val_accuracy: 0.9052 - val_loss: 0.2284 - mcc: 0.8096\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9103 - loss: 0.2194\n","Epoch 36 - MCC: 0.8156\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9101 - loss: 0.2199 - val_accuracy: 0.9079 - val_loss: 0.2220 - mcc: 0.8156\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9064 - loss: 0.2262\n","Epoch 37 - MCC: 0.8173\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9063 - loss: 0.2263 - val_accuracy: 0.9088 - val_loss: 0.2197 - mcc: 0.8173\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9069 - loss: 0.2253\n","Epoch 38 - MCC: 0.8153\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9067 - loss: 0.2257 - val_accuracy: 0.9080 - val_loss: 0.2226 - mcc: 0.8153\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9016 - loss: 0.2367\n","Epoch 39 - MCC: 0.8192\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9016 - loss: 0.2366 - val_accuracy: 0.9100 - val_loss: 0.2171 - mcc: 0.8192\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9092 - loss: 0.2189\n","Epoch 40 - MCC: 0.8174\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9090 - loss: 0.2192 - val_accuracy: 0.9091 - val_loss: 0.2186 - mcc: 0.8174\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9057 - loss: 0.2252\n","Epoch 41 - MCC: 0.8217\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9056 - loss: 0.2253 - val_accuracy: 0.9112 - val_loss: 0.2142 - mcc: 0.8217\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9093 - loss: 0.2191\n","Epoch 42 - MCC: 0.8212\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9092 - loss: 0.2194 - val_accuracy: 0.9109 - val_loss: 0.2143 - mcc: 0.8212\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9070 - loss: 0.2228\n","Epoch 43 - MCC: 0.8181\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9070 - loss: 0.2229 - val_accuracy: 0.9092 - val_loss: 0.2179 - mcc: 0.8181\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9034 - loss: 0.2305\n","Epoch 44 - MCC: 0.8207\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9035 - loss: 0.2303 - val_accuracy: 0.9107 - val_loss: 0.2148 - mcc: 0.8207\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9142 - loss: 0.2093\n","Epoch 45 - MCC: 0.8188\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9139 - loss: 0.2100 - val_accuracy: 0.9098 - val_loss: 0.2156 - mcc: 0.8188\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9021 - loss: 0.2325\n","Epoch 46 - MCC: 0.8256\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9023 - loss: 0.2322 - val_accuracy: 0.9132 - val_loss: 0.2098 - mcc: 0.8256\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9041 - loss: 0.2295\n","Epoch 47 - MCC: 0.8236\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9041 - loss: 0.2295 - val_accuracy: 0.9122 - val_loss: 0.2108 - mcc: 0.8236\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9058 - loss: 0.2269\n","Epoch 48 - MCC: 0.8209\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9059 - loss: 0.2268 - val_accuracy: 0.9106 - val_loss: 0.2155 - mcc: 0.8209\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9095 - loss: 0.2180\n","Epoch 49 - MCC: 0.8250\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9094 - loss: 0.2181 - val_accuracy: 0.9128 - val_loss: 0.2110 - mcc: 0.8250\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9116 - loss: 0.2117\n","Epoch 50 - MCC: 0.8252\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9115 - loss: 0.2121 - val_accuracy: 0.9129 - val_loss: 0.2107 - mcc: 0.8252\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 4\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5738 - loss: 0.6833\n","Epoch 1 - MCC: 0.4430\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 116ms/step - accuracy: 0.5757 - loss: 0.6828 - val_accuracy: 0.7209 - val_loss: 0.6117 - mcc: 0.4430\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7499 - loss: 0.5511\n","Epoch 2 - MCC: 0.6575\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.7510 - loss: 0.5489 - val_accuracy: 0.8291 - val_loss: 0.3873 - mcc: 0.6575\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8307 - loss: 0.3793\n","Epoch 3 - MCC: 0.7143\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.8310 - loss: 0.3788 - val_accuracy: 0.8575 - val_loss: 0.3386 - mcc: 0.7143\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8628 - loss: 0.3265\n","Epoch 4 - MCC: 0.7363\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8628 - loss: 0.3265 - val_accuracy: 0.8682 - val_loss: 0.3151 - mcc: 0.7363\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8657 - loss: 0.3199\n","Epoch 5 - MCC: 0.7524\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.8660 - loss: 0.3194 - val_accuracy: 0.8760 - val_loss: 0.3009 - mcc: 0.7524\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8771 - loss: 0.2940\n","Epoch 6 - MCC: 0.7652\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.8772 - loss: 0.2938 - val_accuracy: 0.8829 - val_loss: 0.2841 - mcc: 0.7652\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8805 - loss: 0.2875\n","Epoch 7 - MCC: 0.7758\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8806 - loss: 0.2874 - val_accuracy: 0.8881 - val_loss: 0.2799 - mcc: 0.7758\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8848 - loss: 0.2781\n","Epoch 8 - MCC: 0.7759\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.8847 - loss: 0.2782 - val_accuracy: 0.8882 - val_loss: 0.2749 - mcc: 0.7759\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8848 - loss: 0.2794\n","Epoch 9 - MCC: 0.7802\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.8849 - loss: 0.2791 - val_accuracy: 0.8903 - val_loss: 0.2716 - mcc: 0.7802\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8864 - loss: 0.2752\n","Epoch 10 - MCC: 0.7774\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8865 - loss: 0.2750 - val_accuracy: 0.8888 - val_loss: 0.2731 - mcc: 0.7774\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8881 - loss: 0.2704\n","Epoch 11 - MCC: 0.7860\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.8881 - loss: 0.2704 - val_accuracy: 0.8933 - val_loss: 0.2658 - mcc: 0.7860\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8952 - loss: 0.2580\n","Epoch 12 - MCC: 0.7855\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.8951 - loss: 0.2583 - val_accuracy: 0.8930 - val_loss: 0.2649 - mcc: 0.7855\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8918 - loss: 0.2630\n","Epoch 13 - MCC: 0.7911\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.8918 - loss: 0.2630 - val_accuracy: 0.8955 - val_loss: 0.2598 - mcc: 0.7911\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8939 - loss: 0.2585\n","Epoch 14 - MCC: 0.7940\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.8939 - loss: 0.2585 - val_accuracy: 0.8972 - val_loss: 0.2539 - mcc: 0.7940\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8902 - loss: 0.2643\n","Epoch 15 - MCC: 0.7913\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8903 - loss: 0.2641 - val_accuracy: 0.8957 - val_loss: 0.2627 - mcc: 0.7913\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8908 - loss: 0.2651\n","Epoch 16 - MCC: 0.7958\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.8910 - loss: 0.2646 - val_accuracy: 0.8981 - val_loss: 0.2514 - mcc: 0.7958\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9023 - loss: 0.2404\n","Epoch 17 - MCC: 0.7990\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9021 - loss: 0.2408 - val_accuracy: 0.8997 - val_loss: 0.2499 - mcc: 0.7990\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9013 - loss: 0.2424\n","Epoch 18 - MCC: 0.7984\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9012 - loss: 0.2426 - val_accuracy: 0.8995 - val_loss: 0.2482 - mcc: 0.7984\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8955 - loss: 0.2546\n","Epoch 19 - MCC: 0.8003\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - accuracy: 0.8956 - loss: 0.2543 - val_accuracy: 0.9003 - val_loss: 0.2432 - mcc: 0.8003\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8979 - loss: 0.2481\n","Epoch 20 - MCC: 0.8039\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8979 - loss: 0.2481 - val_accuracy: 0.9022 - val_loss: 0.2397 - mcc: 0.8039\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9002 - loss: 0.2433\n","Epoch 21 - MCC: 0.8053\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9002 - loss: 0.2433 - val_accuracy: 0.9029 - val_loss: 0.2389 - mcc: 0.8053\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9048 - loss: 0.2340\n","Epoch 22 - MCC: 0.8079\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9047 - loss: 0.2343 - val_accuracy: 0.9041 - val_loss: 0.2386 - mcc: 0.8079\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9039 - loss: 0.2361\n","Epoch 23 - MCC: 0.8072\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9038 - loss: 0.2362 - val_accuracy: 0.9032 - val_loss: 0.2404 - mcc: 0.8072\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9009 - loss: 0.2417\n","Epoch 24 - MCC: 0.8091\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9010 - loss: 0.2416 - val_accuracy: 0.9043 - val_loss: 0.2360 - mcc: 0.8091\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9086 - loss: 0.2258\n","Epoch 25 - MCC: 0.8085\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9084 - loss: 0.2262 - val_accuracy: 0.9044 - val_loss: 0.2343 - mcc: 0.8085\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9048 - loss: 0.2330\n","Epoch 26 - MCC: 0.8085\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9047 - loss: 0.2332 - val_accuracy: 0.9044 - val_loss: 0.2348 - mcc: 0.8085\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8982 - loss: 0.2461\n","Epoch 27 - MCC: 0.8130\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8984 - loss: 0.2458 - val_accuracy: 0.9067 - val_loss: 0.2312 - mcc: 0.8130\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9004 - loss: 0.2397\n","Epoch 28 - MCC: 0.8156\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9006 - loss: 0.2395 - val_accuracy: 0.9080 - val_loss: 0.2289 - mcc: 0.8156\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9029 - loss: 0.2385\n","Epoch 29 - MCC: 0.8052\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9028 - loss: 0.2386 - val_accuracy: 0.9027 - val_loss: 0.2403 - mcc: 0.8052\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9035 - loss: 0.2364\n","Epoch 30 - MCC: 0.8105\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9034 - loss: 0.2364 - val_accuracy: 0.9051 - val_loss: 0.2365 - mcc: 0.8105\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9049 - loss: 0.2328\n","Epoch 31 - MCC: 0.8154\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9050 - loss: 0.2327 - val_accuracy: 0.9077 - val_loss: 0.2292 - mcc: 0.8154\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9045 - loss: 0.2345\n","Epoch 32 - MCC: 0.8163\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9046 - loss: 0.2344 - val_accuracy: 0.9083 - val_loss: 0.2274 - mcc: 0.8163\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9088 - loss: 0.2250\n","Epoch 33 - MCC: 0.8149\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9087 - loss: 0.2252 - val_accuracy: 0.9074 - val_loss: 0.2286 - mcc: 0.8149\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9082 - loss: 0.2246\n","Epoch 34 - MCC: 0.8164\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9082 - loss: 0.2248 - val_accuracy: 0.9083 - val_loss: 0.2279 - mcc: 0.8164\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8991 - loss: 0.2423\n","Epoch 35 - MCC: 0.8095\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.8993 - loss: 0.2420 - val_accuracy: 0.9050 - val_loss: 0.2333 - mcc: 0.8095\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9094 - loss: 0.2224\n","Epoch 36 - MCC: 0.8123\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9092 - loss: 0.2227 - val_accuracy: 0.9064 - val_loss: 0.2293 - mcc: 0.8123\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9051 - loss: 0.2307\n","Epoch 37 - MCC: 0.8070\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9052 - loss: 0.2306 - val_accuracy: 0.9037 - val_loss: 0.2342 - mcc: 0.8070\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9059 - loss: 0.2285\n","Epoch 38 - MCC: 0.8175\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9059 - loss: 0.2285 - val_accuracy: 0.9090 - val_loss: 0.2254 - mcc: 0.8175\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9042 - loss: 0.2317\n","Epoch 39 - MCC: 0.8134\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - accuracy: 0.9042 - loss: 0.2316 - val_accuracy: 0.9069 - val_loss: 0.2293 - mcc: 0.8134\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9099 - loss: 0.2214\n","Epoch 40 - MCC: 0.8157\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9099 - loss: 0.2215 - val_accuracy: 0.9080 - val_loss: 0.2263 - mcc: 0.8157\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9081 - loss: 0.2248\n","Epoch 41 - MCC: 0.8130\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9081 - loss: 0.2249 - val_accuracy: 0.9067 - val_loss: 0.2278 - mcc: 0.8130\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9099 - loss: 0.2210\n","Epoch 42 - MCC: 0.8112\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9099 - loss: 0.2212 - val_accuracy: 0.9058 - val_loss: 0.2283 - mcc: 0.8112\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9053 - loss: 0.2293\n","Epoch 43 - MCC: 0.8131\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9054 - loss: 0.2291 - val_accuracy: 0.9067 - val_loss: 0.2275 - mcc: 0.8131\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9114 - loss: 0.2175\n","Epoch 44 - MCC: 0.8164\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9114 - loss: 0.2176 - val_accuracy: 0.9084 - val_loss: 0.2240 - mcc: 0.8164\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9119 - loss: 0.2167\n","Epoch 45 - MCC: 0.8159\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9118 - loss: 0.2170 - val_accuracy: 0.9082 - val_loss: 0.2238 - mcc: 0.8159\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9099 - loss: 0.2221\n","Epoch 46 - MCC: 0.8186\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9099 - loss: 0.2222 - val_accuracy: 0.9094 - val_loss: 0.2247 - mcc: 0.8186\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9091 - loss: 0.2233\n","Epoch 47 - MCC: 0.8165\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9091 - loss: 0.2233 - val_accuracy: 0.9084 - val_loss: 0.2233 - mcc: 0.8165\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9052 - loss: 0.2304\n","Epoch 48 - MCC: 0.8160\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9053 - loss: 0.2302 - val_accuracy: 0.9080 - val_loss: 0.2252 - mcc: 0.8160\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9108 - loss: 0.2193\n","Epoch 49 - MCC: 0.8167\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9107 - loss: 0.2193 - val_accuracy: 0.9085 - val_loss: 0.2239 - mcc: 0.8167\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9094 - loss: 0.2213\n","Epoch 50 - MCC: 0.8146\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9094 - loss: 0.2213 - val_accuracy: 0.9075 - val_loss: 0.2250 - mcc: 0.8146\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: LSTM_Deep, Fold: 5\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.6225 - loss: 0.6853\n","Epoch 1 - MCC: 0.5178\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - accuracy: 0.6251 - loss: 0.6849 - val_accuracy: 0.7586 - val_loss: 0.6209 - mcc: 0.5178\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7663 - loss: 0.5724\n","Epoch 2 - MCC: 0.6501\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.7669 - loss: 0.5703 - val_accuracy: 0.8254 - val_loss: 0.4059 - mcc: 0.6501\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8331 - loss: 0.3911\n","Epoch 3 - MCC: 0.7077\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8335 - loss: 0.3904 - val_accuracy: 0.8541 - val_loss: 0.3489 - mcc: 0.7077\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8503 - loss: 0.3504\n","Epoch 4 - MCC: 0.7112\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8507 - loss: 0.3497 - val_accuracy: 0.8539 - val_loss: 0.3443 - mcc: 0.7112\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8665 - loss: 0.3207\n","Epoch 5 - MCC: 0.7512\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8667 - loss: 0.3202 - val_accuracy: 0.8756 - val_loss: 0.2992 - mcc: 0.7512\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8764 - loss: 0.2978\n","Epoch 6 - MCC: 0.7624\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.8766 - loss: 0.2975 - val_accuracy: 0.8807 - val_loss: 0.2908 - mcc: 0.7624\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8831 - loss: 0.2854\n","Epoch 7 - MCC: 0.7683\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.8831 - loss: 0.2852 - val_accuracy: 0.8844 - val_loss: 0.2823 - mcc: 0.7683\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8867 - loss: 0.2763\n","Epoch 8 - MCC: 0.7634\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.8868 - loss: 0.2763 - val_accuracy: 0.8815 - val_loss: 0.2849 - mcc: 0.7634\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8827 - loss: 0.2827\n","Epoch 9 - MCC: 0.7777\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.8830 - loss: 0.2822 - val_accuracy: 0.8890 - val_loss: 0.2737 - mcc: 0.7777\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8941 - loss: 0.2612\n","Epoch 10 - MCC: 0.7782\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - accuracy: 0.8940 - loss: 0.2614 - val_accuracy: 0.8887 - val_loss: 0.2725 - mcc: 0.7782\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8920 - loss: 0.2643\n","Epoch 11 - MCC: 0.7783\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.8920 - loss: 0.2644 - val_accuracy: 0.8892 - val_loss: 0.2697 - mcc: 0.7783\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8902 - loss: 0.2690\n","Epoch 12 - MCC: 0.7784\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.8904 - loss: 0.2687 - val_accuracy: 0.8889 - val_loss: 0.2711 - mcc: 0.7784\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8961 - loss: 0.2562\n","Epoch 13 - MCC: 0.7822\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8961 - loss: 0.2563 - val_accuracy: 0.8912 - val_loss: 0.2660 - mcc: 0.7822\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8988 - loss: 0.2520\n","Epoch 14 - MCC: 0.7861\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.8987 - loss: 0.2522 - val_accuracy: 0.8931 - val_loss: 0.2626 - mcc: 0.7861\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8972 - loss: 0.2529\n","Epoch 15 - MCC: 0.7879\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8973 - loss: 0.2528 - val_accuracy: 0.8940 - val_loss: 0.2594 - mcc: 0.7879\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8932 - loss: 0.2606\n","Epoch 16 - MCC: 0.7899\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.8934 - loss: 0.2602 - val_accuracy: 0.8949 - val_loss: 0.2579 - mcc: 0.7899\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9005 - loss: 0.2445\n","Epoch 17 - MCC: 0.7872\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9004 - loss: 0.2447 - val_accuracy: 0.8938 - val_loss: 0.2577 - mcc: 0.7872\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9036 - loss: 0.2381\n","Epoch 18 - MCC: 0.7892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9035 - loss: 0.2384 - val_accuracy: 0.8946 - val_loss: 0.2564 - mcc: 0.7892\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9009 - loss: 0.2431\n","Epoch 19 - MCC: 0.7912\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9009 - loss: 0.2431 - val_accuracy: 0.8958 - val_loss: 0.2524 - mcc: 0.7912\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8988 - loss: 0.2469\n","Epoch 20 - MCC: 0.7958\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.8989 - loss: 0.2467 - val_accuracy: 0.8980 - val_loss: 0.2486 - mcc: 0.7958\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9010 - loss: 0.2433\n","Epoch 21 - MCC: 0.7968\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9010 - loss: 0.2431 - val_accuracy: 0.8986 - val_loss: 0.2476 - mcc: 0.7968\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9036 - loss: 0.2367\n","Epoch 22 - MCC: 0.7943\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9035 - loss: 0.2368 - val_accuracy: 0.8971 - val_loss: 0.2481 - mcc: 0.7943\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9075 - loss: 0.2282\n","Epoch 23 - MCC: 0.7950\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.9073 - loss: 0.2286 - val_accuracy: 0.8973 - val_loss: 0.2499 - mcc: 0.7950\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9033 - loss: 0.2351\n","Epoch 24 - MCC: 0.7989\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.9033 - loss: 0.2352 - val_accuracy: 0.8996 - val_loss: 0.2431 - mcc: 0.7989\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9044 - loss: 0.2334\n","Epoch 25 - MCC: 0.7996\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9043 - loss: 0.2335 - val_accuracy: 0.8996 - val_loss: 0.2452 - mcc: 0.7996\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9045 - loss: 0.2332\n","Epoch 26 - MCC: 0.8003\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.9045 - loss: 0.2332 - val_accuracy: 0.9003 - val_loss: 0.2427 - mcc: 0.8003\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9077 - loss: 0.2246\n","Epoch 27 - MCC: 0.8034\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9076 - loss: 0.2249 - val_accuracy: 0.9017 - val_loss: 0.2392 - mcc: 0.8034\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9084 - loss: 0.2222\n","Epoch 28 - MCC: 0.8042\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9083 - loss: 0.2225 - val_accuracy: 0.9022 - val_loss: 0.2387 - mcc: 0.8042\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9048 - loss: 0.2301\n","Epoch 29 - MCC: 0.8012\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9049 - loss: 0.2301 - val_accuracy: 0.9006 - val_loss: 0.2401 - mcc: 0.8012\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9058 - loss: 0.2279\n","Epoch 30 - MCC: 0.8073\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9058 - loss: 0.2279 - val_accuracy: 0.9036 - val_loss: 0.2344 - mcc: 0.8073\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9138 - loss: 0.2111\n","Epoch 31 - MCC: 0.8063\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9135 - loss: 0.2116 - val_accuracy: 0.9030 - val_loss: 0.2367 - mcc: 0.8063\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9057 - loss: 0.2274\n","Epoch 32 - MCC: 0.8046\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9057 - loss: 0.2274 - val_accuracy: 0.9018 - val_loss: 0.2368 - mcc: 0.8046\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9057 - loss: 0.2273\n","Epoch 33 - MCC: 0.8089\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9058 - loss: 0.2271 - val_accuracy: 0.9042 - val_loss: 0.2336 - mcc: 0.8089\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9086 - loss: 0.2227\n","Epoch 34 - MCC: 0.8101\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9086 - loss: 0.2228 - val_accuracy: 0.9050 - val_loss: 0.2304 - mcc: 0.8101\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9023 - loss: 0.2343\n","Epoch 35 - MCC: 0.8058\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9025 - loss: 0.2338 - val_accuracy: 0.9029 - val_loss: 0.2341 - mcc: 0.8058\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9099 - loss: 0.2181\n","Epoch 36 - MCC: 0.8095\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.9099 - loss: 0.2182 - val_accuracy: 0.9049 - val_loss: 0.2307 - mcc: 0.8095\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9032 - loss: 0.2321\n","Epoch 37 - MCC: 0.8100\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9034 - loss: 0.2316 - val_accuracy: 0.9052 - val_loss: 0.2301 - mcc: 0.8100\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9086 - loss: 0.2204\n","Epoch 38 - MCC: 0.8040\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9086 - loss: 0.2204 - val_accuracy: 0.9017 - val_loss: 0.2350 - mcc: 0.8040\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9112 - loss: 0.2159\n","Epoch 39 - MCC: 0.8074\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9111 - loss: 0.2161 - val_accuracy: 0.9038 - val_loss: 0.2315 - mcc: 0.8074\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9113 - loss: 0.2158\n","Epoch 40 - MCC: 0.8061\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9112 - loss: 0.2160 - val_accuracy: 0.9032 - val_loss: 0.2341 - mcc: 0.8061\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9064 - loss: 0.2260\n","Epoch 41 - MCC: 0.8062\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.9064 - loss: 0.2258 - val_accuracy: 0.9031 - val_loss: 0.2323 - mcc: 0.8062\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9107 - loss: 0.2177\n","Epoch 42 - MCC: 0.8120\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9107 - loss: 0.2177 - val_accuracy: 0.9062 - val_loss: 0.2261 - mcc: 0.8120\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9110 - loss: 0.2149\n","Epoch 43 - MCC: 0.8133\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9110 - loss: 0.2149 - val_accuracy: 0.9067 - val_loss: 0.2254 - mcc: 0.8133\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9132 - loss: 0.2106\n","Epoch 44 - MCC: 0.8095\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9132 - loss: 0.2107 - val_accuracy: 0.9048 - val_loss: 0.2289 - mcc: 0.8095\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9133 - loss: 0.2121\n","Epoch 45 - MCC: 0.8108\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9132 - loss: 0.2122 - val_accuracy: 0.9055 - val_loss: 0.2267 - mcc: 0.8108\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9118 - loss: 0.2125\n","Epoch 46 - MCC: 0.8131\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9118 - loss: 0.2125 - val_accuracy: 0.9062 - val_loss: 0.2293 - mcc: 0.8131\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9127 - loss: 0.2124\n","Epoch 47 - MCC: 0.8141\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9127 - loss: 0.2124 - val_accuracy: 0.9069 - val_loss: 0.2251 - mcc: 0.8141\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9170 - loss: 0.2031\n","Epoch 48 - MCC: 0.8148\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9168 - loss: 0.2034 - val_accuracy: 0.9074 - val_loss: 0.2254 - mcc: 0.8148\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9149 - loss: 0.2073\n","Epoch 49 - MCC: 0.8132\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9148 - loss: 0.2074 - val_accuracy: 0.9067 - val_loss: 0.2240 - mcc: 0.8132\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9138 - loss: 0.2100\n","Epoch 50 - MCC: 0.8122\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9137 - loss: 0.2101 - val_accuracy: 0.9062 - val_loss: 0.2278 - mcc: 0.8122\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9137133333333334,\n","              'mean': 0.9069333333333333,\n","              'min': 0.89428,\n","              'std': 0.006973938788247467},\n"," 'Inference Time (s/sample)': {'max': 0.0035082650184631347,\n","                               'mean': 0.002474419593811035,\n","                               'min': 0.0019860053062438964,\n","                               'std': 0.0005857805985787892},\n"," 'MCC': {'max': 0.8267717820235874,\n","         'mean': 0.8133749476592433,\n","         'min': 0.7881085378497319,\n","         'std': 0.013863609313739967},\n"," 'Parameters': 29857,\n"," 'Train Time (s)': {'max': 156.70258474349976,\n","                    'mean': 146.3319019317627,\n","                    'min': 138.42004299163818,\n","                    'std': 6.208466133445193},\n"," 'Training Accuracy': [[0.6189965605735779,\n","                        0.7690282464027405,\n","                        0.823763370513916,\n","                        0.8493417501449585,\n","                        0.8688432574272156,\n","                        0.8755332827568054,\n","                        0.8799217343330383,\n","                        0.8827332854270935,\n","                        0.8860334157943726,\n","                        0.8876631855964661,\n","                        0.8847299218177795,\n","                        0.8899266123771667,\n","                        0.8917198181152344,\n","                        0.8923133611679077,\n","                        0.8935399651527405,\n","                        0.8951484560966492,\n","                        0.895108163356781,\n","                        0.8941099643707275,\n","                        0.8962416052818298,\n","                        0.8981099724769592,\n","                        0.8952383399009705,\n","                        0.8970600366592407,\n","                        0.8988233804702759,\n","                        0.8985598683357239,\n","                        0.900409996509552,\n","                        0.8998633623123169,\n","                        0.9001701474189758,\n","                        0.9011766910552979,\n","                        0.9019402265548706,\n","                        0.901949942111969,\n","                        0.9025000929832458,\n","                        0.901409924030304,\n","                        0.900941789150238,\n","                        0.9023616909980774,\n","                        0.9032551050186157,\n","                        0.9033418297767639,\n","                        0.9051500558853149,\n","                        0.9057400226593018,\n","                        0.905500054359436,\n","                        0.9061383008956909,\n","                        0.9065433740615845,\n","                        0.906623363494873,\n","                        0.9068616628646851,\n","                        0.9067833423614502,\n","                        0.9069815874099731,\n","                        0.9075900316238403,\n","                        0.9074916243553162,\n","                        0.908121645450592,\n","                        0.9080184102058411,\n","                        0.9065950512886047],\n","                       [0.6498566269874573,\n","                        0.7699882984161377,\n","                        0.8328633308410645,\n","                        0.8593000769615173,\n","                        0.8738633990287781,\n","                        0.8783751130104065,\n","                        0.8830049633979797,\n","                        0.8844084143638611,\n","                        0.88843834400177,\n","                        0.8893583416938782,\n","                        0.89055997133255,\n","                        0.8917749524116516,\n","                        0.8951084017753601,\n","                        0.8967233896255493,\n","                        0.8968633413314819,\n","                        0.8979400396347046,\n","                        0.899953305721283,\n","                        0.8998184204101562,\n","                        0.9009968638420105,\n","                        0.903041660785675,\n","                        0.9028748869895935,\n","                        0.901900053024292,\n","                        0.9030233025550842,\n","                        0.9061034321784973,\n","                        0.9050149321556091,\n","                        0.9048433899879456,\n","                        0.9058434367179871,\n","                        0.9073132872581482,\n","                        0.907145082950592,\n","                        0.9079466462135315,\n","                        0.9096717238426208,\n","                        0.9083867073059082,\n","                        0.8866832852363586,\n","                        0.890920102596283,\n","                        0.9003316760063171,\n","                        0.9033616781234741,\n","                        0.9042865633964539,\n","                        0.9055516123771667,\n","                        0.9057449102401733,\n","                        0.9058099985122681,\n","                        0.9074081182479858,\n","                        0.9061617851257324,\n","                        0.9076899886131287,\n","                        0.9083181619644165,\n","                        0.908414900302887,\n","                        0.908440113067627,\n","                        0.9089001417160034,\n","                        0.9099082350730896,\n","                        0.910248339176178,\n","                        0.9104183912277222],\n","                       [0.6350449919700623,\n","                        0.7737650275230408,\n","                        0.8292083144187927,\n","                        0.8550700545310974,\n","                        0.8662216067314148,\n","                        0.872721791267395,\n","                        0.8766082525253296,\n","                        0.8804966807365417,\n","                        0.8838767409324646,\n","                        0.8856617212295532,\n","                        0.8867750763893127,\n","                        0.8881900906562805,\n","                        0.8892300128936768,\n","                        0.8896900415420532,\n","                        0.8907249569892883,\n","                        0.8910166025161743,\n","                        0.8925648331642151,\n","                        0.8929650187492371,\n","                        0.8943915367126465,\n","                        0.8946566581726074,\n","                        0.8961049914360046,\n","                        0.8957183361053467,\n","                        0.8967233896255493,\n","                        0.8978783488273621,\n","                        0.8983099460601807,\n","                        0.8985017538070679,\n","                        0.8999649882316589,\n","                        0.8996250629425049,\n","                        0.9007233381271362,\n","                        0.901786744594574,\n","                        0.9010750651359558,\n","                        0.9017566442489624,\n","                        0.9019333124160767,\n","                        0.9032299518585205,\n","                        0.902188241481781,\n","                        0.9036150574684143,\n","                        0.9044783711433411,\n","                        0.9023433923721313,\n","                        0.9033600091934204,\n","                        0.9049617648124695,\n","                        0.9050099849700928,\n","                        0.906098484992981,\n","                        0.9061833024024963,\n","                        0.9059067368507385,\n","                        0.9058482646942139,\n","                        0.9062834978103638,\n","                        0.9047783613204956,\n","                        0.9068233370780945,\n","                        0.9073283672332764,\n","                        0.9082033634185791],\n","                       [0.622564971446991,\n","                        0.7788217067718506,\n","                        0.8382883667945862,\n","                        0.8628466725349426,\n","                        0.8716333508491516,\n","                        0.8792833685874939,\n","                        0.8822950124740601,\n","                        0.883604884147644,\n","                        0.8874466419219971,\n","                        0.888313353061676,\n","                        0.8890932202339172,\n","                        0.8910083174705505,\n","                        0.8919664621353149,\n","                        0.8944051265716553,\n","                        0.893828272819519,\n","                        0.8959250450134277,\n","                        0.8965799808502197,\n","                        0.8984065055847168,\n","                        0.899150013923645,\n","                        0.8986483216285706,\n","                        0.8998600244522095,\n","                        0.9013833403587341,\n","                        0.9019365906715393,\n","                        0.9019283056259155,\n","                        0.9034633636474609,\n","                        0.9025582671165466,\n","                        0.9027865529060364,\n","                        0.9037483930587769,\n","                        0.9017516374588013,\n","                        0.9029300212860107,\n","                        0.9057900309562683,\n","                        0.9053598642349243,\n","                        0.9064283967018127,\n","                        0.9061066508293152,\n","                        0.9043432474136353,\n","                        0.905261754989624,\n","                        0.9067266583442688,\n","                        0.9058849811553955,\n","                        0.9060482978820801,\n","                        0.9083784222602844,\n","                        0.9064816832542419,\n","                        0.9077733755111694,\n","                        0.9069399237632751,\n","                        0.9095898270606995,\n","                        0.9087567329406738,\n","                        0.9090881943702698,\n","                        0.9094917178153992,\n","                        0.9079331755638123,\n","                        0.9101598858833313,\n","                        0.9098432660102844],\n","                       [0.6897232532501221,\n","                        0.7827415466308594,\n","                        0.8431715369224548,\n","                        0.8613899350166321,\n","                        0.8728716969490051,\n","                        0.8814550638198853,\n","                        0.885326623916626,\n","                        0.887541651725769,\n","                        0.8894099593162537,\n","                        0.8912783265113831,\n","                        0.8916366100311279,\n","                        0.8938482403755188,\n","                        0.8942350149154663,\n","                        0.8955466747283936,\n","                        0.897368311882019,\n","                        0.897653341293335,\n","                        0.8977116346359253,\n","                        0.8993148803710938,\n","                        0.9001867175102234,\n","                        0.9006083011627197,\n","                        0.9023633599281311,\n","                        0.9016799926757812,\n","                        0.9024750590324402,\n","                        0.9019933342933655,\n","                        0.9030233025550842,\n","                        0.9039314985275269,\n","                        0.9039549827575684,\n","                        0.9052150249481201,\n","                        0.9053865671157837,\n","                        0.9057933688163757,\n","                        0.9069333076477051,\n","                        0.9067217111587524,\n","                        0.9073899984359741,\n","                        0.9076699614524841,\n","                        0.9086199402809143,\n","                        0.9085916876792908,\n","                        0.9096216559410095,\n","                        0.9090965390205383,\n","                        0.9091951251029968,\n","                        0.9094749689102173,\n","                        0.9084015488624573,\n","                        0.9102567434310913,\n","                        0.9112768769264221,\n","                        0.9121715426445007,\n","                        0.9107816815376282,\n","                        0.9120384454727173,\n","                        0.9124065637588501,\n","                        0.9130599498748779,\n","                        0.9124433398246765,\n","                        0.9121749401092529]],\n"," 'Training Loss': [[0.6605088114738464,\n","                    0.49447837471961975,\n","                    0.39488109946250916,\n","                    0.345271497964859,\n","                    0.3121487498283386,\n","                    0.2975776195526123,\n","                    0.28911149501800537,\n","                    0.28355225920677185,\n","                    0.276086688041687,\n","                    0.2728496491909027,\n","                    0.2772415280342102,\n","                    0.2673947811126709,\n","                    0.2631032168865204,\n","                    0.26076656579971313,\n","                    0.25764310359954834,\n","                    0.25476646423339844,\n","                    0.2543684244155884,\n","                    0.2562420666217804,\n","                    0.25087985396385193,\n","                    0.24714328348636627,\n","                    0.2515132427215576,\n","                    0.2482762485742569,\n","                    0.24458947777748108,\n","                    0.2446804791688919,\n","                    0.24079933762550354,\n","                    0.24253131449222565,\n","                    0.24184691905975342,\n","                    0.2393815666437149,\n","                    0.2365880012512207,\n","                    0.2365645170211792,\n","                    0.23660099506378174,\n","                    0.2369820922613144,\n","                    0.2382267564535141,\n","                    0.23486047983169556,\n","                    0.23267236351966858,\n","                    0.23213472962379456,\n","                    0.2293243408203125,\n","                    0.22739920020103455,\n","                    0.22791574895381927,\n","                    0.22618205845355988,\n","                    0.22537082433700562,\n","                    0.22509239614009857,\n","                    0.2246486246585846,\n","                    0.22409726679325104,\n","                    0.22357210516929626,\n","                    0.2224697470664978,\n","                    0.22207844257354736,\n","                    0.22169101238250732,\n","                    0.2208610326051712,\n","                    0.2249545305967331],\n","                   [0.6652815937995911,\n","                    0.5113418698310852,\n","                    0.38720425963401794,\n","                    0.33104512095451355,\n","                    0.30217915773391724,\n","                    0.2914692163467407,\n","                    0.28366807103157043,\n","                    0.2787763178348541,\n","                    0.27018776535987854,\n","                    0.26749154925346375,\n","                    0.26491019129753113,\n","                    0.26273438334465027,\n","                    0.2551420331001282,\n","                    0.2516290843486786,\n","                    0.24984945356845856,\n","                    0.24731098115444183,\n","                    0.24242115020751953,\n","                    0.24280771613121033,\n","                    0.23948994278907776,\n","                    0.23573365807533264,\n","                    0.23671533167362213,\n","                    0.2374897450208664,\n","                    0.23568466305732727,\n","                    0.22861333191394806,\n","                    0.23098735511302948,\n","                    0.22960205376148224,\n","                    0.2283720225095749,\n","                    0.22488214075565338,\n","                    0.2250099778175354,\n","                    0.22431892156600952,\n","                    0.21938271820545197,\n","                    0.22156545519828796,\n","                    0.2901637554168701,\n","                    0.2676028311252594,\n","                    0.2450302094221115,\n","                    0.23783902823925018,\n","                    0.2355475276708603,\n","                    0.23197416961193085,\n","                    0.23038126528263092,\n","                    0.22986724972724915,\n","                    0.22736042737960815,\n","                    0.22857984900474548,\n","                    0.22503714263439178,\n","                    0.22335590422153473,\n","                    0.22333961725234985,\n","                    0.22287234663963318,\n","                    0.22254733741283417,\n","                    0.2197255641222,\n","                    0.21865209937095642,\n","                    0.21808554232120514],\n","                   [0.6598113179206848,\n","                    0.4906468093395233,\n","                    0.3876221776008606,\n","                    0.33778873085975647,\n","                    0.3160642385482788,\n","                    0.30378374457359314,\n","                    0.2958132326602936,\n","                    0.28880372643470764,\n","                    0.28201058506965637,\n","                    0.2775815725326538,\n","                    0.27501580119132996,\n","                    0.27280929684638977,\n","                    0.2706575393676758,\n","                    0.26791146397590637,\n","                    0.26510119438171387,\n","                    0.26498743891716003,\n","                    0.2604960799217224,\n","                    0.2591366469860077,\n","                    0.25522205233573914,\n","                    0.25573039054870605,\n","                    0.25196659564971924,\n","                    0.2517780065536499,\n","                    0.25011298060417175,\n","                    0.24654540419578552,\n","                    0.24481543898582458,\n","                    0.24442510306835175,\n","                    0.2408667355775833,\n","                    0.2407979965209961,\n","                    0.23863817751407623,\n","                    0.23622535169124603,\n","                    0.23831747472286224,\n","                    0.23548461496829987,\n","                    0.23529493808746338,\n","                    0.23310856521129608,\n","                    0.2349863499403,\n","                    0.23166902363300323,\n","                    0.22973033785820007,\n","                    0.23463298380374908,\n","                    0.2326429784297943,\n","                    0.22801412642002106,\n","                    0.2275712788105011,\n","                    0.22531482577323914,\n","                    0.2254943698644638,\n","                    0.22561384737491608,\n","                    0.22527828812599182,\n","                    0.2249504029750824,\n","                    0.2283957302570343,\n","                    0.2236323207616806,\n","                    0.22232620418071747,\n","                    0.2201077789068222],\n","                   [0.6692324876785278,\n","                    0.49414992332458496,\n","                    0.3677513003349304,\n","                    0.3256470859050751,\n","                    0.3078792095184326,\n","                    0.29088371992111206,\n","                    0.2858221232891083,\n","                    0.2806677222251892,\n","                    0.2721201777458191,\n","                    0.2702218294143677,\n","                    0.2689177095890045,\n","                    0.2652454674243927,\n","                    0.2626646161079407,\n","                    0.2578253746032715,\n","                    0.25775444507598877,\n","                    0.2540495991706848,\n","                    0.25201770663261414,\n","                    0.2479158192873001,\n","                    0.2460750937461853,\n","                    0.24678242206573486,\n","                    0.24374565482139587,\n","                    0.24068640172481537,\n","                    0.23951560258865356,\n","                    0.23966597020626068,\n","                    0.23621799051761627,\n","                    0.2380688339471817,\n","                    0.236566424369812,\n","                    0.23346269130706787,\n","                    0.2398114949464798,\n","                    0.23706141114234924,\n","                    0.23032794892787933,\n","                    0.23204559087753296,\n","                    0.22878199815750122,\n","                    0.22962665557861328,\n","                    0.2326403260231018,\n","                    0.23072296380996704,\n","                    0.22820331156253815,\n","                    0.22849661111831665,\n","                    0.22741392254829407,\n","                    0.2238290011882782,\n","                    0.22801810503005981,\n","                    0.22572022676467896,\n","                    0.22603893280029297,\n","                    0.22102057933807373,\n","                    0.22292114794254303,\n","                    0.22293560206890106,\n","                    0.22184224426746368,\n","                    0.22410385310649872,\n","                    0.21939657628536224,\n","                    0.22029215097427368],\n","                   [0.673531174659729,\n","                    0.5178832411766052,\n","                    0.37146663665771484,\n","                    0.3311918377876282,\n","                    0.3078182637691498,\n","                    0.2896697521209717,\n","                    0.2805495262145996,\n","                    0.27571141719818115,\n","                    0.27040135860443115,\n","                    0.26667600870132446,\n","                    0.26520848274230957,\n","                    0.26124295592308044,\n","                    0.259752482175827,\n","                    0.2564564645290375,\n","                    0.25121167302131653,\n","                    0.25084301829338074,\n","                    0.24950015544891357,\n","                    0.24629239737987518,\n","                    0.24466268718242645,\n","                    0.24284490942955017,\n","                    0.23970970511436462,\n","                    0.2397388219833374,\n","                    0.23861278593540192,\n","                    0.23800496757030487,\n","                    0.23551687598228455,\n","                    0.23336787521839142,\n","                    0.23263849318027496,\n","                    0.22946128249168396,\n","                    0.22945767641067505,\n","                    0.22822652757167816,\n","                    0.2249976545572281,\n","                    0.22525614500045776,\n","                    0.2240491807460785,\n","                    0.22393327951431274,\n","                    0.22213314473628998,\n","                    0.22089530527591705,\n","                    0.21896308660507202,\n","                    0.22048363089561462,\n","                    0.21941085159778595,\n","                    0.21923469007015228,\n","                    0.22189563512802124,\n","                    0.21729981899261475,\n","                    0.21525926887989044,\n","                    0.2130175232887268,\n","                    0.21645674109458923,\n","                    0.21268588304519653,\n","                    0.21239294111728668,\n","                    0.21062307059764862,\n","                    0.21180792152881622,\n","                    0.2124214917421341]],\n"," 'Validation Accuracy': [[0.7163066864013672,\n","                          0.8169066905975342,\n","                          0.845133364200592,\n","                          0.8644066452980042,\n","                          0.8788533210754395,\n","                          0.8856534361839294,\n","                          0.8897400498390198,\n","                          0.8924933671951294,\n","                          0.8924533724784851,\n","                          0.8926467299461365,\n","                          0.8910933136940002,\n","                          0.8984066843986511,\n","                          0.8985533118247986,\n","                          0.8984533548355103,\n","                          0.9014267921447754,\n","                          0.9002867341041565,\n","                          0.8998532891273499,\n","                          0.903160035610199,\n","                          0.9026867151260376,\n","                          0.9032467007637024,\n","                          0.9015133380889893,\n","                          0.9046199917793274,\n","                          0.904979944229126,\n","                          0.905773401260376,\n","                          0.9044466614723206,\n","                          0.9032132625579834,\n","                          0.9059399366378784,\n","                          0.9068467020988464,\n","                          0.9068399667739868,\n","                          0.9050666689872742,\n","                          0.9080267548561096,\n","                          0.9046666622161865,\n","                          0.9061734080314636,\n","                          0.9082733392715454,\n","                          0.9093933701515198,\n","                          0.9104866981506348,\n","                          0.9098331928253174,\n","                          0.9094533324241638,\n","                          0.9121266007423401,\n","                          0.9103000164031982,\n","                          0.9105333685874939,\n","                          0.9078333973884583,\n","                          0.9114800095558167,\n","                          0.9110333323478699,\n","                          0.9122599959373474,\n","                          0.9125866889953613,\n","                          0.9112600088119507,\n","                          0.9144999980926514,\n","                          0.9142933487892151,\n","                          0.913713276386261],\n","                         [0.7018866539001465,\n","                          0.8044599890708923,\n","                          0.834226667881012,\n","                          0.8571133613586426,\n","                          0.8662066459655762,\n","                          0.8662733435630798,\n","                          0.8681266903877258,\n","                          0.8732733130455017,\n","                          0.8746334314346313,\n","                          0.8752533197402954,\n","                          0.8773199319839478,\n","                          0.878766655921936,\n","                          0.8806332945823669,\n","                          0.8799600005149841,\n","                          0.8822667002677917,\n","                          0.8837733268737793,\n","                          0.8841733336448669,\n","                          0.8850533962249756,\n","                          0.8873599767684937,\n","                          0.8876332640647888,\n","                          0.8883867859840393,\n","                          0.8881334066390991,\n","                          0.8912267088890076,\n","                          0.8910999298095703,\n","                          0.8897800445556641,\n","                          0.8901732563972473,\n","                          0.8935334086418152,\n","                          0.8944066762924194,\n","                          0.8902199268341064,\n","                          0.8942732810974121,\n","                          0.8903666734695435,\n","                          0.8935600519180298,\n","                          0.864026665687561,\n","                          0.8827065825462341,\n","                          0.8864866495132446,\n","                          0.8902599811553955,\n","                          0.8895200490951538,\n","                          0.8921200633049011,\n","                          0.8919000029563904,\n","                          0.8918066620826721,\n","                          0.8914933800697327,\n","                          0.8940800428390503,\n","                          0.8938133120536804,\n","                          0.8927133083343506,\n","                          0.8940600752830505,\n","                          0.8931800127029419,\n","                          0.8935933709144592,\n","                          0.8952667117118835,\n","                          0.8943467140197754,\n","                          0.8942800164222717],\n","                         [0.7016533613204956,\n","                          0.8142132759094238,\n","                          0.8519999980926514,\n","                          0.8659065961837769,\n","                          0.8755466938018799,\n","                          0.8816066980361938,\n","                          0.8867133855819702,\n","                          0.8904933333396912,\n","                          0.8933732509613037,\n","                          0.8940200209617615,\n","                          0.8955399990081787,\n","                          0.8961799740791321,\n","                          0.8964266777038574,\n","                          0.898080050945282,\n","                          0.8994666934013367,\n","                          0.8988800048828125,\n","                          0.8994734287261963,\n","                          0.9020599126815796,\n","                          0.9011733531951904,\n","                          0.9024066925048828,\n","                          0.9043399691581726,\n","                          0.9002800583839417,\n","                          0.9041600227355957,\n","                          0.9068999290466309,\n","                          0.9051732420921326,\n","                          0.9059666395187378,\n","                          0.9067867398262024,\n","                          0.9059867262840271,\n","                          0.9077666401863098,\n","                          0.9067667126655579,\n","                          0.9081133008003235,\n","                          0.9091200232505798,\n","                          0.9092733860015869,\n","                          0.9066867232322693,\n","                          0.9052132964134216,\n","                          0.9079467058181763,\n","                          0.9088200926780701,\n","                          0.9079933166503906,\n","                          0.9099600315093994,\n","                          0.9091066718101501,\n","                          0.9111732244491577,\n","                          0.9108533263206482,\n","                          0.9091666340827942,\n","                          0.9107133746147156,\n","                          0.9097666144371033,\n","                          0.9131733179092407,\n","                          0.9121532440185547,\n","                          0.9106265306472778,\n","                          0.9128400683403015,\n","                          0.9129467010498047],\n","                         [0.7209000587463379,\n","                          0.8291065692901611,\n","                          0.8575267195701599,\n","                          0.8681933879852295,\n","                          0.8760066032409668,\n","                          0.8828933835029602,\n","                          0.8881399631500244,\n","                          0.8882333636283875,\n","                          0.8903067111968994,\n","                          0.8888466358184814,\n","                          0.8932533264160156,\n","                          0.8930000066757202,\n","                          0.8955133557319641,\n","                          0.897233247756958,\n","                          0.8957067131996155,\n","                          0.8981066942214966,\n","                          0.8997066617012024,\n","                          0.8994600772857666,\n","                          0.9002867341041565,\n","                          0.9022200107574463,\n","                          0.9029067158699036,\n","                          0.9041467308998108,\n","                          0.9032266736030579,\n","                          0.9043334126472473,\n","                          0.904426634311676,\n","                          0.9044333100318909,\n","                          0.9067066311836243,\n","                          0.9079599976539612,\n","                          0.9027266502380371,\n","                          0.9050532579421997,\n","                          0.9077199697494507,\n","                          0.9082933664321899,\n","                          0.9073733687400818,\n","                          0.9083467125892639,\n","                          0.9049733877182007,\n","                          0.9063867330551147,\n","                          0.9037066698074341,\n","                          0.9089532494544983,\n","                          0.9068799614906311,\n","                          0.907966673374176,\n","                          0.906706690788269,\n","                          0.9058066010475159,\n","                          0.9066866040229797,\n","                          0.9084266424179077,\n","                          0.9081733822822571,\n","                          0.9094000458717346,\n","                          0.9084200263023376,\n","                          0.9080333113670349,\n","                          0.908466637134552,\n","                          0.9075466394424438],\n","                         [0.7585866451263428,\n","                          0.825439989566803,\n","                          0.8540933132171631,\n","                          0.8539199829101562,\n","                          0.8756200671195984,\n","                          0.8807133436203003,\n","                          0.884380042552948,\n","                          0.881540060043335,\n","                          0.8890332579612732,\n","                          0.8886799812316895,\n","                          0.8892200589179993,\n","                          0.8889333605766296,\n","                          0.8911733031272888,\n","                          0.8930800557136536,\n","                          0.8940266966819763,\n","                          0.8949200510978699,\n","                          0.8937933444976807,\n","                          0.8945666551589966,\n","                          0.8958132863044739,\n","                          0.8979933261871338,\n","                          0.898580014705658,\n","                          0.8971467018127441,\n","                          0.8972532749176025,\n","                          0.8996067047119141,\n","                          0.8995867371559143,\n","                          0.9003065228462219,\n","                          0.9017465710639954,\n","                          0.9022266864776611,\n","                          0.9005600214004517,\n","                          0.9035799503326416,\n","                          0.9030466675758362,\n","                          0.901846706867218,\n","                          0.9041933417320251,\n","                          0.9050467014312744,\n","                          0.9028666615486145,\n","                          0.9049266576766968,\n","                          0.905180037021637,\n","                          0.9017133116722107,\n","                          0.9038066864013672,\n","                          0.903219997882843,\n","                          0.9031399488449097,\n","                          0.9061800241470337,\n","                          0.90666663646698,\n","                          0.9047600030899048,\n","                          0.9055466651916504,\n","                          0.9061534404754639,\n","                          0.9069200158119202,\n","                          0.9074333906173706,\n","                          0.9067400097846985,\n","                          0.9061800241470337]],\n"," 'Validation Loss': [[0.5842609405517578,\n","                      0.410530686378479,\n","                      0.3539625108242035,\n","                      0.3171117305755615,\n","                      0.2916383147239685,\n","                      0.2838740050792694,\n","                      0.27190423011779785,\n","                      0.26495257019996643,\n","                      0.2623611390590668,\n","                      0.25886499881744385,\n","                      0.2613886892795563,\n","                      0.2499956339597702,\n","                      0.24819429218769073,\n","                      0.24805723130702972,\n","                      0.24241535365581512,\n","                      0.24251364171504974,\n","                      0.24638399481773376,\n","                      0.2379905730485916,\n","                      0.23877567052841187,\n","                      0.2366095334291458,\n","                      0.2449289858341217,\n","                      0.23432831466197968,\n","                      0.23352569341659546,\n","                      0.22896404564380646,\n","                      0.2317465990781784,\n","                      0.23616689443588257,\n","                      0.22878217697143555,\n","                      0.22627604007720947,\n","                      0.22472968697547913,\n","                      0.22998639941215515,\n","                      0.22360946238040924,\n","                      0.2297181934118271,\n","                      0.2264563888311386,\n","                      0.22387370467185974,\n","                      0.2200934737920761,\n","                      0.21967670321464539,\n","                      0.2178838849067688,\n","                      0.21997109055519104,\n","                      0.2145327776670456,\n","                      0.218094140291214,\n","                      0.2182070016860962,\n","                      0.22143974900245667,\n","                      0.21529556810855865,\n","                      0.2162162810564041,\n","                      0.21515464782714844,\n","                      0.21145528554916382,\n","                      0.21609720587730408,\n","                      0.2100343555212021,\n","                      0.20969225466251373,\n","                      0.21300062537193298],\n","                     [0.6112198829650879,\n","                      0.4479217827320099,\n","                      0.3771170675754547,\n","                      0.33620086312294006,\n","                      0.31683528423309326,\n","                      0.316784530878067,\n","                      0.3111417293548584,\n","                      0.299284428358078,\n","                      0.29619255661964417,\n","                      0.2949679493904114,\n","                      0.28986477851867676,\n","                      0.2856329679489136,\n","                      0.2814229130744934,\n","                      0.282657265663147,\n","                      0.27634069323539734,\n","                      0.27386894822120667,\n","                      0.272874653339386,\n","                      0.2713318169116974,\n","                      0.26584798097610474,\n","                      0.2659374177455902,\n","                      0.2681047320365906,\n","                      0.26538723707199097,\n","                      0.25825029611587524,\n","                      0.2570149302482605,\n","                      0.26051774621009827,\n","                      0.26195186376571655,\n","                      0.25485312938690186,\n","                      0.25094112753868103,\n","                      0.25970372557640076,\n","                      0.24924714863300323,\n","                      0.25790509581565857,\n","                      0.25037944316864014,\n","                      0.3249364495277405,\n","                      0.2811267375946045,\n","                      0.27272722125053406,\n","                      0.26389944553375244,\n","                      0.26610323786735535,\n","                      0.2589305639266968,\n","                      0.2593870460987091,\n","                      0.25912296772003174,\n","                      0.25857388973236084,\n","                      0.2549583315849304,\n","                      0.2537444233894348,\n","                      0.2569675147533417,\n","                      0.25428855419158936,\n","                      0.25682029128074646,\n","                      0.2534821927547455,\n","                      0.250655859708786,\n","                      0.2510809004306793,\n","                      0.2512904405593872],\n","                     [0.5905055999755859,\n","                      0.42145293951034546,\n","                      0.33857008814811707,\n","                      0.310980886220932,\n","                      0.2974569797515869,\n","                      0.2825590670108795,\n","                      0.27283596992492676,\n","                      0.2657260000705719,\n","                      0.2595955729484558,\n","                      0.25735560059547424,\n","                      0.25474193692207336,\n","                      0.2534048557281494,\n","                      0.2521374225616455,\n","                      0.24755340814590454,\n","                      0.24716967344284058,\n","                      0.24718455970287323,\n","                      0.24264109134674072,\n","                      0.24090903997421265,\n","                      0.23986659944057465,\n","                      0.2387574166059494,\n","                      0.2348024994134903,\n","                      0.24629129469394684,\n","                      0.23367488384246826,\n","                      0.22821950912475586,\n","                      0.23069117963314056,\n","                      0.22821548581123352,\n","                      0.22586236894130707,\n","                      0.22721563279628754,\n","                      0.22295360267162323,\n","                      0.22366327047348022,\n","                      0.22264042496681213,\n","                      0.21986228227615356,\n","                      0.21989823877811432,\n","                      0.22420631349086761,\n","                      0.2283550202846527,\n","                      0.22204074263572693,\n","                      0.21966621279716492,\n","                      0.22264644503593445,\n","                      0.21705934405326843,\n","                      0.21864359080791473,\n","                      0.21416430175304413,\n","                      0.21426759660243988,\n","                      0.21793332695960999,\n","                      0.21475514769554138,\n","                      0.21562476456165314,\n","                      0.20984190702438354,\n","                      0.21083450317382812,\n","                      0.21545763313770294,\n","                      0.2110411822795868,\n","                      0.210667684674263],\n","                     [0.6116922497749329,\n","                      0.3872738182544708,\n","                      0.3385782241821289,\n","                      0.3151077628135681,\n","                      0.30087366700172424,\n","                      0.28410208225250244,\n","                      0.2798635959625244,\n","                      0.2749369740486145,\n","                      0.2716481685638428,\n","                      0.2730887532234192,\n","                      0.2657511234283447,\n","                      0.2649463713169098,\n","                      0.2597632110118866,\n","                      0.2539416253566742,\n","                      0.2627466320991516,\n","                      0.2514372766017914,\n","                      0.2499217540025711,\n","                      0.24821186065673828,\n","                      0.24316976964473724,\n","                      0.2397121787071228,\n","                      0.2388576865196228,\n","                      0.2385733425617218,\n","                      0.24038240313529968,\n","                      0.23604795336723328,\n","                      0.23430423438549042,\n","                      0.23476159572601318,\n","                      0.23115749657154083,\n","                      0.2288590371608734,\n","                      0.24028170108795166,\n","                      0.2364726960659027,\n","                      0.22921478748321533,\n","                      0.22738097608089447,\n","                      0.22856727242469788,\n","                      0.22789911925792694,\n","                      0.23332475125789642,\n","                      0.229349285364151,\n","                      0.2342042177915573,\n","                      0.22544150054454803,\n","                      0.2293250858783722,\n","                      0.22625532746315002,\n","                      0.2277812957763672,\n","                      0.22832399606704712,\n","                      0.22753536701202393,\n","                      0.2239888310432434,\n","                      0.2238006740808487,\n","                      0.22469356656074524,\n","                      0.22328197956085205,\n","                      0.2251719832420349,\n","                      0.2238803654909134,\n","                      0.22496062517166138],\n","                     [0.6208601593971252,\n","                      0.40588006377220154,\n","                      0.34892815351486206,\n","                      0.34432193636894226,\n","                      0.29920750856399536,\n","                      0.2908243238925934,\n","                      0.28229665756225586,\n","                      0.2849312424659729,\n","                      0.27370962500572205,\n","                      0.27253204584121704,\n","                      0.269746869802475,\n","                      0.27109768986701965,\n","                      0.26599133014678955,\n","                      0.26264944672584534,\n","                      0.2593683898448944,\n","                      0.25793540477752686,\n","                      0.2577286660671234,\n","                      0.256381630897522,\n","                      0.25241053104400635,\n","                      0.24857310950756073,\n","                      0.2476172298192978,\n","                      0.2480686753988266,\n","                      0.24992957711219788,\n","                      0.24313053488731384,\n","                      0.24524959921836853,\n","                      0.24273954331874847,\n","                      0.2391992062330246,\n","                      0.23871244490146637,\n","                      0.24006140232086182,\n","                      0.23439422249794006,\n","                      0.23670010268688202,\n","                      0.23679299652576447,\n","                      0.2336067259311676,\n","                      0.23036713898181915,\n","                      0.2340930998325348,\n","                      0.23070095479488373,\n","                      0.23014716804027557,\n","                      0.234969362616539,\n","                      0.23153668642044067,\n","                      0.23409658670425415,\n","                      0.23230361938476562,\n","                      0.2260521948337555,\n","                      0.22542688250541687,\n","                      0.22892551124095917,\n","                      0.2267312854528427,\n","                      0.22931358218193054,\n","                      0.2250727415084839,\n","                      0.2253958284854889,\n","                      0.22404465079307556,\n","                      0.22781167924404144]],\n"," 'Validation MCC': [[0.429249118692204,\n","                     0.6321198497198653,\n","                     0.6891859392399703,\n","                     0.7276833034654963,\n","                     0.756764140342134,\n","                     0.7710131625777965,\n","                     0.778629092434218,\n","                     0.7844335686360482,\n","                     0.784041693645224,\n","                     0.7845107671860407,\n","                     0.7831977798682345,\n","                     0.7960616264567304,\n","                     0.7964562915504672,\n","                     0.796459251511487,\n","                     0.8020851738841558,\n","                     0.799770322392288,\n","                     0.7995048491872246,\n","                     0.8055526287993304,\n","                     0.8045875986173524,\n","                     0.8057098961778701,\n","                     0.8023554087326539,\n","                     0.8085081320287137,\n","                     0.8097075077034678,\n","                     0.8107986773166345,\n","                     0.8081595927687918,\n","                     0.8057571373196938,\n","                     0.8111860428775004,\n","                     0.8130734361180094,\n","                     0.8130064211920427,\n","                     0.8093803091961708,\n","                     0.8153105387824507,\n","                     0.8088636039976508,\n","                     0.8117961724312835,\n","                     0.8161377712136348,\n","                     0.8180577052905624,\n","                     0.8202978964853969,\n","                     0.8189666533078965,\n","                     0.818196446577807,\n","                     0.8235747162887959,\n","                     0.8198861511805627,\n","                     0.8203830562962734,\n","                     0.8153014133811902,\n","                     0.8224037744219194,\n","                     0.8217493287175939,\n","                     0.8239384012213161,\n","                     0.8247225515321299,\n","                     0.821974889883348,\n","                     0.8283282444083517,\n","                     0.8279247224134942,\n","                     0.8267717820235874],\n","                    [0.40201436862966106,\n","                     0.6079324402440935,\n","                     0.6678415603787082,\n","                     0.7138173354329814,\n","                     0.7318917784590805,\n","                     0.7322167117842281,\n","                     0.7364972826565409,\n","                     0.7459866064603258,\n","                     0.7486222130596136,\n","                     0.7512857828718101,\n","                     0.7540091379096296,\n","                     0.7570610784526066,\n","                     0.760845033502739,\n","                     0.7610616531859874,\n","                     0.76393756843872,\n","                     0.767435018128498,\n","                     0.7680178286480627,\n","                     0.7695994508711869,\n","                     0.7741982206617595,\n","                     0.7750471721299934,\n","                     0.7770450279639892,\n","                     0.7765062990665653,\n","                     0.7825396943191982,\n","                     0.7819459808544906,\n","                     0.7803782288292082,\n","                     0.7818310049971593,\n","                     0.7867561036525293,\n","                     0.7883492233504312,\n","                     0.7800322565422436,\n","                     0.7880818316234862,\n","                     0.7823325988754241,\n","                     0.7865925361517785,\n","                     0.7282800425000906,\n","                     0.7650616210398964,\n","                     0.77355870371598,\n","                     0.7803356906096625,\n","                     0.7786470119134433,\n","                     0.783844748102587,\n","                     0.7836962222997663,\n","                     0.7834910467538657,\n","                     0.7825872905366797,\n","                     0.7876833940144863,\n","                     0.787399221883862,\n","                     0.785042498607978,\n","                     0.7882151604583798,\n","                     0.7863096388411778,\n","                     0.7872982999484476,\n","                     0.7902382437068041,\n","                     0.7885411016688592,\n","                     0.7881085378497319],\n","                    [0.40114558785401455,\n","                     0.630135172730898,\n","                     0.7026063048499639,\n","                     0.7318028476664109,\n","                     0.7509080134719083,\n","                     0.7626412477960898,\n","                     0.772484330944017,\n","                     0.7800233707638831,\n","                     0.7858163783729046,\n","                     0.7871381251867716,\n","                     0.790385267540216,\n","                     0.7914768436204125,\n","                     0.7920053526854895,\n","                     0.7952848983265389,\n","                     0.798051402156373,\n","                     0.7968793417929636,\n","                     0.7980740831027444,\n","                     0.8032692834053441,\n","                     0.8014821194259614,\n","                     0.8039738281396095,\n","                     0.8078773232864944,\n","                     0.7996928185702773,\n","                     0.8075203994409109,\n","                     0.8130289684637746,\n","                     0.8096163401285643,\n","                     0.8111833182387412,\n","                     0.8127638362448638,\n","                     0.8116567303541323,\n","                     0.8148260370430469,\n","                     0.8127830103723583,\n","                     0.8154442243474888,\n","                     0.8174612378711186,\n","                     0.8181086726640704,\n","                     0.812616158447912,\n","                     0.8096047781166069,\n","                     0.815642990617359,\n","                     0.8172608491185833,\n","                     0.8153019251960841,\n","                     0.8191518125435124,\n","                     0.8174267416346606,\n","                     0.8216829955125855,\n","                     0.8212066618686659,\n","                     0.8180868937449304,\n","                     0.8206648498643283,\n","                     0.8187566154645111,\n","                     0.8256168147763437,\n","                     0.8235823087728488,\n","                     0.8208776256245783,\n","                     0.824951482072987,\n","                     0.825209948763336],\n","                    [0.44303271889408197,\n","                     0.6575334754819889,\n","                     0.714288031289158,\n","                     0.7363074498733279,\n","                     0.7524117884747097,\n","                     0.7652097134622556,\n","                     0.7758016033031503,\n","                     0.7759019631472835,\n","                     0.7802417455973627,\n","                     0.7773909742799141,\n","                     0.7860057391583608,\n","                     0.7854950036732591,\n","                     0.7910637000576685,\n","                     0.7940089234388937,\n","                     0.7913322837160791,\n","                     0.7958326870968971,\n","                     0.7989936775682586,\n","                     0.7984358536475619,\n","                     0.8002893614384005,\n","                     0.8039399231939177,\n","                     0.8053250782717698,\n","                     0.8078532875789165,\n","                     0.8071586114147297,\n","                     0.8090836857342768,\n","                     0.8084765416846044,\n","                     0.808490468505007,\n","                     0.8129593322949468,\n","                     0.8156478888533089,\n","                     0.8051635580492207,\n","                     0.8104753997175301,\n","                     0.8153755438895407,\n","                     0.8162736145517858,\n","                     0.814877725291546,\n","                     0.8164118385057888,\n","                     0.809519096044925,\n","                     0.8123358206733697,\n","                     0.8069655102997787,\n","                     0.8175295510626436,\n","                     0.8134437887035637,\n","                     0.8157288644114878,\n","                     0.812954206345563,\n","                     0.8111589816875664,\n","                     0.8130969943628295,\n","                     0.8163885759952895,\n","                     0.8159004981855917,\n","                     0.8185980577287164,\n","                     0.8165030364203083,\n","                     0.8160072051546367,\n","                     0.8166656629580628,\n","                     0.8146299406805056],\n","                    [0.517801246741592,\n","                     0.650120014876492,\n","                     0.7077015273765749,\n","                     0.7112160716646417,\n","                     0.7511521779862184,\n","                     0.7624145648059206,\n","                     0.7683246335002138,\n","                     0.7633745194340162,\n","                     0.7776981742891301,\n","                     0.7782409982313364,\n","                     0.7782708310566462,\n","                     0.778435337358767,\n","                     0.7821857356323525,\n","                     0.7861344127831358,\n","                     0.7879145540482012,\n","                     0.78994896337129,\n","                     0.7872023291053141,\n","                     0.7891604977499679,\n","                     0.7912260224300497,\n","                     0.7957540566133267,\n","                     0.7967679139799569,\n","                     0.7942551933435714,\n","                     0.7949660929407776,\n","                     0.7988651407536395,\n","                     0.7996404710788,\n","                     0.8002611475726356,\n","                     0.80344187896376,\n","                     0.8042007864036753,\n","                     0.8012363586992303,\n","                     0.8072728122699253,\n","                     0.8063326545142844,\n","                     0.8046244793940869,\n","                     0.8089271417716427,\n","                     0.8101267329102826,\n","                     0.8057565882652766,\n","                     0.8094906856088744,\n","                     0.8100496373450748,\n","                     0.8039987877660895,\n","                     0.8074425505821842,\n","                     0.8061055348215194,\n","                     0.8062439352356946,\n","                     0.812004173522663,\n","                     0.8132746650067093,\n","                     0.8094564350154968,\n","                     0.8107989487305937,\n","                     0.8130841970487611,\n","                     0.8141389732385381,\n","                     0.8147791104866545,\n","                     0.8132292937135523,\n","                     0.8121545289790555]]}\n","Training Model: BiLSTM, Fold: 1\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.5748 - loss: 0.6865\n","Epoch 1 - MCC: 0.3676\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 115ms/step - accuracy: 0.5762 - loss: 0.6861 - val_accuracy: 0.6682 - val_loss: 0.6470 - mcc: 0.3676\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6973 - loss: 0.6195\n","Epoch 2 - MCC: 0.6086\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.6986 - loss: 0.6183 - val_accuracy: 0.8014 - val_loss: 0.5034 - mcc: 0.6086\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8235 - loss: 0.4501\n","Epoch 3 - MCC: 0.7536\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.8242 - loss: 0.4484 - val_accuracy: 0.8768 - val_loss: 0.3090 - mcc: 0.7536\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8772 - loss: 0.3009\n","Epoch 4 - MCC: 0.8065\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.8774 - loss: 0.3004 - val_accuracy: 0.9032 - val_loss: 0.2528 - mcc: 0.8065\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8987 - loss: 0.2585\n","Epoch 5 - MCC: 0.8220\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8988 - loss: 0.2581 - val_accuracy: 0.9113 - val_loss: 0.2270 - mcc: 0.8220\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9097 - loss: 0.2296\n","Epoch 6 - MCC: 0.8272\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9096 - loss: 0.2297 - val_accuracy: 0.9139 - val_loss: 0.2160 - mcc: 0.8272\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9104 - loss: 0.2241\n","Epoch 7 - MCC: 0.8397\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9104 - loss: 0.2241 - val_accuracy: 0.9199 - val_loss: 0.2011 - mcc: 0.8397\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9154 - loss: 0.2094\n","Epoch 8 - MCC: 0.8429\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9153 - loss: 0.2095 - val_accuracy: 0.9214 - val_loss: 0.1964 - mcc: 0.8429\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9176 - loss: 0.2037\n","Epoch 9 - MCC: 0.8443\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9175 - loss: 0.2037 - val_accuracy: 0.9224 - val_loss: 0.1897 - mcc: 0.8443\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9220 - loss: 0.1930\n","Epoch 10 - MCC: 0.8512\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.9219 - loss: 0.1933 - val_accuracy: 0.9258 - val_loss: 0.1824 - mcc: 0.8512\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9277 - loss: 0.1801\n","Epoch 11 - MCC: 0.8563\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9274 - loss: 0.1806 - val_accuracy: 0.9285 - val_loss: 0.1764 - mcc: 0.8563\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9192 - loss: 0.1947\n","Epoch 12 - MCC: 0.8602\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9193 - loss: 0.1944 - val_accuracy: 0.9303 - val_loss: 0.1710 - mcc: 0.8602\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9234 - loss: 0.1865\n","Epoch 13 - MCC: 0.8648\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9234 - loss: 0.1864 - val_accuracy: 0.9326 - val_loss: 0.1676 - mcc: 0.8648\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9276 - loss: 0.1768\n","Epoch 14 - MCC: 0.8643\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9275 - loss: 0.1770 - val_accuracy: 0.9324 - val_loss: 0.1655 - mcc: 0.8643\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9282 - loss: 0.1762\n","Epoch 15 - MCC: 0.8708\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9282 - loss: 0.1762 - val_accuracy: 0.9355 - val_loss: 0.1597 - mcc: 0.8708\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9298 - loss: 0.1728\n","Epoch 16 - MCC: 0.8698\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9298 - loss: 0.1729 - val_accuracy: 0.9350 - val_loss: 0.1586 - mcc: 0.8698\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9300 - loss: 0.1722\n","Epoch 17 - MCC: 0.8724\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9300 - loss: 0.1721 - val_accuracy: 0.9364 - val_loss: 0.1560 - mcc: 0.8724\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9263 - loss: 0.1778\n","Epoch 18 - MCC: 0.8739\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9265 - loss: 0.1775 - val_accuracy: 0.9370 - val_loss: 0.1550 - mcc: 0.8739\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9292 - loss: 0.1719\n","Epoch 19 - MCC: 0.8738\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9293 - loss: 0.1717 - val_accuracy: 0.9371 - val_loss: 0.1528 - mcc: 0.8738\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9316 - loss: 0.1681\n","Epoch 20 - MCC: 0.8750\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9316 - loss: 0.1679 - val_accuracy: 0.9377 - val_loss: 0.1514 - mcc: 0.8750\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9313 - loss: 0.1673\n","Epoch 21 - MCC: 0.8785\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9313 - loss: 0.1671 - val_accuracy: 0.9395 - val_loss: 0.1481 - mcc: 0.8785\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9322 - loss: 0.1647\n","Epoch 22 - MCC: 0.8804\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9322 - loss: 0.1645 - val_accuracy: 0.9404 - val_loss: 0.1466 - mcc: 0.8804\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9349 - loss: 0.1594\n","Epoch 23 - MCC: 0.8800\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9350 - loss: 0.1594 - val_accuracy: 0.9402 - val_loss: 0.1455 - mcc: 0.8800\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9348 - loss: 0.1590\n","Epoch 24 - MCC: 0.8799\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9348 - loss: 0.1589 - val_accuracy: 0.9402 - val_loss: 0.1449 - mcc: 0.8799\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9388 - loss: 0.1500\n","Epoch 25 - MCC: 0.8836\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9387 - loss: 0.1502 - val_accuracy: 0.9419 - val_loss: 0.1423 - mcc: 0.8836\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9356 - loss: 0.1570\n","Epoch 26 - MCC: 0.8827\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.9356 - loss: 0.1570 - val_accuracy: 0.9416 - val_loss: 0.1422 - mcc: 0.8827\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9420 - loss: 0.1422\n","Epoch 27 - MCC: 0.8832\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9417 - loss: 0.1427 - val_accuracy: 0.9418 - val_loss: 0.1432 - mcc: 0.8832\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9385 - loss: 0.1495\n","Epoch 28 - MCC: 0.8830\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9384 - loss: 0.1496 - val_accuracy: 0.9417 - val_loss: 0.1424 - mcc: 0.8830\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9356 - loss: 0.1554\n","Epoch 29 - MCC: 0.8833\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9357 - loss: 0.1552 - val_accuracy: 0.9418 - val_loss: 0.1424 - mcc: 0.8833\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9369 - loss: 0.1526\n","Epoch 30 - MCC: 0.8873\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9369 - loss: 0.1525 - val_accuracy: 0.9439 - val_loss: 0.1368 - mcc: 0.8873\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9393 - loss: 0.1501\n","Epoch 31 - MCC: 0.8868\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9393 - loss: 0.1501 - val_accuracy: 0.9436 - val_loss: 0.1380 - mcc: 0.8868\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9414 - loss: 0.1426\n","Epoch 32 - MCC: 0.8875\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9412 - loss: 0.1429 - val_accuracy: 0.9439 - val_loss: 0.1363 - mcc: 0.8875\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9416 - loss: 0.1440\n","Epoch 33 - MCC: 0.8875\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9415 - loss: 0.1442 - val_accuracy: 0.9440 - val_loss: 0.1372 - mcc: 0.8875\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9385 - loss: 0.1487\n","Epoch 34 - MCC: 0.8878\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9386 - loss: 0.1486 - val_accuracy: 0.9441 - val_loss: 0.1369 - mcc: 0.8878\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9356 - loss: 0.1550\n","Epoch 35 - MCC: 0.8884\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9358 - loss: 0.1547 - val_accuracy: 0.9444 - val_loss: 0.1350 - mcc: 0.8884\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9356 - loss: 0.1554\n","Epoch 36 - MCC: 0.8902\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9358 - loss: 0.1551 - val_accuracy: 0.9453 - val_loss: 0.1330 - mcc: 0.8902\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9408 - loss: 0.1446\n","Epoch 37 - MCC: 0.8903\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9408 - loss: 0.1447 - val_accuracy: 0.9453 - val_loss: 0.1334 - mcc: 0.8903\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9419 - loss: 0.1404\n","Epoch 38 - MCC: 0.8886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9418 - loss: 0.1406 - val_accuracy: 0.9445 - val_loss: 0.1367 - mcc: 0.8886\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9385 - loss: 0.1493\n","Epoch 39 - MCC: 0.8894\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9386 - loss: 0.1491 - val_accuracy: 0.9449 - val_loss: 0.1349 - mcc: 0.8894\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9438 - loss: 0.1389\n","Epoch 40 - MCC: 0.8922\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9437 - loss: 0.1391 - val_accuracy: 0.9463 - val_loss: 0.1311 - mcc: 0.8922\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9349 - loss: 0.1565\n","Epoch 41 - MCC: 0.8910\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9351 - loss: 0.1561 - val_accuracy: 0.9456 - val_loss: 0.1326 - mcc: 0.8910\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9423 - loss: 0.1408\n","Epoch 42 - MCC: 0.8922\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9423 - loss: 0.1409 - val_accuracy: 0.9463 - val_loss: 0.1318 - mcc: 0.8922\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9384 - loss: 0.1488\n","Epoch 43 - MCC: 0.8890\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9385 - loss: 0.1487 - val_accuracy: 0.9446 - val_loss: 0.1346 - mcc: 0.8890\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9412 - loss: 0.1443\n","Epoch 44 - MCC: 0.8924\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9413 - loss: 0.1442 - val_accuracy: 0.9464 - val_loss: 0.1307 - mcc: 0.8924\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9430 - loss: 0.1403\n","Epoch 45 - MCC: 0.8931\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9430 - loss: 0.1403 - val_accuracy: 0.9467 - val_loss: 0.1301 - mcc: 0.8931\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9421 - loss: 0.1411\n","Epoch 46 - MCC: 0.8901\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9422 - loss: 0.1411 - val_accuracy: 0.9452 - val_loss: 0.1341 - mcc: 0.8901\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9432 - loss: 0.1388\n","Epoch 47 - MCC: 0.8907\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9432 - loss: 0.1389 - val_accuracy: 0.9455 - val_loss: 0.1329 - mcc: 0.8907\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9408 - loss: 0.1462\n","Epoch 48 - MCC: 0.8922\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9408 - loss: 0.1460 - val_accuracy: 0.9463 - val_loss: 0.1320 - mcc: 0.8922\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9407 - loss: 0.1455\n","Epoch 49 - MCC: 0.8876\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9408 - loss: 0.1453 - val_accuracy: 0.9439 - val_loss: 0.1361 - mcc: 0.8876\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9422 - loss: 0.1405\n","Epoch 50 - MCC: 0.8928\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9423 - loss: 0.1404 - val_accuracy: 0.9466 - val_loss: 0.1294 - mcc: 0.8928\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 2\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5773 - loss: 0.6832\n","Epoch 1 - MCC: 0.2996\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - accuracy: 0.5789 - loss: 0.6827 - val_accuracy: 0.6420 - val_loss: 0.6449 - mcc: 0.2996\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6922 - loss: 0.6092\n","Epoch 2 - MCC: 0.5602\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.6936 - loss: 0.6079 - val_accuracy: 0.7757 - val_loss: 0.5161 - mcc: 0.5602\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8190 - loss: 0.4462\n","Epoch 3 - MCC: 0.6828\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.8197 - loss: 0.4444 - val_accuracy: 0.8400 - val_loss: 0.3631 - mcc: 0.6828\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8675 - loss: 0.3193\n","Epoch 4 - MCC: 0.7509\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8678 - loss: 0.3186 - val_accuracy: 0.8758 - val_loss: 0.3021 - mcc: 0.7509\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8875 - loss: 0.2738\n","Epoch 5 - MCC: 0.7665\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8877 - loss: 0.2733 - val_accuracy: 0.8827 - val_loss: 0.2755 - mcc: 0.7665\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9032 - loss: 0.2399\n","Epoch 6 - MCC: 0.7892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9032 - loss: 0.2397 - val_accuracy: 0.8949 - val_loss: 0.2500 - mcc: 0.7892\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9062 - loss: 0.2280\n","Epoch 7 - MCC: 0.7918\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9065 - loss: 0.2276 - val_accuracy: 0.8961 - val_loss: 0.2381 - mcc: 0.7918\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9152 - loss: 0.2045\n","Epoch 8 - MCC: 0.8062\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9152 - loss: 0.2045 - val_accuracy: 0.9030 - val_loss: 0.2267 - mcc: 0.8062\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9219 - loss: 0.1909\n","Epoch 9 - MCC: 0.8154\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9218 - loss: 0.1910 - val_accuracy: 0.9078 - val_loss: 0.2175 - mcc: 0.8154\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9210 - loss: 0.1903\n","Epoch 10 - MCC: 0.8194\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9211 - loss: 0.1901 - val_accuracy: 0.9099 - val_loss: 0.2098 - mcc: 0.8194\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9276 - loss: 0.1778\n","Epoch 11 - MCC: 0.8266\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9275 - loss: 0.1778 - val_accuracy: 0.9134 - val_loss: 0.2055 - mcc: 0.8266\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9304 - loss: 0.1689\n","Epoch 12 - MCC: 0.8319\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9303 - loss: 0.1691 - val_accuracy: 0.9162 - val_loss: 0.1988 - mcc: 0.8319\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9286 - loss: 0.1728\n","Epoch 13 - MCC: 0.8288\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9287 - loss: 0.1726 - val_accuracy: 0.9144 - val_loss: 0.2001 - mcc: 0.8288\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9371 - loss: 0.1530\n","Epoch 14 - MCC: 0.8321\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9369 - loss: 0.1535 - val_accuracy: 0.9162 - val_loss: 0.1951 - mcc: 0.8321\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9266 - loss: 0.1742\n","Epoch 15 - MCC: 0.8423\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9268 - loss: 0.1738 - val_accuracy: 0.9213 - val_loss: 0.1883 - mcc: 0.8423\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9340 - loss: 0.1602\n","Epoch 16 - MCC: 0.8459\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9340 - loss: 0.1602 - val_accuracy: 0.9231 - val_loss: 0.1839 - mcc: 0.8459\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9331 - loss: 0.1620\n","Epoch 17 - MCC: 0.8459\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9332 - loss: 0.1618 - val_accuracy: 0.9230 - val_loss: 0.1831 - mcc: 0.8459\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9363 - loss: 0.1541\n","Epoch 18 - MCC: 0.8480\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9363 - loss: 0.1542 - val_accuracy: 0.9242 - val_loss: 0.1828 - mcc: 0.8480\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9367 - loss: 0.1547\n","Epoch 19 - MCC: 0.8494\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.9368 - loss: 0.1546 - val_accuracy: 0.9249 - val_loss: 0.1803 - mcc: 0.8494\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9415 - loss: 0.1419\n","Epoch 20 - MCC: 0.8474\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9413 - loss: 0.1423 - val_accuracy: 0.9237 - val_loss: 0.1826 - mcc: 0.8474\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9331 - loss: 0.1609\n","Epoch 21 - MCC: 0.8493\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9333 - loss: 0.1605 - val_accuracy: 0.9247 - val_loss: 0.1805 - mcc: 0.8493\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9360 - loss: 0.1556\n","Epoch 22 - MCC: 0.8539\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9362 - loss: 0.1553 - val_accuracy: 0.9270 - val_loss: 0.1766 - mcc: 0.8539\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9382 - loss: 0.1496\n","Epoch 23 - MCC: 0.8522\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9383 - loss: 0.1494 - val_accuracy: 0.9262 - val_loss: 0.1776 - mcc: 0.8522\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9424 - loss: 0.1408\n","Epoch 24 - MCC: 0.8543\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9423 - loss: 0.1410 - val_accuracy: 0.9272 - val_loss: 0.1751 - mcc: 0.8543\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9407 - loss: 0.1424\n","Epoch 25 - MCC: 0.8505\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9406 - loss: 0.1424 - val_accuracy: 0.9250 - val_loss: 0.1815 - mcc: 0.8505\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9432 - loss: 0.1380\n","Epoch 26 - MCC: 0.8556\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9431 - loss: 0.1382 - val_accuracy: 0.9280 - val_loss: 0.1725 - mcc: 0.8556\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9414 - loss: 0.1422\n","Epoch 27 - MCC: 0.8563\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9414 - loss: 0.1422 - val_accuracy: 0.9281 - val_loss: 0.1756 - mcc: 0.8563\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9416 - loss: 0.1426\n","Epoch 28 - MCC: 0.8567\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9416 - loss: 0.1426 - val_accuracy: 0.9285 - val_loss: 0.1721 - mcc: 0.8567\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9446 - loss: 0.1349\n","Epoch 29 - MCC: 0.8562\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - accuracy: 0.9445 - loss: 0.1351 - val_accuracy: 0.9283 - val_loss: 0.1729 - mcc: 0.8562\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9430 - loss: 0.1370\n","Epoch 30 - MCC: 0.8587\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9429 - loss: 0.1371 - val_accuracy: 0.9295 - val_loss: 0.1727 - mcc: 0.8587\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9430 - loss: 0.1391\n","Epoch 31 - MCC: 0.8591\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9430 - loss: 0.1391 - val_accuracy: 0.9296 - val_loss: 0.1707 - mcc: 0.8591\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9400 - loss: 0.1440\n","Epoch 32 - MCC: 0.8603\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9401 - loss: 0.1437 - val_accuracy: 0.9303 - val_loss: 0.1711 - mcc: 0.8603\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9423 - loss: 0.1396\n","Epoch 33 - MCC: 0.8603\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - accuracy: 0.9424 - loss: 0.1395 - val_accuracy: 0.9303 - val_loss: 0.1685 - mcc: 0.8603\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9407 - loss: 0.1437\n","Epoch 34 - MCC: 0.8623\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9408 - loss: 0.1434 - val_accuracy: 0.9313 - val_loss: 0.1691 - mcc: 0.8623\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9444 - loss: 0.1354\n","Epoch 35 - MCC: 0.8612\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9444 - loss: 0.1354 - val_accuracy: 0.9307 - val_loss: 0.1696 - mcc: 0.8612\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9398 - loss: 0.1439\n","Epoch 36 - MCC: 0.8622\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9400 - loss: 0.1435 - val_accuracy: 0.9312 - val_loss: 0.1682 - mcc: 0.8622\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9483 - loss: 0.1260\n","Epoch 37 - MCC: 0.8584\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9481 - loss: 0.1263 - val_accuracy: 0.9294 - val_loss: 0.1710 - mcc: 0.8584\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9436 - loss: 0.1369\n","Epoch 38 - MCC: 0.8609\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.9436 - loss: 0.1369 - val_accuracy: 0.9306 - val_loss: 0.1692 - mcc: 0.8609\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9478 - loss: 0.1289\n","Epoch 39 - MCC: 0.8624\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9477 - loss: 0.1291 - val_accuracy: 0.9313 - val_loss: 0.1679 - mcc: 0.8624\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9463 - loss: 0.1311\n","Epoch 40 - MCC: 0.8596\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9462 - loss: 0.1312 - val_accuracy: 0.9299 - val_loss: 0.1695 - mcc: 0.8596\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9437 - loss: 0.1367\n","Epoch 41 - MCC: 0.8610\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9437 - loss: 0.1366 - val_accuracy: 0.9306 - val_loss: 0.1688 - mcc: 0.8610\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9424 - loss: 0.1394\n","Epoch 42 - MCC: 0.8620\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9425 - loss: 0.1392 - val_accuracy: 0.9311 - val_loss: 0.1667 - mcc: 0.8620\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9487 - loss: 0.1256\n","Epoch 43 - MCC: 0.8637\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9486 - loss: 0.1258 - val_accuracy: 0.9320 - val_loss: 0.1657 - mcc: 0.8637\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9468 - loss: 0.1293\n","Epoch 44 - MCC: 0.8629\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9468 - loss: 0.1293 - val_accuracy: 0.9315 - val_loss: 0.1684 - mcc: 0.8629\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9480 - loss: 0.1270\n","Epoch 45 - MCC: 0.8652\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9479 - loss: 0.1272 - val_accuracy: 0.9327 - val_loss: 0.1659 - mcc: 0.8652\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9470 - loss: 0.1272\n","Epoch 46 - MCC: 0.8655\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9470 - loss: 0.1273 - val_accuracy: 0.9329 - val_loss: 0.1644 - mcc: 0.8655\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9446 - loss: 0.1340\n","Epoch 47 - MCC: 0.8636\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9446 - loss: 0.1339 - val_accuracy: 0.9320 - val_loss: 0.1656 - mcc: 0.8636\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9458 - loss: 0.1315\n","Epoch 48 - MCC: 0.8649\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9458 - loss: 0.1314 - val_accuracy: 0.9326 - val_loss: 0.1666 - mcc: 0.8649\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9477 - loss: 0.1255\n","Epoch 49 - MCC: 0.8641\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9476 - loss: 0.1256 - val_accuracy: 0.9321 - val_loss: 0.1656 - mcc: 0.8641\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9475 - loss: 0.1280\n","Epoch 50 - MCC: 0.8639\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9474 - loss: 0.1280 - val_accuracy: 0.9321 - val_loss: 0.1668 - mcc: 0.8639\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 3\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6141 - loss: 0.6811\n","Epoch 1 - MCC: 0.3663\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - accuracy: 0.6153 - loss: 0.6806 - val_accuracy: 0.6774 - val_loss: 0.6322 - mcc: 0.3663\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7048 - loss: 0.6053\n","Epoch 2 - MCC: 0.5894\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.7058 - loss: 0.6041 - val_accuracy: 0.7934 - val_loss: 0.4945 - mcc: 0.5894\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8083 - loss: 0.4632\n","Epoch 3 - MCC: 0.7280\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.8090 - loss: 0.4617 - val_accuracy: 0.8627 - val_loss: 0.3359 - mcc: 0.7280\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8661 - loss: 0.3305\n","Epoch 4 - MCC: 0.8146\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - accuracy: 0.8668 - loss: 0.3292 - val_accuracy: 0.9073 - val_loss: 0.2453 - mcc: 0.8146\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8987 - loss: 0.2555\n","Epoch 5 - MCC: 0.8286\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.8990 - loss: 0.2549 - val_accuracy: 0.9141 - val_loss: 0.2198 - mcc: 0.8286\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9150 - loss: 0.2163\n","Epoch 6 - MCC: 0.8370\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9150 - loss: 0.2165 - val_accuracy: 0.9186 - val_loss: 0.2068 - mcc: 0.8370\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9148 - loss: 0.2130\n","Epoch 7 - MCC: 0.8439\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9148 - loss: 0.2129 - val_accuracy: 0.9222 - val_loss: 0.1962 - mcc: 0.8439\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9222 - loss: 0.1951\n","Epoch 8 - MCC: 0.8385\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9221 - loss: 0.1954 - val_accuracy: 0.9189 - val_loss: 0.1975 - mcc: 0.8385\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9210 - loss: 0.1954\n","Epoch 9 - MCC: 0.8459\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9210 - loss: 0.1954 - val_accuracy: 0.9230 - val_loss: 0.1845 - mcc: 0.8459\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9191 - loss: 0.1960\n","Epoch 10 - MCC: 0.8527\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9191 - loss: 0.1959 - val_accuracy: 0.9266 - val_loss: 0.1793 - mcc: 0.8527\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9304 - loss: 0.1739\n","Epoch 11 - MCC: 0.8575\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9302 - loss: 0.1743 - val_accuracy: 0.9290 - val_loss: 0.1717 - mcc: 0.8575\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9287 - loss: 0.1745\n","Epoch 12 - MCC: 0.8606\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9287 - loss: 0.1747 - val_accuracy: 0.9304 - val_loss: 0.1694 - mcc: 0.8606\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9341 - loss: 0.1641\n","Epoch 13 - MCC: 0.8660\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9339 - loss: 0.1645 - val_accuracy: 0.9333 - val_loss: 0.1621 - mcc: 0.8660\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9296 - loss: 0.1714\n","Epoch 14 - MCC: 0.8686\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9297 - loss: 0.1714 - val_accuracy: 0.9345 - val_loss: 0.1611 - mcc: 0.8686\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9297 - loss: 0.1699\n","Epoch 15 - MCC: 0.8717\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9298 - loss: 0.1698 - val_accuracy: 0.9361 - val_loss: 0.1571 - mcc: 0.8717\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9341 - loss: 0.1607\n","Epoch 16 - MCC: 0.8748\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9340 - loss: 0.1609 - val_accuracy: 0.9376 - val_loss: 0.1539 - mcc: 0.8748\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9343 - loss: 0.1603\n","Epoch 17 - MCC: 0.8728\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9343 - loss: 0.1604 - val_accuracy: 0.9365 - val_loss: 0.1551 - mcc: 0.8728\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9315 - loss: 0.1659\n","Epoch 18 - MCC: 0.8767\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - accuracy: 0.9315 - loss: 0.1658 - val_accuracy: 0.9385 - val_loss: 0.1510 - mcc: 0.8767\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9361 - loss: 0.1551\n","Epoch 19 - MCC: 0.8792\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - accuracy: 0.9361 - loss: 0.1552 - val_accuracy: 0.9398 - val_loss: 0.1479 - mcc: 0.8792\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9351 - loss: 0.1575\n","Epoch 20 - MCC: 0.8790\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9351 - loss: 0.1575 - val_accuracy: 0.9396 - val_loss: 0.1485 - mcc: 0.8790\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9361 - loss: 0.1574\n","Epoch 21 - MCC: 0.8749\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9360 - loss: 0.1574 - val_accuracy: 0.9375 - val_loss: 0.1515 - mcc: 0.8749\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9345 - loss: 0.1586\n","Epoch 22 - MCC: 0.8803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9346 - loss: 0.1585 - val_accuracy: 0.9404 - val_loss: 0.1462 - mcc: 0.8803\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9351 - loss: 0.1569\n","Epoch 23 - MCC: 0.8810\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9352 - loss: 0.1568 - val_accuracy: 0.9407 - val_loss: 0.1434 - mcc: 0.8810\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9338 - loss: 0.1612\n","Epoch 24 - MCC: 0.8805\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9339 - loss: 0.1609 - val_accuracy: 0.9405 - val_loss: 0.1442 - mcc: 0.8805\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9409 - loss: 0.1451\n","Epoch 25 - MCC: 0.8822\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9408 - loss: 0.1453 - val_accuracy: 0.9413 - val_loss: 0.1431 - mcc: 0.8822\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9374 - loss: 0.1507\n","Epoch 26 - MCC: 0.8831\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9374 - loss: 0.1507 - val_accuracy: 0.9417 - val_loss: 0.1419 - mcc: 0.8831\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9378 - loss: 0.1512\n","Epoch 27 - MCC: 0.8840\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9379 - loss: 0.1511 - val_accuracy: 0.9422 - val_loss: 0.1410 - mcc: 0.8840\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9380 - loss: 0.1505\n","Epoch 28 - MCC: 0.8824\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9380 - loss: 0.1505 - val_accuracy: 0.9411 - val_loss: 0.1452 - mcc: 0.8824\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9415 - loss: 0.1438\n","Epoch 29 - MCC: 0.8860\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9414 - loss: 0.1440 - val_accuracy: 0.9432 - val_loss: 0.1388 - mcc: 0.8860\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9414 - loss: 0.1412\n","Epoch 30 - MCC: 0.8803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9413 - loss: 0.1414 - val_accuracy: 0.9404 - val_loss: 0.1432 - mcc: 0.8803\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9352 - loss: 0.1560\n","Epoch 31 - MCC: 0.8861\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9353 - loss: 0.1557 - val_accuracy: 0.9433 - val_loss: 0.1404 - mcc: 0.8861\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9413 - loss: 0.1425\n","Epoch 32 - MCC: 0.8841\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9413 - loss: 0.1427 - val_accuracy: 0.9423 - val_loss: 0.1391 - mcc: 0.8841\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9429 - loss: 0.1394\n","Epoch 33 - MCC: 0.8857\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9428 - loss: 0.1396 - val_accuracy: 0.9431 - val_loss: 0.1399 - mcc: 0.8857\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9411 - loss: 0.1430\n","Epoch 34 - MCC: 0.8876\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9410 - loss: 0.1430 - val_accuracy: 0.9440 - val_loss: 0.1369 - mcc: 0.8876\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9415 - loss: 0.1424\n","Epoch 35 - MCC: 0.8867\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9415 - loss: 0.1425 - val_accuracy: 0.9435 - val_loss: 0.1369 - mcc: 0.8867\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9362 - loss: 0.1545\n","Epoch 36 - MCC: 0.8873\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - accuracy: 0.9364 - loss: 0.1540 - val_accuracy: 0.9439 - val_loss: 0.1377 - mcc: 0.8873\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9410 - loss: 0.1438\n","Epoch 37 - MCC: 0.8868\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9410 - loss: 0.1437 - val_accuracy: 0.9436 - val_loss: 0.1364 - mcc: 0.8868\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9428 - loss: 0.1398\n","Epoch 38 - MCC: 0.8892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9428 - loss: 0.1398 - val_accuracy: 0.9447 - val_loss: 0.1357 - mcc: 0.8892\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9404 - loss: 0.1435\n","Epoch 39 - MCC: 0.8894\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9405 - loss: 0.1434 - val_accuracy: 0.9448 - val_loss: 0.1355 - mcc: 0.8894\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9426 - loss: 0.1397\n","Epoch 40 - MCC: 0.8896\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9426 - loss: 0.1397 - val_accuracy: 0.9450 - val_loss: 0.1346 - mcc: 0.8896\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9463 - loss: 0.1309\n","Epoch 41 - MCC: 0.8901\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9462 - loss: 0.1312 - val_accuracy: 0.9452 - val_loss: 0.1346 - mcc: 0.8901\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9442 - loss: 0.1378\n","Epoch 42 - MCC: 0.8890\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9441 - loss: 0.1378 - val_accuracy: 0.9447 - val_loss: 0.1350 - mcc: 0.8890\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9427 - loss: 0.1392\n","Epoch 43 - MCC: 0.8901\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9427 - loss: 0.1391 - val_accuracy: 0.9453 - val_loss: 0.1339 - mcc: 0.8901\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9420 - loss: 0.1423\n","Epoch 44 - MCC: 0.8903\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9421 - loss: 0.1421 - val_accuracy: 0.9453 - val_loss: 0.1340 - mcc: 0.8903\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9418 - loss: 0.1417\n","Epoch 45 - MCC: 0.8904\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - accuracy: 0.9419 - loss: 0.1415 - val_accuracy: 0.9454 - val_loss: 0.1331 - mcc: 0.8904\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9468 - loss: 0.1309\n","Epoch 46 - MCC: 0.8912\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9467 - loss: 0.1311 - val_accuracy: 0.9458 - val_loss: 0.1318 - mcc: 0.8912\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9487 - loss: 0.1262\n","Epoch 47 - MCC: 0.8887\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9485 - loss: 0.1265 - val_accuracy: 0.9443 - val_loss: 0.1362 - mcc: 0.8887\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9395 - loss: 0.1457\n","Epoch 48 - MCC: 0.8919\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9397 - loss: 0.1454 - val_accuracy: 0.9461 - val_loss: 0.1328 - mcc: 0.8919\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9408 - loss: 0.1435\n","Epoch 49 - MCC: 0.8919\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9410 - loss: 0.1432 - val_accuracy: 0.9462 - val_loss: 0.1316 - mcc: 0.8919\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9486 - loss: 0.1275\n","Epoch 50 - MCC: 0.8924\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9484 - loss: 0.1278 - val_accuracy: 0.9464 - val_loss: 0.1309 - mcc: 0.8924\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 4\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5827 - loss: 0.6832\n","Epoch 1 - MCC: 0.3536\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - accuracy: 0.5842 - loss: 0.6827 - val_accuracy: 0.6624 - val_loss: 0.6371 - mcc: 0.3536\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.6916 - loss: 0.6123\n","Epoch 2 - MCC: 0.6382\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.6931 - loss: 0.6111 - val_accuracy: 0.8176 - val_loss: 0.4938 - mcc: 0.6382\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8160 - loss: 0.4651\n","Epoch 3 - MCC: 0.7519\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.8167 - loss: 0.4632 - val_accuracy: 0.8757 - val_loss: 0.3194 - mcc: 0.7519\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8721 - loss: 0.3116\n","Epoch 4 - MCC: 0.8199\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8726 - loss: 0.3106 - val_accuracy: 0.9101 - val_loss: 0.2429 - mcc: 0.8199\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9039 - loss: 0.2479\n","Epoch 5 - MCC: 0.8334\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9040 - loss: 0.2476 - val_accuracy: 0.9167 - val_loss: 0.2176 - mcc: 0.8334\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9129 - loss: 0.2222\n","Epoch 6 - MCC: 0.8416\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9129 - loss: 0.2221 - val_accuracy: 0.9210 - val_loss: 0.2022 - mcc: 0.8416\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9227 - loss: 0.1956\n","Epoch 7 - MCC: 0.8469\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9224 - loss: 0.1960 - val_accuracy: 0.9231 - val_loss: 0.1942 - mcc: 0.8469\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9151 - loss: 0.2066\n","Epoch 8 - MCC: 0.8470\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9153 - loss: 0.2063 - val_accuracy: 0.9237 - val_loss: 0.1883 - mcc: 0.8470\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9188 - loss: 0.1979\n","Epoch 9 - MCC: 0.8527\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9189 - loss: 0.1976 - val_accuracy: 0.9265 - val_loss: 0.1832 - mcc: 0.8527\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9224 - loss: 0.1885\n","Epoch 10 - MCC: 0.8570\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9225 - loss: 0.1885 - val_accuracy: 0.9286 - val_loss: 0.1774 - mcc: 0.8570\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9237 - loss: 0.1860\n","Epoch 11 - MCC: 0.8567\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.9237 - loss: 0.1859 - val_accuracy: 0.9280 - val_loss: 0.1767 - mcc: 0.8567\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9285 - loss: 0.1761\n","Epoch 12 - MCC: 0.8633\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9284 - loss: 0.1761 - val_accuracy: 0.9317 - val_loss: 0.1697 - mcc: 0.8633\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9249 - loss: 0.1783\n","Epoch 13 - MCC: 0.8678\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9250 - loss: 0.1781 - val_accuracy: 0.9341 - val_loss: 0.1640 - mcc: 0.8678\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9306 - loss: 0.1692\n","Epoch 14 - MCC: 0.8715\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9306 - loss: 0.1692 - val_accuracy: 0.9358 - val_loss: 0.1588 - mcc: 0.8715\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9300 - loss: 0.1678\n","Epoch 15 - MCC: 0.8724\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9300 - loss: 0.1677 - val_accuracy: 0.9363 - val_loss: 0.1582 - mcc: 0.8724\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9276 - loss: 0.1737\n","Epoch 16 - MCC: 0.8727\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9278 - loss: 0.1734 - val_accuracy: 0.9365 - val_loss: 0.1560 - mcc: 0.8727\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9312 - loss: 0.1663\n","Epoch 17 - MCC: 0.8731\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9312 - loss: 0.1662 - val_accuracy: 0.9367 - val_loss: 0.1557 - mcc: 0.8731\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9363 - loss: 0.1548\n","Epoch 18 - MCC: 0.8743\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9362 - loss: 0.1550 - val_accuracy: 0.9373 - val_loss: 0.1537 - mcc: 0.8743\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9327 - loss: 0.1618\n","Epoch 19 - MCC: 0.8792\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9327 - loss: 0.1618 - val_accuracy: 0.9397 - val_loss: 0.1497 - mcc: 0.8792\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9333 - loss: 0.1616\n","Epoch 20 - MCC: 0.8791\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9333 - loss: 0.1614 - val_accuracy: 0.9397 - val_loss: 0.1483 - mcc: 0.8791\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9360 - loss: 0.1546\n","Epoch 21 - MCC: 0.8815\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9360 - loss: 0.1546 - val_accuracy: 0.9408 - val_loss: 0.1461 - mcc: 0.8815\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9380 - loss: 0.1495\n","Epoch 22 - MCC: 0.8829\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9379 - loss: 0.1497 - val_accuracy: 0.9416 - val_loss: 0.1446 - mcc: 0.8829\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9347 - loss: 0.1560\n","Epoch 23 - MCC: 0.8827\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9348 - loss: 0.1559 - val_accuracy: 0.9414 - val_loss: 0.1444 - mcc: 0.8827\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9368 - loss: 0.1520\n","Epoch 24 - MCC: 0.8847\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9368 - loss: 0.1519 - val_accuracy: 0.9423 - val_loss: 0.1429 - mcc: 0.8847\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9343 - loss: 0.1573\n","Epoch 25 - MCC: 0.8865\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9344 - loss: 0.1570 - val_accuracy: 0.9433 - val_loss: 0.1409 - mcc: 0.8865\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9383 - loss: 0.1495\n","Epoch 26 - MCC: 0.8830\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9382 - loss: 0.1495 - val_accuracy: 0.9416 - val_loss: 0.1437 - mcc: 0.8830\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9363 - loss: 0.1533\n","Epoch 27 - MCC: 0.8849\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9363 - loss: 0.1532 - val_accuracy: 0.9426 - val_loss: 0.1410 - mcc: 0.8849\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9345 - loss: 0.1577\n","Epoch 28 - MCC: 0.8848\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9346 - loss: 0.1574 - val_accuracy: 0.9425 - val_loss: 0.1406 - mcc: 0.8848\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9377 - loss: 0.1503\n","Epoch 29 - MCC: 0.8850\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9378 - loss: 0.1501 - val_accuracy: 0.9425 - val_loss: 0.1398 - mcc: 0.8850\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9401 - loss: 0.1461\n","Epoch 30 - MCC: 0.8840\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9400 - loss: 0.1462 - val_accuracy: 0.9421 - val_loss: 0.1416 - mcc: 0.8840\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9394 - loss: 0.1449\n","Epoch 31 - MCC: 0.8851\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9394 - loss: 0.1450 - val_accuracy: 0.9427 - val_loss: 0.1411 - mcc: 0.8851\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9449 - loss: 0.1337\n","Epoch 32 - MCC: 0.8886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9446 - loss: 0.1342 - val_accuracy: 0.9444 - val_loss: 0.1371 - mcc: 0.8886\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9409 - loss: 0.1430\n","Epoch 33 - MCC: 0.8872\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9408 - loss: 0.1431 - val_accuracy: 0.9437 - val_loss: 0.1384 - mcc: 0.8872\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9400 - loss: 0.1462\n","Epoch 34 - MCC: 0.8890\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.9400 - loss: 0.1462 - val_accuracy: 0.9446 - val_loss: 0.1365 - mcc: 0.8890\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9422 - loss: 0.1396\n","Epoch 35 - MCC: 0.8861\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9421 - loss: 0.1398 - val_accuracy: 0.9432 - val_loss: 0.1385 - mcc: 0.8861\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9384 - loss: 0.1473\n","Epoch 36 - MCC: 0.8863\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9385 - loss: 0.1472 - val_accuracy: 0.9431 - val_loss: 0.1394 - mcc: 0.8863\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9415 - loss: 0.1415\n","Epoch 37 - MCC: 0.8854\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9415 - loss: 0.1415 - val_accuracy: 0.9428 - val_loss: 0.1400 - mcc: 0.8854\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9370 - loss: 0.1521\n","Epoch 38 - MCC: 0.8892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.9372 - loss: 0.1518 - val_accuracy: 0.9447 - val_loss: 0.1362 - mcc: 0.8892\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9453 - loss: 0.1312\n","Epoch 39 - MCC: 0.8877\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.9452 - loss: 0.1316 - val_accuracy: 0.9440 - val_loss: 0.1361 - mcc: 0.8877\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9434 - loss: 0.1362\n","Epoch 40 - MCC: 0.8893\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9433 - loss: 0.1364 - val_accuracy: 0.9448 - val_loss: 0.1358 - mcc: 0.8893\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9440 - loss: 0.1357\n","Epoch 41 - MCC: 0.8886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9440 - loss: 0.1359 - val_accuracy: 0.9444 - val_loss: 0.1363 - mcc: 0.8886\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9417 - loss: 0.1403\n","Epoch 42 - MCC: 0.8894\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9417 - loss: 0.1403 - val_accuracy: 0.9448 - val_loss: 0.1362 - mcc: 0.8894\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9399 - loss: 0.1458\n","Epoch 43 - MCC: 0.8881\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9400 - loss: 0.1456 - val_accuracy: 0.9441 - val_loss: 0.1377 - mcc: 0.8881\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9391 - loss: 0.1475\n","Epoch 44 - MCC: 0.8901\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9392 - loss: 0.1473 - val_accuracy: 0.9452 - val_loss: 0.1339 - mcc: 0.8901\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9444 - loss: 0.1355\n","Epoch 45 - MCC: 0.8889\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9443 - loss: 0.1357 - val_accuracy: 0.9446 - val_loss: 0.1349 - mcc: 0.8889\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9453 - loss: 0.1341\n","Epoch 46 - MCC: 0.8906\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9452 - loss: 0.1343 - val_accuracy: 0.9454 - val_loss: 0.1333 - mcc: 0.8906\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9423 - loss: 0.1395\n","Epoch 47 - MCC: 0.8913\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9423 - loss: 0.1394 - val_accuracy: 0.9457 - val_loss: 0.1328 - mcc: 0.8913\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9459 - loss: 0.1320\n","Epoch 48 - MCC: 0.8917\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9458 - loss: 0.1322 - val_accuracy: 0.9460 - val_loss: 0.1315 - mcc: 0.8917\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9453 - loss: 0.1323\n","Epoch 49 - MCC: 0.8899\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9452 - loss: 0.1325 - val_accuracy: 0.9451 - val_loss: 0.1326 - mcc: 0.8899\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9435 - loss: 0.1365\n","Epoch 50 - MCC: 0.8918\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9435 - loss: 0.1365 - val_accuracy: 0.9460 - val_loss: 0.1317 - mcc: 0.8918\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM, Fold: 5\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5992 - loss: 0.6846\n","Epoch 1 - MCC: 0.4025\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - accuracy: 0.6003 - loss: 0.6841 - val_accuracy: 0.6793 - val_loss: 0.6360 - mcc: 0.4025\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7115 - loss: 0.6043\n","Epoch 2 - MCC: 0.6042\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7126 - loss: 0.6031 - val_accuracy: 0.7992 - val_loss: 0.4872 - mcc: 0.6042\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8179 - loss: 0.4487\n","Epoch 3 - MCC: 0.7166\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.8185 - loss: 0.4470 - val_accuracy: 0.8583 - val_loss: 0.3340 - mcc: 0.7166\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8716 - loss: 0.3184\n","Epoch 4 - MCC: 0.7854\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.8720 - loss: 0.3176 - val_accuracy: 0.8928 - val_loss: 0.2737 - mcc: 0.7854\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8983 - loss: 0.2581\n","Epoch 5 - MCC: 0.8102\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.8984 - loss: 0.2579 - val_accuracy: 0.9053 - val_loss: 0.2453 - mcc: 0.8102\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9093 - loss: 0.2355\n","Epoch 6 - MCC: 0.8181\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9093 - loss: 0.2353 - val_accuracy: 0.9091 - val_loss: 0.2302 - mcc: 0.8181\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9165 - loss: 0.2158\n","Epoch 7 - MCC: 0.8225\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9164 - loss: 0.2158 - val_accuracy: 0.9114 - val_loss: 0.2204 - mcc: 0.8225\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9176 - loss: 0.2057\n","Epoch 8 - MCC: 0.8308\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9176 - loss: 0.2056 - val_accuracy: 0.9155 - val_loss: 0.2093 - mcc: 0.8308\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9175 - loss: 0.2028\n","Epoch 9 - MCC: 0.8310\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9176 - loss: 0.2026 - val_accuracy: 0.9150 - val_loss: 0.2067 - mcc: 0.8310\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9222 - loss: 0.1911\n","Epoch 10 - MCC: 0.8344\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9222 - loss: 0.1911 - val_accuracy: 0.9170 - val_loss: 0.2003 - mcc: 0.8344\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9240 - loss: 0.1865\n","Epoch 11 - MCC: 0.8370\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9240 - loss: 0.1865 - val_accuracy: 0.9186 - val_loss: 0.1975 - mcc: 0.8370\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9315 - loss: 0.1699\n","Epoch 12 - MCC: 0.8395\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9313 - loss: 0.1704 - val_accuracy: 0.9199 - val_loss: 0.1921 - mcc: 0.8395\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9238 - loss: 0.1844\n","Epoch 13 - MCC: 0.8395\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9238 - loss: 0.1843 - val_accuracy: 0.9198 - val_loss: 0.1923 - mcc: 0.8395\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9289 - loss: 0.1724\n","Epoch 14 - MCC: 0.8456\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9288 - loss: 0.1725 - val_accuracy: 0.9225 - val_loss: 0.1870 - mcc: 0.8456\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9300 - loss: 0.1701\n","Epoch 15 - MCC: 0.8483\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9300 - loss: 0.1702 - val_accuracy: 0.9242 - val_loss: 0.1826 - mcc: 0.8483\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9283 - loss: 0.1727\n","Epoch 16 - MCC: 0.8501\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9283 - loss: 0.1727 - val_accuracy: 0.9252 - val_loss: 0.1792 - mcc: 0.8501\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9292 - loss: 0.1707\n","Epoch 17 - MCC: 0.8540\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9293 - loss: 0.1705 - val_accuracy: 0.9270 - val_loss: 0.1755 - mcc: 0.8540\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9304 - loss: 0.1691\n","Epoch 18 - MCC: 0.8564\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - accuracy: 0.9305 - loss: 0.1688 - val_accuracy: 0.9280 - val_loss: 0.1728 - mcc: 0.8564\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9309 - loss: 0.1668\n","Epoch 19 - MCC: 0.8557\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9310 - loss: 0.1665 - val_accuracy: 0.9280 - val_loss: 0.1710 - mcc: 0.8557\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9330 - loss: 0.1626\n","Epoch 20 - MCC: 0.8603\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9331 - loss: 0.1624 - val_accuracy: 0.9300 - val_loss: 0.1687 - mcc: 0.8603\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9358 - loss: 0.1569\n","Epoch 21 - MCC: 0.8556\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9359 - loss: 0.1568 - val_accuracy: 0.9277 - val_loss: 0.1731 - mcc: 0.8556\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9365 - loss: 0.1538\n","Epoch 22 - MCC: 0.8583\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9365 - loss: 0.1538 - val_accuracy: 0.9291 - val_loss: 0.1692 - mcc: 0.8583\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9382 - loss: 0.1511\n","Epoch 23 - MCC: 0.8611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9382 - loss: 0.1511 - val_accuracy: 0.9307 - val_loss: 0.1635 - mcc: 0.8611\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9393 - loss: 0.1477\n","Epoch 24 - MCC: 0.8585\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9393 - loss: 0.1478 - val_accuracy: 0.9293 - val_loss: 0.1677 - mcc: 0.8585\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9380 - loss: 0.1505\n","Epoch 25 - MCC: 0.8643\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9380 - loss: 0.1506 - val_accuracy: 0.9321 - val_loss: 0.1627 - mcc: 0.8643\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9407 - loss: 0.1443\n","Epoch 26 - MCC: 0.8606\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9406 - loss: 0.1445 - val_accuracy: 0.9303 - val_loss: 0.1641 - mcc: 0.8606\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9431 - loss: 0.1403\n","Epoch 27 - MCC: 0.8618\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9430 - loss: 0.1406 - val_accuracy: 0.9310 - val_loss: 0.1615 - mcc: 0.8618\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9395 - loss: 0.1463\n","Epoch 28 - MCC: 0.8631\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9395 - loss: 0.1463 - val_accuracy: 0.9317 - val_loss: 0.1611 - mcc: 0.8631\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9402 - loss: 0.1462\n","Epoch 29 - MCC: 0.8641\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9402 - loss: 0.1461 - val_accuracy: 0.9320 - val_loss: 0.1605 - mcc: 0.8641\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9418 - loss: 0.1413\n","Epoch 30 - MCC: 0.8668\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9418 - loss: 0.1414 - val_accuracy: 0.9334 - val_loss: 0.1582 - mcc: 0.8668\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9413 - loss: 0.1430\n","Epoch 31 - MCC: 0.8691\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9413 - loss: 0.1430 - val_accuracy: 0.9346 - val_loss: 0.1557 - mcc: 0.8691\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9444 - loss: 0.1358\n","Epoch 32 - MCC: 0.8673\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9443 - loss: 0.1361 - val_accuracy: 0.9338 - val_loss: 0.1564 - mcc: 0.8673\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9433 - loss: 0.1394\n","Epoch 33 - MCC: 0.8654\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9433 - loss: 0.1395 - val_accuracy: 0.9328 - val_loss: 0.1594 - mcc: 0.8654\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9442 - loss: 0.1363\n","Epoch 34 - MCC: 0.8687\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9441 - loss: 0.1364 - val_accuracy: 0.9343 - val_loss: 0.1567 - mcc: 0.8687\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9427 - loss: 0.1387\n","Epoch 35 - MCC: 0.8703\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9427 - loss: 0.1388 - val_accuracy: 0.9352 - val_loss: 0.1541 - mcc: 0.8703\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9415 - loss: 0.1415\n","Epoch 36 - MCC: 0.8684\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9415 - loss: 0.1414 - val_accuracy: 0.9342 - val_loss: 0.1566 - mcc: 0.8684\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9453 - loss: 0.1333\n","Epoch 37 - MCC: 0.8710\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.9452 - loss: 0.1335 - val_accuracy: 0.9355 - val_loss: 0.1555 - mcc: 0.8710\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9418 - loss: 0.1402\n","Epoch 38 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9419 - loss: 0.1401 - val_accuracy: 0.9351 - val_loss: 0.1547 - mcc: 0.8701\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9449 - loss: 0.1355\n","Epoch 39 - MCC: 0.8714\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9448 - loss: 0.1356 - val_accuracy: 0.9358 - val_loss: 0.1537 - mcc: 0.8714\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9442 - loss: 0.1363\n","Epoch 40 - MCC: 0.8712\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9442 - loss: 0.1363 - val_accuracy: 0.9357 - val_loss: 0.1535 - mcc: 0.8712\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9471 - loss: 0.1299\n","Epoch 41 - MCC: 0.8715\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9470 - loss: 0.1301 - val_accuracy: 0.9358 - val_loss: 0.1533 - mcc: 0.8715\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9457 - loss: 0.1318\n","Epoch 42 - MCC: 0.8699\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9456 - loss: 0.1320 - val_accuracy: 0.9351 - val_loss: 0.1548 - mcc: 0.8699\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9401 - loss: 0.1472\n","Epoch 43 - MCC: 0.8702\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9403 - loss: 0.1468 - val_accuracy: 0.9352 - val_loss: 0.1557 - mcc: 0.8702\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9445 - loss: 0.1362\n","Epoch 44 - MCC: 0.8679\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9444 - loss: 0.1363 - val_accuracy: 0.9336 - val_loss: 0.1608 - mcc: 0.8679\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9462 - loss: 0.1319\n","Epoch 45 - MCC: 0.8721\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9461 - loss: 0.1321 - val_accuracy: 0.9362 - val_loss: 0.1523 - mcc: 0.8721\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9459 - loss: 0.1321\n","Epoch 46 - MCC: 0.8727\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9459 - loss: 0.1322 - val_accuracy: 0.9364 - val_loss: 0.1509 - mcc: 0.8727\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9474 - loss: 0.1286\n","Epoch 47 - MCC: 0.8692\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9473 - loss: 0.1288 - val_accuracy: 0.9347 - val_loss: 0.1552 - mcc: 0.8692\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9423 - loss: 0.1394\n","Epoch 48 - MCC: 0.8741\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9424 - loss: 0.1392 - val_accuracy: 0.9372 - val_loss: 0.1518 - mcc: 0.8741\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9477 - loss: 0.1277\n","Epoch 49 - MCC: 0.8751\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9476 - loss: 0.1278 - val_accuracy: 0.9375 - val_loss: 0.1508 - mcc: 0.8751\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9458 - loss: 0.1336\n","Epoch 50 - MCC: 0.8707\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9458 - loss: 0.1335 - val_accuracy: 0.9355 - val_loss: 0.1533 - mcc: 0.8707\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9465666666666667,\n","              'mean': 0.941308,\n","              'min': 0.93208,\n","              'std': 0.006251511319495285},\n"," 'Inference Time (s/sample)': {'max': 0.004024782180786133,\n","                               'mean': 0.003365718841552735,\n","                               'min': 0.002065732479095459,\n","                               'std': 0.0006748899778691915},\n"," 'MCC': {'max': 0.892755582430129,\n","         'mean': 0.8823138609651424,\n","         'min': 0.8638936057138153,\n","         'std': 0.012459102294416114},\n"," 'Parameters': 8769,\n"," 'Train Time (s)': {'max': 148.18627977371216,\n","                    'mean': 146.4290997028351,\n","                    'min': 143.18174624443054,\n","                    'std': 1.7670069402010664},\n"," 'Training Accuracy': [[0.6119266748428345,\n","                        0.7295984029769897,\n","                        0.8403434753417969,\n","                        0.8845982551574707,\n","                        0.9028732776641846,\n","                        0.9071300029754639,\n","                        0.9104316830635071,\n","                        0.9140033721923828,\n","                        0.9165801405906677,\n","                        0.9180349707603455,\n","                        0.9211398959159851,\n","                        0.9220249652862549,\n","                        0.9243283271789551,\n","                        0.9257466197013855,\n","                        0.9266449809074402,\n","                        0.9292533993721008,\n","                        0.9300065636634827,\n","                        0.9299501180648804,\n","                        0.9320200085639954,\n","                        0.932666540145874,\n","                        0.9327866435050964,\n","                        0.9339067339897156,\n","                        0.93531334400177,\n","                        0.9355533719062805,\n","                        0.9362917542457581,\n","                        0.9362766742706299,\n","                        0.9364782571792603,\n","                        0.9371733069419861,\n","                        0.9376550316810608,\n","                        0.9381248950958252,\n","                        0.9380833506584167,\n","                        0.9382932782173157,\n","                        0.9389132857322693,\n","                        0.9393617510795593,\n","                        0.9396767616271973,\n","                        0.9399683475494385,\n","                        0.9399433135986328,\n","                        0.9403316378593445,\n","                        0.9403783679008484,\n","                        0.9409451484680176,\n","                        0.940998375415802,\n","                        0.9411765933036804,\n","                        0.9408016204833984,\n","                        0.941663384437561,\n","                        0.9426934123039246,\n","                        0.9422948956489563,\n","                        0.9426032900810242,\n","                        0.9420833587646484,\n","                        0.9421233534812927,\n","                        0.9428482055664062],\n","                       [0.6189250349998474,\n","                        0.7293399572372437,\n","                        0.8371784090995789,\n","                        0.8754116892814636,\n","                        0.8935967087745667,\n","                        0.9047148823738098,\n","                        0.912024974822998,\n","                        0.9165832996368408,\n","                        0.9194248914718628,\n","                        0.9234150648117065,\n","                        0.9255750179290771,\n","                        0.9276649951934814,\n","                        0.930358350276947,\n","                        0.9303266406059265,\n","                        0.9323367476463318,\n","                        0.9340516924858093,\n","                        0.9354183673858643,\n","                        0.9358932971954346,\n","                        0.9376184344291687,\n","                        0.9371832013130188,\n","                        0.9385015964508057,\n","                        0.9396399855613708,\n","                        0.940143346786499,\n","                        0.9397983551025391,\n","                        0.9403449892997742,\n","                        0.9406516551971436,\n","                        0.941246747970581,\n","                        0.9413900971412659,\n","                        0.9419800639152527,\n","                        0.9424816966056824,\n","                        0.9429932236671448,\n","                        0.9436633586883545,\n","                        0.9432416558265686,\n","                        0.9437199234962463,\n","                        0.944254994392395,\n","                        0.9442983269691467,\n","                        0.9446917176246643,\n","                        0.9443400502204895,\n","                        0.9455982446670532,\n","                        0.9455083608627319,\n","                        0.9457117319107056,\n","                        0.9452732801437378,\n","                        0.9458749890327454,\n","                        0.946215033531189,\n","                        0.9461817741394043,\n","                        0.9461215734481812,\n","                        0.9462634325027466,\n","                        0.9471350908279419,\n","                        0.9469484090805054,\n","                        0.9467650651931763],\n","                       [0.6456582546234131,\n","                        0.7317900061607361,\n","                        0.8275268077850342,\n","                        0.8841500282287598,\n","                        0.906748354434967,\n","                        0.9127349257469177,\n","                        0.9163282513618469,\n","                        0.9181750416755676,\n","                        0.9207366704940796,\n","                        0.9212750196456909,\n","                        0.9251183867454529,\n","                        0.9271484613418579,\n","                        0.9286250472068787,\n","                        0.9302499890327454,\n","                        0.9313633441925049,\n","                        0.9317882657051086,\n","                        0.9323633313179016,\n","                        0.9332516193389893,\n","                        0.9345566034317017,\n","                        0.9352900981903076,\n","                        0.9351215362548828,\n","                        0.9361783862113953,\n","                        0.9367515444755554,\n","                        0.9371833801269531,\n","                        0.9376633167266846,\n","                        0.9382749795913696,\n","                        0.9387600421905518,\n","                        0.9380849599838257,\n","                        0.9394165873527527,\n","                        0.938839852809906,\n","                        0.9386782646179199,\n","                        0.939903199672699,\n","                        0.9405550360679626,\n","                        0.9401533007621765,\n","                        0.9411615133285522,\n","                        0.9413782358169556,\n","                        0.9415716528892517,\n","                        0.9417199492454529,\n","                        0.9424216747283936,\n","                        0.9428349137306213,\n","                        0.9429133534431458,\n","                        0.9430133700370789,\n","                        0.9432016611099243,\n","                        0.9437999725341797,\n","                        0.9439465999603271,\n","                        0.944040060043335,\n","                        0.944546639919281,\n","                        0.9438999891281128,\n","                        0.9444865584373474,\n","                        0.9452367424964905],\n","                       [0.6220500469207764,\n","                        0.7281949520111084,\n","                        0.8353883624076843,\n","                        0.8839582800865173,\n","                        0.9060100317001343,\n","                        0.9131516218185425,\n","                        0.9159965515136719,\n","                        0.9193716645240784,\n","                        0.9215967059135437,\n","                        0.9227566719055176,\n","                        0.924155056476593,\n","                        0.927294909954071,\n","                        0.9278984069824219,\n","                        0.9302082657814026,\n","                        0.9313600659370422,\n","                        0.9307798743247986,\n","                        0.9326799511909485,\n","                        0.9337016344070435,\n","                        0.9330049753189087,\n","                        0.9347383379936218,\n","                        0.9355950355529785,\n","                        0.935646653175354,\n","                        0.9365883469581604,\n","                        0.9373167157173157,\n","                        0.9376032948493958,\n","                        0.937174916267395,\n","                        0.9378767609596252,\n","                        0.9385049343109131,\n","                        0.9390115141868591,\n","                        0.9395866990089417,\n","                        0.9389615654945374,\n","                        0.9388566613197327,\n","                        0.9398132562637329,\n","                        0.9400866627693176,\n","                        0.9401583671569824,\n","                        0.9403931498527527,\n","                        0.9407016038894653,\n","                        0.940998375415802,\n","                        0.9410200715065002,\n","                        0.9409334063529968,\n","                        0.9418516755104065,\n","                        0.9418751001358032,\n","                        0.9423249959945679,\n","                        0.9414699077606201,\n","                        0.9417599439620972,\n","                        0.9431067705154419,\n","                        0.9435915946960449,\n","                        0.9436751008033752,\n","                        0.9435415863990784,\n","                        0.9441365599632263],\n","                       [0.6260533928871155,\n","                        0.739715039730072,\n","                        0.8347001075744629,\n","                        0.8823032975196838,\n","                        0.9007917046546936,\n","                        0.9090548753738403,\n","                        0.9152333736419678,\n","                        0.917959988117218,\n","                        0.9202466607093811,\n","                        0.9226183295249939,\n","                        0.9236633777618408,\n","                        0.9253082275390625,\n","                        0.9253416657447815,\n","                        0.927163302898407,\n","                        0.928748369216919,\n","                        0.9293016791343689,\n","                        0.9312267303466797,\n","                        0.933003306388855,\n","                        0.9350466728210449,\n","                        0.9357200860977173,\n","                        0.936680018901825,\n","                        0.9362000226974487,\n","                        0.9380233883857727,\n","                        0.9377400279045105,\n","                        0.938154935836792,\n","                        0.9388817548751831,\n","                        0.9399032592773438,\n","                        0.9394199252128601,\n","                        0.9405484199523926,\n","                        0.9403948783874512,\n","                        0.9416699409484863,\n","                        0.9422032833099365,\n","                        0.9418200850486755,\n","                        0.9425565600395203,\n","                        0.9425249695777893,\n","                        0.9431300163269043,\n","                        0.9434200525283813,\n","                        0.9432600140571594,\n","                        0.9432315826416016,\n","                        0.9444550275802612,\n","                        0.9442517161369324,\n","                        0.9438983201980591,\n","                        0.9443649053573608,\n","                        0.9432950615882874,\n","                        0.9447516798973083,\n","                        0.9448899626731873,\n","                        0.9453299641609192,\n","                        0.9451082348823547,\n","                        0.9459702372550964,\n","                        0.9458999633789062]],\n"," 'Training Loss': [[0.6766158938407898,\n","                    0.5874537229537964,\n","                    0.40430697798728943,\n","                    0.2873327136039734,\n","                    0.24939775466918945,\n","                    0.23343119025230408,\n","                    0.22203929722309113,\n","                    0.2116648554801941,\n","                    0.20448185503482819,\n","                    0.20005963742733002,\n","                    0.19283077120780945,\n","                    0.18907928466796875,\n","                    0.18427056074142456,\n","                    0.1805727630853653,\n","                    0.1774047315120697,\n","                    0.1730250120162964,\n","                    0.1709614247083664,\n","                    0.17048078775405884,\n","                    0.16660228371620178,\n","                    0.16494838893413544,\n","                    0.1636318415403366,\n","                    0.16111131012439728,\n","                    0.1586867868900299,\n","                    0.15732166171073914,\n","                    0.15553420782089233,\n","                    0.15518352389335632,\n","                    0.15523755550384521,\n","                    0.1526755839586258,\n","                    0.15204709768295288,\n","                    0.15051907300949097,\n","                    0.15087375044822693,\n","                    0.15012353658676147,\n","                    0.1489110141992569,\n","                    0.1480262130498886,\n","                    0.14679767191410065,\n","                    0.14623485505580902,\n","                    0.14617887139320374,\n","                    0.145085409283638,\n","                    0.14547264575958252,\n","                    0.14448212087154388,\n","                    0.14369957149028778,\n","                    0.14305126667022705,\n","                    0.14411120116710663,\n","                    0.1421031653881073,\n","                    0.14021533727645874,\n","                    0.14050906896591187,\n","                    0.1406933069229126,\n","                    0.14177405834197998,\n","                    0.14118759334087372,\n","                    0.13954728841781616],\n","                   [0.6712031364440918,\n","                    0.5755097270011902,\n","                    0.40062251687049866,\n","                    0.3024104833602905,\n","                    0.2616459131240845,\n","                    0.23479479551315308,\n","                    0.21587805449962616,\n","                    0.20298336446285248,\n","                    0.1939314901828766,\n","                    0.18507002294063568,\n","                    0.18024581670761108,\n","                    0.17399582266807556,\n","                    0.1687849462032318,\n","                    0.16679611802101135,\n","                    0.16338390111923218,\n","                    0.1596718728542328,\n","                    0.15612976253032684,\n","                    0.15518306195735931,\n","                    0.15182872116565704,\n","                    0.15197452902793884,\n","                    0.149479478597641,\n","                    0.14704464375972748,\n","                    0.145956352353096,\n","                    0.14563389122486115,\n","                    0.14414499700069427,\n","                    0.1439901441335678,\n","                    0.14218731224536896,\n","                    0.14275281131267548,\n","                    0.14043477177619934,\n","                    0.13897959887981415,\n","                    0.1386812925338745,\n","                    0.1365654170513153,\n","                    0.1370043158531189,\n","                    0.13621342182159424,\n","                    0.13517022132873535,\n","                    0.13468579947948456,\n","                    0.13409720361232758,\n","                    0.1351361870765686,\n","                    0.13225582242012024,\n","                    0.13282565772533417,\n","                    0.13259154558181763,\n","                    0.1326080858707428,\n","                    0.1315322369337082,\n","                    0.13031457364559174,\n","                    0.1307794153690338,\n","                    0.13055558502674103,\n","                    0.1300080418586731,\n","                    0.12866170704364777,\n","                    0.12834058701992035,\n","                    0.12898221611976624],\n","                   [0.6685076951980591,\n","                    0.5747836232185364,\n","                    0.4231542944908142,\n","                    0.29528555274009705,\n","                    0.23973415791988373,\n","                    0.2205808013677597,\n","                    0.2087378352880478,\n","                    0.20271286368370056,\n","                    0.19537116587162018,\n","                    0.19188757240772247,\n","                    0.18410171568393707,\n","                    0.17768770456314087,\n","                    0.17465153336524963,\n","                    0.16978788375854492,\n","                    0.16716788709163666,\n","                    0.16607743501663208,\n","                    0.16368454694747925,\n","                    0.162566140294075,\n","                    0.1591200977563858,\n","                    0.15654033422470093,\n","                    0.1572202742099762,\n","                    0.15518993139266968,\n","                    0.1531676948070526,\n","                    0.15313611924648285,\n","                    0.1506642997264862,\n","                    0.14984986186027527,\n","                    0.14870306849479675,\n","                    0.14986759424209595,\n","                    0.1476832628250122,\n","                    0.14733310043811798,\n","                    0.14772666990756989,\n","                    0.14643774926662445,\n","                    0.14436113834381104,\n","                    0.14497050642967224,\n","                    0.1432139277458191,\n","                    0.14254598319530487,\n","                    0.14144770801067352,\n","                    0.14193782210350037,\n","                    0.14021074771881104,\n","                    0.1393173187971115,\n","                    0.13930697739124298,\n","                    0.1391163468360901,\n","                    0.1380789577960968,\n","                    0.13758626580238342,\n","                    0.13709485530853271,\n","                    0.13645128905773163,\n","                    0.13563747704029083,\n","                    0.13636085391044617,\n","                    0.135187029838562,\n","                    0.1345318853855133],\n","                   [0.6710624694824219,\n","                    0.5796008706092834,\n","                    0.4160066545009613,\n","                    0.28728774189949036,\n","                    0.24035030603408813,\n","                    0.21888227760791779,\n","                    0.2067016065120697,\n","                    0.19806556403636932,\n","                    0.19087384641170502,\n","                    0.1876147836446762,\n","                    0.18374794721603394,\n","                    0.17660216987133026,\n","                    0.1733347773551941,\n","                    0.1692604124546051,\n","                    0.16616636514663696,\n","                    0.1669674664735794,\n","                    0.16300976276397705,\n","                    0.16050374507904053,\n","                    0.16165000200271606,\n","                    0.15756084024906158,\n","                    0.15533417463302612,\n","                    0.1543169915676117,\n","                    0.15251396596431732,\n","                    0.15162687003612518,\n","                    0.15054382383823395,\n","                    0.15088440477848053,\n","                    0.15022945404052734,\n","                    0.14833568036556244,\n","                    0.14701484143733978,\n","                    0.14697834849357605,\n","                    0.14681190252304077,\n","                    0.14722242951393127,\n","                    0.1450737863779068,\n","                    0.14447559416294098,\n","                    0.14404523372650146,\n","                    0.14361849427223206,\n","                    0.1432264745235443,\n","                    0.14288173615932465,\n","                    0.1421598196029663,\n","                    0.14233706891536713,\n","                    0.14096909761428833,\n","                    0.14032146334648132,\n","                    0.13997197151184082,\n","                    0.14200088381767273,\n","                    0.14123040437698364,\n","                    0.13827981054782867,\n","                    0.13683418929576874,\n","                    0.1372050642967224,\n","                    0.13628989458084106,\n","                    0.13543957471847534],\n","                   [0.6731628179550171,\n","                    0.5726335048675537,\n","                    0.40441688895225525,\n","                    0.29706043004989624,\n","                    0.2531185448169708,\n","                    0.23212389647960663,\n","                    0.215927392244339,\n","                    0.20517511665821075,\n","                    0.19702327251434326,\n","                    0.1906527876853943,\n","                    0.186881884932518,\n","                    0.1825181543827057,\n","                    0.18143413960933685,\n","                    0.17656032741069794,\n","                    0.17306049168109894,\n","                    0.170977383852005,\n","                    0.16634313762187958,\n","                    0.16247062385082245,\n","                    0.15860790014266968,\n","                    0.1564120352268219,\n","                    0.15441708266735077,\n","                    0.15463989973068237,\n","                    0.1513429582118988,\n","                    0.15138529241085052,\n","                    0.15103797614574432,\n","                    0.148258775472641,\n","                    0.1464959979057312,\n","                    0.14656907320022583,\n","                    0.14577065408229828,\n","                    0.14474689960479736,\n","                    0.1423226147890091,\n","                    0.1415720134973526,\n","                    0.14156556129455566,\n","                    0.13984866440296173,\n","                    0.1402590423822403,\n","                    0.13865415751934052,\n","                    0.13771581649780273,\n","                    0.13787180185317993,\n","                    0.13884060084819794,\n","                    0.1362217366695404,\n","                    0.13640648126602173,\n","                    0.13658036291599274,\n","                    0.13626863062381744,\n","                    0.13808287680149078,\n","                    0.13542766869068146,\n","                    0.13391652703285217,\n","                    0.13323260843753815,\n","                    0.13408492505550385,\n","                    0.13201409578323364,\n","                    0.13207000494003296]],\n"," 'Validation Accuracy': [[0.6682466268539429,\n","                          0.8014199733734131,\n","                          0.8768466114997864,\n","                          0.9032333493232727,\n","                          0.9113467335700989,\n","                          0.9138532876968384,\n","                          0.9199466705322266,\n","                          0.921446681022644,\n","                          0.9224200248718262,\n","                          0.9258333444595337,\n","                          0.928453266620636,\n","                          0.9303267002105713,\n","                          0.9325599670410156,\n","                          0.9323733448982239,\n","                          0.9355133175849915,\n","                          0.9350000023841858,\n","                          0.9364466071128845,\n","                          0.9370466470718384,\n","                          0.9370867013931274,\n","                          0.9377267360687256,\n","                          0.9394933581352234,\n","                          0.9403733611106873,\n","                          0.9402466416358948,\n","                          0.9401732683181763,\n","                          0.9418599605560303,\n","                          0.9415733218193054,\n","                          0.9417932629585266,\n","                          0.9416932463645935,\n","                          0.9418399930000305,\n","                          0.9438601136207581,\n","                          0.9436067342758179,\n","                          0.9439466595649719,\n","                          0.943953275680542,\n","                          0.9440933465957642,\n","                          0.9443800449371338,\n","                          0.9453266859054565,\n","                          0.9452533721923828,\n","                          0.9444866180419922,\n","                          0.9449065923690796,\n","                          0.946306586265564,\n","                          0.9456466436386108,\n","                          0.9462933540344238,\n","                          0.9445798993110657,\n","                          0.9464000463485718,\n","                          0.9467399716377258,\n","                          0.9451599717140198,\n","                          0.945533275604248,\n","                          0.9462599754333496,\n","                          0.9439266920089722,\n","                          0.946566641330719],\n","                         [0.641986608505249,\n","                          0.7757132649421692,\n","                          0.8400466442108154,\n","                          0.875760018825531,\n","                          0.8826667070388794,\n","                          0.8948532938957214,\n","                          0.8960933685302734,\n","                          0.9030266404151917,\n","                          0.9077733755111694,\n","                          0.9099066257476807,\n","                          0.9134199619293213,\n","                          0.916159987449646,\n","                          0.914366602897644,\n","                          0.9161533117294312,\n","                          0.9213066697120667,\n","                          0.9230866432189941,\n","                          0.9230066537857056,\n","                          0.9241799712181091,\n","                          0.9248732924461365,\n","                          0.9236932992935181,\n","                          0.9247466921806335,\n","                          0.9270132184028625,\n","                          0.9262199997901917,\n","                          0.9272466897964478,\n","                          0.9250199794769287,\n","                          0.9279866814613342,\n","                          0.9281200170516968,\n","                          0.928493320941925,\n","                          0.9282733798027039,\n","                          0.9294999837875366,\n","                          0.9296467304229736,\n","                          0.9303200244903564,\n","                          0.9303133487701416,\n","                          0.9312866926193237,\n","                          0.9307466745376587,\n","                          0.9312466382980347,\n","                          0.9293733239173889,\n","                          0.9306000471115112,\n","                          0.9313200116157532,\n","                          0.9299067854881287,\n","                          0.9306132793426514,\n","                          0.9311332702636719,\n","                          0.9320132732391357,\n","                          0.9314800500869751,\n","                          0.9326933026313782,\n","                          0.9328733086585999,\n","                          0.9319599866867065,\n","                          0.9326199293136597,\n","                          0.9321000576019287,\n","                          0.9320800304412842],\n","                         [0.6773932576179504,\n","                          0.7934399247169495,\n","                          0.8627133965492249,\n","                          0.9073066711425781,\n","                          0.9141267538070679,\n","                          0.9186199903488159,\n","                          0.922166645526886,\n","                          0.9189134240150452,\n","                          0.9230400323867798,\n","                          0.9266400337219238,\n","                          0.9290466904640198,\n","                          0.9304333329200745,\n","                          0.9332934021949768,\n","                          0.9344733357429504,\n","                          0.9361000061035156,\n","                          0.9376134276390076,\n","                          0.936519980430603,\n","                          0.9385333061218262,\n","                          0.9398133754730225,\n","                          0.9395933747291565,\n","                          0.9375066161155701,\n","                          0.9403799176216125,\n","                          0.9407466053962708,\n","                          0.9404866695404053,\n","                          0.9413266777992249,\n","                          0.9417466521263123,\n","                          0.9422266483306885,\n","                          0.9411399960517883,\n","                          0.9432466626167297,\n","                          0.9404067397117615,\n","                          0.9432533383369446,\n","                          0.9422800540924072,\n","                          0.9430601000785828,\n","                          0.9440132975578308,\n","                          0.9435266852378845,\n","                          0.9438533186912537,\n","                          0.9436132907867432,\n","                          0.9447399973869324,\n","                          0.9448400139808655,\n","                          0.9449799060821533,\n","                          0.9452400207519531,\n","                          0.9446732997894287,\n","                          0.9452599883079529,\n","                          0.9453466534614563,\n","                          0.9454066753387451,\n","                          0.9457999467849731,\n","                          0.9443133473396301,\n","                          0.9461400508880615,\n","                          0.946179986000061,\n","                          0.946399986743927],\n","                         [0.6623666286468506,\n","                          0.8175600171089172,\n","                          0.8756866455078125,\n","                          0.9101132750511169,\n","                          0.9167199730873108,\n","                          0.9209799766540527,\n","                          0.9230800867080688,\n","                          0.923706591129303,\n","                          0.9264534115791321,\n","                          0.9286133646965027,\n","                          0.9280266761779785,\n","                          0.9317266941070557,\n","                          0.9340600371360779,\n","                          0.9358133673667908,\n","                          0.9363000392913818,\n","                          0.9364800453186035,\n","                          0.9366533160209656,\n","                          0.9372934103012085,\n","                          0.939740002155304,\n","                          0.9396733045578003,\n","                          0.9408400058746338,\n","                          0.9415732622146606,\n","                          0.941433310508728,\n","                          0.9423334002494812,\n","                          0.9433133006095886,\n","                          0.9415934085845947,\n","                          0.9425733089447021,\n","                          0.9425466656684875,\n","                          0.9425266981124878,\n","                          0.9421465992927551,\n","                          0.9426599740982056,\n","                          0.9444200396537781,\n","                          0.9436599612236023,\n","                          0.9445866942405701,\n","                          0.9431800246238708,\n","                          0.9431400299072266,\n","                          0.9427933692932129,\n","                          0.9447466135025024,\n","                          0.9440000057220459,\n","                          0.9447799921035767,\n","                          0.944379985332489,\n","                          0.9448201060295105,\n","                          0.9440733194351196,\n","                          0.9451866149902344,\n","                          0.9445533752441406,\n","                          0.9453800320625305,\n","                          0.945746660232544,\n","                          0.9459599256515503,\n","                          0.9450666904449463,\n","                          0.9460399746894836],\n","                         [0.6792666912078857,\n","                          0.7991800904273987,\n","                          0.8582533001899719,\n","                          0.8927799463272095,\n","                          0.9052600860595703,\n","                          0.9090933203697205,\n","                          0.9113532900810242,\n","                          0.9154666066169739,\n","                          0.9150000214576721,\n","                          0.9170399308204651,\n","                          0.9185600280761719,\n","                          0.9199000000953674,\n","                          0.9198333621025085,\n","                          0.9224733710289001,\n","                          0.9242000579833984,\n","                          0.9252000451087952,\n","                          0.9269733428955078,\n","                          0.9279934167861938,\n","                          0.928006649017334,\n","                          0.9300466775894165,\n","                          0.92767333984375,\n","                          0.929080069065094,\n","                          0.9306666851043701,\n","                          0.9292799234390259,\n","                          0.9320533275604248,\n","                          0.9302999973297119,\n","                          0.9310266971588135,\n","                          0.9316733479499817,\n","                          0.9319999814033508,\n","                          0.9333532452583313,\n","                          0.9346200823783875,\n","                          0.9337667226791382,\n","                          0.9327932596206665,\n","                          0.9343066215515137,\n","                          0.9352400302886963,\n","                          0.9342199563980103,\n","                          0.9355400204658508,\n","                          0.9351000189781189,\n","                          0.935753345489502,\n","                          0.9357333183288574,\n","                          0.9358466863632202,\n","                          0.9350866079330444,\n","                          0.9351733326911926,\n","                          0.9335533380508423,\n","                          0.9361534118652344,\n","                          0.9363800287246704,\n","                          0.9346733689308167,\n","                          0.9371800422668457,\n","                          0.9374933838844299,\n","                          0.9354533553123474]],\n"," 'Validation Loss': [[0.6470491290092468,\n","                      0.5033769607543945,\n","                      0.3090415298938751,\n","                      0.25284111499786377,\n","                      0.22700782120227814,\n","                      0.21602308750152588,\n","                      0.20112329721450806,\n","                      0.19638891518115997,\n","                      0.1897394359111786,\n","                      0.18243925273418427,\n","                      0.17642568051815033,\n","                      0.17097915709018707,\n","                      0.1676061600446701,\n","                      0.16550111770629883,\n","                      0.1597256064414978,\n","                      0.15855728089809418,\n","                      0.15602177381515503,\n","                      0.15497057139873505,\n","                      0.15282674133777618,\n","                      0.1513790339231491,\n","                      0.14810295403003693,\n","                      0.14656080305576324,\n","                      0.14551520347595215,\n","                      0.14487577974796295,\n","                      0.14225351810455322,\n","                      0.14224329590797424,\n","                      0.14321236312389374,\n","                      0.14242710173130035,\n","                      0.14239545166492462,\n","                      0.13683150708675385,\n","                      0.137999027967453,\n","                      0.1363016963005066,\n","                      0.1371726095676422,\n","                      0.13691754639148712,\n","                      0.13502731919288635,\n","                      0.13303281366825104,\n","                      0.13338108360767365,\n","                      0.13669897615909576,\n","                      0.13494008779525757,\n","                      0.13108016550540924,\n","                      0.13258635997772217,\n","                      0.13176484405994415,\n","                      0.1345997005701065,\n","                      0.13067932426929474,\n","                      0.13007275760173798,\n","                      0.13410355150699615,\n","                      0.132906973361969,\n","                      0.13203677535057068,\n","                      0.13605475425720215,\n","                      0.12941919267177582],\n","                     [0.644888162612915,\n","                      0.5161376595497131,\n","                      0.3631068468093872,\n","                      0.3021242916584015,\n","                      0.27554967999458313,\n","                      0.24996261298656464,\n","                      0.238062784075737,\n","                      0.2266715168952942,\n","                      0.2174997180700302,\n","                      0.20975802838802338,\n","                      0.20552414655685425,\n","                      0.19877399504184723,\n","                      0.20010067522525787,\n","                      0.1951068639755249,\n","                      0.18829387426376343,\n","                      0.18389073014259338,\n","                      0.1830601841211319,\n","                      0.18276292085647583,\n","                      0.18026494979858398,\n","                      0.18255524337291718,\n","                      0.180466428399086,\n","                      0.17662659287452698,\n","                      0.1776270717382431,\n","                      0.175072580575943,\n","                      0.18148770928382874,\n","                      0.17252111434936523,\n","                      0.17559701204299927,\n","                      0.17213793098926544,\n","                      0.17285571992397308,\n","                      0.17268984019756317,\n","                      0.17067962884902954,\n","                      0.17108221352100372,\n","                      0.16852115094661713,\n","                      0.16910570859909058,\n","                      0.1696489155292511,\n","                      0.16821488738059998,\n","                      0.17098942399024963,\n","                      0.16915163397789001,\n","                      0.16794079542160034,\n","                      0.16950936615467072,\n","                      0.16883954405784607,\n","                      0.16671614348888397,\n","                      0.1657445728778839,\n","                      0.16843725740909576,\n","                      0.16592487692832947,\n","                      0.164401113986969,\n","                      0.16562892496585846,\n","                      0.16661401093006134,\n","                      0.16563871502876282,\n","                      0.16677044332027435],\n","                     [0.6321783661842346,\n","                      0.4945089817047119,\n","                      0.3359231948852539,\n","                      0.24530130624771118,\n","                      0.2198328822851181,\n","                      0.20676162838935852,\n","                      0.19620710611343384,\n","                      0.1975090056657791,\n","                      0.18445684015750885,\n","                      0.17929592728614807,\n","                      0.17172689735889435,\n","                      0.16937686502933502,\n","                      0.16214771568775177,\n","                      0.16107597947120667,\n","                      0.15707038342952728,\n","                      0.1538977473974228,\n","                      0.15511858463287354,\n","                      0.1510237455368042,\n","                      0.14786070585250854,\n","                      0.14854855835437775,\n","                      0.1515326350927353,\n","                      0.14619027078151703,\n","                      0.14342160522937775,\n","                      0.14424638450145721,\n","                      0.14314718544483185,\n","                      0.14194710552692413,\n","                      0.14104756712913513,\n","                      0.14522579312324524,\n","                      0.13875067234039307,\n","                      0.14319002628326416,\n","                      0.1403520703315735,\n","                      0.13912899792194366,\n","                      0.13990145921707153,\n","                      0.13694393634796143,\n","                      0.13693395256996155,\n","                      0.1376621574163437,\n","                      0.13642609119415283,\n","                      0.1357012540102005,\n","                      0.13553652167320251,\n","                      0.1346055269241333,\n","                      0.13464313745498657,\n","                      0.13498596847057343,\n","                      0.1339174062013626,\n","                      0.13403205573558807,\n","                      0.13313041627407074,\n","                      0.131825253367424,\n","                      0.136243537068367,\n","                      0.13276268541812897,\n","                      0.13160187005996704,\n","                      0.13087719678878784],\n","                     [0.6371355056762695,\n","                      0.49380719661712646,\n","                      0.3194385766983032,\n","                      0.24288338422775269,\n","                      0.21757559478282928,\n","                      0.2021632194519043,\n","                      0.1941678673028946,\n","                      0.18829332292079926,\n","                      0.1832103133201599,\n","                      0.17742204666137695,\n","                      0.1767473667860031,\n","                      0.1697445660829544,\n","                      0.1640324741601944,\n","                      0.15883111953735352,\n","                      0.1582438051700592,\n","                      0.1559707224369049,\n","                      0.15570057928562164,\n","                      0.15368135273456573,\n","                      0.14972645044326782,\n","                      0.14834809303283691,\n","                      0.14611287415027618,\n","                      0.14457716047763824,\n","                      0.1444099247455597,\n","                      0.14286087453365326,\n","                      0.14085939526557922,\n","                      0.14371663331985474,\n","                      0.14104916155338287,\n","                      0.14055685698986053,\n","                      0.13980598747730255,\n","                      0.14164048433303833,\n","                      0.1411222219467163,\n","                      0.1371203064918518,\n","                      0.13836459815502167,\n","                      0.1364685446023941,\n","                      0.13849452137947083,\n","                      0.1393578201532364,\n","                      0.13995495438575745,\n","                      0.13621917366981506,\n","                      0.1361299455165863,\n","                      0.1358480602502823,\n","                      0.136262908577919,\n","                      0.1362036168575287,\n","                      0.13769273459911346,\n","                      0.1338685303926468,\n","                      0.13493579626083374,\n","                      0.1333051472902298,\n","                      0.13277481496334076,\n","                      0.13149003684520721,\n","                      0.13264760375022888,\n","                      0.13167959451675415],\n","                     [0.6359960436820984,\n","                      0.4871590733528137,\n","                      0.333983838558197,\n","                      0.27368801832199097,\n","                      0.2452845573425293,\n","                      0.2301589846611023,\n","                      0.22036509215831757,\n","                      0.20933300256729126,\n","                      0.20673206448554993,\n","                      0.20034632086753845,\n","                      0.19754071533679962,\n","                      0.1921302080154419,\n","                      0.19226133823394775,\n","                      0.18699632585048676,\n","                      0.18261706829071045,\n","                      0.1791662722826004,\n","                      0.17548532783985138,\n","                      0.17283934354782104,\n","                      0.17101112008094788,\n","                      0.168683722615242,\n","                      0.17311571538448334,\n","                      0.1691822111606598,\n","                      0.163492351770401,\n","                      0.16773027181625366,\n","                      0.16272594034671783,\n","                      0.16408546268939972,\n","                      0.16152995824813843,\n","                      0.1611240953207016,\n","                      0.16051001846790314,\n","                      0.1581757813692093,\n","                      0.15574030578136444,\n","                      0.15643656253814697,\n","                      0.1593543440103531,\n","                      0.15669140219688416,\n","                      0.1540779173374176,\n","                      0.15662872791290283,\n","                      0.15546712279319763,\n","                      0.15468594431877136,\n","                      0.15370836853981018,\n","                      0.15351037681102753,\n","                      0.1533188372850418,\n","                      0.15476664900779724,\n","                      0.15566380321979523,\n","                      0.1607956886291504,\n","                      0.15229377150535583,\n","                      0.15085746347904205,\n","                      0.15516795217990875,\n","                      0.15180954337120056,\n","                      0.15081806480884552,\n","                      0.15326188504695892]],\n"," 'Validation MCC': [[0.36756651965199033,\n","                     0.6085524399745722,\n","                     0.7535685040852461,\n","                     0.8064725954614281,\n","                     0.821996814494624,\n","                     0.827249062339539,\n","                     0.8396613733362578,\n","                     0.8428861135120543,\n","                     0.8442917054202567,\n","                     0.8511988246752624,\n","                     0.8563492289010323,\n","                     0.8601928625976838,\n","                     0.8648433959681613,\n","                     0.8643148299835429,\n","                     0.8707504302981359,\n","                     0.8698487625415425,\n","                     0.8723996013444139,\n","                     0.8738623738534118,\n","                     0.8737831764764136,\n","                     0.8749714410404407,\n","                     0.8785314753537731,\n","                     0.8803575118959144,\n","                     0.8800467550903661,\n","                     0.8799265104375896,\n","                     0.8835904585273988,\n","                     0.8827177421977956,\n","                     0.8831689665672487,\n","                     0.882968554062682,\n","                     0.8833206408436636,\n","                     0.8872892278394784,\n","                     0.8867872863139062,\n","                     0.8874609731894515,\n","                     0.8875158051470692,\n","                     0.8877554468119405,\n","                     0.8883984876038458,\n","                     0.8902423216374203,\n","                     0.8902602742796758,\n","                     0.8885557747294499,\n","                     0.8893992480847324,\n","                     0.8922477209192001,\n","                     0.8910335124976427,\n","                     0.8921748464956195,\n","                     0.8889832699574034,\n","                     0.8924022995241554,\n","                     0.8930772765148579,\n","                     0.8900765811074635,\n","                     0.8906711576438506,\n","                     0.8922460939379362,\n","                     0.8875638968695094,\n","                     0.892755582430129],\n","                    [0.29959541906131426,\n","                     0.5602325320537126,\n","                     0.682792148490419,\n","                     0.7509093109336955,\n","                     0.7665452966843005,\n","                     0.7891960449573566,\n","                     0.7918111603957539,\n","                     0.8061907002167212,\n","                     0.8153628998234289,\n","                     0.8193853224289412,\n","                     0.82660339952585,\n","                     0.831911520753238,\n","                     0.8288352315693954,\n","                     0.8320991038876471,\n","                     0.8422663156898503,\n","                     0.8458910615624676,\n","                     0.8459182474965911,\n","                     0.8480248402607337,\n","                     0.8493970180879626,\n","                     0.8473606884502679,\n","                     0.849303794297277,\n","                     0.8538694478288168,\n","                     0.8522493103655091,\n","                     0.8543486372515087,\n","                     0.8505265829153905,\n","                     0.8556336506684592,\n","                     0.856336553646761,\n","                     0.8566769521697597,\n","                     0.8562293476095219,\n","                     0.8586720437177896,\n","                     0.8590837327838335,\n","                     0.8603070236709619,\n","                     0.8602934248720339,\n","                     0.8622867353627642,\n","                     0.8611875127130004,\n","                     0.8621647697900968,\n","                     0.8584108510736179,\n","                     0.8609185204083346,\n","                     0.8623751867720103,\n","                     0.8595684085892898,\n","                     0.8609644143980569,\n","                     0.861981042811856,\n","                     0.8637490474185773,\n","                     0.8628568308701203,\n","                     0.8651546055155398,\n","                     0.8655210931288594,\n","                     0.8636344213588952,\n","                     0.8649195134126512,\n","                     0.8640531829579361,\n","                     0.8638936057138153],\n","                    [0.3662975439662921,\n","                     0.5893856099968137,\n","                     0.7280367205017227,\n","                     0.8145849275604135,\n","                     0.8286450899148001,\n","                     0.8369700570886514,\n","                     0.8439464358613737,\n","                     0.8385224285412852,\n","                     0.8459073843887257,\n","                     0.8527168574905478,\n","                     0.8575226989310785,\n","                     0.8606007683610138,\n","                     0.8660272520868643,\n","                     0.8685615992730862,\n","                     0.8716767619558463,\n","                     0.8747891858864252,\n","                     0.8727883902969923,\n","                     0.8766593750433818,\n","                     0.8791963247367031,\n","                     0.8790307996521168,\n","                     0.8749096498393708,\n","                     0.8803367415591388,\n","                     0.8810075664128685,\n","                     0.880485808651014,\n","                     0.8821682850816546,\n","                     0.8831171790585686,\n","                     0.8840469363809774,\n","                     0.8824125845220118,\n","                     0.8860375746958782,\n","                     0.8803350903828314,\n","                     0.8861015075435641,\n","                     0.884144587137821,\n","                     0.8856690343483016,\n","                     0.8875835463095609,\n","                     0.8866623493106824,\n","                     0.887329426989233,\n","                     0.8867921832278312,\n","                     0.8891502997417304,\n","                     0.8893872402217583,\n","                     0.8895591163740786,\n","                     0.8901356919456329,\n","                     0.8890043342117653,\n","                     0.8901073394555282,\n","                     0.8902577298720268,\n","                     0.8903712082555554,\n","                     0.8911523127946781,\n","                     0.8886782450396774,\n","                     0.8918887250438968,\n","                     0.8919234822998945,\n","                     0.8924197383171061],\n","                    [0.35364441801470226,\n","                     0.638216068777369,\n","                     0.7518635574882958,\n","                     0.8199467645167918,\n","                     0.8333763208228236,\n","                     0.8415665543631649,\n","                     0.8468635151311312,\n","                     0.8470341597787973,\n","                     0.8527248410832394,\n","                     0.8569977949140029,\n","                     0.8566762397510312,\n","                     0.8633331302177875,\n","                     0.8678162193934676,\n","                     0.8714856527062628,\n","                     0.8724093772920521,\n","                     0.8726515666510326,\n","                     0.8730504233760725,\n","                     0.8742803881019799,\n","                     0.8791918709835859,\n","                     0.879093585614901,\n","                     0.8814629649082822,\n","                     0.8828808098321349,\n","                     0.8827123503341903,\n","                     0.8846778800510439,\n","                     0.886498505385003,\n","                     0.8829594352463794,\n","                     0.8849103327050716,\n","                     0.8848189707087026,\n","                     0.8850124914904233,\n","                     0.8840147243166926,\n","                     0.8850627868085704,\n","                     0.8886045416206501,\n","                     0.8871755355437787,\n","                     0.8889844793480725,\n","                     0.8861029562339834,\n","                     0.886256556172877,\n","                     0.8854012050662403,\n","                     0.8892468662446565,\n","                     0.8877457983181775,\n","                     0.8893370970078882,\n","                     0.8886049802548888,\n","                     0.8894059776049665,\n","                     0.8880873183819736,\n","                     0.8901246269961133,\n","                     0.8888772345655661,\n","                     0.8906425003504902,\n","                     0.8912600862280877,\n","                     0.891738542702646,\n","                     0.8898753936456857,\n","                     0.8918194665195284],\n","                    [0.402471769131502,\n","                     0.6042001321678616,\n","                     0.7165560576080007,\n","                     0.7853661723036355,\n","                     0.8101883765443002,\n","                     0.8181356366348612,\n","                     0.8224966497637339,\n","                     0.8307943230213202,\n","                     0.8310499797182572,\n","                     0.8343905872274886,\n","                     0.8369592751067696,\n","                     0.8395310473804363,\n","                     0.8394849358890314,\n","                     0.8455656355596742,\n","                     0.8483236813642092,\n","                     0.8501218230981265,\n","                     0.8539876358213364,\n","                     0.8564488416886142,\n","                     0.8557464053831555,\n","                     0.8603218134052188,\n","                     0.855568290089556,\n","                     0.8582895355466155,\n","                     0.8610943498432245,\n","                     0.8584620590730957,\n","                     0.8643169033091267,\n","                     0.8606285097530516,\n","                     0.8618054834925117,\n","                     0.8631016077170574,\n","                     0.8641165441222818,\n","                     0.8667669477480805,\n","                     0.8691084468085527,\n","                     0.8673087418816656,\n","                     0.8654346627069264,\n","                     0.8686642704762803,\n","                     0.870303686803678,\n","                     0.8684130513038928,\n","                     0.8710300318529588,\n","                     0.8700764705986057,\n","                     0.8714068304171348,\n","                     0.8712307787018385,\n","                     0.8715482080170296,\n","                     0.8699364416800094,\n","                     0.8701881746229513,\n","                     0.8678759469426598,\n","                     0.8720857186881985,\n","                     0.8726904450837654,\n","                     0.8692006817168942,\n","                     0.8741390836845595,\n","                     0.8751105130888251,\n","                     0.8706809118451335]]}\n","Training Model: BiLSTM_Dense, Fold: 1\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.5717 - loss: 0.6928\n","Epoch 1 - MCC: 0.4666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.5739 - loss: 0.6924 - val_accuracy: 0.6966 - val_loss: 0.6567 - mcc: 0.4666\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7142 - loss: 0.6399\n","Epoch 2 - MCC: 0.6012\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - accuracy: 0.7149 - loss: 0.6389 - val_accuracy: 0.7892 - val_loss: 0.5390 - mcc: 0.6012\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8103 - loss: 0.4996\n","Epoch 3 - MCC: 0.7836\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.8115 - loss: 0.4973 - val_accuracy: 0.8918 - val_loss: 0.3145 - mcc: 0.7836\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8867 - loss: 0.2991\n","Epoch 4 - MCC: 0.8320\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.8871 - loss: 0.2978 - val_accuracy: 0.9163 - val_loss: 0.2109 - mcc: 0.8320\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9148 - loss: 0.2107\n","Epoch 5 - MCC: 0.8470\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9148 - loss: 0.2106 - val_accuracy: 0.9234 - val_loss: 0.1853 - mcc: 0.8470\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9193 - loss: 0.1945\n","Epoch 6 - MCC: 0.8560\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9193 - loss: 0.1944 - val_accuracy: 0.9282 - val_loss: 0.1721 - mcc: 0.8560\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9288 - loss: 0.1740\n","Epoch 7 - MCC: 0.8564\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9285 - loss: 0.1745 - val_accuracy: 0.9282 - val_loss: 0.1709 - mcc: 0.8564\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9239 - loss: 0.1848\n","Epoch 8 - MCC: 0.8666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9239 - loss: 0.1847 - val_accuracy: 0.9335 - val_loss: 0.1596 - mcc: 0.8666\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9276 - loss: 0.1750\n","Epoch 9 - MCC: 0.8722\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9276 - loss: 0.1750 - val_accuracy: 0.9364 - val_loss: 0.1538 - mcc: 0.8722\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9278 - loss: 0.1745\n","Epoch 10 - MCC: 0.8758\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 278ms/step - accuracy: 0.9279 - loss: 0.1743 - val_accuracy: 0.9382 - val_loss: 0.1491 - mcc: 0.8758\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9337 - loss: 0.1605\n","Epoch 11 - MCC: 0.8761\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 101ms/step - accuracy: 0.9337 - loss: 0.1606 - val_accuracy: 0.9382 - val_loss: 0.1487 - mcc: 0.8761\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9350 - loss: 0.1568\n","Epoch 12 - MCC: 0.8803\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9349 - loss: 0.1570 - val_accuracy: 0.9403 - val_loss: 0.1443 - mcc: 0.8803\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9297 - loss: 0.1687\n","Epoch 13 - MCC: 0.8826\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9298 - loss: 0.1683 - val_accuracy: 0.9414 - val_loss: 0.1435 - mcc: 0.8826\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9401 - loss: 0.1466\n","Epoch 14 - MCC: 0.8845\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9400 - loss: 0.1470 - val_accuracy: 0.9425 - val_loss: 0.1402 - mcc: 0.8845\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9379 - loss: 0.1498\n","Epoch 15 - MCC: 0.8832\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9379 - loss: 0.1500 - val_accuracy: 0.9416 - val_loss: 0.1428 - mcc: 0.8832\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9366 - loss: 0.1530\n","Epoch 16 - MCC: 0.8871\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9366 - loss: 0.1531 - val_accuracy: 0.9438 - val_loss: 0.1365 - mcc: 0.8871\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9406 - loss: 0.1454\n","Epoch 17 - MCC: 0.8890\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9404 - loss: 0.1457 - val_accuracy: 0.9447 - val_loss: 0.1365 - mcc: 0.8890\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9348 - loss: 0.1596\n","Epoch 18 - MCC: 0.8865\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9349 - loss: 0.1593 - val_accuracy: 0.9435 - val_loss: 0.1389 - mcc: 0.8865\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9420 - loss: 0.1405\n","Epoch 19 - MCC: 0.8908\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.9419 - loss: 0.1408 - val_accuracy: 0.9456 - val_loss: 0.1330 - mcc: 0.8908\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9415 - loss: 0.1429\n","Epoch 20 - MCC: 0.8912\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 117ms/step - accuracy: 0.9414 - loss: 0.1430 - val_accuracy: 0.9458 - val_loss: 0.1325 - mcc: 0.8912\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9414 - loss: 0.1440\n","Epoch 21 - MCC: 0.8928\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9413 - loss: 0.1441 - val_accuracy: 0.9466 - val_loss: 0.1301 - mcc: 0.8928\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9403 - loss: 0.1457\n","Epoch 22 - MCC: 0.8921\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9403 - loss: 0.1457 - val_accuracy: 0.9461 - val_loss: 0.1322 - mcc: 0.8921\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9411 - loss: 0.1437\n","Epoch 23 - MCC: 0.8942\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9411 - loss: 0.1437 - val_accuracy: 0.9473 - val_loss: 0.1290 - mcc: 0.8942\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9412 - loss: 0.1433\n","Epoch 24 - MCC: 0.8909\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9412 - loss: 0.1433 - val_accuracy: 0.9455 - val_loss: 0.1338 - mcc: 0.8909\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9392 - loss: 0.1471\n","Epoch 25 - MCC: 0.8950\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - accuracy: 0.9392 - loss: 0.1470 - val_accuracy: 0.9477 - val_loss: 0.1277 - mcc: 0.8950\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9397 - loss: 0.1473\n","Epoch 26 - MCC: 0.8956\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9398 - loss: 0.1471 - val_accuracy: 0.9480 - val_loss: 0.1260 - mcc: 0.8956\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9452 - loss: 0.1336\n","Epoch 27 - MCC: 0.8950\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9451 - loss: 0.1338 - val_accuracy: 0.9477 - val_loss: 0.1265 - mcc: 0.8950\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9434 - loss: 0.1381\n","Epoch 28 - MCC: 0.8974\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9434 - loss: 0.1381 - val_accuracy: 0.9489 - val_loss: 0.1253 - mcc: 0.8974\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9410 - loss: 0.1444\n","Epoch 29 - MCC: 0.8977\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - accuracy: 0.9411 - loss: 0.1442 - val_accuracy: 0.9490 - val_loss: 0.1257 - mcc: 0.8977\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9462 - loss: 0.1308\n","Epoch 30 - MCC: 0.8972\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9461 - loss: 0.1310 - val_accuracy: 0.9488 - val_loss: 0.1251 - mcc: 0.8972\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9464 - loss: 0.1297\n","Epoch 31 - MCC: 0.8969\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.9463 - loss: 0.1300 - val_accuracy: 0.9487 - val_loss: 0.1247 - mcc: 0.8969\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9466 - loss: 0.1306\n","Epoch 32 - MCC: 0.8992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9465 - loss: 0.1308 - val_accuracy: 0.9498 - val_loss: 0.1217 - mcc: 0.8992\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9483 - loss: 0.1279\n","Epoch 33 - MCC: 0.9005\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9481 - loss: 0.1281 - val_accuracy: 0.9504 - val_loss: 0.1222 - mcc: 0.9005\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9434 - loss: 0.1381\n","Epoch 34 - MCC: 0.8973\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9434 - loss: 0.1379 - val_accuracy: 0.9487 - val_loss: 0.1253 - mcc: 0.8973\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9455 - loss: 0.1340\n","Epoch 35 - MCC: 0.8977\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9455 - loss: 0.1340 - val_accuracy: 0.9488 - val_loss: 0.1259 - mcc: 0.8977\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9440 - loss: 0.1375\n","Epoch 36 - MCC: 0.9014\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9441 - loss: 0.1373 - val_accuracy: 0.9508 - val_loss: 0.1201 - mcc: 0.9014\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9409 - loss: 0.1430\n","Epoch 37 - MCC: 0.8986\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9410 - loss: 0.1426 - val_accuracy: 0.9495 - val_loss: 0.1236 - mcc: 0.8986\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9453 - loss: 0.1342\n","Epoch 38 - MCC: 0.9034\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9453 - loss: 0.1341 - val_accuracy: 0.9519 - val_loss: 0.1181 - mcc: 0.9034\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9481 - loss: 0.1270\n","Epoch 39 - MCC: 0.9019\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9481 - loss: 0.1272 - val_accuracy: 0.9510 - val_loss: 0.1209 - mcc: 0.9019\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9438 - loss: 0.1373\n","Epoch 40 - MCC: 0.8972\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9439 - loss: 0.1372 - val_accuracy: 0.9488 - val_loss: 0.1240 - mcc: 0.8972\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9498 - loss: 0.1241\n","Epoch 41 - MCC: 0.9014\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9497 - loss: 0.1244 - val_accuracy: 0.9506 - val_loss: 0.1207 - mcc: 0.9014\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9471 - loss: 0.1313\n","Epoch 42 - MCC: 0.8990\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9471 - loss: 0.1314 - val_accuracy: 0.9496 - val_loss: 0.1238 - mcc: 0.8990\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9469 - loss: 0.1300\n","Epoch 43 - MCC: 0.9014\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - accuracy: 0.9469 - loss: 0.1301 - val_accuracy: 0.9509 - val_loss: 0.1199 - mcc: 0.9014\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9474 - loss: 0.1306\n","Epoch 44 - MCC: 0.9043\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9474 - loss: 0.1305 - val_accuracy: 0.9523 - val_loss: 0.1160 - mcc: 0.9043\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9505 - loss: 0.1222\n","Epoch 45 - MCC: 0.9024\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9504 - loss: 0.1224 - val_accuracy: 0.9514 - val_loss: 0.1181 - mcc: 0.9024\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9482 - loss: 0.1260\n","Epoch 46 - MCC: 0.9030\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9482 - loss: 0.1260 - val_accuracy: 0.9517 - val_loss: 0.1178 - mcc: 0.9030\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9461 - loss: 0.1308\n","Epoch 47 - MCC: 0.9058\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9462 - loss: 0.1306 - val_accuracy: 0.9531 - val_loss: 0.1142 - mcc: 0.9058\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9505 - loss: 0.1232\n","Epoch 48 - MCC: 0.9056\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9504 - loss: 0.1232 - val_accuracy: 0.9530 - val_loss: 0.1137 - mcc: 0.9056\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9521 - loss: 0.1192\n","Epoch 49 - MCC: 0.9074\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9520 - loss: 0.1195 - val_accuracy: 0.9539 - val_loss: 0.1142 - mcc: 0.9074\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9508 - loss: 0.1225\n","Epoch 50 - MCC: 0.9050\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9507 - loss: 0.1226 - val_accuracy: 0.9527 - val_loss: 0.1148 - mcc: 0.9050\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 2\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6036 - loss: 0.6816\n","Epoch 1 - MCC: 0.4943\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 130ms/step - accuracy: 0.6062 - loss: 0.6811 - val_accuracy: 0.7311 - val_loss: 0.6458 - mcc: 0.4943\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7622 - loss: 0.6097\n","Epoch 2 - MCC: 0.5960\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.7629 - loss: 0.6081 - val_accuracy: 0.7945 - val_loss: 0.4920 - mcc: 0.5960\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8400 - loss: 0.4126\n","Epoch 3 - MCC: 0.7423\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.8406 - loss: 0.4110 - val_accuracy: 0.8688 - val_loss: 0.3138 - mcc: 0.7423\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8881 - loss: 0.2687\n","Epoch 4 - MCC: 0.7925\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.8884 - loss: 0.2680 - val_accuracy: 0.8965 - val_loss: 0.2465 - mcc: 0.7925\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9094 - loss: 0.2185\n","Epoch 5 - MCC: 0.8120\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9094 - loss: 0.2183 - val_accuracy: 0.9061 - val_loss: 0.2224 - mcc: 0.8120\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9161 - loss: 0.1988\n","Epoch 6 - MCC: 0.8214\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9161 - loss: 0.1988 - val_accuracy: 0.9106 - val_loss: 0.2132 - mcc: 0.8214\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9219 - loss: 0.1866\n","Epoch 7 - MCC: 0.8214\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9219 - loss: 0.1866 - val_accuracy: 0.9109 - val_loss: 0.2087 - mcc: 0.8214\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9310 - loss: 0.1685\n","Epoch 8 - MCC: 0.8309\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9308 - loss: 0.1689 - val_accuracy: 0.9154 - val_loss: 0.2012 - mcc: 0.8309\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9293 - loss: 0.1707\n","Epoch 9 - MCC: 0.8356\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9292 - loss: 0.1709 - val_accuracy: 0.9178 - val_loss: 0.1969 - mcc: 0.8356\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9293 - loss: 0.1721\n","Epoch 10 - MCC: 0.8334\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9293 - loss: 0.1721 - val_accuracy: 0.9160 - val_loss: 0.2037 - mcc: 0.8334\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9321 - loss: 0.1657\n","Epoch 11 - MCC: 0.8345\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9321 - loss: 0.1657 - val_accuracy: 0.9172 - val_loss: 0.1954 - mcc: 0.8345\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9347 - loss: 0.1580\n","Epoch 12 - MCC: 0.8461\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9347 - loss: 0.1581 - val_accuracy: 0.9229 - val_loss: 0.1856 - mcc: 0.8461\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9343 - loss: 0.1597\n","Epoch 13 - MCC: 0.8497\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9343 - loss: 0.1596 - val_accuracy: 0.9247 - val_loss: 0.1834 - mcc: 0.8497\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9345 - loss: 0.1577\n","Epoch 14 - MCC: 0.8427\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9345 - loss: 0.1577 - val_accuracy: 0.9213 - val_loss: 0.1864 - mcc: 0.8427\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9361 - loss: 0.1558\n","Epoch 15 - MCC: 0.8453\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9362 - loss: 0.1557 - val_accuracy: 0.9226 - val_loss: 0.1834 - mcc: 0.8453\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9381 - loss: 0.1493\n","Epoch 16 - MCC: 0.8534\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9381 - loss: 0.1493 - val_accuracy: 0.9265 - val_loss: 0.1776 - mcc: 0.8534\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9395 - loss: 0.1486\n","Epoch 17 - MCC: 0.8573\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - accuracy: 0.9394 - loss: 0.1487 - val_accuracy: 0.9287 - val_loss: 0.1727 - mcc: 0.8573\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.9400 - loss: 0.1455\n","Epoch 18 - MCC: 0.8531\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 0.9400 - loss: 0.1455 - val_accuracy: 0.9267 - val_loss: 0.1744 - mcc: 0.8531\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9416 - loss: 0.1419\n","Epoch 19 - MCC: 0.8532\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9416 - loss: 0.1421 - val_accuracy: 0.9264 - val_loss: 0.1758 - mcc: 0.8532\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9408 - loss: 0.1450\n","Epoch 20 - MCC: 0.8593\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9409 - loss: 0.1449 - val_accuracy: 0.9298 - val_loss: 0.1702 - mcc: 0.8593\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9450 - loss: 0.1357\n","Epoch 21 - MCC: 0.8572\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9449 - loss: 0.1358 - val_accuracy: 0.9288 - val_loss: 0.1720 - mcc: 0.8572\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9404 - loss: 0.1459\n","Epoch 22 - MCC: 0.8567\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9405 - loss: 0.1457 - val_accuracy: 0.9284 - val_loss: 0.1712 - mcc: 0.8567\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9410 - loss: 0.1421\n","Epoch 23 - MCC: 0.8607\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9411 - loss: 0.1420 - val_accuracy: 0.9305 - val_loss: 0.1661 - mcc: 0.8607\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9422 - loss: 0.1405\n","Epoch 24 - MCC: 0.8640\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9423 - loss: 0.1403 - val_accuracy: 0.9318 - val_loss: 0.1666 - mcc: 0.8640\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9425 - loss: 0.1393\n","Epoch 25 - MCC: 0.8648\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9425 - loss: 0.1392 - val_accuracy: 0.9325 - val_loss: 0.1654 - mcc: 0.8648\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9390 - loss: 0.1471\n","Epoch 26 - MCC: 0.8652\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9392 - loss: 0.1466 - val_accuracy: 0.9328 - val_loss: 0.1633 - mcc: 0.8652\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9474 - loss: 0.1294\n","Epoch 27 - MCC: 0.8666\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9474 - loss: 0.1296 - val_accuracy: 0.9334 - val_loss: 0.1620 - mcc: 0.8666\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9444 - loss: 0.1353\n","Epoch 28 - MCC: 0.8669\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - accuracy: 0.9445 - loss: 0.1352 - val_accuracy: 0.9336 - val_loss: 0.1622 - mcc: 0.8669\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9471 - loss: 0.1287\n","Epoch 29 - MCC: 0.8686\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.9470 - loss: 0.1288 - val_accuracy: 0.9344 - val_loss: 0.1609 - mcc: 0.8686\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9458 - loss: 0.1314\n","Epoch 30 - MCC: 0.8671\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9458 - loss: 0.1313 - val_accuracy: 0.9337 - val_loss: 0.1609 - mcc: 0.8671\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9410 - loss: 0.1444\n","Epoch 31 - MCC: 0.8681\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9413 - loss: 0.1438 - val_accuracy: 0.9341 - val_loss: 0.1620 - mcc: 0.8681\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9471 - loss: 0.1300\n","Epoch 32 - MCC: 0.8692\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - accuracy: 0.9471 - loss: 0.1301 - val_accuracy: 0.9347 - val_loss: 0.1599 - mcc: 0.8692\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9474 - loss: 0.1293\n","Epoch 33 - MCC: 0.8687\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9474 - loss: 0.1293 - val_accuracy: 0.9345 - val_loss: 0.1605 - mcc: 0.8687\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9471 - loss: 0.1297\n","Epoch 34 - MCC: 0.8698\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9471 - loss: 0.1297 - val_accuracy: 0.9350 - val_loss: 0.1587 - mcc: 0.8698\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9498 - loss: 0.1222\n","Epoch 35 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9497 - loss: 0.1225 - val_accuracy: 0.9351 - val_loss: 0.1598 - mcc: 0.8701\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9475 - loss: 0.1287\n","Epoch 36 - MCC: 0.8642\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9475 - loss: 0.1287 - val_accuracy: 0.9322 - val_loss: 0.1653 - mcc: 0.8642\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9463 - loss: 0.1322\n","Epoch 37 - MCC: 0.8719\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9464 - loss: 0.1320 - val_accuracy: 0.9360 - val_loss: 0.1593 - mcc: 0.8719\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9477 - loss: 0.1280\n","Epoch 38 - MCC: 0.8720\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.9477 - loss: 0.1279 - val_accuracy: 0.9360 - val_loss: 0.1593 - mcc: 0.8720\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9492 - loss: 0.1253\n","Epoch 39 - MCC: 0.8713\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9492 - loss: 0.1252 - val_accuracy: 0.9358 - val_loss: 0.1586 - mcc: 0.8713\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9518 - loss: 0.1172\n","Epoch 40 - MCC: 0.8726\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9517 - loss: 0.1175 - val_accuracy: 0.9364 - val_loss: 0.1571 - mcc: 0.8726\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9479 - loss: 0.1268\n","Epoch 41 - MCC: 0.8715\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9480 - loss: 0.1267 - val_accuracy: 0.9359 - val_loss: 0.1571 - mcc: 0.8715\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9493 - loss: 0.1249\n","Epoch 42 - MCC: 0.8741\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - accuracy: 0.9493 - loss: 0.1248 - val_accuracy: 0.9372 - val_loss: 0.1577 - mcc: 0.8741\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9493 - loss: 0.1229\n","Epoch 43 - MCC: 0.8736\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.9493 - loss: 0.1229 - val_accuracy: 0.9369 - val_loss: 0.1555 - mcc: 0.8736\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9498 - loss: 0.1225\n","Epoch 44 - MCC: 0.8720\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9498 - loss: 0.1226 - val_accuracy: 0.9361 - val_loss: 0.1578 - mcc: 0.8720\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9485 - loss: 0.1262\n","Epoch 45 - MCC: 0.8718\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9485 - loss: 0.1262 - val_accuracy: 0.9360 - val_loss: 0.1574 - mcc: 0.8718\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9514 - loss: 0.1189\n","Epoch 46 - MCC: 0.8692\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9513 - loss: 0.1190 - val_accuracy: 0.9347 - val_loss: 0.1596 - mcc: 0.8692\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9484 - loss: 0.1267\n","Epoch 47 - MCC: 0.8686\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9485 - loss: 0.1265 - val_accuracy: 0.9343 - val_loss: 0.1610 - mcc: 0.8686\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9519 - loss: 0.1183\n","Epoch 48 - MCC: 0.8751\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9519 - loss: 0.1184 - val_accuracy: 0.9376 - val_loss: 0.1547 - mcc: 0.8751\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9545 - loss: 0.1129\n","Epoch 49 - MCC: 0.8741\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9544 - loss: 0.1132 - val_accuracy: 0.9372 - val_loss: 0.1557 - mcc: 0.8741\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9506 - loss: 0.1214\n","Epoch 50 - MCC: 0.8715\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9506 - loss: 0.1213 - val_accuracy: 0.9358 - val_loss: 0.1577 - mcc: 0.8715\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 3\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6267 - loss: 0.6791\n","Epoch 1 - MCC: 0.5517\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - accuracy: 0.6295 - loss: 0.6785 - val_accuracy: 0.7725 - val_loss: 0.6238 - mcc: 0.5517\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7689 - loss: 0.5945\n","Epoch 2 - MCC: 0.6612\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - accuracy: 0.7696 - loss: 0.5928 - val_accuracy: 0.8309 - val_loss: 0.4472 - mcc: 0.6612\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8527 - loss: 0.3963\n","Epoch 3 - MCC: 0.8017\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8532 - loss: 0.3944 - val_accuracy: 0.9010 - val_loss: 0.2440 - mcc: 0.8017\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9107 - loss: 0.2253\n","Epoch 4 - MCC: 0.8455\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.9107 - loss: 0.2250 - val_accuracy: 0.9231 - val_loss: 0.1895 - mcc: 0.8455\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9224 - loss: 0.1903\n","Epoch 5 - MCC: 0.8488\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9223 - loss: 0.1904 - val_accuracy: 0.9241 - val_loss: 0.1828 - mcc: 0.8488\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9251 - loss: 0.1824\n","Epoch 6 - MCC: 0.8611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9251 - loss: 0.1823 - val_accuracy: 0.9308 - val_loss: 0.1673 - mcc: 0.8611\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9319 - loss: 0.1653\n","Epoch 7 - MCC: 0.8618\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.9317 - loss: 0.1656 - val_accuracy: 0.9307 - val_loss: 0.1662 - mcc: 0.8618\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9313 - loss: 0.1655\n","Epoch 8 - MCC: 0.8700\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9313 - loss: 0.1656 - val_accuracy: 0.9352 - val_loss: 0.1574 - mcc: 0.8700\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9343 - loss: 0.1585\n","Epoch 9 - MCC: 0.8736\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9341 - loss: 0.1587 - val_accuracy: 0.9370 - val_loss: 0.1520 - mcc: 0.8736\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9340 - loss: 0.1593\n","Epoch 10 - MCC: 0.8778\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9340 - loss: 0.1593 - val_accuracy: 0.9391 - val_loss: 0.1500 - mcc: 0.8778\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9319 - loss: 0.1645\n","Epoch 11 - MCC: 0.8753\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - accuracy: 0.9320 - loss: 0.1643 - val_accuracy: 0.9379 - val_loss: 0.1506 - mcc: 0.8753\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9358 - loss: 0.1553\n","Epoch 12 - MCC: 0.8788\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9358 - loss: 0.1553 - val_accuracy: 0.9396 - val_loss: 0.1473 - mcc: 0.8788\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9345 - loss: 0.1575\n","Epoch 13 - MCC: 0.8795\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9346 - loss: 0.1574 - val_accuracy: 0.9398 - val_loss: 0.1485 - mcc: 0.8795\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9353 - loss: 0.1580\n","Epoch 14 - MCC: 0.8827\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9354 - loss: 0.1578 - val_accuracy: 0.9416 - val_loss: 0.1434 - mcc: 0.8827\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9342 - loss: 0.1578\n","Epoch 15 - MCC: 0.8831\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9344 - loss: 0.1575 - val_accuracy: 0.9417 - val_loss: 0.1424 - mcc: 0.8831\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9367 - loss: 0.1544\n","Epoch 16 - MCC: 0.8796\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9367 - loss: 0.1542 - val_accuracy: 0.9400 - val_loss: 0.1458 - mcc: 0.8796\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9435 - loss: 0.1397\n","Epoch 17 - MCC: 0.8853\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9433 - loss: 0.1401 - val_accuracy: 0.9428 - val_loss: 0.1406 - mcc: 0.8853\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9450 - loss: 0.1346\n","Epoch 18 - MCC: 0.8862\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9448 - loss: 0.1351 - val_accuracy: 0.9433 - val_loss: 0.1396 - mcc: 0.8862\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9414 - loss: 0.1433\n","Epoch 19 - MCC: 0.8880\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.9414 - loss: 0.1433 - val_accuracy: 0.9442 - val_loss: 0.1385 - mcc: 0.8880\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9449 - loss: 0.1361\n","Epoch 20 - MCC: 0.8852\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - accuracy: 0.9447 - loss: 0.1364 - val_accuracy: 0.9427 - val_loss: 0.1408 - mcc: 0.8852\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9420 - loss: 0.1414\n","Epoch 21 - MCC: 0.8864\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9420 - loss: 0.1415 - val_accuracy: 0.9434 - val_loss: 0.1390 - mcc: 0.8864\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9396 - loss: 0.1488\n","Epoch 22 - MCC: 0.8872\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9397 - loss: 0.1486 - val_accuracy: 0.9437 - val_loss: 0.1381 - mcc: 0.8872\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9409 - loss: 0.1428\n","Epoch 23 - MCC: 0.8889\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9410 - loss: 0.1428 - val_accuracy: 0.9447 - val_loss: 0.1362 - mcc: 0.8889\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9370 - loss: 0.1532\n","Epoch 24 - MCC: 0.8900\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9372 - loss: 0.1528 - val_accuracy: 0.9452 - val_loss: 0.1354 - mcc: 0.8900\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9443 - loss: 0.1386\n","Epoch 25 - MCC: 0.8912\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9442 - loss: 0.1386 - val_accuracy: 0.9458 - val_loss: 0.1340 - mcc: 0.8912\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9448 - loss: 0.1354\n","Epoch 26 - MCC: 0.8908\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9447 - loss: 0.1357 - val_accuracy: 0.9456 - val_loss: 0.1332 - mcc: 0.8908\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9478 - loss: 0.1288\n","Epoch 27 - MCC: 0.8888\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9477 - loss: 0.1292 - val_accuracy: 0.9445 - val_loss: 0.1368 - mcc: 0.8888\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9454 - loss: 0.1339\n","Epoch 28 - MCC: 0.8893\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9453 - loss: 0.1341 - val_accuracy: 0.9449 - val_loss: 0.1355 - mcc: 0.8893\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9405 - loss: 0.1440\n","Epoch 29 - MCC: 0.8919\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9406 - loss: 0.1438 - val_accuracy: 0.9462 - val_loss: 0.1331 - mcc: 0.8919\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9420 - loss: 0.1444\n","Epoch 30 - MCC: 0.8934\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9420 - loss: 0.1442 - val_accuracy: 0.9469 - val_loss: 0.1328 - mcc: 0.8934\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9457 - loss: 0.1324\n","Epoch 31 - MCC: 0.8955\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9456 - loss: 0.1326 - val_accuracy: 0.9480 - val_loss: 0.1296 - mcc: 0.8955\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9467 - loss: 0.1304\n","Epoch 32 - MCC: 0.8939\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9466 - loss: 0.1305 - val_accuracy: 0.9472 - val_loss: 0.1301 - mcc: 0.8939\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9495 - loss: 0.1246\n","Epoch 33 - MCC: 0.8956\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9494 - loss: 0.1249 - val_accuracy: 0.9480 - val_loss: 0.1292 - mcc: 0.8956\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9479 - loss: 0.1269\n","Epoch 34 - MCC: 0.8898\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9478 - loss: 0.1272 - val_accuracy: 0.9449 - val_loss: 0.1343 - mcc: 0.8898\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9466 - loss: 0.1335\n","Epoch 35 - MCC: 0.8929\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9466 - loss: 0.1335 - val_accuracy: 0.9466 - val_loss: 0.1334 - mcc: 0.8929\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9473 - loss: 0.1286\n","Epoch 36 - MCC: 0.8920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9473 - loss: 0.1288 - val_accuracy: 0.9461 - val_loss: 0.1315 - mcc: 0.8920\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9502 - loss: 0.1234\n","Epoch 37 - MCC: 0.8956\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.9501 - loss: 0.1237 - val_accuracy: 0.9480 - val_loss: 0.1278 - mcc: 0.8956\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9481 - loss: 0.1271\n","Epoch 38 - MCC: 0.8962\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - accuracy: 0.9480 - loss: 0.1272 - val_accuracy: 0.9483 - val_loss: 0.1279 - mcc: 0.8962\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9488 - loss: 0.1284\n","Epoch 39 - MCC: 0.8958\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - accuracy: 0.9487 - loss: 0.1285 - val_accuracy: 0.9481 - val_loss: 0.1274 - mcc: 0.8958\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9460 - loss: 0.1328\n","Epoch 40 - MCC: 0.8955\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9460 - loss: 0.1327 - val_accuracy: 0.9479 - val_loss: 0.1294 - mcc: 0.8955\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9482 - loss: 0.1276\n","Epoch 41 - MCC: 0.8946\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9481 - loss: 0.1277 - val_accuracy: 0.9475 - val_loss: 0.1294 - mcc: 0.8946\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9453 - loss: 0.1354\n","Epoch 42 - MCC: 0.8920\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9453 - loss: 0.1352 - val_accuracy: 0.9461 - val_loss: 0.1334 - mcc: 0.8920\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9491 - loss: 0.1252\n","Epoch 43 - MCC: 0.8979\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9490 - loss: 0.1254 - val_accuracy: 0.9492 - val_loss: 0.1258 - mcc: 0.8979\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9519 - loss: 0.1205\n","Epoch 44 - MCC: 0.8976\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9518 - loss: 0.1207 - val_accuracy: 0.9490 - val_loss: 0.1256 - mcc: 0.8976\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9484 - loss: 0.1265\n","Epoch 45 - MCC: 0.8957\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9484 - loss: 0.1265 - val_accuracy: 0.9481 - val_loss: 0.1271 - mcc: 0.8957\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9502 - loss: 0.1230\n","Epoch 46 - MCC: 0.8988\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9501 - loss: 0.1232 - val_accuracy: 0.9496 - val_loss: 0.1252 - mcc: 0.8988\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9537 - loss: 0.1148\n","Epoch 47 - MCC: 0.8979\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9535 - loss: 0.1153 - val_accuracy: 0.9491 - val_loss: 0.1252 - mcc: 0.8979\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9505 - loss: 0.1208\n","Epoch 48 - MCC: 0.8952\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9505 - loss: 0.1210 - val_accuracy: 0.9478 - val_loss: 0.1296 - mcc: 0.8952\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9479 - loss: 0.1278\n","Epoch 49 - MCC: 0.8980\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9479 - loss: 0.1278 - val_accuracy: 0.9492 - val_loss: 0.1265 - mcc: 0.8980\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9477 - loss: 0.1285\n","Epoch 50 - MCC: 0.8965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9478 - loss: 0.1285 - val_accuracy: 0.9484 - val_loss: 0.1281 - mcc: 0.8965\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 4\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6802 - loss: 0.6813\n","Epoch 1 - MCC: 0.5405\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.6819 - loss: 0.6808 - val_accuracy: 0.7401 - val_loss: 0.6339 - mcc: 0.5405\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7481 - loss: 0.6012\n","Epoch 2 - MCC: 0.7275\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.7490 - loss: 0.5998 - val_accuracy: 0.8597 - val_loss: 0.4611 - mcc: 0.7275\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8548 - loss: 0.4132\n","Epoch 3 - MCC: 0.7992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.8555 - loss: 0.4109 - val_accuracy: 0.8997 - val_loss: 0.2545 - mcc: 0.7992\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9020 - loss: 0.2419\n","Epoch 4 - MCC: 0.8269\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9020 - loss: 0.2417 - val_accuracy: 0.9136 - val_loss: 0.2112 - mcc: 0.8269\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9167 - loss: 0.2035\n","Epoch 5 - MCC: 0.8440\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9166 - loss: 0.2036 - val_accuracy: 0.9220 - val_loss: 0.1911 - mcc: 0.8440\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9189 - loss: 0.1967\n","Epoch 6 - MCC: 0.8507\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9190 - loss: 0.1965 - val_accuracy: 0.9251 - val_loss: 0.1824 - mcc: 0.8507\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9251 - loss: 0.1819\n","Epoch 7 - MCC: 0.8566\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9250 - loss: 0.1821 - val_accuracy: 0.9282 - val_loss: 0.1739 - mcc: 0.8566\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9215 - loss: 0.1885\n","Epoch 8 - MCC: 0.8608\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9216 - loss: 0.1882 - val_accuracy: 0.9304 - val_loss: 0.1688 - mcc: 0.8608\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9291 - loss: 0.1707\n","Epoch 9 - MCC: 0.8624\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - accuracy: 0.9289 - loss: 0.1711 - val_accuracy: 0.9314 - val_loss: 0.1655 - mcc: 0.8624\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9305 - loss: 0.1672\n","Epoch 10 - MCC: 0.8649\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9304 - loss: 0.1674 - val_accuracy: 0.9325 - val_loss: 0.1636 - mcc: 0.8649\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9276 - loss: 0.1715\n","Epoch 11 - MCC: 0.8675\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9277 - loss: 0.1714 - val_accuracy: 0.9337 - val_loss: 0.1614 - mcc: 0.8675\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9261 - loss: 0.1754\n","Epoch 12 - MCC: 0.8630\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9263 - loss: 0.1751 - val_accuracy: 0.9309 - val_loss: 0.1672 - mcc: 0.8630\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9325 - loss: 0.1624\n","Epoch 13 - MCC: 0.8708\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9325 - loss: 0.1626 - val_accuracy: 0.9355 - val_loss: 0.1573 - mcc: 0.8708\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9271 - loss: 0.1728\n","Epoch 14 - MCC: 0.8719\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - accuracy: 0.9273 - loss: 0.1724 - val_accuracy: 0.9358 - val_loss: 0.1580 - mcc: 0.8719\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9336 - loss: 0.1616\n","Epoch 15 - MCC: 0.8714\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.9336 - loss: 0.1616 - val_accuracy: 0.9358 - val_loss: 0.1550 - mcc: 0.8714\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9332 - loss: 0.1614\n","Epoch 16 - MCC: 0.8722\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9332 - loss: 0.1614 - val_accuracy: 0.9357 - val_loss: 0.1580 - mcc: 0.8722\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9296 - loss: 0.1698\n","Epoch 17 - MCC: 0.8728\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.9297 - loss: 0.1695 - val_accuracy: 0.9365 - val_loss: 0.1539 - mcc: 0.8728\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9342 - loss: 0.1586\n","Epoch 18 - MCC: 0.8769\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.9342 - loss: 0.1586 - val_accuracy: 0.9385 - val_loss: 0.1505 - mcc: 0.8769\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9334 - loss: 0.1615\n","Epoch 19 - MCC: 0.8756\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9334 - loss: 0.1614 - val_accuracy: 0.9376 - val_loss: 0.1522 - mcc: 0.8756\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9407 - loss: 0.1461\n","Epoch 20 - MCC: 0.8761\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.9405 - loss: 0.1465 - val_accuracy: 0.9382 - val_loss: 0.1503 - mcc: 0.8761\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9390 - loss: 0.1490\n","Epoch 21 - MCC: 0.8766\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9388 - loss: 0.1492 - val_accuracy: 0.9380 - val_loss: 0.1510 - mcc: 0.8766\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9353 - loss: 0.1579\n","Epoch 22 - MCC: 0.8829\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9353 - loss: 0.1577 - val_accuracy: 0.9415 - val_loss: 0.1441 - mcc: 0.8829\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9395 - loss: 0.1479\n","Epoch 23 - MCC: 0.8797\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - accuracy: 0.9394 - loss: 0.1482 - val_accuracy: 0.9400 - val_loss: 0.1468 - mcc: 0.8797\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9397 - loss: 0.1468\n","Epoch 24 - MCC: 0.8810\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9396 - loss: 0.1471 - val_accuracy: 0.9406 - val_loss: 0.1453 - mcc: 0.8810\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9439 - loss: 0.1385\n","Epoch 25 - MCC: 0.8841\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9437 - loss: 0.1391 - val_accuracy: 0.9421 - val_loss: 0.1413 - mcc: 0.8841\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9361 - loss: 0.1554\n","Epoch 26 - MCC: 0.8874\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9362 - loss: 0.1551 - val_accuracy: 0.9438 - val_loss: 0.1384 - mcc: 0.8874\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9403 - loss: 0.1461\n","Epoch 27 - MCC: 0.8853\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9403 - loss: 0.1461 - val_accuracy: 0.9428 - val_loss: 0.1387 - mcc: 0.8853\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9394 - loss: 0.1472\n","Epoch 28 - MCC: 0.8907\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9395 - loss: 0.1471 - val_accuracy: 0.9454 - val_loss: 0.1355 - mcc: 0.8907\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9426 - loss: 0.1395\n","Epoch 29 - MCC: 0.8899\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9426 - loss: 0.1396 - val_accuracy: 0.9451 - val_loss: 0.1348 - mcc: 0.8899\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9422 - loss: 0.1419\n","Epoch 30 - MCC: 0.8899\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9422 - loss: 0.1419 - val_accuracy: 0.9449 - val_loss: 0.1362 - mcc: 0.8899\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9454 - loss: 0.1345\n","Epoch 31 - MCC: 0.8915\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9452 - loss: 0.1348 - val_accuracy: 0.9457 - val_loss: 0.1353 - mcc: 0.8915\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9392 - loss: 0.1473\n","Epoch 32 - MCC: 0.8949\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9393 - loss: 0.1470 - val_accuracy: 0.9475 - val_loss: 0.1297 - mcc: 0.8949\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9450 - loss: 0.1351\n","Epoch 33 - MCC: 0.8929\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9449 - loss: 0.1353 - val_accuracy: 0.9466 - val_loss: 0.1315 - mcc: 0.8929\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9440 - loss: 0.1362\n","Epoch 34 - MCC: 0.8957\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9440 - loss: 0.1363 - val_accuracy: 0.9479 - val_loss: 0.1286 - mcc: 0.8957\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9434 - loss: 0.1399\n","Epoch 35 - MCC: 0.8924\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9435 - loss: 0.1398 - val_accuracy: 0.9463 - val_loss: 0.1312 - mcc: 0.8924\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9449 - loss: 0.1366\n","Epoch 36 - MCC: 0.8965\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9449 - loss: 0.1365 - val_accuracy: 0.9483 - val_loss: 0.1283 - mcc: 0.8965\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9432 - loss: 0.1387\n","Epoch 37 - MCC: 0.8964\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - accuracy: 0.9433 - loss: 0.1385 - val_accuracy: 0.9483 - val_loss: 0.1282 - mcc: 0.8964\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9491 - loss: 0.1258\n","Epoch 38 - MCC: 0.8957\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9490 - loss: 0.1262 - val_accuracy: 0.9480 - val_loss: 0.1277 - mcc: 0.8957\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9443 - loss: 0.1353\n","Epoch 39 - MCC: 0.8974\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9444 - loss: 0.1352 - val_accuracy: 0.9488 - val_loss: 0.1255 - mcc: 0.8974\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9479 - loss: 0.1289\n","Epoch 40 - MCC: 0.8958\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9478 - loss: 0.1291 - val_accuracy: 0.9480 - val_loss: 0.1270 - mcc: 0.8958\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9454 - loss: 0.1336\n","Epoch 41 - MCC: 0.8996\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9454 - loss: 0.1336 - val_accuracy: 0.9498 - val_loss: 0.1241 - mcc: 0.8996\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9457 - loss: 0.1345\n","Epoch 42 - MCC: 0.8986\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.9457 - loss: 0.1344 - val_accuracy: 0.9493 - val_loss: 0.1256 - mcc: 0.8986\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9463 - loss: 0.1329\n","Epoch 43 - MCC: 0.8941\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9463 - loss: 0.1329 - val_accuracy: 0.9471 - val_loss: 0.1291 - mcc: 0.8941\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9441 - loss: 0.1376\n","Epoch 44 - MCC: 0.8971\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9442 - loss: 0.1375 - val_accuracy: 0.9487 - val_loss: 0.1263 - mcc: 0.8971\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9420 - loss: 0.1417\n","Epoch 45 - MCC: 0.8973\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9421 - loss: 0.1414 - val_accuracy: 0.9488 - val_loss: 0.1266 - mcc: 0.8973\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9469 - loss: 0.1311\n","Epoch 46 - MCC: 0.8998\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - accuracy: 0.9469 - loss: 0.1311 - val_accuracy: 0.9499 - val_loss: 0.1247 - mcc: 0.8998\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9467 - loss: 0.1307\n","Epoch 47 - MCC: 0.9002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9467 - loss: 0.1307 - val_accuracy: 0.9502 - val_loss: 0.1231 - mcc: 0.9002\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9486 - loss: 0.1275\n","Epoch 48 - MCC: 0.9008\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9486 - loss: 0.1275 - val_accuracy: 0.9505 - val_loss: 0.1221 - mcc: 0.9008\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9495 - loss: 0.1241\n","Epoch 49 - MCC: 0.8993\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9494 - loss: 0.1243 - val_accuracy: 0.9496 - val_loss: 0.1248 - mcc: 0.8993\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9467 - loss: 0.1310\n","Epoch 50 - MCC: 0.8994\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9468 - loss: 0.1308 - val_accuracy: 0.9498 - val_loss: 0.1226 - mcc: 0.8994\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Dense, Fold: 5\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6344 - loss: 0.6817\n","Epoch 1 - MCC: 0.5520\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 0.6367 - loss: 0.6813 - val_accuracy: 0.7518 - val_loss: 0.6396 - mcc: 0.5520\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7657 - loss: 0.6125\n","Epoch 2 - MCC: 0.6401\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 0.7661 - loss: 0.6113 - val_accuracy: 0.8164 - val_loss: 0.4777 - mcc: 0.6401\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8490 - loss: 0.4272\n","Epoch 3 - MCC: 0.7784\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.8497 - loss: 0.4252 - val_accuracy: 0.8883 - val_loss: 0.2841 - mcc: 0.7784\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9026 - loss: 0.2527\n","Epoch 4 - MCC: 0.8187\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9027 - loss: 0.2523 - val_accuracy: 0.9082 - val_loss: 0.2261 - mcc: 0.8187\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9254 - loss: 0.1915\n","Epoch 5 - MCC: 0.8288\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9251 - loss: 0.1920 - val_accuracy: 0.9134 - val_loss: 0.2119 - mcc: 0.8288\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9218 - loss: 0.1914\n","Epoch 6 - MCC: 0.8405\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9219 - loss: 0.1913 - val_accuracy: 0.9203 - val_loss: 0.1935 - mcc: 0.8405\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9238 - loss: 0.1863\n","Epoch 7 - MCC: 0.8431\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9238 - loss: 0.1861 - val_accuracy: 0.9210 - val_loss: 0.1930 - mcc: 0.8431\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9228 - loss: 0.1853\n","Epoch 8 - MCC: 0.8400\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9229 - loss: 0.1852 - val_accuracy: 0.9201 - val_loss: 0.1914 - mcc: 0.8400\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9269 - loss: 0.1767\n","Epoch 9 - MCC: 0.8495\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9270 - loss: 0.1765 - val_accuracy: 0.9248 - val_loss: 0.1797 - mcc: 0.8495\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9281 - loss: 0.1732\n","Epoch 10 - MCC: 0.8508\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.9282 - loss: 0.1729 - val_accuracy: 0.9254 - val_loss: 0.1780 - mcc: 0.8508\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9329 - loss: 0.1645\n","Epoch 11 - MCC: 0.8564\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9329 - loss: 0.1645 - val_accuracy: 0.9282 - val_loss: 0.1743 - mcc: 0.8564\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9376 - loss: 0.1536\n","Epoch 12 - MCC: 0.8563\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9374 - loss: 0.1540 - val_accuracy: 0.9279 - val_loss: 0.1774 - mcc: 0.8563\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9335 - loss: 0.1621\n","Epoch 13 - MCC: 0.8611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - accuracy: 0.9336 - loss: 0.1620 - val_accuracy: 0.9307 - val_loss: 0.1674 - mcc: 0.8611\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9317 - loss: 0.1646\n","Epoch 14 - MCC: 0.8636\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9319 - loss: 0.1643 - val_accuracy: 0.9316 - val_loss: 0.1656 - mcc: 0.8636\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9362 - loss: 0.1542\n","Epoch 15 - MCC: 0.8673\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.9363 - loss: 0.1541 - val_accuracy: 0.9335 - val_loss: 0.1630 - mcc: 0.8673\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9425 - loss: 0.1423\n","Epoch 16 - MCC: 0.8673\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.9424 - loss: 0.1425 - val_accuracy: 0.9338 - val_loss: 0.1580 - mcc: 0.8673\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9406 - loss: 0.1452\n","Epoch 17 - MCC: 0.8663\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9406 - loss: 0.1452 - val_accuracy: 0.9327 - val_loss: 0.1635 - mcc: 0.8663\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9411 - loss: 0.1438\n","Epoch 18 - MCC: 0.8680\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9410 - loss: 0.1439 - val_accuracy: 0.9341 - val_loss: 0.1570 - mcc: 0.8680\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9413 - loss: 0.1415\n","Epoch 19 - MCC: 0.8705\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9413 - loss: 0.1416 - val_accuracy: 0.9353 - val_loss: 0.1557 - mcc: 0.8705\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9425 - loss: 0.1406\n","Epoch 20 - MCC: 0.8712\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9425 - loss: 0.1406 - val_accuracy: 0.9356 - val_loss: 0.1560 - mcc: 0.8712\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9419 - loss: 0.1430\n","Epoch 21 - MCC: 0.8731\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - accuracy: 0.9419 - loss: 0.1430 - val_accuracy: 0.9364 - val_loss: 0.1548 - mcc: 0.8731\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9455 - loss: 0.1338\n","Epoch 22 - MCC: 0.8715\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.9454 - loss: 0.1340 - val_accuracy: 0.9358 - val_loss: 0.1535 - mcc: 0.8715\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9436 - loss: 0.1394\n","Epoch 23 - MCC: 0.8755\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9436 - loss: 0.1394 - val_accuracy: 0.9377 - val_loss: 0.1505 - mcc: 0.8755\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9423 - loss: 0.1408\n","Epoch 24 - MCC: 0.8768\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9423 - loss: 0.1407 - val_accuracy: 0.9382 - val_loss: 0.1516 - mcc: 0.8768\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9448 - loss: 0.1360\n","Epoch 25 - MCC: 0.8745\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9448 - loss: 0.1360 - val_accuracy: 0.9373 - val_loss: 0.1492 - mcc: 0.8745\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9370 - loss: 0.1521\n","Epoch 26 - MCC: 0.8744\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - accuracy: 0.9373 - loss: 0.1514 - val_accuracy: 0.9373 - val_loss: 0.1497 - mcc: 0.8744\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9458 - loss: 0.1336\n","Epoch 27 - MCC: 0.8764\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9457 - loss: 0.1336 - val_accuracy: 0.9383 - val_loss: 0.1471 - mcc: 0.8764\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9500 - loss: 0.1232\n","Epoch 28 - MCC: 0.8754\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9498 - loss: 0.1236 - val_accuracy: 0.9378 - val_loss: 0.1487 - mcc: 0.8754\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9467 - loss: 0.1301\n","Epoch 29 - MCC: 0.8755\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.9467 - loss: 0.1302 - val_accuracy: 0.9379 - val_loss: 0.1469 - mcc: 0.8755\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9457 - loss: 0.1327\n","Epoch 30 - MCC: 0.8748\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.9457 - loss: 0.1327 - val_accuracy: 0.9374 - val_loss: 0.1490 - mcc: 0.8748\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9481 - loss: 0.1280\n","Epoch 31 - MCC: 0.8806\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9480 - loss: 0.1282 - val_accuracy: 0.9401 - val_loss: 0.1466 - mcc: 0.8806\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9469 - loss: 0.1307\n","Epoch 32 - MCC: 0.8784\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9468 - loss: 0.1307 - val_accuracy: 0.9393 - val_loss: 0.1457 - mcc: 0.8784\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9476 - loss: 0.1290\n","Epoch 33 - MCC: 0.8772\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - accuracy: 0.9475 - loss: 0.1291 - val_accuracy: 0.9387 - val_loss: 0.1458 - mcc: 0.8772\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9439 - loss: 0.1370\n","Epoch 34 - MCC: 0.8808\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - accuracy: 0.9440 - loss: 0.1368 - val_accuracy: 0.9404 - val_loss: 0.1441 - mcc: 0.8808\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9498 - loss: 0.1226\n","Epoch 35 - MCC: 0.8781\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.9497 - loss: 0.1229 - val_accuracy: 0.9392 - val_loss: 0.1442 - mcc: 0.8781\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9476 - loss: 0.1283\n","Epoch 36 - MCC: 0.8793\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.9476 - loss: 0.1283 - val_accuracy: 0.9398 - val_loss: 0.1430 - mcc: 0.8793\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9500 - loss: 0.1224\n","Epoch 37 - MCC: 0.8813\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - accuracy: 0.9499 - loss: 0.1226 - val_accuracy: 0.9405 - val_loss: 0.1446 - mcc: 0.8813\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9490 - loss: 0.1266\n","Epoch 38 - MCC: 0.8791\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.9490 - loss: 0.1266 - val_accuracy: 0.9396 - val_loss: 0.1433 - mcc: 0.8791\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9478 - loss: 0.1286\n","Epoch 39 - MCC: 0.8779\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9479 - loss: 0.1286 - val_accuracy: 0.9390 - val_loss: 0.1454 - mcc: 0.8779\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9509 - loss: 0.1207\n","Epoch 40 - MCC: 0.8822\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9508 - loss: 0.1210 - val_accuracy: 0.9412 - val_loss: 0.1416 - mcc: 0.8822\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9442 - loss: 0.1364\n","Epoch 41 - MCC: 0.8819\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.9444 - loss: 0.1361 - val_accuracy: 0.9408 - val_loss: 0.1421 - mcc: 0.8819\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9481 - loss: 0.1270\n","Epoch 42 - MCC: 0.8791\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - accuracy: 0.9482 - loss: 0.1270 - val_accuracy: 0.9391 - val_loss: 0.1492 - mcc: 0.8791\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9484 - loss: 0.1265\n","Epoch 43 - MCC: 0.8828\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.9484 - loss: 0.1265 - val_accuracy: 0.9415 - val_loss: 0.1406 - mcc: 0.8828\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9499 - loss: 0.1237\n","Epoch 44 - MCC: 0.8812\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9499 - loss: 0.1237 - val_accuracy: 0.9406 - val_loss: 0.1448 - mcc: 0.8812\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9507 - loss: 0.1217\n","Epoch 45 - MCC: 0.8829\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9506 - loss: 0.1218 - val_accuracy: 0.9415 - val_loss: 0.1401 - mcc: 0.8829\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9501 - loss: 0.1232\n","Epoch 46 - MCC: 0.8819\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.9500 - loss: 0.1233 - val_accuracy: 0.9410 - val_loss: 0.1404 - mcc: 0.8819\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9518 - loss: 0.1187\n","Epoch 47 - MCC: 0.8813\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - accuracy: 0.9517 - loss: 0.1189 - val_accuracy: 0.9407 - val_loss: 0.1405 - mcc: 0.8813\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9494 - loss: 0.1238\n","Epoch 48 - MCC: 0.8839\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9494 - loss: 0.1238 - val_accuracy: 0.9419 - val_loss: 0.1408 - mcc: 0.8839\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9466 - loss: 0.1300\n","Epoch 49 - MCC: 0.8834\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9468 - loss: 0.1297 - val_accuracy: 0.9417 - val_loss: 0.1405 - mcc: 0.8834\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9517 - loss: 0.1202\n","Epoch 50 - MCC: 0.8826\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9517 - loss: 0.1202 - val_accuracy: 0.9413 - val_loss: 0.1399 - mcc: 0.8826\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9526533333333334,\n","              'mean': 0.9456013333333333,\n","              'min': 0.9358266666666667,\n","              'std': 0.006144662580013823},\n"," 'Inference Time (s/sample)': {'max': 0.003861398696899414,\n","                               'mean': 0.003637311935424805,\n","                               'min': 0.0032679057121276856,\n","                               'std': 0.0002066710836646875},\n"," 'MCC': {'max': 0.9049885142752772,\n","         'mean': 0.8909969515821938,\n","         'min': 0.8715491519243017,\n","         'std': 0.012216385301479166},\n"," 'Parameters': 9761,\n"," 'Train Time (s)': {'max': 161.33348608016968,\n","                    'mean': 150.4443459033966,\n","                    'min': 142.74560284614563,\n","                    'std': 6.9818524590983335},\n"," 'Training Accuracy': [[0.629569947719574,\n","                        0.7321267127990723,\n","                        0.8404266834259033,\n","                        0.8973917961120605,\n","                        0.9153532981872559,\n","                        0.9196434020996094,\n","                        0.9218666553497314,\n","                        0.9251783490180969,\n","                        0.9278066158294678,\n","                        0.930044949054718,\n","                        0.9317450523376465,\n","                        0.932696521282196,\n","                        0.9341399669647217,\n","                        0.9353917837142944,\n","                        0.9357849955558777,\n","                        0.9360133409500122,\n","                        0.9372066259384155,\n","                        0.9375482797622681,\n","                        0.9385599493980408,\n","                        0.9397733807563782,\n","                        0.9389582276344299,\n","                        0.9404900074005127,\n","                        0.9406883716583252,\n","                        0.9410083293914795,\n","                        0.9408367276191711,\n","                        0.942026674747467,\n","                        0.9428648948669434,\n","                        0.9431601166725159,\n","                        0.9427531957626343,\n","                        0.9434200525283813,\n","                        0.9436498880386353,\n","                        0.9441133141517639,\n","                        0.9451266527175903,\n","                        0.9451767206192017,\n","                        0.9453482627868652,\n","                        0.9453866481781006,\n","                        0.944638192653656,\n","                        0.9466667175292969,\n","                        0.9466716647148132,\n","                        0.945609986782074,\n","                        0.9467799663543701,\n","                        0.9461483955383301,\n","                        0.9454032182693481,\n","                        0.947753369808197,\n","                        0.948604941368103,\n","                        0.9482234120368958,\n","                        0.9476749897003174,\n","                        0.9488250017166138,\n","                        0.9492316246032715,\n","                        0.9497749209403992],\n","                       [0.6716232895851135,\n","                        0.7800400257110596,\n","                        0.8533067107200623,\n","                        0.8972499966621399,\n","                        0.9112666845321655,\n","                        0.916411817073822,\n","                        0.9222415685653687,\n","                        0.9257016777992249,\n","                        0.9271716475486755,\n","                        0.9287300705909729,\n","                        0.9311667680740356,\n","                        0.9338015913963318,\n","                        0.9343183636665344,\n","                        0.9348066449165344,\n","                        0.9371950030326843,\n","                        0.9381267428398132,\n","                        0.9385067820549011,\n","                        0.940488338470459,\n","                        0.9402933716773987,\n","                        0.9417516589164734,\n","                        0.9426133036613464,\n","                        0.9424149990081787,\n","                        0.9417715668678284,\n","                        0.9438716173171997,\n","                        0.9439817667007446,\n","                        0.9443501234054565,\n","                        0.945975124835968,\n","                        0.9456149339675903,\n","                        0.9464666247367859,\n","                        0.9463814496994019,\n","                        0.9467616081237793,\n","                        0.9464484453201294,\n","                        0.9472399950027466,\n","                        0.9463967084884644,\n","                        0.9477216601371765,\n","                        0.947836697101593,\n","                        0.9479416608810425,\n","                        0.9488016366958618,\n","                        0.9492065906524658,\n","                        0.9494099617004395,\n","                        0.9500765800476074,\n","                        0.9499849677085876,\n","                        0.9497483968734741,\n","                        0.9493415951728821,\n","                        0.9487617015838623,\n","                        0.9500982761383057,\n","                        0.95052170753479,\n","                        0.9510999917984009,\n","                        0.951323390007019,\n","                        0.9517232775688171],\n","                       [0.6979100108146667,\n","                        0.7880983948707581,\n","                        0.8663799166679382,\n","                        0.9112334251403809,\n","                        0.9205349683761597,\n","                        0.9253515601158142,\n","                        0.9284732937812805,\n","                        0.9303367137908936,\n","                        0.9314117431640625,\n","                        0.9344316124916077,\n","                        0.9340217709541321,\n","                        0.9358016848564148,\n","                        0.9368449449539185,\n","                        0.9374882578849792,\n","                        0.9382749795913696,\n","                        0.9381933808326721,\n","                        0.9386782050132751,\n","                        0.9396133422851562,\n","                        0.9411683678627014,\n","                        0.9412266612052917,\n","                        0.9412634372711182,\n","                        0.9417849183082581,\n","                        0.9413816332817078,\n","                        0.9421716332435608,\n","                        0.9432083368301392,\n","                        0.9419733285903931,\n","                        0.9440667629241943,\n","                        0.9435501098632812,\n","                        0.9436084032058716,\n","                        0.9434551000595093,\n","                        0.9443733096122742,\n","                        0.9456418752670288,\n","                        0.9458132982254028,\n","                        0.9449917078018188,\n","                        0.9461816549301147,\n","                        0.9460033178329468,\n","                        0.9464882016181946,\n","                        0.9465683698654175,\n","                        0.9472934007644653,\n","                        0.9471850395202637,\n","                        0.9468433260917664,\n","                        0.9466000199317932,\n","                        0.9475350379943848,\n","                        0.9484899044036865,\n","                        0.9484717845916748,\n","                        0.9485566020011902,\n","                        0.9481584429740906,\n","                        0.9486583471298218,\n","                        0.9477416276931763,\n","                        0.9490534067153931],\n","                       [0.7244666218757629,\n","                        0.7721783518791199,\n","                        0.8739901185035706,\n","                        0.9023832678794861,\n","                        0.9146415591239929,\n","                        0.9206499457359314,\n","                        0.9217850565910339,\n","                        0.9250649213790894,\n","                        0.9247501492500305,\n","                        0.92774498462677,\n","                        0.9289766550064087,\n","                        0.9299049377441406,\n","                        0.9310615658760071,\n","                        0.9317533373832703,\n","                        0.9336099028587341,\n","                        0.9325599074363708,\n","                        0.9333632588386536,\n","                        0.9345449805259705,\n","                        0.9349416494369507,\n","                        0.9357299208641052,\n","                        0.9356966614723206,\n","                        0.9373065829277039,\n","                        0.9365850687026978,\n","                        0.9368649125099182,\n","                        0.9373866319656372,\n","                        0.9386934041976929,\n","                        0.940049946308136,\n","                        0.9404783844947815,\n","                        0.9410433173179626,\n","                        0.9419350624084473,\n","                        0.941501796245575,\n","                        0.9427517056465149,\n","                        0.9429066181182861,\n","                        0.9435716867446899,\n","                        0.9442616105079651,\n","                        0.9448549747467041,\n","                        0.9445750713348389,\n","                        0.9454267024993896,\n","                        0.9454666972160339,\n","                        0.9459382891654968,\n","                        0.9465417265892029,\n","                        0.9460365772247314,\n","                        0.9458751082420349,\n","                        0.9453150033950806,\n","                        0.9466750621795654,\n","                        0.9467184543609619,\n","                        0.947806715965271,\n","                        0.9486448764801025,\n","                        0.9475499987602234,\n","                        0.9480999112129211],\n","                       [0.6950716376304626,\n","                        0.7751383781433105,\n","                        0.8664566874504089,\n","                        0.9056099653244019,\n","                        0.918171763420105,\n","                        0.9225283265113831,\n","                        0.9249783158302307,\n","                        0.9250566959381104,\n","                        0.9293232560157776,\n","                        0.9310965538024902,\n","                        0.932034969329834,\n","                        0.9339714646339417,\n","                        0.9346134066581726,\n","                        0.936291515827179,\n","                        0.9375117421150208,\n","                        0.9395983219146729,\n","                        0.9400098919868469,\n","                        0.9403199553489685,\n","                        0.9406834244728088,\n","                        0.9425783753395081,\n","                        0.9419699311256409,\n","                        0.9430266618728638,\n","                        0.9437149167060852,\n","                        0.9432666897773743,\n","                        0.9442116022109985,\n","                        0.9447066783905029,\n","                        0.9453549981117249,\n","                        0.945253312587738,\n","                        0.9456832408905029,\n","                        0.9455100893974304,\n","                        0.9455899000167847,\n","                        0.9455582499504089,\n","                        0.9463033080101013,\n","                        0.9466550350189209,\n","                        0.94691002368927,\n","                        0.9475148916244507,\n","                        0.947421669960022,\n","                        0.9484251141548157,\n","                        0.9482417106628418,\n","                        0.9477717876434326,\n","                        0.9482749700546265,\n","                        0.948590099811554,\n","                        0.9489983320236206,\n","                        0.9497199058532715,\n","                        0.9493000507354736,\n","                        0.9492866396903992,\n","                        0.9500533938407898,\n","                        0.9502183794975281,\n","                        0.949989914894104,\n","                        0.9508918523788452]],\n"," 'Training Loss': [[0.6820106506347656,\n","                    0.6159961819648743,\n","                    0.44135308265686035,\n","                    0.2640235722064972,\n","                    0.2075539231300354,\n","                    0.1932222545146942,\n","                    0.18819892406463623,\n","                    0.18099889159202576,\n","                    0.1743091344833374,\n","                    0.16865220665931702,\n","                    0.16467241942882538,\n","                    0.1621757298707962,\n","                    0.1591210663318634,\n","                    0.15736077725887299,\n","                    0.1547938734292984,\n","                    0.15478582680225372,\n","                    0.15344025194644928,\n","                    0.15249134600162506,\n","                    0.14937540888786316,\n","                    0.14648641645908356,\n","                    0.14801086485385895,\n","                    0.1451251059770584,\n","                    0.1438770741224289,\n","                    0.14375039935112,\n","                    0.14406265318393707,\n","                    0.14151351153850555,\n","                    0.13995924592018127,\n","                    0.13900426030158997,\n","                    0.1398128867149353,\n","                    0.13816750049591064,\n","                    0.13687828183174133,\n","                    0.1361074596643448,\n","                    0.13454985618591309,\n","                    0.1342489868402481,\n","                    0.13421477377414703,\n","                    0.13415133953094482,\n","                    0.1349201649427414,\n","                    0.13058488070964813,\n","                    0.13107457756996155,\n","                    0.13265591859817505,\n","                    0.1308523714542389,\n","                    0.1324971616268158,\n","                    0.13332723081111908,\n","                    0.12833140790462494,\n","                    0.12636922299861908,\n","                    0.12683069705963135,\n","                    0.12729696929454803,\n","                    0.12552310526371002,\n","                    0.12506943941116333,\n","                    0.12334904074668884],\n","                   [0.6697113513946533,\n","                    0.567670464515686,\n","                    0.36956271529197693,\n","                    0.2484411597251892,\n","                    0.21281760931015015,\n","                    0.19829463958740234,\n","                    0.18579155206680298,\n","                    0.178472638130188,\n","                    0.17549210786819458,\n","                    0.1713792085647583,\n","                    0.16698330640792847,\n","                    0.15983587503433228,\n","                    0.15881234407424927,\n","                    0.15764646232128143,\n","                    0.15351808071136475,\n","                    0.1500275582075119,\n","                    0.14951586723327637,\n","                    0.14456906914710999,\n","                    0.14531996846199036,\n","                    0.14218544960021973,\n","                    0.139713853597641,\n","                    0.14015381038188934,\n","                    0.14134451746940613,\n","                    0.13651221990585327,\n","                    0.13625748455524445,\n","                    0.1359712779521942,\n","                    0.1328582912683487,\n","                    0.1326473206281662,\n","                    0.13051338493824005,\n","                    0.13089483976364136,\n","                    0.12991183996200562,\n","                    0.1309455931186676,\n","                    0.12952028214931488,\n","                    0.13057182729244232,\n","                    0.1279239058494568,\n","                    0.12788541615009308,\n","                    0.12770400941371918,\n","                    0.1254304051399231,\n","                    0.1241697371006012,\n","                    0.1238243356347084,\n","                    0.12235525995492935,\n","                    0.12321338802576065,\n","                    0.1230633556842804,\n","                    0.12453654408454895,\n","                    0.12594909965991974,\n","                    0.12190938740968704,\n","                    0.12191104143857956,\n","                    0.1205221489071846,\n","                    0.11962945014238358,\n","                    0.11946827173233032],\n","                   [0.6643399000167847,\n","                    0.5508689284324646,\n","                    0.34684839844703674,\n","                    0.21892105042934418,\n","                    0.1930031180381775,\n","                    0.18084383010864258,\n","                    0.17261913418769836,\n","                    0.1679006963968277,\n","                    0.16499406099319458,\n","                    0.15874619781970978,\n","                    0.15964563190937042,\n","                    0.155369371175766,\n","                    0.15274681150913239,\n","                    0.1522248387336731,\n","                    0.1501207798719406,\n","                    0.1503623127937317,\n","                    0.14901186525821686,\n","                    0.1466405987739563,\n","                    0.14413326978683472,\n","                    0.14395280182361603,\n","                    0.143636554479599,\n","                    0.14364495873451233,\n","                    0.14274753630161285,\n","                    0.14154210686683655,\n","                    0.13947774469852448,\n","                    0.14161449670791626,\n","                    0.13741415739059448,\n","                    0.13838648796081543,\n","                    0.13793398439884186,\n","                    0.1385442316532135,\n","                    0.1364678293466568,\n","                    0.13336016237735748,\n","                    0.13285623490810394,\n","                    0.13405382633209229,\n","                    0.1328115016222,\n","                    0.13269072771072388,\n","                    0.13119065761566162,\n","                    0.13104978203773499,\n","                    0.13040752708911896,\n","                    0.13087791204452515,\n","                    0.13016892969608307,\n","                    0.13093239068984985,\n","                    0.12960313260555267,\n","                    0.12708310782909393,\n","                    0.12683485448360443,\n","                    0.12720991671085358,\n","                    0.12730687856674194,\n","                    0.1265287548303604,\n","                    0.12931571900844574,\n","                    0.12598465383052826],\n","                   [0.6688700318336487,\n","                    0.5648329854011536,\n","                    0.3545945882797241,\n","                    0.2372080385684967,\n","                    0.2061099112033844,\n","                    0.19178889691829681,\n","                    0.1872648000717163,\n","                    0.1797652691602707,\n","                    0.18014484643936157,\n","                    0.173398956656456,\n","                    0.16928242146968842,\n","                    0.16800281405448914,\n","                    0.1663181483745575,\n","                    0.16383962333202362,\n","                    0.16175712645053864,\n","                    0.16236156225204468,\n","                    0.16138280928134918,\n","                    0.1584881693124771,\n","                    0.15776166319847107,\n","                    0.15660695731639862,\n","                    0.15578842163085938,\n","                    0.15366263687610626,\n","                    0.15499980747699738,\n","                    0.15372347831726074,\n","                    0.1524343639612198,\n","                    0.14831355214118958,\n","                    0.14633148908615112,\n","                    0.14453712105751038,\n","                    0.14297175407409668,\n","                    0.14178413152694702,\n","                    0.14213012158870697,\n","                    0.13930119574069977,\n","                    0.13929110765457153,\n","                    0.13784904778003693,\n","                    0.13713468611240387,\n","                    0.13550062477588654,\n","                    0.13499197363853455,\n","                    0.13396185636520386,\n","                    0.1330840140581131,\n","                    0.1333022266626358,\n","                    0.13178855180740356,\n","                    0.13205228745937347,\n","                    0.1326131969690323,\n","                    0.1348341554403305,\n","                    0.1315687596797943,\n","                    0.13107460737228394,\n","                    0.12837882339954376,\n","                    0.1269705891609192,\n","                    0.12875588238239288,\n","                    0.12780418992042542],\n","                   [0.6695510745048523,\n","                    0.5790471434593201,\n","                    0.37640058994293213,\n","                    0.2404344230890274,\n","                    0.20287470519542694,\n","                    0.18847638368606567,\n","                    0.18148475885391235,\n","                    0.18167562782764435,\n","                    0.1718183308839798,\n","                    0.1664951890707016,\n","                    0.16569580137729645,\n","                    0.16162805259227753,\n","                    0.1600933074951172,\n","                    0.15551632642745972,\n","                    0.15197572112083435,\n","                    0.14774127304553986,\n","                    0.14673981070518494,\n","                    0.14590156078338623,\n","                    0.14357969164848328,\n","                    0.14082488417625427,\n","                    0.14170947670936584,\n","                    0.13892227411270142,\n","                    0.13803331553936005,\n","                    0.13841886818408966,\n","                    0.13637496531009674,\n","                    0.1346951574087143,\n","                    0.13384580612182617,\n","                    0.13343796133995056,\n","                    0.1324869990348816,\n","                    0.13252849876880646,\n","                    0.13302572071552277,\n","                    0.13274288177490234,\n","                    0.1314287632703781,\n","                    0.13116022944450378,\n","                    0.13011351227760315,\n","                    0.1284138709306717,\n","                    0.1276639997959137,\n","                    0.12730593979358673,\n","                    0.12711969017982483,\n","                    0.12784527242183685,\n","                    0.12707187235355377,\n","                    0.12640836834907532,\n","                    0.1254313588142395,\n","                    0.12391199171543121,\n","                    0.12478180974721909,\n","                    0.12449871748685837,\n","                    0.12299844622612,\n","                    0.12289050221443176,\n","                    0.12323413044214249,\n","                    0.12111477553844452]],\n"," 'Validation Accuracy': [[0.6965733170509338,\n","                          0.7891532778739929,\n","                          0.8917933702468872,\n","                          0.9163333773612976,\n","                          0.9234200119972229,\n","                          0.9281933307647705,\n","                          0.9281667470932007,\n","                          0.9335333108901978,\n","                          0.9363666772842407,\n","                          0.9381533861160278,\n","                          0.9382266998291016,\n","                          0.9403466582298279,\n","                          0.9413665533065796,\n","                          0.9424799084663391,\n","                          0.9416332244873047,\n","                          0.9437599778175354,\n","                          0.9446600079536438,\n","                          0.9434733390808105,\n","                          0.9456267356872559,\n","                          0.9457666873931885,\n","                          0.9465933442115784,\n","                          0.9461266994476318,\n","                          0.9472600817680359,\n","                          0.9454666972160339,\n","                          0.9476866722106934,\n","                          0.9480200409889221,\n","                          0.9476933479309082,\n","                          0.9488866329193115,\n","                          0.9490200281143188,\n","                          0.948753297328949,\n","                          0.9486600756645203,\n","                          0.9497866630554199,\n","                          0.9503931999206543,\n","                          0.9487200379371643,\n","                          0.9488334059715271,\n","                          0.9508000016212463,\n","                          0.9495067000389099,\n","                          0.9518799781799316,\n","                          0.9510465860366821,\n","                          0.9488133192062378,\n","                          0.9506332874298096,\n","                          0.9495533108711243,\n","                          0.9508666396141052,\n","                          0.9523133039474487,\n","                          0.951373279094696,\n","                          0.9516866207122803,\n","                          0.9530666470527649,\n","                          0.9529600739479065,\n","                          0.9538933634757996,\n","                          0.9526533484458923],\n","                         [0.7311267256736755,\n","                          0.7944733500480652,\n","                          0.868773341178894,\n","                          0.8964666724205017,\n","                          0.9061267375946045,\n","                          0.9106199741363525,\n","                          0.9108932614326477,\n","                          0.9153799414634705,\n","                          0.9178000688552856,\n","                          0.9160199761390686,\n","                          0.9171867370605469,\n","                          0.922926664352417,\n","                          0.9246665835380554,\n","                          0.9212667346000671,\n","                          0.922553300857544,\n","                          0.9264533519744873,\n","                          0.9287066459655762,\n","                          0.926693320274353,\n","                          0.9264333248138428,\n","                          0.9297733306884766,\n","                          0.9287667274475098,\n","                          0.9283533692359924,\n","                          0.9305000305175781,\n","                          0.9318466186523438,\n","                          0.9325266480445862,\n","                          0.9327733516693115,\n","                          0.9334400296211243,\n","                          0.9336199760437012,\n","                          0.9344266653060913,\n","                          0.9336866736412048,\n","                          0.9340999722480774,\n","                          0.9347133040428162,\n","                          0.9344599843025208,\n","                          0.9350399971008301,\n","                          0.9350666999816895,\n","                          0.9322400093078613,\n","                          0.9360466003417969,\n","                          0.9360200762748718,\n","                          0.935759961605072,\n","                          0.9364399909973145,\n","                          0.9358866810798645,\n","                          0.9371866583824158,\n","                          0.9369399547576904,\n","                          0.9361266493797302,\n","                          0.9360466003417969,\n","                          0.9347267150878906,\n","                          0.934273362159729,\n","                          0.9376400113105774,\n","                          0.9372133612632751,\n","                          0.9358266592025757],\n","                         [0.7725399732589722,\n","                          0.8309266567230225,\n","                          0.9010266661643982,\n","                          0.9230599403381348,\n","                          0.9240866899490356,\n","                          0.930786669254303,\n","                          0.9307267069816589,\n","                          0.9352333545684814,\n","                          0.936959981918335,\n","                          0.9390999674797058,\n","                          0.9379066228866577,\n","                          0.9395999312400818,\n","                          0.9398000240325928,\n","                          0.9415732622146606,\n","                          0.9417333006858826,\n","                          0.9399867057800293,\n","                          0.9428266882896423,\n","                          0.9433133006095886,\n","                          0.9442200660705566,\n","                          0.9426599740982056,\n","                          0.9434266686439514,\n","                          0.9436733722686768,\n","                          0.9447000026702881,\n","                          0.9452466368675232,\n","                          0.9458132982254028,\n","                          0.9455732703208923,\n","                          0.9444866180419922,\n","                          0.9448666572570801,\n","                          0.9461599588394165,\n","                          0.9468933343887329,\n","                          0.9479666948318481,\n","                          0.9471732974052429,\n","                          0.9480133652687073,\n","                          0.9448866248130798,\n","                          0.9466067552566528,\n","                          0.9461399912834167,\n","                          0.9479800462722778,\n","                          0.9482599496841431,\n","                          0.9480599761009216,\n","                          0.947939932346344,\n","                          0.9474599957466125,\n","                          0.9460800290107727,\n","                          0.9491667151451111,\n","                          0.9490132927894592,\n","                          0.9480800628662109,\n","                          0.9496266841888428,\n","                          0.9491332769393921,\n","                          0.9477733373641968,\n","                          0.9492266774177551,\n","                          0.9483599662780762],\n","                         [0.7401333451271057,\n","                          0.8596667647361755,\n","                          0.8997200131416321,\n","                          0.9136399030685425,\n","                          0.9220066666603088,\n","                          0.9251000285148621,\n","                          0.9282466173171997,\n","                          0.9304398894309998,\n","                          0.9313666820526123,\n","                          0.9325466752052307,\n","                          0.9336667060852051,\n","                          0.930899977684021,\n","                          0.9354532361030579,\n","                          0.9357666969299316,\n","                          0.9358199834823608,\n","                          0.9357067346572876,\n","                          0.936519980430603,\n","                          0.9384999871253967,\n","                          0.9376066327095032,\n","                          0.9381799101829529,\n","                          0.9380266070365906,\n","                          0.9415267109870911,\n","                          0.9399600028991699,\n","                          0.9406465888023376,\n","                          0.9421133399009705,\n","                          0.9437733292579651,\n","                          0.9427533745765686,\n","                          0.9453667402267456,\n","                          0.9450865983963013,\n","                          0.9448933601379395,\n","                          0.9456533193588257,\n","                          0.9475399851799011,\n","                          0.9465866684913635,\n","                          0.9479266405105591,\n","                          0.9463133811950684,\n","                          0.948253333568573,\n","                          0.9483000040054321,\n","                          0.9479533433914185,\n","                          0.9488067626953125,\n","                          0.9479799866676331,\n","                          0.9498332738876343,\n","                          0.9492732882499695,\n","                          0.9470865726470947,\n","                          0.9486666321754456,\n","                          0.9487866163253784,\n","                          0.9499465823173523,\n","                          0.9501934051513672,\n","                          0.9505400061607361,\n","                          0.949606716632843,\n","                          0.9498200416564941],\n","                         [0.7518267035484314,\n","                          0.8164000511169434,\n","                          0.8882666826248169,\n","                          0.9081599712371826,\n","                          0.9133867025375366,\n","                          0.9203066825866699,\n","                          0.9210400581359863,\n","                          0.9200533032417297,\n","                          0.9248467087745667,\n","                          0.9254466891288757,\n","                          0.928213357925415,\n","                          0.9278733134269714,\n","                          0.9306866526603699,\n","                          0.9316466450691223,\n","                          0.9334800243377686,\n","                          0.9337733387947083,\n","                          0.9327199459075928,\n","                          0.9340799450874329,\n","                          0.935333251953125,\n","                          0.9356334209442139,\n","                          0.9364199638366699,\n","                          0.9358466863632202,\n","                          0.9376667141914368,\n","                          0.9381933808326721,\n","                          0.9372733235359192,\n","                          0.9373133778572083,\n","                          0.9382733106613159,\n","                          0.9377866387367249,\n","                          0.9378800392150879,\n","                          0.9373800158500671,\n","                          0.9401400685310364,\n","                          0.9393332600593567,\n","                          0.9387199878692627,\n","                          0.9403599500656128,\n","                          0.9391599297523499,\n","                          0.9397599697113037,\n","                          0.9404733180999756,\n","                          0.9396266341209412,\n","                          0.9390133619308472,\n","                          0.9412000179290771,\n","                          0.9408332705497742,\n","                          0.9390866756439209,\n","                          0.9414867162704468,\n","                          0.9405534267425537,\n","                          0.9415066242218018,\n","                          0.9410133361816406,\n","                          0.940726637840271,\n","                          0.9419201016426086,\n","                          0.9416533708572388,\n","                          0.9413467645645142]],\n"," 'Validation Loss': [[0.6567414999008179,\n","                      0.538987934589386,\n","                      0.3145139813423157,\n","                      0.21094617247581482,\n","                      0.1852588653564453,\n","                      0.17210204899311066,\n","                      0.17087125778198242,\n","                      0.1595742106437683,\n","                      0.15384168922901154,\n","                      0.14909295737743378,\n","                      0.14869408309459686,\n","                      0.1442761868238449,\n","                      0.14353197813034058,\n","                      0.1402469128370285,\n","                      0.14282841980457306,\n","                      0.13650932908058167,\n","                      0.13646350800991058,\n","                      0.13891378045082092,\n","                      0.13302727043628693,\n","                      0.13250313699245453,\n","                      0.1300836056470871,\n","                      0.13222505152225494,\n","                      0.12902571260929108,\n","                      0.13384582102298737,\n","                      0.12766313552856445,\n","                      0.12598349153995514,\n","                      0.12654276192188263,\n","                      0.12525765597820282,\n","                      0.12572506070137024,\n","                      0.12511183321475983,\n","                      0.12465789169073105,\n","                      0.12166967242956161,\n","                      0.12223462760448456,\n","                      0.12527835369110107,\n","                      0.12588995695114136,\n","                      0.12005014717578888,\n","                      0.12363136559724808,\n","                      0.11807318031787872,\n","                      0.12092775106430054,\n","                      0.12397374957799911,\n","                      0.12065828591585159,\n","                      0.12379972636699677,\n","                      0.1198936402797699,\n","                      0.11595769226551056,\n","                      0.1180846244096756,\n","                      0.11779224127531052,\n","                      0.11415465176105499,\n","                      0.11370352655649185,\n","                      0.11419268697500229,\n","                      0.11477173119783401],\n","                     [0.6458485126495361,\n","                      0.49196872115135193,\n","                      0.3138103485107422,\n","                      0.24647502601146698,\n","                      0.22235891222953796,\n","                      0.21324394643306732,\n","                      0.20867866277694702,\n","                      0.2012237012386322,\n","                      0.1968805491924286,\n","                      0.203663632273674,\n","                      0.19538047909736633,\n","                      0.18562889099121094,\n","                      0.183428555727005,\n","                      0.1864365190267563,\n","                      0.18337304890155792,\n","                      0.1775808483362198,\n","                      0.17272968590259552,\n","                      0.17441554367542267,\n","                      0.17581762373447418,\n","                      0.17016878724098206,\n","                      0.1719941943883896,\n","                      0.1712370365858078,\n","                      0.1660890132188797,\n","                      0.16664743423461914,\n","                      0.16540247201919556,\n","                      0.1633337438106537,\n","                      0.16204185783863068,\n","                      0.1622186154127121,\n","                      0.1609182506799698,\n","                      0.16090461611747742,\n","                      0.16202837228775024,\n","                      0.15985122323036194,\n","                      0.16048431396484375,\n","                      0.15869849920272827,\n","                      0.15981082618236542,\n","                      0.16532260179519653,\n","                      0.15925173461437225,\n","                      0.15928952395915985,\n","                      0.15855033695697784,\n","                      0.15714462101459503,\n","                      0.15713441371917725,\n","                      0.15768800675868988,\n","                      0.15546084940433502,\n","                      0.15783406794071198,\n","                      0.15743587911128998,\n","                      0.15964403748512268,\n","                      0.16103824973106384,\n","                      0.15471334755420685,\n","                      0.15572530031204224,\n","                      0.15772974491119385],\n","                     [0.6238152980804443,\n","                      0.44722792506217957,\n","                      0.24395883083343506,\n","                      0.18952232599258423,\n","                      0.18279929459095,\n","                      0.1672571897506714,\n","                      0.166182741522789,\n","                      0.1573726236820221,\n","                      0.15203000605106354,\n","                      0.14998286962509155,\n","                      0.1506165862083435,\n","                      0.1473178267478943,\n","                      0.14853884279727936,\n","                      0.14336419105529785,\n","                      0.14238213002681732,\n","                      0.14580082893371582,\n","                      0.14061592519283295,\n","                      0.1395580768585205,\n","                      0.138454869389534,\n","                      0.14082448184490204,\n","                      0.1389843225479126,\n","                      0.1381259709596634,\n","                      0.13615307211875916,\n","                      0.13539434969425201,\n","                      0.13396863639354706,\n","                      0.1332164704799652,\n","                      0.13681091368198395,\n","                      0.13545331358909607,\n","                      0.1331024169921875,\n","                      0.13275077939033508,\n","                      0.12963540852069855,\n","                      0.13011948764324188,\n","                      0.12918508052825928,\n","                      0.13434375822544098,\n","                      0.13343925774097443,\n","                      0.131460502743721,\n","                      0.12776651978492737,\n","                      0.12786169350147247,\n","                      0.12736226618289948,\n","                      0.12936779856681824,\n","                      0.1294296383857727,\n","                      0.1334017515182495,\n","                      0.1258402168750763,\n","                      0.12557244300842285,\n","                      0.12706464529037476,\n","                      0.12522505223751068,\n","                      0.12517932057380676,\n","                      0.12955313920974731,\n","                      0.12646597623825073,\n","                      0.12812335789203644],\n","                     [0.6338522434234619,\n","                      0.46113350987434387,\n","                      0.25446152687072754,\n","                      0.2111768275499344,\n","                      0.19110187888145447,\n","                      0.1823773980140686,\n","                      0.17389991879463196,\n","                      0.16876833140850067,\n","                      0.1654973030090332,\n","                      0.16359728574752808,\n","                      0.16140353679656982,\n","                      0.16722670197486877,\n","                      0.15727660059928894,\n","                      0.15795020759105682,\n","                      0.1550263911485672,\n","                      0.1579943597316742,\n","                      0.15390217304229736,\n","                      0.15045005083084106,\n","                      0.15223664045333862,\n","                      0.15033385157585144,\n","                      0.1509736031293869,\n","                      0.14414724707603455,\n","                      0.1467828005552292,\n","                      0.14530690014362335,\n","                      0.14129513502120972,\n","                      0.13835176825523376,\n","                      0.13871441781520844,\n","                      0.13551464676856995,\n","                      0.13482698798179626,\n","                      0.13620083034038544,\n","                      0.13532279431819916,\n","                      0.12972033023834229,\n","                      0.1315472573041916,\n","                      0.12857963144779205,\n","                      0.13120785355567932,\n","                      0.1282672882080078,\n","                      0.12821818888187408,\n","                      0.12772776186466217,\n","                      0.1255308836698532,\n","                      0.1270035356283188,\n","                      0.12411670386791229,\n","                      0.1255531758069992,\n","                      0.12914542853832245,\n","                      0.12631791830062866,\n","                      0.1266242414712906,\n","                      0.12470551580190659,\n","                      0.12310073524713516,\n","                      0.12205107510089874,\n","                      0.12484057247638702,\n","                      0.12258397787809372],\n","                     [0.6395862102508545,\n","                      0.477748304605484,\n","                      0.284115195274353,\n","                      0.22611436247825623,\n","                      0.2119448035955429,\n","                      0.1935131996870041,\n","                      0.19297762215137482,\n","                      0.191427081823349,\n","                      0.1797402799129486,\n","                      0.17796938121318817,\n","                      0.1743168830871582,\n","                      0.17737802863121033,\n","                      0.16735868155956268,\n","                      0.16561554372310638,\n","                      0.16301923990249634,\n","                      0.1579829901456833,\n","                      0.16346707940101624,\n","                      0.1570175439119339,\n","                      0.15568043291568756,\n","                      0.1559676080942154,\n","                      0.1548362523317337,\n","                      0.15345875918865204,\n","                      0.1505432426929474,\n","                      0.15156778693199158,\n","                      0.14919590950012207,\n","                      0.1496545672416687,\n","                      0.1471349596977234,\n","                      0.1486947238445282,\n","                      0.1468585878610611,\n","                      0.14896322786808014,\n","                      0.14655523002147675,\n","                      0.145703986287117,\n","                      0.14582395553588867,\n","                      0.14408163726329803,\n","                      0.14421170949935913,\n","                      0.14304740726947784,\n","                      0.14455899596214294,\n","                      0.14327114820480347,\n","                      0.1454458236694336,\n","                      0.14162234961986542,\n","                      0.1421438753604889,\n","                      0.1492186188697815,\n","                      0.14062285423278809,\n","                      0.14475174248218536,\n","                      0.14008696377277374,\n","                      0.1403517872095108,\n","                      0.140491783618927,\n","                      0.14083588123321533,\n","                      0.14045587182044983,\n","                      0.1398663967847824]],\n"," 'Validation MCC': [[0.46659214721381875,\n","                     0.6012146110179216,\n","                     0.7836309073491726,\n","                     0.8320142191843801,\n","                     0.8469836933679044,\n","                     0.8559959116630178,\n","                     0.8564399987320892,\n","                     0.8666235024119961,\n","                     0.8722436400023308,\n","                     0.8758270074844233,\n","                     0.8760700404527332,\n","                     0.8802928489664067,\n","                     0.8825713290115258,\n","                     0.8845204895757395,\n","                     0.8832484090477184,\n","                     0.8870945422144184,\n","                     0.8890280778153277,\n","                     0.8865117836707376,\n","                     0.8908445918355733,\n","                     0.8911602135924468,\n","                     0.8928195229706767,\n","                     0.8920708554831573,\n","                     0.8942028637410885,\n","                     0.8909197141800002,\n","                     0.8950082752935236,\n","                     0.8956465190176076,\n","                     0.894991705772691,\n","                     0.8973932796346892,\n","                     0.8976502332268614,\n","                     0.8972160932232491,\n","                     0.8969262380638279,\n","                     0.8992435420173978,\n","                     0.9004670226554002,\n","                     0.8972529188332264,\n","                     0.8976727345749114,\n","                     0.9013778144840192,\n","                     0.8986266285599586,\n","                     0.9033966559482567,\n","                     0.9019162461051613,\n","                     0.8972343106329392,\n","                     0.9013522911447421,\n","                     0.8989670762042983,\n","                     0.9013573816045053,\n","                     0.9042630380839086,\n","                     0.9023864613139143,\n","                     0.9030240060695442,\n","                     0.9057773354693068,\n","                     0.9055810676540789,\n","                     0.9074353063340149,\n","                     0.9049885142752772],\n","                    [0.4943160407009841,\n","                     0.5959514809202598,\n","                     0.7423375260145336,\n","                     0.7925348859938863,\n","                     0.8119867962277628,\n","                     0.8213555157013473,\n","                     0.821366691952512,\n","                     0.8308847547470057,\n","                     0.8356051251679163,\n","                     0.8334078133922669,\n","                     0.834507580103544,\n","                     0.8460697420946457,\n","                     0.8497481177882236,\n","                     0.8426578690988862,\n","                     0.845329244794401,\n","                     0.8534309895145034,\n","                     0.8573053382289932,\n","                     0.8531420844523934,\n","                     0.8531569636963702,\n","                     0.8592917941451101,\n","                     0.8572340667654974,\n","                     0.8567289004306428,\n","                     0.8607232443227978,\n","                     0.8640300672161056,\n","                     0.8647616544354021,\n","                     0.8652291428015014,\n","                     0.866590573064816,\n","                     0.866937754533494,\n","                     0.8685589207835904,\n","                     0.8670591266333578,\n","                     0.8681327959778062,\n","                     0.8691879923126181,\n","                     0.868679610611471,\n","                     0.8698026828891859,\n","                     0.870134779700182,\n","                     0.8641739212065173,\n","                     0.8718639505146091,\n","                     0.8719503467550466,\n","                     0.8712558705247998,\n","                     0.8725937806930382,\n","                     0.8714852421973321,\n","                     0.8741234010180229,\n","                     0.8735807651131264,\n","                     0.8719519489605393,\n","                     0.8718393287294018,\n","                     0.8691741592633491,\n","                     0.8686100038605084,\n","                     0.8751477954665757,\n","                     0.8741352998322148,\n","                     0.8715491519243017],\n","                    "]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["[0.5516876081188758,\n","                     0.661188691364668,\n","                     0.8017243152418764,\n","                     0.8454851949232801,\n","                     0.8488306781998949,\n","                     0.8610990919097142,\n","                     0.8618268419351565,\n","                     0.8699747668387131,\n","                     0.8735616677070202,\n","                     0.8777509965320582,\n","                     0.8752997670901727,\n","                     0.8787586850913078,\n","                     0.8794997139237147,\n","                     0.8826833230380638,\n","                     0.8831068566285173,\n","                     0.8796004523657402,\n","                     0.8853429227667405,\n","                     0.8861601551723409,\n","                     0.8879795598173822,\n","                     0.885169416904841,\n","                     0.8864069026773418,\n","                     0.8871924290170525,\n","                     0.8889424784829698,\n","                     0.8900414327271584,\n","                     0.8912412920298148,\n","                     0.8908317469410163,\n","                     0.8887522653025663,\n","                     0.8893212397279642,\n","                     0.8919443736943958,\n","                     0.89335353255248,\n","                     0.8955134667665687,\n","                     0.8939138210442559,\n","                     0.8956011149668835,\n","                     0.8897769278330631,\n","                     0.8929177124542941,\n","                     0.8919503314880397,\n","                     0.8955903321614174,\n","                     0.896217650379033,\n","                     0.8957594605501356,\n","                     0.8954671565650322,\n","                     0.8945504059177227,\n","                     0.8919944217674157,\n","                     0.8979158325262775,\n","                     0.8976215863051094,\n","                     0.8957330115328035,\n","                     0.8988428954937144,\n","                     0.8978570356132789,\n","                     0.89524234438141,\n","                     0.898037487649221,\n","                     0.8964938618969301],\n","                    [0.5404771414729624,\n","                     0.7275412861281796,\n","                     0.7991931162739284,\n","                     0.8269444069806624,\n","                     0.8439743572904834,\n","                     0.8506940491979379,\n","                     0.8566088436616738,\n","                     0.8607817991911701,\n","                     0.862397392771433,\n","                     0.8648898971645177,\n","                     0.8674681375668368,\n","                     0.8630377215723742,\n","                     0.8707989470753982,\n","                     0.8718962258798179,\n","                     0.8713512726257661,\n","                     0.8722494499071075,\n","                     0.8727557395687319,\n","                     0.876857753709679,\n","                     0.8755967991871783,\n","                     0.8761302393868267,\n","                     0.8766093796178253,\n","                     0.8828716407337733,\n","                     0.8796644195526452,\n","                     0.8810231020748789,\n","                     0.8840539178270282,\n","                     0.8873852181220047,\n","                     0.8852806918465382,\n","                     0.8907187435665525,\n","                     0.8899088710088823,\n","                     0.8898970088645163,\n","                     0.8915203317723487,\n","                     0.8948731425885904,\n","                     0.8929188275146587,\n","                     0.8957065503543947,\n","                     0.8923668686805091,\n","                     0.896465750653046,\n","                     0.8964086340768361,\n","                     0.8957305662815548,\n","                     0.8973693416338414,\n","                     0.895788818370678,\n","                     0.8996255491284985,\n","                     0.8985568689882013,\n","                     0.8940820456274857,\n","                     0.8971086704196999,\n","                     0.8973362160275888,\n","                     0.8997845241188286,\n","                     0.9001574289552917,\n","                     0.9008436102678191,\n","                     0.8993280230325787,\n","                     0.8994028465938747],\n","                    [0.5519751405626734,\n","                     0.6401314725880838,\n","                     0.7783568709590368,\n","                     0.8186728238570221,\n","                     0.8287621034022568,\n","                     0.8404780886416807,\n","                     0.8430534007226966,\n","                     0.8399970387648719,\n","                     0.8495130636859873,\n","                     0.8508380383762802,\n","                     0.8564213938273773,\n","                     0.8563384842407706,\n","                     0.8611372245962958,\n","                     0.8636434895482007,\n","                     0.8672883480868573,\n","                     0.867330306041046,\n","                     0.8663438948590638,\n","                     0.867951257554029,\n","                     0.8705375517658126,\n","                     0.8712211921836265,\n","                     0.873147456001113,\n","                     0.8714812076723504,\n","                     0.8755096332879827,\n","                     0.8768377686410769,\n","                     0.8745248163596321,\n","                     0.8744076251401492,\n","                     0.876385610038299,\n","                     0.87536012220639,\n","                     0.8755360447340671,\n","                     0.8748197837660238,\n","                     0.8806042483891223,\n","                     0.8784477206977376,\n","                     0.8772232445536379,\n","                     0.8808173781869509,\n","                     0.878102619191422,\n","                     0.8793415256955002,\n","                     0.8813318019819592,\n","                     0.8791354564994703,\n","                     0.877903911133507,\n","                     0.8821888322464279,\n","                     0.881856362109992,\n","                     0.8790679994281678,\n","                     0.882807159535606,\n","                     0.8811736624506247,\n","                     0.8829063903605209,\n","                     0.8819317245207281,\n","                     0.88129912280376,\n","                     0.8838720392117704,\n","                     0.8833510263360813,\n","                     0.8825503832205848]]}\n","Training Model: BiLSTM_Deep, Fold: 1\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.6049 - loss: 0.6701\n","Epoch 1 - MCC: 0.6375\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 229ms/step - accuracy: 0.6075 - loss: 0.6686 - val_accuracy: 0.8190 - val_loss: 0.4741 - mcc: 0.6375\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.8462 - loss: 0.3847\n","Epoch 2 - MCC: 0.8349\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 182ms/step - accuracy: 0.8471 - loss: 0.3822 - val_accuracy: 0.9164 - val_loss: 0.2088 - mcc: 0.8349\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9116 - loss: 0.2223\n","Epoch 3 - MCC: 0.8467\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 179ms/step - accuracy: 0.9118 - loss: 0.2218 - val_accuracy: 0.9233 - val_loss: 0.1868 - mcc: 0.8467\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9248 - loss: 0.1848\n","Epoch 4 - MCC: 0.8801\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9249 - loss: 0.1847 - val_accuracy: 0.9403 - val_loss: 0.1517 - mcc: 0.8801\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9336 - loss: 0.1662\n","Epoch 5 - MCC: 0.8862\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.9336 - loss: 0.1663 - val_accuracy: 0.9433 - val_loss: 0.1419 - mcc: 0.8862\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9338 - loss: 0.1621\n","Epoch 6 - MCC: 0.8914\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9339 - loss: 0.1620 - val_accuracy: 0.9459 - val_loss: 0.1331 - mcc: 0.8914\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9328 - loss: 0.1647\n","Epoch 7 - MCC: 0.8916\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 185ms/step - accuracy: 0.9329 - loss: 0.1643 - val_accuracy: 0.9458 - val_loss: 0.1314 - mcc: 0.8916\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9378 - loss: 0.1522\n","Epoch 8 - MCC: 0.8964\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 177ms/step - accuracy: 0.9378 - loss: 0.1521 - val_accuracy: 0.9484 - val_loss: 0.1261 - mcc: 0.8964\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9407 - loss: 0.1476\n","Epoch 9 - MCC: 0.8960\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9406 - loss: 0.1477 - val_accuracy: 0.9482 - val_loss: 0.1267 - mcc: 0.8960\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9426 - loss: 0.1387\n","Epoch 10 - MCC: 0.8978\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.9425 - loss: 0.1389 - val_accuracy: 0.9491 - val_loss: 0.1261 - mcc: 0.8978\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9426 - loss: 0.1400\n","Epoch 11 - MCC: 0.8954\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.9426 - loss: 0.1400 - val_accuracy: 0.9479 - val_loss: 0.1276 - mcc: 0.8954\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9413 - loss: 0.1432\n","Epoch 12 - MCC: 0.8996\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9414 - loss: 0.1430 - val_accuracy: 0.9500 - val_loss: 0.1210 - mcc: 0.8996\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9440 - loss: 0.1383\n","Epoch 13 - MCC: 0.8997\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 197ms/step - accuracy: 0.9440 - loss: 0.1384 - val_accuracy: 0.9500 - val_loss: 0.1256 - mcc: 0.8997\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9461 - loss: 0.1334\n","Epoch 14 - MCC: 0.9015\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9460 - loss: 0.1337 - val_accuracy: 0.9510 - val_loss: 0.1219 - mcc: 0.9015\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9437 - loss: 0.1398\n","Epoch 15 - MCC: 0.9016\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 184ms/step - accuracy: 0.9438 - loss: 0.1397 - val_accuracy: 0.9510 - val_loss: 0.1199 - mcc: 0.9016\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.9463 - loss: 0.1318\n","Epoch 16 - MCC: 0.9026\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - accuracy: 0.9462 - loss: 0.1319 - val_accuracy: 0.9515 - val_loss: 0.1177 - mcc: 0.9026\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9430 - loss: 0.1396\n","Epoch 17 - MCC: 0.9080\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - accuracy: 0.9432 - loss: 0.1393 - val_accuracy: 0.9542 - val_loss: 0.1112 - mcc: 0.9080\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9509 - loss: 0.1213\n","Epoch 18 - MCC: 0.9031\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 187ms/step - accuracy: 0.9508 - loss: 0.1215 - val_accuracy: 0.9517 - val_loss: 0.1155 - mcc: 0.9031\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9475 - loss: 0.1290\n","Epoch 19 - MCC: 0.9103\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 174ms/step - accuracy: 0.9475 - loss: 0.1290 - val_accuracy: 0.9553 - val_loss: 0.1099 - mcc: 0.9103\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9457 - loss: 0.1348\n","Epoch 20 - MCC: 0.9096\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.9458 - loss: 0.1345 - val_accuracy: 0.9550 - val_loss: 0.1087 - mcc: 0.9096\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9503 - loss: 0.1234\n","Epoch 21 - MCC: 0.9135\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 198ms/step - accuracy: 0.9503 - loss: 0.1233 - val_accuracy: 0.9569 - val_loss: 0.1051 - mcc: 0.9135\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9535 - loss: 0.1163\n","Epoch 22 - MCC: 0.9126\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9534 - loss: 0.1164 - val_accuracy: 0.9565 - val_loss: 0.1043 - mcc: 0.9126\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9497 - loss: 0.1227\n","Epoch 23 - MCC: 0.9147\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 169ms/step - accuracy: 0.9497 - loss: 0.1227 - val_accuracy: 0.9575 - val_loss: 0.1064 - mcc: 0.9147\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9509 - loss: 0.1214\n","Epoch 24 - MCC: 0.9175\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 198ms/step - accuracy: 0.9509 - loss: 0.1213 - val_accuracy: 0.9589 - val_loss: 0.1006 - mcc: 0.9175\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9513 - loss: 0.1191\n","Epoch 25 - MCC: 0.9142\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9514 - loss: 0.1190 - val_accuracy: 0.9572 - val_loss: 0.1043 - mcc: 0.9142\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9598 - loss: 0.0996\n","Epoch 26 - MCC: 0.9164\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 182ms/step - accuracy: 0.9596 - loss: 0.1002 - val_accuracy: 0.9583 - val_loss: 0.1015 - mcc: 0.9164\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.9547 - loss: 0.1129\n","Epoch 27 - MCC: 0.9157\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9546 - loss: 0.1130 - val_accuracy: 0.9580 - val_loss: 0.1019 - mcc: 0.9157\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9533 - loss: 0.1138\n","Epoch 28 - MCC: 0.9186\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9533 - loss: 0.1138 - val_accuracy: 0.9595 - val_loss: 0.0979 - mcc: 0.9186\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9543 - loss: 0.1107\n","Epoch 29 - MCC: 0.9201\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 206ms/step - accuracy: 0.9544 - loss: 0.1106 - val_accuracy: 0.9602 - val_loss: 0.0971 - mcc: 0.9201\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9560 - loss: 0.1072\n","Epoch 30 - MCC: 0.9213\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9559 - loss: 0.1072 - val_accuracy: 0.9608 - val_loss: 0.0958 - mcc: 0.9213\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9568 - loss: 0.1074\n","Epoch 31 - MCC: 0.9206\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 184ms/step - accuracy: 0.9568 - loss: 0.1074 - val_accuracy: 0.9604 - val_loss: 0.0960 - mcc: 0.9206\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9532 - loss: 0.1157\n","Epoch 32 - MCC: 0.9203\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 192ms/step - accuracy: 0.9533 - loss: 0.1155 - val_accuracy: 0.9603 - val_loss: 0.0968 - mcc: 0.9203\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9579 - loss: 0.1034\n","Epoch 33 - MCC: 0.9168\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9578 - loss: 0.1036 - val_accuracy: 0.9585 - val_loss: 0.1031 - mcc: 0.9168\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9505 - loss: 0.1189\n","Epoch 34 - MCC: 0.8861\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 211ms/step - accuracy: 0.9506 - loss: 0.1188 - val_accuracy: 0.9432 - val_loss: 0.1349 - mcc: 0.8861\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9525 - loss: 0.1167\n","Epoch 35 - MCC: 0.9151\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9524 - loss: 0.1168 - val_accuracy: 0.9577 - val_loss: 0.1035 - mcc: 0.9151\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9531 - loss: 0.1150\n","Epoch 36 - MCC: 0.9182\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - accuracy: 0.9531 - loss: 0.1150 - val_accuracy: 0.9592 - val_loss: 0.0983 - mcc: 0.9182\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9562 - loss: 0.1098\n","Epoch 37 - MCC: 0.9264\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - accuracy: 0.9562 - loss: 0.1097 - val_accuracy: 0.9633 - val_loss: 0.0908 - mcc: 0.9264\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9595 - loss: 0.0996\n","Epoch 38 - MCC: 0.9071\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - accuracy: 0.9594 - loss: 0.0998 - val_accuracy: 0.9537 - val_loss: 0.1091 - mcc: 0.9071\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9449 - loss: 0.1332\n","Epoch 39 - MCC: 0.9135\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9451 - loss: 0.1329 - val_accuracy: 0.9569 - val_loss: 0.1035 - mcc: 0.9135\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9512 - loss: 0.1186\n","Epoch 40 - MCC: 0.9147\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 182ms/step - accuracy: 0.9513 - loss: 0.1185 - val_accuracy: 0.9575 - val_loss: 0.1031 - mcc: 0.9147\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9570 - loss: 0.1053\n","Epoch 41 - MCC: 0.9241\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9570 - loss: 0.1055 - val_accuracy: 0.9622 - val_loss: 0.0932 - mcc: 0.9241\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9617 - loss: 0.0957\n","Epoch 42 - MCC: 0.9244\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 192ms/step - accuracy: 0.9616 - loss: 0.0961 - val_accuracy: 0.9623 - val_loss: 0.0923 - mcc: 0.9244\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9612 - loss: 0.0952\n","Epoch 43 - MCC: 0.9233\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - accuracy: 0.9610 - loss: 0.0957 - val_accuracy: 0.9618 - val_loss: 0.0937 - mcc: 0.9233\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9598 - loss: 0.1008\n","Epoch 44 - MCC: 0.9269\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9598 - loss: 0.1010 - val_accuracy: 0.9635 - val_loss: 0.0889 - mcc: 0.9269\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9579 - loss: 0.1047\n","Epoch 45 - MCC: 0.9247\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 199ms/step - accuracy: 0.9579 - loss: 0.1046 - val_accuracy: 0.9625 - val_loss: 0.0923 - mcc: 0.9247\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9577 - loss: 0.1036\n","Epoch 46 - MCC: 0.9246\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9577 - loss: 0.1035 - val_accuracy: 0.9624 - val_loss: 0.0920 - mcc: 0.9246\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9612 - loss: 0.0964\n","Epoch 47 - MCC: 0.9278\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9612 - loss: 0.0965 - val_accuracy: 0.9640 - val_loss: 0.0875 - mcc: 0.9278\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9607 - loss: 0.0969\n","Epoch 48 - MCC: 0.9239\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 175ms/step - accuracy: 0.9607 - loss: 0.0969 - val_accuracy: 0.9621 - val_loss: 0.0925 - mcc: 0.9239\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9605 - loss: 0.0979\n","Epoch 49 - MCC: 0.9259\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9605 - loss: 0.0979 - val_accuracy: 0.9631 - val_loss: 0.0919 - mcc: 0.9259\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9604 - loss: 0.0985\n","Epoch 50 - MCC: 0.9295\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 186ms/step - accuracy: 0.9604 - loss: 0.0985 - val_accuracy: 0.9648 - val_loss: 0.0867 - mcc: 0.9295\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 2\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.6789 - loss: 0.6486\n","Epoch 1 - MCC: 0.6678\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.6815 - loss: 0.6460 - val_accuracy: 0.8335 - val_loss: 0.3970 - mcc: 0.6678\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8640 - loss: 0.3262\n","Epoch 2 - MCC: 0.7881\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.8644 - loss: 0.3251 - val_accuracy: 0.8939 - val_loss: 0.2531 - mcc: 0.7881\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9110 - loss: 0.2123\n","Epoch 3 - MCC: 0.8302\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 207ms/step - accuracy: 0.9113 - loss: 0.2119 - val_accuracy: 0.9151 - val_loss: 0.2023 - mcc: 0.8302\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9268 - loss: 0.1775\n","Epoch 4 - MCC: 0.8354\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9267 - loss: 0.1777 - val_accuracy: 0.9177 - val_loss: 0.1983 - mcc: 0.8354\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9344 - loss: 0.1629\n","Epoch 5 - MCC: 0.8439\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9343 - loss: 0.1630 - val_accuracy: 0.9217 - val_loss: 0.1885 - mcc: 0.8439\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9315 - loss: 0.1703\n","Epoch 6 - MCC: 0.8539\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.9317 - loss: 0.1698 - val_accuracy: 0.9270 - val_loss: 0.1792 - mcc: 0.8539\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9346 - loss: 0.1570\n","Epoch 7 - MCC: 0.8606\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9347 - loss: 0.1567 - val_accuracy: 0.9304 - val_loss: 0.1705 - mcc: 0.8606\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9396 - loss: 0.1475\n","Epoch 8 - MCC: 0.8509\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9396 - loss: 0.1474 - val_accuracy: 0.9254 - val_loss: 0.1779 - mcc: 0.8509\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9354 - loss: 0.1564\n","Epoch 9 - MCC: 0.8623\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9356 - loss: 0.1561 - val_accuracy: 0.9312 - val_loss: 0.1651 - mcc: 0.8623\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9417 - loss: 0.1412\n","Epoch 10 - MCC: 0.8669\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9416 - loss: 0.1414 - val_accuracy: 0.9335 - val_loss: 0.1599 - mcc: 0.8669\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9465 - loss: 0.1319\n","Epoch 11 - MCC: 0.8611\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 188ms/step - accuracy: 0.9464 - loss: 0.1320 - val_accuracy: 0.9302 - val_loss: 0.1701 - mcc: 0.8611\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9438 - loss: 0.1358\n","Epoch 12 - MCC: 0.8686\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9438 - loss: 0.1357 - val_accuracy: 0.9344 - val_loss: 0.1585 - mcc: 0.8686\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9420 - loss: 0.1427\n","Epoch 13 - MCC: 0.8643\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9420 - loss: 0.1427 - val_accuracy: 0.9322 - val_loss: 0.1662 - mcc: 0.8643\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9477 - loss: 0.1277\n","Epoch 14 - MCC: 0.8643\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.9476 - loss: 0.1279 - val_accuracy: 0.9319 - val_loss: 0.1649 - mcc: 0.8643\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9464 - loss: 0.1318\n","Epoch 15 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9463 - loss: 0.1319 - val_accuracy: 0.9351 - val_loss: 0.1612 - mcc: 0.8701\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9454 - loss: 0.1330\n","Epoch 16 - MCC: 0.8705\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.9454 - loss: 0.1329 - val_accuracy: 0.9353 - val_loss: 0.1599 - mcc: 0.8705\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9406 - loss: 0.1460\n","Epoch 17 - MCC: 0.8721\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 184ms/step - accuracy: 0.9408 - loss: 0.1455 - val_accuracy: 0.9361 - val_loss: 0.1566 - mcc: 0.8721\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9500 - loss: 0.1218\n","Epoch 18 - MCC: 0.8748\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9499 - loss: 0.1219 - val_accuracy: 0.9375 - val_loss: 0.1507 - mcc: 0.8748\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9482 - loss: 0.1268\n","Epoch 19 - MCC: 0.8734\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 187ms/step - accuracy: 0.9483 - loss: 0.1266 - val_accuracy: 0.9368 - val_loss: 0.1514 - mcc: 0.8734\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9469 - loss: 0.1283\n","Epoch 20 - MCC: 0.8752\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9471 - loss: 0.1280 - val_accuracy: 0.9377 - val_loss: 0.1489 - mcc: 0.8752\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9453 - loss: 0.1329\n","Epoch 21 - MCC: 0.8790\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9455 - loss: 0.1324 - val_accuracy: 0.9396 - val_loss: 0.1464 - mcc: 0.8790\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9570 - loss: 0.1055\n","Epoch 22 - MCC: 0.8741\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 205ms/step - accuracy: 0.9568 - loss: 0.1059 - val_accuracy: 0.9371 - val_loss: 0.1541 - mcc: 0.8741\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9539 - loss: 0.1113\n","Epoch 23 - MCC: 0.8761\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9538 - loss: 0.1116 - val_accuracy: 0.9381 - val_loss: 0.1485 - mcc: 0.8761\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9515 - loss: 0.1181\n","Epoch 24 - MCC: 0.8825\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 181ms/step - accuracy: 0.9515 - loss: 0.1180 - val_accuracy: 0.9414 - val_loss: 0.1448 - mcc: 0.8825\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9573 - loss: 0.1067\n","Epoch 25 - MCC: 0.8790\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - accuracy: 0.9572 - loss: 0.1068 - val_accuracy: 0.9395 - val_loss: 0.1474 - mcc: 0.8790\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9558 - loss: 0.1083\n","Epoch 26 - MCC: 0.8850\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9558 - loss: 0.1085 - val_accuracy: 0.9426 - val_loss: 0.1410 - mcc: 0.8850\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9570 - loss: 0.1068\n","Epoch 27 - MCC: 0.8848\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 199ms/step - accuracy: 0.9569 - loss: 0.1070 - val_accuracy: 0.9425 - val_loss: 0.1400 - mcc: 0.8848\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9537 - loss: 0.1137\n","Epoch 28 - MCC: 0.8841\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9538 - loss: 0.1135 - val_accuracy: 0.9422 - val_loss: 0.1396 - mcc: 0.8841\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9550 - loss: 0.1093\n","Epoch 29 - MCC: 0.8856\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9550 - loss: 0.1093 - val_accuracy: 0.9427 - val_loss: 0.1421 - mcc: 0.8856\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9551 - loss: 0.1106\n","Epoch 30 - MCC: 0.8806\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 208ms/step - accuracy: 0.9552 - loss: 0.1105 - val_accuracy: 0.9404 - val_loss: 0.1497 - mcc: 0.8806\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9586 - loss: 0.1015\n","Epoch 31 - MCC: 0.8873\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - accuracy: 0.9586 - loss: 0.1016 - val_accuracy: 0.9436 - val_loss: 0.1386 - mcc: 0.8873\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9577 - loss: 0.1058\n","Epoch 32 - MCC: 0.8808\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 179ms/step - accuracy: 0.9576 - loss: 0.1060 - val_accuracy: 0.9405 - val_loss: 0.1450 - mcc: 0.8808\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9553 - loss: 0.1114\n","Epoch 33 - MCC: 0.8882\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9554 - loss: 0.1112 - val_accuracy: 0.9442 - val_loss: 0.1362 - mcc: 0.8882\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9630 - loss: 0.0924\n","Epoch 34 - MCC: 0.8865\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.9629 - loss: 0.0928 - val_accuracy: 0.9433 - val_loss: 0.1413 - mcc: 0.8865\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9559 - loss: 0.1095\n","Epoch 35 - MCC: 0.8886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9560 - loss: 0.1092 - val_accuracy: 0.9444 - val_loss: 0.1367 - mcc: 0.8886\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9612 - loss: 0.0963\n","Epoch 36 - MCC: 0.8879\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9611 - loss: 0.0965 - val_accuracy: 0.9441 - val_loss: 0.1367 - mcc: 0.8879\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9629 - loss: 0.0915\n","Epoch 37 - MCC: 0.8858\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.9629 - loss: 0.0917 - val_accuracy: 0.9430 - val_loss: 0.1372 - mcc: 0.8858\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9623 - loss: 0.0926\n","Epoch 38 - MCC: 0.8909\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9623 - loss: 0.0928 - val_accuracy: 0.9453 - val_loss: 0.1387 - mcc: 0.8909\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9611 - loss: 0.0999\n","Epoch 39 - MCC: 0.8892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.9611 - loss: 0.0998 - val_accuracy: 0.9447 - val_loss: 0.1347 - mcc: 0.8892\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9595 - loss: 0.0989\n","Epoch 40 - MCC: 0.8938\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 169ms/step - accuracy: 0.9595 - loss: 0.0989 - val_accuracy: 0.9470 - val_loss: 0.1287 - mcc: 0.8938\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9628 - loss: 0.0943\n","Epoch 41 - MCC: 0.8856\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9628 - loss: 0.0944 - val_accuracy: 0.9429 - val_loss: 0.1367 - mcc: 0.8856\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9595 - loss: 0.0987\n","Epoch 42 - MCC: 0.8889\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.9594 - loss: 0.0990 - val_accuracy: 0.9445 - val_loss: 0.1348 - mcc: 0.8889\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9604 - loss: 0.0990\n","Epoch 43 - MCC: 0.8949\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9604 - loss: 0.0990 - val_accuracy: 0.9476 - val_loss: 0.1311 - mcc: 0.8949\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9620 - loss: 0.0933\n","Epoch 44 - MCC: 0.8932\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9620 - loss: 0.0933 - val_accuracy: 0.9467 - val_loss: 0.1302 - mcc: 0.8932\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9672 - loss: 0.0806\n","Epoch 45 - MCC: 0.8929\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 205ms/step - accuracy: 0.9671 - loss: 0.0810 - val_accuracy: 0.9464 - val_loss: 0.1336 - mcc: 0.8929\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9632 - loss: 0.0898\n","Epoch 46 - MCC: 0.8929\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9632 - loss: 0.0898 - val_accuracy: 0.9465 - val_loss: 0.1326 - mcc: 0.8929\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9605 - loss: 0.0964\n","Epoch 47 - MCC: 0.8960\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.9606 - loss: 0.0962 - val_accuracy: 0.9481 - val_loss: 0.1278 - mcc: 0.8960\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9648 - loss: 0.0873\n","Epoch 48 - MCC: 0.8960\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 183ms/step - accuracy: 0.9648 - loss: 0.0873 - val_accuracy: 0.9481 - val_loss: 0.1274 - mcc: 0.8960\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9643 - loss: 0.0873\n","Epoch 49 - MCC: 0.8942\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9643 - loss: 0.0874 - val_accuracy: 0.9472 - val_loss: 0.1269 - mcc: 0.8942\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9664 - loss: 0.0852\n","Epoch 50 - MCC: 0.8961\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - accuracy: 0.9663 - loss: 0.0852 - val_accuracy: 0.9481 - val_loss: 0.1302 - mcc: 0.8961\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 3\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.6162 - loss: 0.6654\n","Epoch 1 - MCC: 0.6677\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.6187 - loss: 0.6636 - val_accuracy: 0.8346 - val_loss: 0.4583 - mcc: 0.6677\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8470 - loss: 0.3969\n","Epoch 2 - MCC: 0.7925\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 190ms/step - accuracy: 0.8476 - loss: 0.3948 - val_accuracy: 0.8958 - val_loss: 0.2476 - mcc: 0.7925\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.8969 - loss: 0.2473\n","Epoch 3 - MCC: 0.8450\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.8972 - loss: 0.2465 - val_accuracy: 0.9222 - val_loss: 0.1895 - mcc: 0.8450\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9223 - loss: 0.1885\n","Epoch 4 - MCC: 0.8533\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.9223 - loss: 0.1884 - val_accuracy: 0.9269 - val_loss: 0.1753 - mcc: 0.8533\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9287 - loss: 0.1736\n","Epoch 5 - MCC: 0.8705\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9288 - loss: 0.1734 - val_accuracy: 0.9355 - val_loss: 0.1578 - mcc: 0.8705\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9361 - loss: 0.1571\n","Epoch 6 - MCC: 0.8703\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9360 - loss: 0.1572 - val_accuracy: 0.9353 - val_loss: 0.1556 - mcc: 0.8703\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9359 - loss: 0.1559\n","Epoch 7 - MCC: 0.8536\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 202ms/step - accuracy: 0.9358 - loss: 0.1559 - val_accuracy: 0.9266 - val_loss: 0.1726 - mcc: 0.8536\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9306 - loss: 0.1652\n","Epoch 8 - MCC: 0.8829\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9307 - loss: 0.1650 - val_accuracy: 0.9414 - val_loss: 0.1409 - mcc: 0.8829\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9406 - loss: 0.1439\n","Epoch 9 - MCC: 0.8905\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9405 - loss: 0.1439 - val_accuracy: 0.9455 - val_loss: 0.1344 - mcc: 0.8905\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9360 - loss: 0.1546\n","Epoch 10 - MCC: 0.8910\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.9362 - loss: 0.1542 - val_accuracy: 0.9456 - val_loss: 0.1334 - mcc: 0.8910\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9437 - loss: 0.1384\n","Epoch 11 - MCC: 0.8890\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9437 - loss: 0.1384 - val_accuracy: 0.9447 - val_loss: 0.1333 - mcc: 0.8890\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9490 - loss: 0.1256\n","Epoch 12 - MCC: 0.8883\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9488 - loss: 0.1261 - val_accuracy: 0.9443 - val_loss: 0.1347 - mcc: 0.8883\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9444 - loss: 0.1347\n","Epoch 13 - MCC: 0.8877\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.9444 - loss: 0.1348 - val_accuracy: 0.9440 - val_loss: 0.1342 - mcc: 0.8877\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9473 - loss: 0.1292\n","Epoch 14 - MCC: 0.8937\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9471 - loss: 0.1294 - val_accuracy: 0.9471 - val_loss: 0.1284 - mcc: 0.8937\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9426 - loss: 0.1394\n","Epoch 15 - MCC: 0.8936\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9427 - loss: 0.1391 - val_accuracy: 0.9470 - val_loss: 0.1289 - mcc: 0.8936\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9481 - loss: 0.1258\n","Epoch 16 - MCC: 0.8881\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 196ms/step - accuracy: 0.9480 - loss: 0.1261 - val_accuracy: 0.9443 - val_loss: 0.1385 - mcc: 0.8881\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9435 - loss: 0.1383\n","Epoch 17 - MCC: 0.8963\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9435 - loss: 0.1381 - val_accuracy: 0.9483 - val_loss: 0.1267 - mcc: 0.8963\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9499 - loss: 0.1235\n","Epoch 18 - MCC: 0.8952\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9498 - loss: 0.1238 - val_accuracy: 0.9478 - val_loss: 0.1290 - mcc: 0.8952\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9463 - loss: 0.1288\n","Epoch 19 - MCC: 0.8930\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 193ms/step - accuracy: 0.9463 - loss: 0.1289 - val_accuracy: 0.9464 - val_loss: 0.1303 - mcc: 0.8930\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9512 - loss: 0.1190\n","Epoch 20 - MCC: 0.8947\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9510 - loss: 0.1194 - val_accuracy: 0.9476 - val_loss: 0.1265 - mcc: 0.8947\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9483 - loss: 0.1253\n","Epoch 21 - MCC: 0.8992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 185ms/step - accuracy: 0.9483 - loss: 0.1253 - val_accuracy: 0.9498 - val_loss: 0.1249 - mcc: 0.8992\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9506 - loss: 0.1201\n","Epoch 22 - MCC: 0.9026\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - accuracy: 0.9505 - loss: 0.1202 - val_accuracy: 0.9515 - val_loss: 0.1175 - mcc: 0.9026\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9479 - loss: 0.1281\n","Epoch 23 - MCC: 0.8990\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9480 - loss: 0.1279 - val_accuracy: 0.9497 - val_loss: 0.1223 - mcc: 0.8990\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9524 - loss: 0.1174\n","Epoch 24 - MCC: 0.9018\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 204ms/step - accuracy: 0.9523 - loss: 0.1174 - val_accuracy: 0.9511 - val_loss: 0.1216 - mcc: 0.9018\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9506 - loss: 0.1218\n","Epoch 25 - MCC: 0.9043\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9506 - loss: 0.1217 - val_accuracy: 0.9523 - val_loss: 0.1175 - mcc: 0.9043\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9548 - loss: 0.1108\n","Epoch 26 - MCC: 0.9070\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9547 - loss: 0.1110 - val_accuracy: 0.9537 - val_loss: 0.1154 - mcc: 0.9070\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9528 - loss: 0.1154\n","Epoch 27 - MCC: 0.9077\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 196ms/step - accuracy: 0.9528 - loss: 0.1154 - val_accuracy: 0.9540 - val_loss: 0.1159 - mcc: 0.9077\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9543 - loss: 0.1144\n","Epoch 28 - MCC: 0.9083\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9543 - loss: 0.1144 - val_accuracy: 0.9543 - val_loss: 0.1135 - mcc: 0.9083\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9560 - loss: 0.1086\n","Epoch 29 - MCC: 0.9081\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 167ms/step - accuracy: 0.9560 - loss: 0.1086 - val_accuracy: 0.9542 - val_loss: 0.1135 - mcc: 0.9081\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9549 - loss: 0.1100\n","Epoch 30 - MCC: 0.9085\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - accuracy: 0.9549 - loss: 0.1100 - val_accuracy: 0.9544 - val_loss: 0.1132 - mcc: 0.9085\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9594 - loss: 0.1007\n","Epoch 31 - MCC: 0.9052\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9593 - loss: 0.1009 - val_accuracy: 0.9527 - val_loss: 0.1171 - mcc: 0.9052\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9583 - loss: 0.1029\n","Epoch 32 - MCC: 0.9134\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 174ms/step - accuracy: 0.9582 - loss: 0.1031 - val_accuracy: 0.9568 - val_loss: 0.1097 - mcc: 0.9134\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9540 - loss: 0.1144\n","Epoch 33 - MCC: 0.9120\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.9541 - loss: 0.1141 - val_accuracy: 0.9562 - val_loss: 0.1084 - mcc: 0.9120\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9640 - loss: 0.0900\n","Epoch 34 - MCC: 0.9120\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9638 - loss: 0.0904 - val_accuracy: 0.9562 - val_loss: 0.1102 - mcc: 0.9120\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9614 - loss: 0.0965\n","Epoch 35 - MCC: 0.9055\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 0.9613 - loss: 0.0968 - val_accuracy: 0.9527 - val_loss: 0.1201 - mcc: 0.9055\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9538 - loss: 0.1126\n","Epoch 36 - MCC: 0.9056\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 174ms/step - accuracy: 0.9538 - loss: 0.1126 - val_accuracy: 0.9530 - val_loss: 0.1166 - mcc: 0.9056\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9558 - loss: 0.1092\n","Epoch 37 - MCC: 0.9100\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9557 - loss: 0.1092 - val_accuracy: 0.9552 - val_loss: 0.1124 - mcc: 0.9100\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9556 - loss: 0.1083\n","Epoch 38 - MCC: 0.9100\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 205ms/step - accuracy: 0.9556 - loss: 0.1082 - val_accuracy: 0.9551 - val_loss: 0.1100 - mcc: 0.9100\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9597 - loss: 0.1006\n","Epoch 39 - MCC: 0.9149\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.9597 - loss: 0.1006 - val_accuracy: 0.9576 - val_loss: 0.1055 - mcc: 0.9149\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9582 - loss: 0.1020\n","Epoch 40 - MCC: 0.9113\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 177ms/step - accuracy: 0.9583 - loss: 0.1019 - val_accuracy: 0.9558 - val_loss: 0.1128 - mcc: 0.9113\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9589 - loss: 0.1021\n","Epoch 41 - MCC: 0.9169\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9589 - loss: 0.1020 - val_accuracy: 0.9586 - val_loss: 0.1044 - mcc: 0.9169\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9605 - loss: 0.0984\n","Epoch 42 - MCC: 0.9133\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9605 - loss: 0.0985 - val_accuracy: 0.9568 - val_loss: 0.1077 - mcc: 0.9133\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.9580 - loss: 0.1019\n","Epoch 43 - MCC: 0.9179\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 201ms/step - accuracy: 0.9580 - loss: 0.1017 - val_accuracy: 0.9591 - val_loss: 0.1020 - mcc: 0.9179\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9632 - loss: 0.0909\n","Epoch 44 - MCC: 0.9056\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9630 - loss: 0.0914 - val_accuracy: 0.9530 - val_loss: 0.1163 - mcc: 0.9056\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9557 - loss: 0.1079\n","Epoch 45 - MCC: 0.9146\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 160ms/step - accuracy: 0.9558 - loss: 0.1076 - val_accuracy: 0.9575 - val_loss: 0.1042 - mcc: 0.9146\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9633 - loss: 0.0891\n","Epoch 46 - MCC: 0.9138\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 205ms/step - accuracy: 0.9632 - loss: 0.0894 - val_accuracy: 0.9571 - val_loss: 0.1050 - mcc: 0.9138\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9638 - loss: 0.0925\n","Epoch 47 - MCC: 0.9171\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9638 - loss: 0.0925 - val_accuracy: 0.9587 - val_loss: 0.1030 - mcc: 0.9171\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9596 - loss: 0.0988\n","Epoch 48 - MCC: 0.9183\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 182ms/step - accuracy: 0.9596 - loss: 0.0987 - val_accuracy: 0.9593 - val_loss: 0.0998 - mcc: 0.9183\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9604 - loss: 0.0987\n","Epoch 49 - MCC: 0.9183\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 175ms/step - accuracy: 0.9605 - loss: 0.0984 - val_accuracy: 0.9593 - val_loss: 0.1042 - mcc: 0.9183\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9625 - loss: 0.0914\n","Epoch 50 - MCC: 0.9190\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9625 - loss: 0.0915 - val_accuracy: 0.9597 - val_loss: 0.1003 - mcc: 0.9190\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 4\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.6165 - loss: 0.6651\n","Epoch 1 - MCC: 0.6360\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.6192 - loss: 0.6631 - val_accuracy: 0.8185 - val_loss: 0.4415 - mcc: 0.6360\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8299 - loss: 0.3906\n","Epoch 2 - MCC: 0.8223\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - accuracy: 0.8314 - loss: 0.3877 - val_accuracy: 0.9105 - val_loss: 0.2149 - mcc: 0.8223\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9128 - loss: 0.2090\n","Epoch 3 - MCC: 0.8194\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9129 - loss: 0.2086 - val_accuracy: 0.9087 - val_loss: 0.2131 - mcc: 0.8194\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9190 - loss: 0.1973\n","Epoch 4 - MCC: 0.8707\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9191 - loss: 0.1971 - val_accuracy: 0.9352 - val_loss: 0.1631 - mcc: 0.8707\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9229 - loss: 0.1850\n","Epoch 5 - MCC: 0.8653\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 202ms/step - accuracy: 0.9231 - loss: 0.1846 - val_accuracy: 0.9322 - val_loss: 0.1695 - mcc: 0.8653\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9265 - loss: 0.1761\n","Epoch 6 - MCC: 0.8836\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9267 - loss: 0.1758 - val_accuracy: 0.9419 - val_loss: 0.1459 - mcc: 0.8836\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9371 - loss: 0.1540\n","Epoch 7 - MCC: 0.8864\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9371 - loss: 0.1539 - val_accuracy: 0.9432 - val_loss: 0.1416 - mcc: 0.8864\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9398 - loss: 0.1461\n","Epoch 8 - MCC: 0.8817\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 194ms/step - accuracy: 0.9398 - loss: 0.1462 - val_accuracy: 0.9408 - val_loss: 0.1450 - mcc: 0.8817\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9394 - loss: 0.1463\n","Epoch 9 - MCC: 0.8892\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9394 - loss: 0.1464 - val_accuracy: 0.9445 - val_loss: 0.1366 - mcc: 0.8892\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9370 - loss: 0.1533\n","Epoch 10 - MCC: 0.8922\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9371 - loss: 0.1530 - val_accuracy: 0.9462 - val_loss: 0.1333 - mcc: 0.8922\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9411 - loss: 0.1432\n","Epoch 11 - MCC: 0.8886\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.9411 - loss: 0.1432 - val_accuracy: 0.9444 - val_loss: 0.1377 - mcc: 0.8886\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9432 - loss: 0.1388\n","Epoch 12 - MCC: 0.8913\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9432 - loss: 0.1388 - val_accuracy: 0.9457 - val_loss: 0.1325 - mcc: 0.8913\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9442 - loss: 0.1361\n","Epoch 13 - MCC: 0.8953\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 183ms/step - accuracy: 0.9441 - loss: 0.1362 - val_accuracy: 0.9478 - val_loss: 0.1287 - mcc: 0.8953\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9446 - loss: 0.1340\n","Epoch 14 - MCC: 0.8908\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - accuracy: 0.9445 - loss: 0.1342 - val_accuracy: 0.9455 - val_loss: 0.1328 - mcc: 0.8908\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9409 - loss: 0.1437\n","Epoch 15 - MCC: 0.8967\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 161ms/step - accuracy: 0.9410 - loss: 0.1435 - val_accuracy: 0.9485 - val_loss: 0.1274 - mcc: 0.8967\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9464 - loss: 0.1315\n","Epoch 16 - MCC: 0.8937\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 203ms/step - accuracy: 0.9463 - loss: 0.1317 - val_accuracy: 0.9469 - val_loss: 0.1313 - mcc: 0.8937\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9461 - loss: 0.1305\n","Epoch 17 - MCC: 0.8994\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9460 - loss: 0.1307 - val_accuracy: 0.9497 - val_loss: 0.1242 - mcc: 0.8994\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9433 - loss: 0.1383\n","Epoch 18 - MCC: 0.8943\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9434 - loss: 0.1383 - val_accuracy: 0.9472 - val_loss: 0.1316 - mcc: 0.8943\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9410 - loss: 0.1428\n","Epoch 19 - MCC: 0.8968\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 204ms/step - accuracy: 0.9410 - loss: 0.1426 - val_accuracy: 0.9482 - val_loss: 0.1305 - mcc: 0.8968\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9455 - loss: 0.1331\n","Epoch 20 - MCC: 0.8916\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9455 - loss: 0.1331 - val_accuracy: 0.9455 - val_loss: 0.1362 - mcc: 0.8916\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9456 - loss: 0.1329\n","Epoch 21 - MCC: 0.8972\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9456 - loss: 0.1329 - val_accuracy: 0.9487 - val_loss: 0.1268 - mcc: 0.8972\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9522 - loss: 0.1176\n","Epoch 22 - MCC: 0.8934\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step - accuracy: 0.9519 - loss: 0.1181 - val_accuracy: 0.9466 - val_loss: 0.1277 - mcc: 0.8934\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9484 - loss: 0.1264\n","Epoch 23 - MCC: 0.9005\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9483 - loss: 0.1266 - val_accuracy: 0.9503 - val_loss: 0.1244 - mcc: 0.9005\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9458 - loss: 0.1324\n","Epoch 24 - MCC: 0.8989\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - accuracy: 0.9459 - loss: 0.1323 - val_accuracy: 0.9495 - val_loss: 0.1252 - mcc: 0.8989\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9505 - loss: 0.1186\n","Epoch 25 - MCC: 0.9009\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9505 - loss: 0.1189 - val_accuracy: 0.9505 - val_loss: 0.1214 - mcc: 0.9009\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9445 - loss: 0.1336\n","Epoch 26 - MCC: 0.9045\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9447 - loss: 0.1332 - val_accuracy: 0.9523 - val_loss: 0.1182 - mcc: 0.9045\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9562 - loss: 0.1078\n","Epoch 27 - MCC: 0.9039\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 193ms/step - accuracy: 0.9560 - loss: 0.1083 - val_accuracy: 0.9519 - val_loss: 0.1214 - mcc: 0.9039\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9505 - loss: 0.1216\n","Epoch 28 - MCC: 0.8979\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9505 - loss: 0.1216 - val_accuracy: 0.9490 - val_loss: 0.1245 - mcc: 0.8979\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9500 - loss: 0.1213\n","Epoch 29 - MCC: 0.9041\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9500 - loss: 0.1214 - val_accuracy: 0.9521 - val_loss: 0.1183 - mcc: 0.9041\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9500 - loss: 0.1189\n","Epoch 30 - MCC: 0.9088\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 204ms/step - accuracy: 0.9500 - loss: 0.1189 - val_accuracy: 0.9544 - val_loss: 0.1128 - mcc: 0.9088\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9542 - loss: 0.1102\n","Epoch 31 - MCC: 0.9058\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9541 - loss: 0.1104 - val_accuracy: 0.9528 - val_loss: 0.1208 - mcc: 0.9058\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9515 - loss: 0.1183\n","Epoch 32 - MCC: 0.9105\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9515 - loss: 0.1183 - val_accuracy: 0.9553 - val_loss: 0.1118 - mcc: 0.9105\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9563 - loss: 0.1077\n","Epoch 33 - MCC: 0.9151\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.9562 - loss: 0.1079 - val_accuracy: 0.9575 - val_loss: 0.1110 - mcc: 0.9151\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9512 - loss: 0.1196\n","Epoch 34 - MCC: 0.9121\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.9513 - loss: 0.1194 - val_accuracy: 0.9561 - val_loss: 0.1100 - mcc: 0.9121\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9559 - loss: 0.1089\n","Epoch 35 - MCC: 0.9163\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - accuracy: 0.9559 - loss: 0.1089 - val_accuracy: 0.9582 - val_loss: 0.1069 - mcc: 0.9163\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.9538 - loss: 0.1120\n","Epoch 36 - MCC: 0.9114\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - accuracy: 0.9538 - loss: 0.1120 - val_accuracy: 0.9556 - val_loss: 0.1121 - mcc: 0.9114\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9502 - loss: 0.1201\n","Epoch 37 - MCC: 0.9152\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9504 - loss: 0.1197 - val_accuracy: 0.9575 - val_loss: 0.1106 - mcc: 0.9152\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.9536 - loss: 0.1137\n","Epoch 38 - MCC: 0.9161\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 189ms/step - accuracy: 0.9537 - loss: 0.1135 - val_accuracy: 0.9581 - val_loss: 0.1079 - mcc: 0.9161\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9591 - loss: 0.1001\n","Epoch 39 - MCC: 0.9166\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9590 - loss: 0.1002 - val_accuracy: 0.9583 - val_loss: 0.1052 - mcc: 0.9166\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9578 - loss: 0.1031\n","Epoch 40 - MCC: 0.9185\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - accuracy: 0.9578 - loss: 0.1032 - val_accuracy: 0.9593 - val_loss: 0.1038 - mcc: 0.9185\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9591 - loss: 0.0998\n","Epoch 41 - MCC: 0.9216\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 187ms/step - accuracy: 0.9590 - loss: 0.1000 - val_accuracy: 0.9609 - val_loss: 0.1007 - mcc: 0.9216\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9567 - loss: 0.1065\n","Epoch 42 - MCC: 0.9187\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9568 - loss: 0.1063 - val_accuracy: 0.9594 - val_loss: 0.1032 - mcc: 0.9187\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9544 - loss: 0.1102\n","Epoch 43 - MCC: 0.9208\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.9546 - loss: 0.1099 - val_accuracy: 0.9605 - val_loss: 0.1006 - mcc: 0.9208\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9602 - loss: 0.0991\n","Epoch 44 - MCC: 0.9178\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9601 - loss: 0.0992 - val_accuracy: 0.9590 - val_loss: 0.1025 - mcc: 0.9178\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9607 - loss: 0.0961\n","Epoch 45 - MCC: 0.9222\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9606 - loss: 0.0963 - val_accuracy: 0.9611 - val_loss: 0.1000 - mcc: 0.9222\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9607 - loss: 0.0963\n","Epoch 46 - MCC: 0.9174\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 195ms/step - accuracy: 0.9606 - loss: 0.0965 - val_accuracy: 0.9588 - val_loss: 0.1049 - mcc: 0.9174\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9585 - loss: 0.1008\n","Epoch 47 - MCC: 0.9198\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9585 - loss: 0.1008 - val_accuracy: 0.9600 - val_loss: 0.1029 - mcc: 0.9198\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9605 - loss: 0.0953\n","Epoch 48 - MCC: 0.9194\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9604 - loss: 0.0955 - val_accuracy: 0.9598 - val_loss: 0.1025 - mcc: 0.9194\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9602 - loss: 0.0955\n","Epoch 49 - MCC: 0.9197\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 205ms/step - accuracy: 0.9601 - loss: 0.0957 - val_accuracy: 0.9599 - val_loss: 0.0994 - mcc: 0.9197\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9555 - loss: 0.1063\n","Epoch 50 - MCC: 0.9221\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9555 - loss: 0.1061 - val_accuracy: 0.9611 - val_loss: 0.0989 - mcc: 0.9221\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Training Model: BiLSTM_Deep, Fold: 5\n","Epoch 1/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6455 - loss: 0.6600\n","Epoch 1 - MCC: 0.6646\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 243ms/step - accuracy: 0.6490 - loss: 0.6580 - val_accuracy: 0.8317 - val_loss: 0.4312 - mcc: 0.6646\n","Epoch 2/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8502 - loss: 0.3678\n","Epoch 2 - MCC: 0.7868\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 202ms/step - accuracy: 0.8508 - loss: 0.3660 - val_accuracy: 0.8905 - val_loss: 0.2568 - mcc: 0.7868\n","Epoch 3/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9073 - loss: 0.2255\n","Epoch 3 - MCC: 0.8324\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9075 - loss: 0.2250 - val_accuracy: 0.9147 - val_loss: 0.2063 - mcc: 0.8324\n","Epoch 4/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9200 - loss: 0.1935\n","Epoch 4 - MCC: 0.8454\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9201 - loss: 0.1932 - val_accuracy: 0.9228 - val_loss: 0.1869 - mcc: 0.8454\n","Epoch 5/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9270 - loss: 0.1788\n","Epoch 5 - MCC: 0.8500\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.9271 - loss: 0.1785 - val_accuracy: 0.9248 - val_loss: 0.1867 - mcc: 0.8500\n","Epoch 6/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9292 - loss: 0.1734\n","Epoch 6 - MCC: 0.8408\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9293 - loss: 0.1732 - val_accuracy: 0.9197 - val_loss: 0.2010 - mcc: 0.8408\n","Epoch 7/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9261 - loss: 0.1812\n","Epoch 7 - MCC: 0.8546\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 186ms/step - accuracy: 0.9262 - loss: 0.1808 - val_accuracy: 0.9272 - val_loss: 0.1776 - mcc: 0.8546\n","Epoch 8/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9352 - loss: 0.1571\n","Epoch 8 - MCC: 0.8610\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9352 - loss: 0.1573 - val_accuracy: 0.9303 - val_loss: 0.1725 - mcc: 0.8610\n","Epoch 9/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9393 - loss: 0.1492\n","Epoch 9 - MCC: 0.8577\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9392 - loss: 0.1495 - val_accuracy: 0.9288 - val_loss: 0.1730 - mcc: 0.8577\n","Epoch 10/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9318 - loss: 0.1662\n","Epoch 10 - MCC: 0.8525\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.9318 - loss: 0.1662 - val_accuracy: 0.9248 - val_loss: 0.1880 - mcc: 0.8525\n","Epoch 11/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9356 - loss: 0.1566\n","Epoch 11 - MCC: 0.8625\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9356 - loss: 0.1566 - val_accuracy: 0.9307 - val_loss: 0.1700 - mcc: 0.8625\n","Epoch 12/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9425 - loss: 0.1406\n","Epoch 12 - MCC: 0.8701\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9425 - loss: 0.1407 - val_accuracy: 0.9351 - val_loss: 0.1531 - mcc: 0.8701\n","Epoch 13/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9396 - loss: 0.1506\n","Epoch 13 - MCC: 0.8719\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 172ms/step - accuracy: 0.9398 - loss: 0.1501 - val_accuracy: 0.9355 - val_loss: 0.1546 - mcc: 0.8719\n","Epoch 14/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9440 - loss: 0.1373\n","Epoch 14 - MCC: 0.8747\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9441 - loss: 0.1372 - val_accuracy: 0.9375 - val_loss: 0.1507 - mcc: 0.8747\n","Epoch 15/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9442 - loss: 0.1353\n","Epoch 15 - MCC: 0.8798\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 202ms/step - accuracy: 0.9442 - loss: 0.1353 - val_accuracy: 0.9400 - val_loss: 0.1443 - mcc: 0.8798\n","Epoch 16/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9420 - loss: 0.1413\n","Epoch 16 - MCC: 0.8812\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.9421 - loss: 0.1410 - val_accuracy: 0.9406 - val_loss: 0.1436 - mcc: 0.8812\n","Epoch 17/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9484 - loss: 0.1253\n","Epoch 17 - MCC: 0.8842\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.9484 - loss: 0.1254 - val_accuracy: 0.9420 - val_loss: 0.1397 - mcc: 0.8842\n","Epoch 18/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9424 - loss: 0.1395\n","Epoch 18 - MCC: 0.8842\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - accuracy: 0.9426 - loss: 0.1390 - val_accuracy: 0.9421 - val_loss: 0.1404 - mcc: 0.8842\n","Epoch 19/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9502 - loss: 0.1199\n","Epoch 19 - MCC: 0.8849\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9501 - loss: 0.1201 - val_accuracy: 0.9425 - val_loss: 0.1385 - mcc: 0.8849\n","Epoch 20/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9512 - loss: 0.1209\n","Epoch 20 - MCC: 0.8832\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 200ms/step - accuracy: 0.9511 - loss: 0.1211 - val_accuracy: 0.9416 - val_loss: 0.1393 - mcc: 0.8832\n","Epoch 21/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9474 - loss: 0.1265\n","Epoch 21 - MCC: 0.8856\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9475 - loss: 0.1265 - val_accuracy: 0.9424 - val_loss: 0.1426 - mcc: 0.8856\n","Epoch 22/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9492 - loss: 0.1225\n","Epoch 22 - MCC: 0.8883\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9491 - loss: 0.1227 - val_accuracy: 0.9442 - val_loss: 0.1368 - mcc: 0.8883\n","Epoch 23/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9522 - loss: 0.1182\n","Epoch 23 - MCC: 0.8813\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 204ms/step - accuracy: 0.9522 - loss: 0.1183 - val_accuracy: 0.9407 - val_loss: 0.1419 - mcc: 0.8813\n","Epoch 24/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9472 - loss: 0.1276\n","Epoch 24 - MCC: 0.8849\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9473 - loss: 0.1273 - val_accuracy: 0.9425 - val_loss: 0.1401 - mcc: 0.8849\n","Epoch 25/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9560 - loss: 0.1110\n","Epoch 25 - MCC: 0.8868\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9558 - loss: 0.1113 - val_accuracy: 0.9433 - val_loss: 0.1373 - mcc: 0.8868\n","Epoch 26/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9514 - loss: 0.1201\n","Epoch 26 - MCC: 0.8930\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 208ms/step - accuracy: 0.9515 - loss: 0.1200 - val_accuracy: 0.9464 - val_loss: 0.1311 - mcc: 0.8930\n","Epoch 27/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9533 - loss: 0.1149\n","Epoch 27 - MCC: 0.8952\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 183ms/step - accuracy: 0.9532 - loss: 0.1150 - val_accuracy: 0.9476 - val_loss: 0.1297 - mcc: 0.8952\n","Epoch 28/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9560 - loss: 0.1082\n","Epoch 28 - MCC: 0.8849\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - accuracy: 0.9559 - loss: 0.1084 - val_accuracy: 0.9425 - val_loss: 0.1369 - mcc: 0.8849\n","Epoch 29/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9567 - loss: 0.1056\n","Epoch 29 - MCC: 0.8899\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9566 - loss: 0.1059 - val_accuracy: 0.9450 - val_loss: 0.1315 - mcc: 0.8899\n","Epoch 30/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9530 - loss: 0.1155\n","Epoch 30 - MCC: 0.8974\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 170ms/step - accuracy: 0.9530 - loss: 0.1155 - val_accuracy: 0.9487 - val_loss: 0.1265 - mcc: 0.8974\n","Epoch 31/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9533 - loss: 0.1153\n","Epoch 31 - MCC: 0.8937\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 174ms/step - accuracy: 0.9533 - loss: 0.1151 - val_accuracy: 0.9469 - val_loss: 0.1291 - mcc: 0.8937\n","Epoch 32/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9513 - loss: 0.1181\n","Epoch 32 - MCC: 0.8958\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9515 - loss: 0.1178 - val_accuracy: 0.9479 - val_loss: 0.1258 - mcc: 0.8958\n","Epoch 33/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9554 - loss: 0.1106\n","Epoch 33 - MCC: 0.8971\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 198ms/step - accuracy: 0.9555 - loss: 0.1105 - val_accuracy: 0.9486 - val_loss: 0.1258 - mcc: 0.8971\n","Epoch 34/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9562 - loss: 0.1082\n","Epoch 34 - MCC: 0.9002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9563 - loss: 0.1081 - val_accuracy: 0.9501 - val_loss: 0.1242 - mcc: 0.9002\n","Epoch 35/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9597 - loss: 0.0982\n","Epoch 35 - MCC: 0.8956\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 165ms/step - accuracy: 0.9596 - loss: 0.0984 - val_accuracy: 0.9478 - val_loss: 0.1278 - mcc: 0.8956\n","Epoch 36/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9587 - loss: 0.1044\n","Epoch 36 - MCC: 0.8992\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - accuracy: 0.9586 - loss: 0.1045 - val_accuracy: 0.9496 - val_loss: 0.1250 - mcc: 0.8992\n","Epoch 37/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9584 - loss: 0.1037\n","Epoch 37 - MCC: 0.9002\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.9583 - loss: 0.1037 - val_accuracy: 0.9502 - val_loss: 0.1236 - mcc: 0.9002\n","Epoch 38/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9562 - loss: 0.1070\n","Epoch 38 - MCC: 0.9000\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 178ms/step - accuracy: 0.9563 - loss: 0.1069 - val_accuracy: 0.9501 - val_loss: 0.1222 - mcc: 0.9000\n","Epoch 39/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9578 - loss: 0.1025\n","Epoch 39 - MCC: 0.9014\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 164ms/step - accuracy: 0.9578 - loss: 0.1025 - val_accuracy: 0.9506 - val_loss: 0.1215 - mcc: 0.9014\n","Epoch 40/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9607 - loss: 0.0952\n","Epoch 40 - MCC: 0.9016\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9606 - loss: 0.0955 - val_accuracy: 0.9508 - val_loss: 0.1201 - mcc: 0.9016\n","Epoch 41/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9577 - loss: 0.1056\n","Epoch 41 - MCC: 0.8978\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 183ms/step - accuracy: 0.9577 - loss: 0.1056 - val_accuracy: 0.9487 - val_loss: 0.1284 - mcc: 0.8978\n","Epoch 42/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9581 - loss: 0.1044\n","Epoch 42 - MCC: 0.9020\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.9581 - loss: 0.1044 - val_accuracy: 0.9509 - val_loss: 0.1207 - mcc: 0.9020\n","Epoch 43/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9622 - loss: 0.0932\n","Epoch 43 - MCC: 0.9062\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.9621 - loss: 0.0935 - val_accuracy: 0.9531 - val_loss: 0.1177 - mcc: 0.9062\n","Epoch 44/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9633 - loss: 0.0900\n","Epoch 44 - MCC: 0.9038\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 186ms/step - accuracy: 0.9632 - loss: 0.0903 - val_accuracy: 0.9519 - val_loss: 0.1172 - mcc: 0.9038\n","Epoch 45/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9611 - loss: 0.0957\n","Epoch 45 - MCC: 0.9051\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 160ms/step - accuracy: 0.9611 - loss: 0.0958 - val_accuracy: 0.9526 - val_loss: 0.1181 - mcc: 0.9051\n","Epoch 46/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9602 - loss: 0.0984\n","Epoch 46 - MCC: 0.9074\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9603 - loss: 0.0983 - val_accuracy: 0.9537 - val_loss: 0.1171 - mcc: 0.9074\n","Epoch 47/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9601 - loss: 0.0991\n","Epoch 47 - MCC: 0.9051\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 197ms/step - accuracy: 0.9601 - loss: 0.0991 - val_accuracy: 0.9522 - val_loss: 0.1242 - mcc: 0.9051\n","Epoch 48/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9619 - loss: 0.0947\n","Epoch 48 - MCC: 0.9072\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9619 - loss: 0.0948 - val_accuracy: 0.9537 - val_loss: 0.1137 - mcc: 0.9072\n","Epoch 49/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9605 - loss: 0.0947\n","Epoch 49 - MCC: 0.9006\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9605 - loss: 0.0947 - val_accuracy: 0.9503 - val_loss: 0.1204 - mcc: 0.9006\n","Epoch 50/50\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9552 - loss: 0.1084\n","Epoch 50 - MCC: 0.9044\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 200ms/step - accuracy: 0.9555 - loss: 0.1078 - val_accuracy: 0.9521 - val_loss: 0.1188 - mcc: 0.9044\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["{'Accuracy': {'max': 0.9648466666666666,\n","              'mean': 0.9571626666666667,\n","              'min': 0.94808,\n","              'std': 0.006159279322922265},\n"," 'Inference Time (s/sample)': {'max': 0.0035810399055480955,\n","                               'mean': 0.0035000085830688477,\n","                               'min': 0.0033470749855041503,\n","                               'std': 8.026211736623289e-05},\n"," 'MCC': {'max': 0.9295314778327362,\n","         'mean': 0.914217540236238,\n","         'min': 0.8960745714453434,\n","         'std': 0.012217934326062305},\n"," 'Parameters': 76065,\n"," 'Train Time (s)': {'max': 255.8758499622345,\n","                    'mean': 247.70022249221802,\n","                    'min': 241.05864095687866,\n","                    'std': 5.697644946302527},\n"," 'Training Accuracy': [[0.6716265678405762,\n","                        0.8692701458930969,\n","                        0.916606605052948,\n","                        0.9255232214927673,\n","                        0.9321717619895935,\n","                        0.9355266690254211,\n","                        0.9372082352638245,\n","                        0.9390250444412231,\n","                        0.9394983649253845,\n","                        0.9409949779510498,\n","                        0.942796528339386,\n","                        0.9440366625785828,\n","                        0.9429367780685425,\n","                        0.9433634281158447,\n","                        0.9452615976333618,\n","                        0.9451383948326111,\n","                        0.9468900561332703,\n","                        0.948598325252533,\n","                        0.9478681683540344,\n","                        0.9493682980537415,\n","                        0.9500884413719177,\n","                        0.9514582753181458,\n","                        0.950765073299408,\n","                        0.9513999819755554,\n","                        0.9530299305915833,\n","                        0.9536750316619873,\n","                        0.9532500505447388,\n","                        0.9534649848937988,\n","                        0.9558233618736267,\n","                        0.955496609210968,\n","                        0.9564017653465271,\n","                        0.9561417102813721,\n","                        0.9560566544532776,\n","                        0.9529833793640137,\n","                        0.9513483643531799,\n","                        0.953461766242981,\n","                        0.9564067125320435,\n","                        0.957028329372406,\n","                        0.9485317468643188,\n","                        0.953533411026001,\n","                        0.9558982849121094,\n","                        0.9577734470367432,\n","                        0.9564766883850098,\n","                        0.9576098918914795,\n","                        0.958098292350769,\n","                        0.9586284160614014,\n","                        0.9597949981689453,\n","                        0.9604365825653076,\n","                        0.9599500298500061,\n","                        0.9606083631515503],\n","                       [0.7466248869895935,\n","                        0.8755283355712891,\n","                        0.9168766736984253,\n","                        0.9248084425926208,\n","                        0.9321016073226929,\n","                        0.9346799254417419,\n","                        0.93836510181427,\n","                        0.9398650527000427,\n","                        0.9382216930389404,\n","                        0.9409183263778687,\n","                        0.9440298676490784,\n","                        0.9448165893554688,\n","                        0.9418015480041504,\n","                        0.9447349309921265,\n","                        0.9451199173927307,\n","                        0.9467350840568542,\n","                        0.9453449845314026,\n","                        0.9480900764465332,\n","                        0.949963390827179,\n","                        0.9507018327713013,\n","                        0.9505617618560791,\n","                        0.9521664977073669,\n","                        0.9515249729156494,\n","                        0.9528999924659729,\n","                        0.9552649855613708,\n","                        0.9550016522407532,\n","                        0.9553365111351013,\n","                        0.9559565782546997,\n","                        0.955353319644928,\n","                        0.9560867547988892,\n","                        0.9571583271026611,\n","                        0.9552932977676392,\n","                        0.9568167328834534,\n","                        0.9595249891281128,\n","                        0.9583433270454407,\n","                        0.9586968421936035,\n","                        0.9606965780258179,\n","                        0.9610232710838318,\n","                        0.9612167477607727,\n","                        0.9596783518791199,\n","                        0.9617300629615784,\n","                        0.9570534229278564,\n","                        0.9594216346740723,\n","                        0.9618698954582214,\n","                        0.9632563591003418,\n","                        0.9627932906150818,\n","                        0.9630998373031616,\n","                        0.9647149443626404,\n","                        0.9638015031814575,\n","                        0.9646599292755127],\n","                       [0.6818233728408813,\n","                        0.8640883564949036,\n","                        0.9061934947967529,\n","                        0.9233448505401611,\n","                        0.9312116503715515,\n","                        0.934333324432373,\n","                        0.9351049661636353,\n","                        0.9333799481391907,\n","                        0.9394716024398804,\n","                        0.9412615299224854,\n","                        0.9436666369438171,\n","                        0.9429600238800049,\n","                        0.9430550336837769,\n","                        0.9441966414451599,\n","                        0.9452866911888123,\n","                        0.944474995136261,\n","                        0.9454217553138733,\n","                        0.9464299082756042,\n","                        0.94638991355896,\n","                        0.9468166828155518,\n","                        0.9486950635910034,\n","                        0.9485799670219421,\n","                        0.9498999118804932,\n","                        0.951388418674469,\n","                        0.9512099623680115,\n","                        0.9520016312599182,\n","                        0.9525815844535828,\n","                        0.9539083242416382,\n","                        0.9553916454315186,\n","                        0.9546767473220825,\n","                        0.9557851552963257,\n","                        0.9558300971984863,\n","                        0.9564200043678284,\n","                        0.9592817425727844,\n","                        0.9585650563240051,\n","                        0.9537684917449951,\n","                        0.9552168250083923,\n","                        0.956839919090271,\n","                        0.959735095500946,\n","                        0.9600266814231873,\n","                        0.9593198895454407,\n","                        0.959431529045105,\n","                        0.96014004945755,\n","                        0.957173228263855,\n","                        0.9585699439048767,\n","                        0.9610615372657776,\n","                        0.9625000953674316,\n","                        0.9609048962593079,\n","                        0.9628783464431763,\n","                        0.9621549844741821],\n","                       [0.6881816387176514,\n","                        0.8688533306121826,\n","                        0.9163267016410828,\n","                        0.9213016033172607,\n","                        0.927808403968811,\n","                        0.9310516119003296,\n","                        0.9375967383384705,\n","                        0.9383416175842285,\n","                        0.9382033348083496,\n","                        0.9398866891860962,\n","                        0.9418032169342041,\n","                        0.9431384205818176,\n","                        0.9421967267990112,\n","                        0.9434031844139099,\n","                        0.9427933692932129,\n","                        0.9447733163833618,\n","                        0.9442183971405029,\n","                        0.9439617395401001,\n","                        0.9429684281349182,\n","                        0.9457767605781555,\n","                        0.9461765885353088,\n","                        0.9461933374404907,\n","                        0.9460132718086243,\n","                        0.947353184223175,\n","                        0.9480316042900085,\n","                        0.9494149088859558,\n","                        0.9498584270477295,\n","                        0.9496833682060242,\n","                        0.9500265717506409,\n","                        0.9513366222381592,\n","                        0.9517950415611267,\n","                        0.951021671295166,\n","                        0.9541648626327515,\n","                        0.9536117315292358,\n","                        0.9552099704742432,\n","                        0.9541900753974915,\n","                        0.954990029335022,\n","                        0.9561083316802979,\n","                        0.9570348858833313,\n","                        0.9559200406074524,\n","                        0.9564915299415588,\n","                        0.9586833715438843,\n","                        0.9583050012588501,\n","                        0.9592483639717102,\n","                        0.9586132764816284,\n","                        0.9592248797416687,\n","                        0.9584200382232666,\n","                        0.9590499997138977,\n","                        0.9577583074569702,\n","                        0.9579249024391174],\n","                       [0.7352150678634644,\n","                        0.8668999671936035,\n","                        0.9132465124130249,\n","                        0.923988401889801,\n","                        0.930278480052948,\n","                        0.931380033493042,\n","                        0.9303183555603027,\n","                        0.9335949420928955,\n","                        0.9359983205795288,\n","                        0.932075023651123,\n","                        0.9362733364105225,\n","                        0.941301703453064,\n","                        0.9442999958992004,\n","                        0.9451133608818054,\n","                        0.9440184235572815,\n","                        0.9456350207328796,\n","                        0.9481765627861023,\n","                        0.9478132128715515,\n","                        0.9480999708175659,\n","                        0.9479315876960754,\n","                        0.9484367370605469,\n","                        0.9482166767120361,\n","                        0.9510383009910583,\n","                        0.9506882429122925,\n","                        0.951335072517395,\n","                        0.9523766040802002,\n","                        0.9518998861312866,\n","                        0.9532216787338257,\n","                        0.953666627407074,\n","                        0.952720046043396,\n","                        0.9552650451660156,\n","                        0.954728364944458,\n","                        0.9561033844947815,\n","                        0.9570868015289307,\n","                        0.9575883746147156,\n","                        0.956299901008606,\n","                        0.9576866030693054,\n","                        0.9575150012969971,\n","                        0.958401620388031,\n","                        0.9577018022537231,\n","                        0.9574449062347412,\n","                        0.9574581980705261,\n","                        0.9591017961502075,\n","                        0.9603165984153748,\n","                        0.9607784152030945,\n","                        0.9612566232681274,\n","                        0.959850013256073,\n","                        0.9608933925628662,\n","                        0.9610984325408936,\n","                        0.9619584679603577]],\n"," 'Training Loss': [[0.631186842918396,\n","                    0.320125550031662,\n","                    0.20703592896461487,\n","                    0.18375568091869354,\n","                    0.16821813583374023,\n","                    0.15815576910972595,\n","                    0.15404611825942993,\n","                    0.14981244504451752,\n","                    0.14986681938171387,\n","                    0.14411358535289764,\n","                    0.14009439945220947,\n","                    0.13850724697113037,\n","                    0.14128509163856506,\n","                    0.14114561676979065,\n","                    0.1355087161064148,\n","                    0.13416090607643127,\n","                    0.13115112483501434,\n","                    0.12651799619197845,\n","                    0.12831902503967285,\n","                    0.12588927149772644,\n","                    0.12257187813520432,\n","                    0.11944654583930969,\n","                    0.12064877152442932,\n","                    0.12016191333532333,\n","                    0.1162179559469223,\n","                    0.11490091681480408,\n","                    0.11597184091806412,\n","                    0.11367504298686981,\n","                    0.10879047214984894,\n","                    0.10909834504127502,\n","                    0.10826360434293747,\n","                    0.10898749530315399,\n","                    0.10806815326213837,\n","                    0.11426570266485214,\n","                    0.11895633488893509,\n","                    0.11461387574672699,\n","                    0.10731209814548492,\n","                    0.10454609990119934,\n","                    0.12539422512054443,\n","                    0.11424922943115234,\n","                    0.10816042125225067,\n","                    0.10436967015266418,\n","                    0.10717470198869705,\n","                    0.10518693923950195,\n","                    0.10293776541948318,\n","                    0.10119927674531937,\n","                    0.09967884421348572,\n","                    0.09786819666624069,\n","                    0.09866204857826233,\n","                    0.09702564030885696],\n","                   [0.5812520384788513,\n","                    0.2953192889690399,\n","                    0.2009885013103485,\n","                    0.1827407330274582,\n","                    0.16700440645217896,\n","                    0.15887850522994995,\n","                    0.14999455213546753,\n","                    0.14581944048404694,\n","                    0.14976242184638977,\n","                    0.14461374282836914,\n","                    0.13579200208187103,\n","                    0.13377578556537628,\n","                    0.14319166541099548,\n","                    0.13384772837162018,\n","                    0.13369426131248474,\n","                    0.13147509098052979,\n","                    0.1335982084274292,\n","                    0.12584997713565826,\n","                    0.12138063460588455,\n","                    0.11960557103157043,\n","                    0.12043581157922745,\n","                    0.11643562465906143,\n","                    0.11868702620267868,\n","                    0.1156553253531456,\n","                    0.11060802638530731,\n","                    0.11118254065513611,\n","                    0.11022781580686569,\n","                    0.10852626711130142,\n","                    0.10998759418725967,\n","                    0.10862863808870316,\n","                    0.10541568696498871,\n","                    0.1110333725810051,\n","                    0.10742201656103134,\n","                    0.1014537587761879,\n","                    0.1032237634062767,\n","                    0.1019243523478508,\n","                    0.09747271239757538,\n","                    0.09619955718517303,\n","                    0.09842338413000107,\n","                    0.0987873300909996,\n","                    0.09699025750160217,\n","                    0.1048065647482872,\n","                    0.10015258193016052,\n","                    0.09444983303546906,\n","                    0.09094680100679398,\n","                    0.0913376733660698,\n","                    0.09044429659843445,\n","                    0.08823467046022415,\n","                    0.08911392092704773,\n","                    0.08721482008695602],\n","                   [0.6181890964508057,\n","                    0.34185588359832764,\n","                    0.22782957553863525,\n","                    0.18664684891700745,\n","                    0.16800235211849213,\n","                    0.16031980514526367,\n","                    0.15572375059127808,\n","                    0.15927939116954803,\n","                    0.1459261029958725,\n","                    0.1429014801979065,\n","                    0.1374150514602661,\n","                    0.1385042816400528,\n","                    0.13787856698036194,\n","                    0.13499411940574646,\n","                    0.1323484480381012,\n","                    0.1336522102355957,\n","                    0.13337041437625885,\n","                    0.13051775097846985,\n","                    0.12967482209205627,\n","                    0.12821927666664124,\n","                    0.12530860304832458,\n","                    0.1239185780286789,\n","                    0.12261076271533966,\n","                    0.11871886998414993,\n","                    0.11892244219779968,\n","                    0.11674255132675171,\n","                    0.11629103869199753,\n","                    0.11369884759187698,\n","                    0.10983449965715408,\n","                    0.10985251516103745,\n","                    0.10811781138181686,\n","                    0.10839396715164185,\n","                    0.10708116739988327,\n","                    0.1014384999871254,\n","                    0.10265469551086426,\n","                    0.11291307210922241,\n","                    0.10921740531921387,\n","                    0.10571709275245667,\n","                    0.1001366525888443,\n","                    0.09876006841659546,\n","                    0.09987157583236694,\n","                    0.10082297027111053,\n","                    0.09847122430801392,\n","                    0.10499674826860428,\n","                    0.10192812234163284,\n","                    0.09560251981019974,\n","                    0.093508780002594,\n","                    0.09572213143110275,\n","                    0.09196078032255173,\n","                    0.09326402842998505],\n","                   [0.613409161567688,\n","                    0.31552815437316895,\n","                    0.20014144480228424,\n","                    0.1921393722295761,\n","                    0.17514421045780182,\n","                    0.1668570637702942,\n","                    0.1515854299068451,\n","                    0.14869701862335205,\n","                    0.1489882916212082,\n","                    0.14581173658370972,\n","                    0.14124996960163116,\n","                    0.1395634114742279,\n","                    0.1404130905866623,\n","                    0.13804267346858978,\n","                    0.13916319608688354,\n","                    0.13541482388973236,\n","                    0.13555753231048584,\n","                    0.13635072112083435,\n","                    0.1386645883321762,\n","                    0.1320171356201172,\n","                    0.13144297897815704,\n","                    0.12998241186141968,\n","                    0.13103710114955902,\n","                    0.1284153014421463,\n","                    0.1258929818868637,\n","                    0.12310761213302612,\n","                    0.12148448079824448,\n","                    0.12226604670286179,\n","                    0.12209401279687881,\n","                    0.11849886924028397,\n","                    0.11644691228866577,\n","                    0.11892791837453842,\n","                    0.11258871108293533,\n","                    0.11395908147096634,\n","                    0.1104687973856926,\n","                    0.11227878928184509,\n","                    0.11003126949071884,\n","                    0.10844238102436066,\n","                    0.10502476990222931,\n","                    0.10679107904434204,\n","                    0.10461640357971191,\n","                    0.10057391971349716,\n","                    0.10094626247882843,\n","                    0.0996614620089531,\n","                    0.10132408887147903,\n","                    0.09979847818613052,\n","                    0.10087175667285919,\n","                    0.09954901784658432,\n","                    0.10182322561740875,\n","                    0.10150919109582901],\n","                   [0.608299195766449,\n","                    0.3201541602611542,\n","                    0.21260100603103638,\n","                    0.18544644117355347,\n","                    0.1711340695619583,\n","                    0.1685144603252411,\n","                    0.17123009264469147,\n","                    0.1617685705423355,\n","                    0.15612100064754486,\n","                    0.1659010946750641,\n","                    0.1559123694896698,\n","                    0.14298228919506073,\n","                    0.1378328502178192,\n","                    0.1346903294324875,\n","                    0.13468335568904877,\n","                    0.1334606111049652,\n","                    0.12671874463558197,\n","                    0.12656034529209137,\n","                    0.12586970627307892,\n","                    0.12713876366615295,\n","                    0.12554991245269775,\n","                    0.12640519440174103,\n","                    0.12143142521381378,\n","                    0.12037213146686554,\n","                    0.12032105773687363,\n","                    0.11775722354650497,\n","                    0.11714403331279755,\n","                    0.114694744348526,\n","                    0.11336357891559601,\n","                    0.11556681990623474,\n","                    0.11004474759101868,\n","                    0.11102793365716934,\n","                    0.1079096794128418,\n","                    0.10580453276634216,\n","                    0.10402429848909378,\n","                    0.10861298441886902,\n","                    0.10446489602327347,\n","                    0.10439645498991013,\n","                    0.10165490210056305,\n","                    0.10300882160663605,\n","                    0.10573506355285645,\n","                    0.10471200942993164,\n","                    0.10065310448408127,\n","                    0.09689260274171829,\n","                    0.09676051139831543,\n","                    0.09484804421663284,\n","                    0.09835952520370483,\n","                    0.09613695740699768,\n","                    0.09405411034822464,\n","                    0.09334754943847656]],\n"," 'Validation Accuracy': [[0.8189533948898315,\n","                          0.916439950466156,\n","                          0.923266589641571,\n","                          0.9402599930763245,\n","                          0.9432532787322998,\n","                          0.945900022983551,\n","                          0.9457733631134033,\n","                          0.9483934044837952,\n","                          0.9481532573699951,\n","                          0.9490799903869629,\n","                          0.9478533864021301,\n","                          0.9499533772468567,\n","                          0.9499732851982117,\n","                          0.9509533643722534,\n","                          0.9509666562080383,\n","                          0.9514533281326294,\n","                          0.9541666507720947,\n","                          0.9517000317573547,\n","                          0.9553133249282837,\n","                          0.9549600481987,\n","                          0.9568666219711304,\n","                          0.9564533233642578,\n","                          0.9574799537658691,\n","                          0.9589200019836426,\n","                          0.9572399854660034,\n","                          0.9583333730697632,\n","                          0.958026647567749,\n","                          0.9594734311103821,\n","                          0.9602000713348389,\n","                          0.9607999920845032,\n","                          0.9604266881942749,\n","                          0.9602999687194824,\n","                          0.9585466980934143,\n","                          0.94322669506073,\n","                          0.9577000141143799,\n","                          0.9592000842094421,\n","                          0.9633467197418213,\n","                          0.953706681728363,\n","                          0.9568933844566345,\n","                          0.9575266242027283,\n","                          0.9621732831001282,\n","                          0.9623265862464905,\n","                          0.9617533087730408,\n","                          0.963533341884613,\n","                          0.9624533653259277,\n","                          0.9624333381652832,\n","                          0.9640333652496338,\n","                          0.9620733857154846,\n","                          0.963086724281311,\n","                          0.964846670627594],\n","                         [0.8334733843803406,\n","                          0.8938867449760437,\n","                          0.9150933623313904,\n","                          0.9177267551422119,\n","                          0.921739935874939,\n","                          0.9270266890525818,\n","                          0.9304333329200745,\n","                          0.9253866672515869,\n","                          0.931233286857605,\n","                          0.9335132837295532,\n","                          0.930213212966919,\n","                          0.9344400763511658,\n","                          0.9321733713150024,\n","                          0.9318599700927734,\n","                          0.9350866675376892,\n","                          0.9353200793266296,\n","                          0.9361400008201599,\n","                          0.9374932646751404,\n","                          0.9368400573730469,\n","                          0.9377332925796509,\n","                          0.9395933747291565,\n","                          0.9371333122253418,\n","                          0.9381466507911682,\n","                          0.9413667917251587,\n","                          0.9395199418067932,\n","                          0.942639946937561,\n","                          0.9424733519554138,\n","                          0.9421600103378296,\n","                          0.9426666498184204,\n","                          0.9404399991035461,\n","                          0.943579912185669,\n","                          0.9405333995819092,\n","                          0.9442200660705566,\n","                          0.9433333873748779,\n","                          0.9444267153739929,\n","                          0.9440533518791199,\n","                          0.9429932236671448,\n","                          0.9453198909759521,\n","                          0.9447267055511475,\n","                          0.9470133185386658,\n","                          0.9429199695587158,\n","                          0.9445200562477112,\n","                          0.9475599527359009,\n","                          0.9467000365257263,\n","                          0.9463533163070679,\n","                          0.9465200304985046,\n","                          0.9480866193771362,\n","                          0.9480933547019958,\n","                          0.9472066760063171,\n","                          0.9480800628662109],\n","                         [0.8346200585365295,\n","                          0.8958200216293335,\n","                          0.9221667647361755,\n","                          0.9269132018089294,\n","                          0.9354866147041321,\n","                          0.9353334307670593,\n","                          0.9266000390052795,\n","                          0.9414399862289429,\n","                          0.9454934000968933,\n","                          0.9456199407577515,\n","                          0.9447000026702881,\n","                          0.9443399906158447,\n","                          0.9440200328826904,\n","                          0.9470599293708801,\n","                          0.9469999670982361,\n","                          0.9442933797836304,\n","                          0.9483466148376465,\n","                          0.9478133916854858,\n","                          0.9464133381843567,\n","                          0.9475599527359009,\n","                          0.9498133063316345,\n","                          0.9514666795730591,\n","                          0.949686586856842,\n","                          0.9511067271232605,\n","                          0.9523466229438782,\n","                          0.9536933302879333,\n","                          0.9540266394615173,\n","                          0.9543467164039612,\n","                          0.9542067050933838,\n","                          0.9544000029563904,\n","                          0.9527133107185364,\n","                          0.9568133354187012,\n","                          0.9561533331871033,\n","                          0.9561866521835327,\n","                          0.9526933431625366,\n","                          0.9529999494552612,\n","                          0.9551533460617065,\n","                          0.9550800323486328,\n","                          0.9576332569122314,\n","                          0.9558200240135193,\n","                          0.9586066603660583,\n","                          0.9568200707435608,\n","                          0.9591200351715088,\n","                          0.9529666304588318,\n","                          0.9574933648109436,\n","                          0.9570667147636414,\n","                          0.9587266445159912,\n","                          0.9593200087547302,\n","                          0.9592999219894409,\n","                          0.9596734046936035],\n","                         [0.8184800148010254,\n","                          0.9104600548744202,\n","                          0.9086799621582031,\n","                          0.9352333545684814,\n","                          0.9322267174720764,\n","                          0.9419400095939636,\n","                          0.9432266354560852,\n","                          0.9407600164413452,\n","                          0.9445400238037109,\n","                          0.9462266564369202,\n","                          0.9443933367729187,\n","                          0.945726752281189,\n","                          0.9477866888046265,\n","                          0.9454999566078186,\n","                          0.9484599828720093,\n","                          0.946933388710022,\n","                          0.9496932029724121,\n","                          0.9472467303276062,\n","                          0.948193371295929,\n","                          0.9454866647720337,\n","                          0.9486532807350159,\n","                          0.9466466307640076,\n","                          0.9503200650215149,\n","                          0.9494866132736206,\n","                          0.9505200386047363,\n","                          0.9522666335105896,\n","                          0.9519200325012207,\n","                          0.9490467309951782,\n","                          0.9521333575248718,\n","                          0.9544132947921753,\n","                          0.9528467059135437,\n","                          0.955299973487854,\n","                          0.9575266242027283,\n","                          0.9561399221420288,\n","                          0.9581533670425415,\n","                          0.955613374710083,\n","                          0.957486629486084,\n","                          0.9580532908439636,\n","                          0.9583399891853333,\n","                          0.9593333601951599,\n","                          0.9608666300773621,\n","                          0.959393322467804,\n","                          0.9604867696762085,\n","                          0.9589999914169312,\n","                          0.9611467123031616,\n","                          0.9587600827217102,\n","                          0.9599533677101135,\n","                          0.9598132967948914,\n","                          0.9599465727806091,\n","                          0.9611467123031616],\n","                         [0.8316667079925537,\n","                          0.8905467391014099,\n","                          0.9146599769592285,\n","                          0.9228267073631287,\n","                          0.9247866868972778,\n","                          0.9196534156799316,\n","                          0.9271799325942993,\n","                          0.9303466081619263,\n","                          0.9287665486335754,\n","                          0.9247733354568481,\n","                          0.9306866526603699,\n","                          0.935106635093689,\n","                          0.9355400204658508,\n","                          0.9374533891677856,\n","                          0.9400131702423096,\n","                          0.9406266808509827,\n","                          0.9419733285903931,\n","                          0.9420933723449707,\n","                          0.9425400495529175,\n","                          0.9416000247001648,\n","                          0.9424332976341248,\n","                          0.9442199468612671,\n","                          0.9406999945640564,\n","                          0.9424933791160583,\n","                          0.9432666301727295,\n","                          0.9463866353034973,\n","                          0.9475866556167603,\n","                          0.9424933791160583,\n","                          0.9449666738510132,\n","                          0.948733389377594,\n","                          0.9469332695007324,\n","                          0.9479465484619141,\n","                          0.9486200213432312,\n","                          0.9500600695610046,\n","                          0.9477800130844116,\n","                          0.9496133327484131,\n","                          0.950166642665863,\n","                          0.9500600695610046,\n","                          0.9506133198738098,\n","                          0.9507666826248169,\n","                          0.94868004322052,\n","                          0.9508800506591797,\n","                          0.9531266689300537,\n","                          0.9519132971763611,\n","                          0.9525667428970337,\n","                          0.9536799788475037,\n","                          0.9522466063499451,\n","                          0.9536734223365784,\n","                          0.9502933621406555,\n","                          0.9520666599273682]],\n"," 'Validation Loss': [[0.47413092851638794,\n","                      0.20882238447666168,\n","                      0.18682163953781128,\n","                      0.15166863799095154,\n","                      0.14186660945415497,\n","                      0.1330602765083313,\n","                      0.13140112161636353,\n","                      0.12614738941192627,\n","                      0.12665797770023346,\n","                      0.1260964572429657,\n","                      0.1275845617055893,\n","                      0.12100536376237869,\n","                      0.12555177509784698,\n","                      0.12187483161687851,\n","                      0.11993145197629929,\n","                      0.11773078143596649,\n","                      0.11118903011083603,\n","                      0.11549586057662964,\n","                      0.1099269762635231,\n","                      0.10869229584932327,\n","                      0.10514422506093979,\n","                      0.10427246242761612,\n","                      0.10637602210044861,\n","                      0.10062851011753082,\n","                      0.10434922575950623,\n","                      0.10151947289705276,\n","                      0.10188616812229156,\n","                      0.09787674248218536,\n","                      0.0971071645617485,\n","                      0.0957975685596466,\n","                      0.09596135467290878,\n","                      0.09679891914129257,\n","                      0.10307431221008301,\n","                      0.13488119840621948,\n","                      0.10345813632011414,\n","                      0.0982554703950882,\n","                      0.09083990752696991,\n","                      0.10913470387458801,\n","                      0.10352389514446259,\n","                      0.10308418422937393,\n","                      0.09317783266305923,\n","                      0.09231898933649063,\n","                      0.09373364597558975,\n","                      0.08889731764793396,\n","                      0.0922660157084465,\n","                      0.09196247160434723,\n","                      0.08752652257680893,\n","                      0.09254258126020432,\n","                      0.09192874282598495,\n","                      0.08669601380825043],\n","                     [0.39695850014686584,\n","                      0.2530514597892761,\n","                      0.20230557024478912,\n","                      0.19826331734657288,\n","                      0.18854592740535736,\n","                      0.17917536199092865,\n","                      0.1704828143119812,\n","                      0.17788104712963104,\n","                      0.1651371717453003,\n","                      0.15988689661026,\n","                      0.17008493840694427,\n","                      0.15852494537830353,\n","                      0.1661984622478485,\n","                      0.164881631731987,\n","                      0.1612095832824707,\n","                      0.15988092124462128,\n","                      0.15663322806358337,\n","                      0.150667205452919,\n","                      0.15141528844833374,\n","                      0.14889556169509888,\n","                      0.14643190801143646,\n","                      0.15407335758209229,\n","                      0.14853371679782867,\n","                      0.1448366641998291,\n","                      0.14735856652259827,\n","                      0.14104224741458893,\n","                      0.1399969458580017,\n","                      0.13961488008499146,\n","                      0.1420772224664688,\n","                      0.14966820180416107,\n","                      0.13861443102359772,\n","                      0.14499138295650482,\n","                      0.13619917631149292,\n","                      0.14128321409225464,\n","                      0.13667511940002441,\n","                      0.13667616248130798,\n","                      0.13719969987869263,\n","                      0.1386992335319519,\n","                      0.13473555445671082,\n","                      0.12865637242794037,\n","                      0.13667862117290497,\n","                      0.1348239630460739,\n","                      0.13109493255615234,\n","                      0.13015304505825043,\n","                      0.13359422981739044,\n","                      0.13257044553756714,\n","                      0.12780338525772095,\n","                      0.127382293343544,\n","                      0.12688246369361877,\n","                      0.13021866977214813],\n","                     [0.458274245262146,\n","                      0.24760384857654572,\n","                      0.1895080953836441,\n","                      0.17532701790332794,\n","                      0.15776227414608002,\n","                      0.155600443482399,\n","                      0.1725916862487793,\n","                      0.14085324108600616,\n","                      0.13438114523887634,\n","                      0.13336408138275146,\n","                      0.13326552510261536,\n","                      0.13465555012226105,\n","                      0.1342155933380127,\n","                      0.1283538043498993,\n","                      0.12890581786632538,\n","                      0.1384630799293518,\n","                      0.12668699026107788,\n","                      0.12904880940914154,\n","                      0.13029062747955322,\n","                      0.1265157163143158,\n","                      0.12494245171546936,\n","                      0.11753159761428833,\n","                      0.1223488450050354,\n","                      0.12161793559789658,\n","                      0.11754808574914932,\n","                      0.1154162809252739,\n","                      0.11588063091039658,\n","                      0.11346502602100372,\n","                      0.11347480863332748,\n","                      0.11323085427284241,\n","                      0.11710865050554276,\n","                      0.10971058905124664,\n","                      0.1083536222577095,\n","                      0.11023211479187012,\n","                      0.12006965279579163,\n","                      0.11662688851356506,\n","                      0.11236533522605896,\n","                      0.10996662825345993,\n","                      0.1054919883608818,\n","                      0.11279113590717316,\n","                      0.10443229973316193,\n","                      0.10766945779323578,\n","                      0.10197251290082932,\n","                      0.1163436770439148,\n","                      0.10416784137487411,\n","                      0.10501483827829361,\n","                      0.1030493900179863,\n","                      0.09978583455085754,\n","                      0.10420434176921844,\n","                      0.10033459961414337],\n","                     [0.44148924946784973,\n","                      0.21487446129322052,\n","                      0.21308006346225739,\n","                      0.16313646733760834,\n","                      0.16949374973773956,\n","                      0.1459309607744217,\n","                      0.14159582555294037,\n","                      0.14504873752593994,\n","                      0.1366487592458725,\n","                      0.1333407759666443,\n","                      0.13769668340682983,\n","                      0.13254962861537933,\n","                      0.12871293723583221,\n","                      0.13283026218414307,\n","                      0.12738466262817383,\n","                      0.1313341110944748,\n","                      0.12418457120656967,\n","                      0.13155514001846313,\n","                      0.13050274550914764,\n","                      0.1362072378396988,\n","                      0.12683475017547607,\n","                      0.12772558629512787,\n","                      0.12440761923789978,\n","                      0.12521810829639435,\n","                      0.12140840291976929,\n","                      0.11821036040782928,\n","                      0.12144547700881958,\n","                      0.12452428042888641,\n","                      0.1183045357465744,\n","                      0.11283626407384872,\n","                      0.12082362174987793,\n","                      0.1117880716919899,\n","                      0.11101854592561722,\n","                      0.10996972769498825,\n","                      0.10693088173866272,\n","                      0.11211749911308289,\n","                      0.11061637103557587,\n","                      0.10785999149084091,\n","                      0.10519421845674515,\n","                      0.10379748046398163,\n","                      0.10070937126874924,\n","                      0.10321106761693954,\n","                      0.10061652213335037,\n","                      0.10246489197015762,\n","                      0.09999392181634903,\n","                      0.10489824414253235,\n","                      0.10285184532403946,\n","                      0.10251419991254807,\n","                      0.09938038885593414,\n","                      0.09890692681074142],\n","                     [0.43120864033699036,\n","                      0.2568356692790985,\n","                      0.20633439719676971,\n","                      0.18694990873336792,\n","                      0.18672867119312286,\n","                      0.2010451853275299,\n","                      0.17757996916770935,\n","                      0.1725269854068756,\n","                      0.1729954481124878,\n","                      0.1879669725894928,\n","                      0.16998761892318726,\n","                      0.15311147272586823,\n","                      0.15460199117660522,\n","                      0.15072260797023773,\n","                      0.14432963728904724,\n","                      0.14363369345664978,\n","                      0.13967962563037872,\n","                      0.14035460352897644,\n","                      0.13845646381378174,\n","                      0.1392505168914795,\n","                      0.1425967663526535,\n","                      0.13675612211227417,\n","                      0.14189176261425018,\n","                      0.14009569585323334,\n","                      0.13726896047592163,\n","                      0.13113223016262054,\n","                      0.12969310581684113,\n","                      0.13685333728790283,\n","                      0.13145318627357483,\n","                      0.12646523118019104,\n","                      0.12914659082889557,\n","                      0.12582017481327057,\n","                      0.12577775120735168,\n","                      0.124229297041893,\n","                      0.12779255211353302,\n","                      0.1249723806977272,\n","                      0.12358243018388748,\n","                      0.12216611206531525,\n","                      0.12153129279613495,\n","                      0.12011831253767014,\n","                      0.12836521863937378,\n","                      0.12070805579423904,\n","                      0.11774329841136932,\n","                      0.11722934991121292,\n","                      0.11808814108371735,\n","                      0.11714302748441696,\n","                      0.12419918924570084,\n","                      0.11365152150392532,\n","                      0.1204393208026886,\n","                      0.11877728253602982]],\n"," 'Validation MCC': [[0.6374988952104893,\n","                     0.8348656824065285,\n","                     0.8467330142152786,\n","                     0.8800990970974258,\n","                     0.8861996365425061,\n","                     0.891445151388677,\n","                     0.8915651552758175,\n","                     0.8963922440484108,\n","                     0.8960430160042544,\n","                     0.8978032410702432,\n","                     0.8953739448565092,\n","                     0.8995529111500559,\n","                     0.8996628502380908,\n","                     0.9015389392543132,\n","                     0.901561435638027,\n","                     0.9026033848559695,\n","                     0.9079882711008369,\n","                     0.9030544640653306,\n","                     0.9103370509280847,\n","                     0.9095828652285788,\n","                     0.9134947187842525,\n","                     0.9126110557117527,\n","                     0.9147356420952705,\n","                     0.9175340646655709,\n","                     0.9141912728371464,\n","                     0.9163514009529983,\n","                     0.9157440148755749,\n","                     0.9186466177059522,\n","                     0.920117542287812,\n","                     0.9213408491435718,\n","                     0.9205613935103802,\n","                     0.920301314903973,\n","                     0.9167789420128916,\n","                     0.8860703860879422,\n","                     0.9151192268656996,\n","                     0.9181525237385425,\n","                     0.9264200213494436,\n","                     0.9070672047688013,\n","                     0.9135362792893259,\n","                     0.9147435235618737,\n","                     0.9240616141743411,\n","                     0.9243688661367172,\n","                     0.9232865315497703,\n","                     0.9269158520761749,\n","                     0.924650491765394,\n","                     0.9245927448763143,\n","                     0.9278022627840672,\n","                     0.9238682843411976,\n","                     0.9259435255862973,\n","                     0.9295314778327362],\n","                    [0.6678295417885686,\n","                     0.7881331580537334,\n","                     0.8301958535599558,\n","                     0.8353815887069531,\n","                     0.8438848083494204,\n","                     0.8538676284039741,\n","                     0.8605623471206114,\n","                     0.8509331139867791,\n","                     0.8623472298011817,\n","                     0.8669049108370117,\n","                     0.8610899984830769,\n","                     0.8685762008047396,\n","                     0.8642647141424482,\n","                     0.8643342111750846,\n","                     0.8700546535895842,\n","                     0.8705258440919812,\n","                     0.8720583830059916,\n","                     0.8748112171725101,\n","                     0.8734109197787651,\n","                     0.8751866995377031,\n","                     0.8790033705781115,\n","                     0.8741261496389682,\n","                     0.8760580464814813,\n","                     0.8825198447150973,\n","                     0.879006106987995,\n","                     0.8850153353569199,\n","                     0.8847761009492374,\n","                     0.884062640103967,\n","                     0.8856237307544084,\n","                     0.8806069077426404,\n","                     0.887330562271744,\n","                     0.8808385529709606,\n","                     0.8881822926292533,\n","                     0.8864531232131537,\n","                     0.8885930861635585,\n","                     0.887910721860989,\n","                     0.8857932746750015,\n","                     0.8909150190248657,\n","                     0.8891953575811183,\n","                     0.8938199772552357,\n","                     0.8855761922684141,\n","                     0.8888645618115129,\n","                     0.8949350839801977,\n","                     0.8931569544167741,\n","                     0.8928616099559855,\n","                     0.8928904706141209,\n","                     0.8959806446373199,\n","                     0.8959921293064509,\n","                     0.8941727483772974,\n","                     0.8960745714453434],\n","                    [0.6677295021662083,\n","                     0.7924627362166444,\n","                     0.8449726838713147,\n","                     0.8533070248424336,\n","                     0.8705446974199613,\n","                     0.8703050456284017,\n","                     0.8535941358618528,\n","                     0.8828980258311266,\n","                     0.8905403058991356,\n","                     0.8909702922679772,\n","                     0.888965159923224,\n","                     0.888281208654916,\n","                     0.887679152557726,\n","                     0.8937047103305054,\n","                     0.8936149362366642,\n","                     0.8881282860638854,\n","                     0.8962725866239343,\n","                     0.8951975005789092,\n","                     0.8930497427958614,\n","                     0.8946876058052655,\n","                     0.8992290540712071,\n","                     0.9025653192063589,\n","                     0.8989598141746572,\n","                     0.9018171497578121,\n","                     0.9043028400455824,\n","                     0.9070386017191024,\n","                     0.9076956481100489,\n","                     0.9083248155760496,\n","                     0.9080516716747383,\n","                     0.9085296886841179,\n","                     0.9051908560791719,\n","                     0.9134158773353006,\n","                     0.9119557839255934,\n","                     0.9120217872139106,\n","                     0.9055283304160565,\n","                     0.9056494261672562,\n","                     0.9099726264525873,\n","                     0.9099963741827661,\n","                     0.9149261680831764,\n","                     0.9112888439817172,\n","                     0.916876367458145,\n","                     0.9133350957339238,\n","                     0.9179099550906307,\n","                     0.9056258607603075,\n","                     0.9146408837464972,\n","                     0.9138019822179062,\n","                     0.9171289622921922,\n","                     0.9183095106847751,\n","                     0.9182874976704096,\n","                     0.9190221821757365],\n","                    [0.6360424660445911,\n","                     0.8223230380628075,\n","                     0.8194107695238185,\n","                     0.870730279866231,\n","                     0.8653319503585643,\n","                     0.8835980870369831,\n","                     0.8864252430852195,\n","                     0.8817184975399339,\n","                     0.8891628659478352,\n","                     0.8922447549745677,\n","                     0.8885841208988579,\n","                     0.8912510874011464,\n","                     0.8953419481690168,\n","                     0.8907874903753925,\n","                     0.8967373092978815,\n","                     0.8937018062882784,\n","                     0.8994326654167626,\n","                     0.8943311461152869,\n","                     0.8968437502752915,\n","                     0.8915707070848674,\n","                     0.8971788925038471,\n","                     0.8933792914650412,\n","                     0.9004739111832638,\n","                     0.898895070339811,\n","                     0.9009115489181537,\n","                     0.9044520423764331,\n","                     0.9039217954622003,\n","                     0.8978749683469723,\n","                     0.9041442093553205,\n","                     0.9087579697396064,\n","                     0.9057744499184707,\n","                     0.910492150162967,\n","                     0.9150870331188555,\n","                     0.9121001871802703,\n","                     0.9162770889490002,\n","                     0.9113959137956024,\n","                     0.915247217982793,\n","                     0.9161448104914173,\n","                     0.9165771638777938,\n","                     0.9185446767764652,\n","                     0.9216349608650621,\n","                     0.9186724819536269,\n","                     0.9207959881401806,\n","                     0.917804546259629,\n","                     0.9222044875668585,\n","                     0.9173719628886411,\n","                     0.9198037971471499,\n","                     0.9194413892579893,\n","                     0.9197331623205793,\n","                     0.922108628140672],\n","                    [0.6646080164294107,\n","                     0.7867943389518047,\n","                     0.8323556899265194,\n","                     0.8454230284369298,\n","                     0.8499823719363121,\n","                     0.8407502748998241,\n","                     0.8545907084576111,\n","                     0.8609564098127435,\n","                     0.8576827550001415,\n","                     0.8525265366128498,\n","                     0.8625318692787521,\n","                     0.8700683885599053,\n","                     0.8718882481557524,\n","                     0.8746980724749707,\n","                     0.8798480684842734,\n","                     0.8812174353076379,\n","                     0.8841542311574009,\n","                     0.8842093811155447,\n","                     0.8848928749620384,\n","                     0.883196146598912,\n","                     0.8856403255391925,\n","                     0.8882568001393989,\n","                     0.8813128539946541,\n","                     0.8848661398587437,\n","                     0.886788544221761,\n","                     0.8929607512357041,\n","                     0.8951834660580047,\n","                     0.8848834229459888,\n","                     0.8899036391409229,\n","                     0.8974294082189791,\n","                     0.8936742353767368,\n","                     0.8957911337125071,\n","                     0.8971100691222713,\n","                     0.9001853506313944,\n","                     0.8956000025899497,\n","                     0.8992265020050879,\n","                     0.9002073791341139,\n","                     0.899995116040722,\n","                     0.901369224990683,\n","                     0.901559010264776,\n","                     0.8977571118227513,\n","                     0.9019723004525408,\n","                     0.9061707253641105,\n","                     0.9037670239417936,\n","                     0.9050896021661191,\n","                     0.9074072691063931,\n","                     0.9051041473163394,\n","                     0.9072161634091495,\n","                     0.9005666290842407,\n","                     0.9043508415867022]]}\n"]}]},{"cell_type":"markdown","metadata":{"id":"SaQotIXc5Bqh"},"source":["## k-fold with Noise Level Performance Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yee8zcei5Gkh"},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import matthews_corrcoef, accuracy_score\n","from collections import defaultdict\n","\n","# Assume we have a noise label array (same length as X)\n","# Example: noise_labels = np.random.randint(0, 3, len(X))  # Simulating 3 noise levels: 0, 1, 2\n","noise_labels = ...  # This should be an array of shape (len(X),)\n","\n","# Create directory for model storage\n","os.makedirs(\"saved_models\", exist_ok=True)\n","\n","# Define model architectures\n","model_dict = {\n","    \"Simple_LSTM\": lambda input_shape, num_classes: tf.keras.Sequential([\n","        tf.keras.layers.LSTM(32, return_sequences=True, input_shape=input_shape),\n","        tf.keras.layers.Dense(num_classes, activation='softmax')\n","    ]),\n","}\n","\n","# Number of folds\n","k_folds = 5\n","skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n","\n","# Initialize results storage\n","model_results = {model_name: {} for model_name in model_dict.keys()}\n","trained_models = {model_name: [] for model_name in model_dict.keys()}\n","\n","# Convert y to a format compatible with StratifiedKFold\n","y_flat = np.argmax(y, axis=-1) if y.ndim == 3 else y  # Ensure shape (samples,)\n","\n","for model_name, model_fn in model_dict.items():\n","    acc_scores, mcc_scores, train_times, infer_times = [], [], [], []\n","    noise_performance = defaultdict(lambda: {\"acc\": [], \"mcc\": []})  # Store per noise level\n","\n","    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_flat[:, 0])):\n","        X_train, X_val = X[train_idx], X[val_idx]\n","        y_train, y_val = y[train_idx], y[val_idx]\n","        val_noise_levels = noise_labels[val_idx]  # Get noise levels for validation set\n","\n","        # Build and compile model\n","        model = model_fn(input_shape=X.shape[1:], num_classes=5)\n","        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","        # Train model and measure time\n","        start_train = time.time()\n","        model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_val, y_val), verbose=1)\n","        train_time = time.time() - start_train\n","\n","        # Inference speed measurement\n","        start_infer = time.time()\n","        y_pred_probs = model.predict(X_val)\n","        infer_time = (time.time() - start_infer) / len(X_val)  # Time per sample\n","\n","        # Compute metrics\n","        y_pred = np.argmax(y_pred_probs, axis=-1).flatten()\n","        y_true = y_val.flatten()\n","\n","        acc = accuracy_score(y_true, y_pred)\n","        mcc = matthews_corrcoef(y_true, y_pred)\n","\n","        acc_scores.append(acc)\n","        mcc_scores.append(mcc)\n","        train_times.append(train_time)\n","        infer_times.append(infer_time)\n","\n","        # Save model for this fold\n","        model_save_path = f\"saved_models/{model_name}_fold_{fold+1}.h5\"\n","        model.save(model_save_path)\n","        trained_models[model_name].append(model_save_path)\n","\n","        # **Noise-Level Performance Tracking**\n","        for noise_level in np.unique(val_noise_levels):\n","            mask = val_noise_levels == noise_level  # Get indices of this noise level\n","            y_true_noise = y_true[mask]\n","            y_pred_noise = y_pred[mask]\n","\n","            if len(y_true_noise) > 0:  # Avoid empty cases\n","                acc_noise = accuracy_score(y_true_noise, y_pred_noise)\n","                mcc_noise = matthews_corrcoef(y_true_noise, y_pred_noise)\n","\n","                noise_performance[noise_level][\"acc\"].append(acc_noise)\n","                noise_performance[noise_level][\"mcc\"].append(mcc_noise)\n","\n","    # Store aggregated results\n","    model_results[model_name] = {\n","        \"Accuracy\": {\"mean\": np.mean(acc_scores), \"std\": np.std(acc_scores), \"min\": np.min(acc_scores), \"max\": np.max(acc_scores)},\n","        \"MCC\": {\"mean\": np.mean(mcc_scores), \"std\": np.std(mcc_scores), \"min\": np.min(mcc_scores), \"max\": np.max(mcc_scores)},\n","        \"Train Time (s)\": {\"mean\": np.mean(train_times), \"std\": np.std(train_times), \"min\": np.min(train_times), \"max\": np.max(train_times)},\n","        \"Inference Time (s/sample)\": {\"mean\": np.mean(infer_times), \"std\": np.std(infer_times), \"min\": np.min(infer_times), \"max\": np.max(infer_times)},\n","        \"Noise Performance\": {\n","            noise_level: {\n","                \"Accuracy\": {\"mean\": np.mean(scores[\"acc\"]), \"std\": np.std(scores[\"acc\"])},\n","                \"MCC\": {\"mean\": np.mean(scores[\"mcc\"]), \"std\": np.std(scores[\"mcc\"])}\n","            } for noise_level, scores in noise_performance.items()\n","        }\n","    }\n","\n","    # Save the final trained model from the last fold\n","    final_model_save_path = f\"saved_models/{model_name}_final.h5\"\n","    model.save(final_model_save_path)\n","\n","# Print results\n","import pprint\n","pprint.pprint(model_results)\n","\n","# Save results as JSON\n","import json\n","with open(\"model_results.json\", \"w\") as f:\n","    json.dump(model_results, f, indent=4)\n"]},{"cell_type":"markdown","metadata":{"id":"kOMZpon3uxRc"},"source":["## Visualize Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBU92PtSuy4m"},"outputs":[],"source":["def plot_confusion_matrix(cm, class_names, title=\"Confusion Matrix\"):\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n","    plt.title(title)\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpIZsmrqvfRf"},"outputs":[],"source":["def plot_confusion_matrix(cm, class_names, title=\"Confusion Matrix\"):\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n","    plt.title(title)\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHFU5WeGvhAf"},"outputs":[],"source":["# Loss curve\n","def plot_loss_curves(history):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.title('Loss Curves')\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"KK5JS28qvsHK"},"source":["Compuatational Efficiency"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7SxJzkovu51"},"outputs":[],"source":["def measure_inference_time(model, X_test):\n","    start_time = time.time()\n","    _ = model.predict(X_test)\n","    inference_time = time.time() - start_time\n","    return inference_time\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQwjR9RXvwLB"},"outputs":[],"source":["def measure_inference_time(model, X_test):\n","    start_time = time.time()\n","    _ = model.predict(X_test)\n","    inference_time = time.time() - start_time\n","    return inference_time\n"]},{"cell_type":"markdown","metadata":{"id":"JieiAj4uvy0N"},"source":["## Result Report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ttc7Vezv0cX"},"outputs":[],"source":["results = {\n","    \"Model\": [],\n","    \"Accuracy\": [],\n","    \"MAE\": [],\n","    \"F1-Score\": [],\n","    \"Training Time (s)\": [],\n","    \"Inference Time (s)\": []\n","}\n","\n","# Assuming `models` is a list of your models\n","for model_name, model in models.items():\n","    metrics = evaluate_model(model, X_train, y_train, X_val, y_val)\n","    inference_time = measure_inference_time(model, X_test)\n","\n","    results[\"Model\"].append(model_name)\n","    results[\"Accuracy\"].append(metrics[\"accuracy\"])\n","    results[\"MAE\"].append(metrics[\"mae\"])\n","    results[\"F1-Score\"].append(metrics[\"f1\"])\n","    results[\"Training Time (s)\"].append(metrics[\"training_time\"])\n","    results[\"Inference Time (s)\"].append(inference_time)\n","\n","# Convert to DataFrame for better visualization\n","results_df = pd.DataFrame(results)\n","print(results_df)\n"]}],"metadata":{"colab":{"collapsed_sections":["SaQotIXc5Bqh","kOMZpon3uxRc","JieiAj4uvy0N"],"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyPzUe9dr7KTXPf4XUoy/eBa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}